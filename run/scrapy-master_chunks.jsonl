{"id": "/Users/zack.alatrash/Downloads/scrapy-master/conftest.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/conftest.py", "rel_path": "conftest.py", "module": "conftest", "ext": "py", "chunk_number": 1, "symbols": ["_py_files", "mockserver", "reactor_pytest", "only_asyncio", "only_not_asyncio", "requires_uvloop", "requires_botocore", "requires_boto3", "pytest_configure", "encoding", "proactor", "append", "file", "spiders", "future", "typ", "checking", "path", "get", "closest", "pytest", "async", "crawler", "ignores", "here", "selector", "handlers", "fixture", "localhost", "docs", "line", "without", "config", "http", "runner", "testproc", "typing", "decide", "return", "only", "not", "annotations", "option", "files", "deps", "some", "noqa", "twisted", "generate", "requires"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nimport pytest\nfrom twisted.web.http import H2_ENABLED\n\nfrom scrapy.utils.reactor import set_asyncio_event_loop_policy\nfrom tests.keys import generate_keys\nfrom tests.mockserver.http import MockServer\n\nif TYPE_CHECKING:\n    from collections.abc import Generator\n\n\ndef _py_files(folder):\n    return (str(p) for p in Path(folder).rglob(\"*.py\"))\n\n\ncollect_ignore = [\n    # may need extra deps\n    \"docs/_ext\",\n    # not a test, but looks like a test\n    \"scrapy/utils/testproc.py\",\n    \"scrapy/utils/testsite.py\",\n    \"tests/ftpserver.py\",\n    \"tests/mockserver.py\",\n    \"tests/pipelines.py\",\n    \"tests/spiders.py\",\n    # contains scripts to be run by tests/test_crawler.py::AsyncCrawlerProcessSubprocess\n    *_py_files(\"tests/AsyncCrawlerProcess\"),\n    # contains scripts to be run by tests/test_crawler.py::AsyncCrawlerRunnerSubprocess\n    *_py_files(\"tests/AsyncCrawlerRunner\"),\n    # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n    *_py_files(\"tests/CrawlerProcess\"),\n    # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess\n    *_py_files(\"tests/CrawlerRunner\"),\n]\n\nbase_dir = Path(__file__).parent\nignore_file_path = base_dir / \"tests\" / \"ignores.txt\"\nwith ignore_file_path.open(encoding=\"utf-8\") as reader:\n    for line in reader:\n        file_path = line.strip()\n        if file_path and file_path[0] != \"#\":\n            collect_ignore.append(file_path)\n\nif not H2_ENABLED:\n    collect_ignore.extend(\n        (\n            \"scrapy/core/downloader/handlers/http2.py\",\n            *_py_files(\"scrapy/core/http2\"),\n        )\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef mockserver() -> Generator[MockServer]:\n    with MockServer() as mockserver:\n        yield mockserver\n\n\n@pytest.fixture(scope=\"session\")\ndef reactor_pytest(request) -> str:\n    return request.config.getoption(\"--reactor\")\n\n\n@pytest.fixture(autouse=True)\ndef only_asyncio(request, reactor_pytest):\n    if request.node.get_closest_marker(\"only_asyncio\") and reactor_pytest != \"asyncio\":\n        pytest.skip(\"This test is only run with --reactor=asyncio\")\n\n\n@pytest.fixture(autouse=True)\ndef only_not_asyncio(request, reactor_pytest):\n    if (\n        request.node.get_closest_marker(\"only_not_asyncio\")\n        and reactor_pytest == \"asyncio\"\n    ):\n        pytest.skip(\"This test is only run without --reactor=asyncio\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_uvloop(request):\n    if not request.node.get_closest_marker(\"requires_uvloop\"):\n        return\n    try:\n        import uvloop  # noqa: PLC0415\n\n        del uvloop\n    except ImportError:\n        pytest.skip(\"uvloop is not installed\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_botocore(request):\n    if not request.node.get_closest_marker(\"requires_botocore\"):\n        return\n    try:\n        import botocore  # noqa: PLC0415\n\n        del botocore\n    except ImportError:\n        pytest.skip(\"botocore is not installed\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_boto3(request):\n    if not request.node.get_closest_marker(\"requires_boto3\"):\n        return\n    try:\n        import boto3  # noqa: PLC0415\n\n        del boto3\n    except ImportError:\n        pytest.skip(\"boto3 is not installed\")\n\n\ndef pytest_configure(config):\n    if config.getoption(\"--reactor\") == \"asyncio\":\n        # Needed on Windows to switch from proactor to selector for Twisted reactor compatibility.\n        # If we decide to run tests with both, we will need to add a new option and check it here.\n        set_asyncio_event_loop_policy()\n\n\n# Generate localhost certificate files, needed by some tests\ngenerate_keys()\n", "n_tokens": 872, "byte_len": 3687, "file_sha1": "0b7ca9e55e2534f465ebfd70421d977a8e38cf81", "start_line": 1, "end_line": 129}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_dict.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_dict.py", "rel_path": "tests/test_request_dict.py", "module": "tests.test_request_dict", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_basic", "test_all_attributes", "test_latin1_body", "test_utf8_body", "_assert_serializes_ok", "_assert_same_request", "test_request_class", "test_callback_serialization", "test_reference_callback_serialization", "test_private_reference_callback_serialization", "test_private_callback_serialization", "test_mixin_private_callback_serialization", "test_delegated_callback_serialization", "test_unserializable_callback1", "test_unserializable_callback2", "test_unserializable_callback3", "CustomRequest", "TestRequestSerialization", "MySpider", "encoding", "method", "test", "all", "private", "cookies", "instance", "spider", "name", "dont", "parse", "test_callback_not_available", "__mixin_callback", "delegated_callback", "parse_item", "handle_error", "private_parse_item", "private_handle_error", "__init__", "__parse_item_private", "SpiderMixin", "SpiderDelegation", "MethodsSpider", "callback", "filter", "passed", "removed", "item", "unused", "request"], "ast_kind": "class_or_type", "text": "import pytest\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import FormRequest, JsonRequest\nfrom scrapy.utils.request import request_from_dict\n\n\nclass CustomRequest(Request):\n    pass\n\n\nclass TestRequestSerialization:\n    def setup_method(self):\n        self.spider = MethodsSpider()\n\n    def test_basic(self):\n        r = Request(\"http://www.example.com\")\n        self._assert_serializes_ok(r)\n\n    def test_all_attributes(self):\n        r = Request(\n            url=\"http://www.example.com\",\n            callback=self.spider.parse_item,\n            errback=self.spider.handle_error,\n            method=\"POST\",\n            body=b\"some body\",\n            headers={\"content-encoding\": \"text/html; charset=latin-1\"},\n            cookies={\"currency\": \"руб\"},\n            encoding=\"latin-1\",\n            priority=20,\n            meta={\"a\": \"b\"},\n            cb_kwargs={\"k\": \"v\"},\n            flags=[\"testFlag\"],\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_latin1_body(self):\n        r = Request(\"http://www.example.com\", body=b\"\\xa3\")\n        self._assert_serializes_ok(r)\n\n    def test_utf8_body(self):\n        r = Request(\"http://www.example.com\", body=b\"\\xc2\\xa3\")\n        self._assert_serializes_ok(r)\n\n    def _assert_serializes_ok(self, request, spider=None):\n        d = request.to_dict(spider=spider)\n        request2 = request_from_dict(d, spider=spider)\n        self._assert_same_request(request, request2)\n\n    def _assert_same_request(self, r1, r2):\n        assert r1.__class__ == r2.__class__\n        assert r1.url == r2.url\n        assert r1.callback == r2.callback\n        assert r1.errback == r2.errback\n        assert r1.method == r2.method\n        assert r1.body == r2.body\n        assert r1.headers == r2.headers\n        assert r1.cookies == r2.cookies\n        assert r1.meta == r2.meta\n        assert r1.cb_kwargs == r2.cb_kwargs\n        assert r1.encoding == r2.encoding\n        assert r1._encoding == r2._encoding\n        assert r1.priority == r2.priority\n        assert r1.dont_filter == r2.dont_filter\n        assert r1.flags == r2.flags\n        if isinstance(r1, JsonRequest):\n            assert r1.dumps_kwargs == r2.dumps_kwargs\n\n    def test_request_class(self):\n        r1 = FormRequest(\"http://www.example.com\")\n        self._assert_serializes_ok(r1, spider=self.spider)\n        r2 = CustomRequest(\"http://www.example.com\")\n        self._assert_serializes_ok(r2, spider=self.spider)\n        r3 = JsonRequest(\"http://www.example.com\", dumps_kwargs={\"indent\": 4})\n        self._assert_serializes_ok(r3, spider=self.spider)\n\n    def test_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider.parse_item,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_reference_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider.parse_item_reference,\n            errback=self.spider.handle_error_reference,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n        request_dict = r.to_dict(spider=self.spider)\n        assert request_dict[\"callback\"] == \"parse_item_reference\"\n        assert request_dict[\"errback\"] == \"handle_error_reference\"\n\n    def test_private_reference_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider._MethodsSpider__parse_item_reference,\n            errback=self.spider._MethodsSpider__handle_error_reference,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n        request_dict = r.to_dict(spider=self.spider)\n        assert request_dict[\"callback\"] == \"_MethodsSpider__parse_item_reference\"\n        assert request_dict[\"errback\"] == \"_MethodsSpider__handle_error_reference\"\n\n    def test_private_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider._MethodsSpider__parse_item_private,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_mixin_private_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider._SpiderMixin__mixin_callback,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_delegated_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider.delegated_callback,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_unserializable_callback1(self):\n        r = Request(\"http://www.example.com\", callback=lambda x: x)\n        with pytest.raises(\n            ValueError, match=\"is not an instance method in: <MethodsSpider\"\n        ):\n            r.to_dict(spider=self.spider)\n\n    def test_unserializable_callback2(self):\n        r = Request(\"http://www.example.com\", callback=self.spider.parse_item)\n        with pytest.raises(ValueError, match=\"is not an instance method in: None\"):\n            r.to_dict(spider=None)\n\n    def test_unserializable_callback3(self):\n        \"\"\"Parser method is removed or replaced dynamically.\"\"\"\n\n        class MySpider(Spider):\n            name = \"my_spider\"\n", "n_tokens": 1218, "byte_len": 5443, "file_sha1": "3b218fed7a2bf279df892a220388dd074151ff50", "start_line": 1, "end_line": 148}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_dict.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_dict.py", "rel_path": "tests/test_request_dict.py", "module": "tests.test_request_dict", "ext": "py", "chunk_number": 2, "symbols": ["parse", "test_callback_not_available", "__mixin_callback", "delegated_callback", "parse_item", "handle_error", "private_parse_item", "private_handle_error", "__init__", "__parse_item_private", "SpiderMixin", "SpiderDelegation", "MethodsSpider", "method", "dict", "pass", "from", "test", "callback", "spider", "instance", "methods", "member", "value", "error", "name", "private", "item", "class", "with", "setup_method", "test_basic", "test_all_attributes", "test_latin1_body", "test_utf8_body", "_assert_serializes_ok", "_assert_same_request", "test_request_class", "test_callback_serialization", "test_reference_callback_serialization", "test_private_reference_callback_serialization", "test_private_callback_serialization", "test_mixin_private_callback_serialization", "test_delegated_callback_serialization", "test_unserializable_callback1", "test_unserializable_callback2", "test_unserializable_callback3", "CustomRequest", "TestRequestSerialization", "MySpider"], "ast_kind": "class_or_type", "text": "            def parse(self, response):\n                pass\n\n        spider = MySpider()\n        r = Request(\"http://www.example.com\", callback=spider.parse)\n        spider.parse = None\n        with pytest.raises(ValueError, match=\"is not an instance method in: <MySpider\"):\n            r.to_dict(spider=spider)\n\n    def test_callback_not_available(self):\n        \"\"\"Callback method is not available in the spider passed to from_dict\"\"\"\n        spider = SpiderDelegation()\n        r = Request(\"http://www.example.com\", callback=spider.delegated_callback)\n        d = r.to_dict(spider=spider)\n        with pytest.raises(\n            ValueError, match=\"Method 'delegated_callback' not found in: <Spider\"\n        ):\n            request_from_dict(d, spider=Spider(\"foo\"))\n\n\nclass SpiderMixin:\n    def __mixin_callback(self, response):  # pylint: disable=unused-private-member\n        pass\n\n\nclass SpiderDelegation:\n    def delegated_callback(self, response):\n        pass\n\n\ndef parse_item(response):\n    pass\n\n\ndef handle_error(failure):\n    pass\n\n\ndef private_parse_item(response):\n    pass\n\n\ndef private_handle_error(failure):\n    pass\n\n\nclass MethodsSpider(Spider, SpiderMixin):\n    name = \"test\"\n    parse_item_reference = parse_item\n    handle_error_reference = handle_error\n    __parse_item_reference = private_parse_item\n    __handle_error_reference = private_handle_error\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.delegated_callback = SpiderDelegation().delegated_callback\n\n    def parse_item(self, response):\n        pass\n\n    def handle_error(self, failure):\n        pass\n\n    def __parse_item_private(self, response):  # pylint: disable=unused-private-member\n        pass\n", "n_tokens": 379, "byte_len": 1720, "file_sha1": "3b218fed7a2bf279df892a220388dd074151ff50", "start_line": 149, "end_line": 214}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_signal.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_signal.py", "rel_path": "tests/test_utils_signal.py", "module": "tests.test_utils_signal", "ext": "py", "chunk_number": 1, "symbols": ["test_send_catch_log", "_get_result", "error_handler", "ok_handler", "test_error_logged_if_deferred_not_supported", "test_handler", "TestSendCatchLog", "TestSendCatchLogDeferred", "TestSendCatchLogDeferred2", "TestSendCatchLogDeferredAsyncDef", "TestSendCatchLogDeferredAsyncio", "TestSendCatchLogAsync", "TestSendCatchLogAsync2", "TestSendCatchLogAsyncAsyncDef", "TestSendCatchLogAsyncAsyncio", "TestSendCatchLog2", "disconnect", "failure", "async", "test", "send", "levelname", "call", "later", "get", "result", "await", "internet", "python", "signal", "twisted", "return", "catch", "mark", "class", "only", "asyncio", "sleep", "with", "error", "handler", "scrapy", "record", "defer", "testfixtures", "deferred", "from", "pylint", "succeed", "pydispatch"], "ast_kind": "class_or_type", "text": "import asyncio\n\nimport pytest\nfrom pydispatch import dispatcher\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.defer import deferred_from_coro\nfrom scrapy.utils.signal import (\n    send_catch_log,\n    send_catch_log_async,\n    send_catch_log_deferred,\n)\nfrom scrapy.utils.test import get_from_asyncio_queue\n\n\nclass TestSendCatchLog:\n    @inlineCallbacks\n    def test_send_catch_log(self):\n        test_signal = object()\n        handlers_called = set()\n\n        dispatcher.connect(self.error_handler, signal=test_signal)\n        dispatcher.connect(self.ok_handler, signal=test_signal)\n        with LogCapture() as log:\n            result = yield defer.maybeDeferred(\n                self._get_result,\n                test_signal,\n                arg=\"test\",\n                handlers_called=handlers_called,\n            )\n\n        assert self.error_handler in handlers_called\n        assert self.ok_handler in handlers_called\n        assert len(log.records) == 1\n        record = log.records[0]\n        assert \"error_handler\" in record.getMessage()\n        assert record.levelname == \"ERROR\"\n        assert result[0][0] == self.error_handler  # pylint: disable=comparison-with-callable\n        assert isinstance(result[0][1], Failure)\n        assert result[1] == (self.ok_handler, \"OK\")\n\n        dispatcher.disconnect(self.error_handler, signal=test_signal)\n        dispatcher.disconnect(self.ok_handler, signal=test_signal)\n\n    def _get_result(self, signal, *a, **kw):\n        return send_catch_log(signal, *a, **kw)\n\n    def error_handler(self, arg, handlers_called):\n        handlers_called.add(self.error_handler)\n        1 / 0\n\n    def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        return \"OK\"\n\n\nclass TestSendCatchLogDeferred(TestSendCatchLog):\n    def _get_result(self, signal, *a, **kw):\n        return send_catch_log_deferred(signal, *a, **kw)\n\n\nclass TestSendCatchLogDeferred2(TestSendCatchLogDeferred):\n    def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        d = defer.Deferred()\n        call_later(0, d.callback, \"OK\")\n        return d\n\n\nclass TestSendCatchLogDeferredAsyncDef(TestSendCatchLogDeferred):\n    async def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        await defer.succeed(42)\n        return \"OK\"\n\n\n@pytest.mark.only_asyncio\nclass TestSendCatchLogDeferredAsyncio(TestSendCatchLogDeferred):\n    async def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        await asyncio.sleep(0.2)\n        return await get_from_asyncio_queue(\"OK\")\n\n\nclass TestSendCatchLogAsync(TestSendCatchLog):\n    def _get_result(self, signal, *a, **kw):\n        return deferred_from_coro(send_catch_log_async(signal, *a, **kw))\n\n\nclass TestSendCatchLogAsync2(TestSendCatchLogAsync):\n    def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        d = defer.Deferred()\n        call_later(0, d.callback, \"OK\")\n        return d\n\n\nclass TestSendCatchLogAsyncAsyncDef(TestSendCatchLogAsync):\n    async def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        await defer.succeed(42)\n        return \"OK\"\n\n\n@pytest.mark.only_asyncio\nclass TestSendCatchLogAsyncAsyncio(TestSendCatchLogAsync):\n    async def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        await asyncio.sleep(0.2)\n        return await get_from_asyncio_queue(\"OK\")\n\n\nclass TestSendCatchLog2:\n    def test_error_logged_if_deferred_not_supported(self):\n        def test_handler():\n            return defer.Deferred()\n\n        test_signal = object()\n        dispatcher.connect(test_handler, test_signal)\n        with LogCapture() as log:\n            send_catch_log(test_signal)\n        assert len(log.records) == 1\n        assert \"Cannot return deferreds from signal handler\" in str(log)\n        dispatcher.disconnect(test_handler, test_signal)\n", "n_tokens": 959, "byte_len": 4378, "file_sha1": "19c8fb48e854690faf388d808a8f8322fc58b0b1", "start_line": 1, "end_line": 136}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py", "rel_path": "tests/test_contracts.py", "module": "tests.test_contracts", "ext": "py", "chunk_number": 1, "symbols": ["adjust_request_args", "returns_request", "returns_item", "returns_request_cb_kwargs", "returns_item_cb_kwargs", "returns_item_cb_kwargs_error_unexpected_keyword", "returns_item_cb_kwargs_error_missing_argument", "returns_dict_item", "returns_fail", "returns_dict_fail", "scrapes_item_ok", "scrapes_dict_item_ok", "scrapes_item_fail", "scrapes_dict_item_fail", "scrapes_multiple_missing_fields", "parse_no_url", "custom_form", "invalid_regex", "invalid_regex_with_valid_contract", "returns_request_meta", "DemoItem", "ResponseMock", "ResponseMetaMock", "CustomSuccessContract", "CustomFailContract", "CustomFormContract", "DemoSpider", "method", "custom", "fail", "returns_item_meta", "returns_error_missing_meta", "parse", "setup_method", "should_succeed", "should_fail", "should_error", "test_contracts", "test_cb_kwargs", "test_meta", "test_returns", "test_returns_async", "test_scrapes", "test_regex", "test_custom_contracts", "test_errback", "test_same_url", "__init__", "parse_first", "parse_second"], "ast_kind": "class_or_type", "text": "from unittest import TextTestResult\n\nimport pytest\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.python import failure\n\nfrom scrapy import FormRequest\nfrom scrapy.contracts import Contract, ContractsManager\nfrom scrapy.contracts.default import (\n    CallbackKeywordArgumentsContract,\n    MetadataContract,\n    ReturnsContract,\n    ScrapesContract,\n    UrlContract,\n)\nfrom scrapy.http import Request\nfrom scrapy.item import Field, Item\nfrom scrapy.spidermiddlewares.httperror import HttpError\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\n\n\nclass DemoItem(Item):\n    name = Field()\n    url = Field()\n\n\nclass ResponseMock:\n    url = \"http://scrapy.org\"\n\n\nclass ResponseMetaMock(ResponseMock):\n    meta = None\n\n\nclass CustomSuccessContract(Contract):\n    name = \"custom_success_contract\"\n\n    def adjust_request_args(self, args):\n        args[\"url\"] = \"http://scrapy.org\"\n        return args\n\n\nclass CustomFailContract(Contract):\n    name = \"custom_fail_contract\"\n\n    def adjust_request_args(self, args):\n        raise TypeError(\"Error in adjust_request_args\")\n\n\nclass CustomFormContract(Contract):\n    name = \"custom_form\"\n    request_cls = FormRequest\n\n    def adjust_request_args(self, args):\n        args[\"formdata\"] = {\"name\": \"scrapy\"}\n        return args\n\n\nclass DemoSpider(Spider):\n    name = \"demo_spider\"\n\n    def returns_request(self, response):\n        \"\"\"method which returns request\n        @url http://scrapy.org\n        @returns requests 1\n        \"\"\"\n        return Request(\"http://scrapy.org\", callback=self.returns_item)\n\n    async def returns_request_async(self, response):\n        \"\"\"async method which returns request\n        @url http://scrapy.org\n        @returns requests 1\n        \"\"\"\n        return Request(\"http://scrapy.org\", callback=self.returns_item)\n\n    def returns_item(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return DemoItem(url=response.url)\n\n    def returns_request_cb_kwargs(self, response, url):\n        \"\"\"method which returns request\n        @url https://example.org\n        @cb_kwargs {\"url\": \"http://scrapy.org\"}\n        @returns requests 1\n        \"\"\"\n        return Request(url, callback=self.returns_item_cb_kwargs)\n\n    def returns_item_cb_kwargs(self, response, name):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @cb_kwargs {\"name\": \"Scrapy\"}\n        @returns items 1 1\n        \"\"\"\n        return DemoItem(name=name, url=response.url)\n\n    def returns_item_cb_kwargs_error_unexpected_keyword(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @cb_kwargs {\"arg\": \"value\"}\n        @returns items 1 1\n        \"\"\"\n        return DemoItem(url=response.url)\n\n    def returns_item_cb_kwargs_error_missing_argument(self, response, arg):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return DemoItem(url=response.url)\n\n    def returns_dict_item(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return {\"url\": response.url}\n\n    def returns_fail(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 0 0\n        \"\"\"\n        return DemoItem(url=response.url)\n\n    def returns_dict_fail(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 0 0\n        \"\"\"\n        return {\"url\": response.url}\n\n    def scrapes_item_ok(self, response):\n        \"\"\"returns item with name and url\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return DemoItem(name=\"test\", url=response.url)\n\n    def scrapes_dict_item_ok(self, response):\n        \"\"\"returns item with name and url\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return {\"name\": \"test\", \"url\": response.url}\n\n    def scrapes_item_fail(self, response):\n        \"\"\"returns item with no name\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return DemoItem(url=response.url)\n\n    def scrapes_dict_item_fail(self, response):\n        \"\"\"returns item with no name\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return {\"url\": response.url}\n\n    def scrapes_multiple_missing_fields(self, response):\n        \"\"\"returns item with no name\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return {}\n\n    def parse_no_url(self, response):\n        \"\"\"method with no url\n        @returns items 1 1\n        \"\"\"\n\n    def custom_form(self, response):\n        \"\"\"\n        @url http://scrapy.org\n        @custom_form\n        \"\"\"\n\n    def invalid_regex(self, response):\n        \"\"\"method with invalid regex\n        @ Scrapy is awsome\n        \"\"\"\n\n    def invalid_regex_with_valid_contract(self, response):\n        \"\"\"method with invalid regex\n        @ scrapy is awsome\n        @url http://scrapy.org\n        \"\"\"\n\n    def returns_request_meta(self, response):\n        \"\"\"method which returns request\n        @url https://example.org\n        @meta {\"cookiejar\": \"session1\"}\n        @returns requests 1\n        \"\"\"\n        return Request(\n            \"https://example.org\", meta=response.meta, callback=self.returns_item_meta\n        )\n", "n_tokens": 1270, "byte_len": 5594, "file_sha1": "1eda76f54c334ca3953374ab340c61005e9950a6", "start_line": 1, "end_line": 208}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py", "rel_path": "tests/test_contracts.py", "module": "tests.test_contracts", "ext": "py", "chunk_number": 2, "symbols": ["returns_item_meta", "returns_error_missing_meta", "parse", "setup_method", "should_succeed", "should_fail", "should_error", "test_contracts", "test_cb_kwargs", "CustomContractSuccessSpider", "CustomContractFailSpider", "InheritsDemoSpider", "TestContractsManager", "method", "custom", "fail", "returns", "contract", "success", "demo", "spider", "stream", "name", "form", "doesn", "missing", "conman", "extract", "items", "results", "adjust_request_args", "returns_request", "returns_item", "returns_request_cb_kwargs", "returns_item_cb_kwargs", "returns_item_cb_kwargs_error_unexpected_keyword", "returns_item_cb_kwargs_error_missing_argument", "returns_dict_item", "returns_fail", "returns_dict_fail", "scrapes_item_ok", "scrapes_dict_item_ok", "scrapes_item_fail", "scrapes_dict_item_fail", "scrapes_multiple_missing_fields", "parse_no_url", "custom_form", "invalid_regex", "invalid_regex_with_valid_contract", "returns_request_meta"], "ast_kind": "class_or_type", "text": "    def returns_item_meta(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @meta {\"key\": \"example\"}\n        @returns items 1 1\n        \"\"\"\n        return DemoItem(name=\"example\", url=response.url)\n\n    def returns_error_missing_meta(self, response):\n        \"\"\"method which depends of metadata be defined\n\n        @url http://scrapy.org\n        @returns items 1\n        \"\"\"\n        key = response.meta[\"key\"]\n        yield {key: \"value\"}\n\n\nclass CustomContractSuccessSpider(Spider):\n    name = \"custom_contract_success_spider\"\n\n    def parse(self, response):\n        \"\"\"\n        @custom_success_contract\n        \"\"\"\n\n\nclass CustomContractFailSpider(Spider):\n    name = \"custom_contract_fail_spider\"\n\n    def parse(self, response):\n        \"\"\"\n        @custom_fail_contract\n        \"\"\"\n\n\nclass InheritsDemoSpider(DemoSpider):\n    name = \"inherits_demo_spider\"\n\n\nclass TestContractsManager:\n    contracts = [\n        UrlContract,\n        CallbackKeywordArgumentsContract,\n        MetadataContract,\n        ReturnsContract,\n        ScrapesContract,\n        CustomFormContract,\n        CustomSuccessContract,\n        CustomFailContract,\n    ]\n\n    def setup_method(self):\n        self.conman = ContractsManager(self.contracts)\n        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n\n    def should_succeed(self):\n        assert not self.results.failures\n        assert not self.results.errors\n\n    def should_fail(self):\n        assert self.results.failures\n        assert not self.results.errors\n\n    def should_error(self):\n        assert self.results.errors\n\n    def test_contracts(self):\n        spider = DemoSpider()\n\n        # extract contracts correctly\n        contracts = self.conman.extract_contracts(spider.returns_request)\n        assert len(contracts) == 2\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, ReturnsContract]\n        )\n\n        # returns request for valid method\n        request = self.conman.from_method(spider.returns_request, self.results)\n        assert request is not None\n\n        # no request for missing url\n        request = self.conman.from_method(spider.parse_no_url, self.results)\n        assert request is None\n\n    def test_cb_kwargs(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n\n        # extract contracts correctly\n        contracts = self.conman.extract_contracts(spider.returns_request_cb_kwargs)\n        assert len(contracts) == 3\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]\n        )\n\n        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs)\n        assert len(contracts) == 3\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]\n        )\n\n        contracts = self.conman.extract_contracts(\n            spider.returns_item_cb_kwargs_error_unexpected_keyword\n        )\n        assert len(contracts) == 3\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]\n        )\n\n        contracts = self.conman.extract_contracts(\n            spider.returns_item_cb_kwargs_error_missing_argument\n        )\n        assert len(contracts) == 2\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, ReturnsContract]\n        )\n\n        # returns_request\n        request = self.conman.from_method(\n            spider.returns_request_cb_kwargs, self.results\n        )\n        request.callback(response, **request.cb_kwargs)\n        self.should_succeed()\n\n        # returns_item\n        request = self.conman.from_method(spider.returns_item_cb_kwargs, self.results)\n        request.callback(response, **request.cb_kwargs)\n        self.should_succeed()\n\n        # returns_item (error, callback doesn't take keyword arguments)\n        request = self.conman.from_method(\n            spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results\n        )\n        request.callback(response, **request.cb_kwargs)\n        self.should_error()\n\n        # returns_item (error, contract doesn't provide keyword arguments)\n        request = self.conman.from_method(\n            spider.returns_item_cb_kwargs_error_missing_argument, self.results\n        )\n        request.callback(response, **request.cb_kwargs)\n        self.should_error()\n", "n_tokens": 940, "byte_len": 4547, "file_sha1": "1eda76f54c334ca3953374ab340c61005e9950a6", "start_line": 209, "end_line": 352}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py", "rel_path": "tests/test_contracts.py", "module": "tests.test_contracts", "ext": "py", "chunk_number": 3, "symbols": ["test_meta", "test_returns", "test_returns_async", "test_scrapes", "test_regex", "test_custom_contracts", "test_errback", "test_same_url", "__init__", "parse_first", "TestSameUrlSpider", "failure", "async", "http", "error", "returns", "contract", "test", "regex", "custom", "demo", "spider", "from", "name", "visited", "conman", "extract", "scrapes", "results", "self", "adjust_request_args", "returns_request", "returns_item", "returns_request_cb_kwargs", "returns_item_cb_kwargs", "returns_item_cb_kwargs_error_unexpected_keyword", "returns_item_cb_kwargs_error_missing_argument", "returns_dict_item", "returns_fail", "returns_dict_fail", "scrapes_item_ok", "scrapes_dict_item_ok", "scrapes_item_fail", "scrapes_dict_item_fail", "scrapes_multiple_missing_fields", "parse_no_url", "custom_form", "invalid_regex", "invalid_regex_with_valid_contract", "returns_request_meta"], "ast_kind": "class_or_type", "text": "    def test_meta(self):\n        spider = DemoSpider()\n\n        # extract contracts correctly\n        contracts = self.conman.extract_contracts(spider.returns_request_meta)\n        assert len(contracts) == 3\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, MetadataContract, ReturnsContract]\n        )\n\n        contracts = self.conman.extract_contracts(spider.returns_item_meta)\n        assert len(contracts) == 3\n        assert frozenset(type(x) for x in contracts) == frozenset(\n            [UrlContract, MetadataContract, ReturnsContract]\n        )\n\n        response = ResponseMetaMock()\n\n        # returns_request\n        request = self.conman.from_method(spider.returns_request_meta, self.results)\n        assert request.meta[\"cookiejar\"] == \"session1\"\n        response.meta = request.meta\n        request.callback(response)\n        assert response.meta[\"cookiejar\"] == \"session1\"\n        self.should_succeed()\n\n        response = ResponseMetaMock()\n\n        # returns_item\n        request = self.conman.from_method(spider.returns_item_meta, self.results)\n        assert request.meta[\"key\"] == \"example\"\n        response.meta = request.meta\n        request.callback(ResponseMetaMock)\n        assert response.meta[\"key\"] == \"example\"\n        self.should_succeed()\n\n        response = ResponseMetaMock()\n\n        request = self.conman.from_method(\n            spider.returns_error_missing_meta, self.results\n        )\n        request.callback(response)\n        self.should_error()\n\n    def test_returns(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n\n        # returns_item\n        request = self.conman.from_method(spider.returns_item, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # returns_dict_item\n        request = self.conman.from_method(spider.returns_dict_item, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # returns_request\n        request = self.conman.from_method(spider.returns_request, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # returns_fail\n        request = self.conman.from_method(spider.returns_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n        # returns_dict_fail\n        request = self.conman.from_method(spider.returns_dict_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n    def test_returns_async(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n\n        request = self.conman.from_method(spider.returns_request_async, self.results)\n        request.callback(response)\n        self.should_error()\n\n    def test_scrapes(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n\n        # scrapes_item_ok\n        request = self.conman.from_method(spider.scrapes_item_ok, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # scrapes_dict_item_ok\n        request = self.conman.from_method(spider.scrapes_dict_item_ok, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # scrapes_item_fail\n        request = self.conman.from_method(spider.scrapes_item_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n        # scrapes_dict_item_fail\n        request = self.conman.from_method(spider.scrapes_dict_item_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n        # scrapes_multiple_missing_fields\n        request = self.conman.from_method(\n            spider.scrapes_multiple_missing_fields, self.results\n        )\n        request.callback(response)\n        self.should_fail()\n        message = \"ContractFail: Missing fields: name, url\"\n        assert message in self.results.failures[-1][-1]\n\n    def test_regex(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n\n        # invalid regex\n        request = self.conman.from_method(spider.invalid_regex, self.results)\n        self.should_succeed()\n\n        # invalid regex with valid contract\n        request = self.conman.from_method(\n            spider.invalid_regex_with_valid_contract, self.results\n        )\n        self.should_succeed()\n        request.callback(response)\n\n    def test_custom_contracts(self):\n        self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n        self.should_succeed()\n\n        self.conman.from_spider(CustomContractFailSpider(), self.results)\n        self.should_error()\n\n    def test_errback(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n\n        try:\n            raise HttpError(response, \"Ignoring non-200 response\")\n        except HttpError:\n            failure_mock = failure.Failure()\n\n        request = self.conman.from_method(spider.returns_request, self.results)\n        request.errback(failure_mock)\n\n        assert not self.results.failures\n        assert self.results.errors\n\n    @inlineCallbacks\n    def test_same_url(self):\n        class TestSameUrlSpider(Spider):\n            name = \"test_same_url\"\n\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.visited = 0\n\n            async def start(self_):  # pylint: disable=no-self-argument\n                for item_or_request in self.conman.from_spider(self_, self.results):\n                    yield item_or_request\n\n            def parse_first(self, response):\n                self.visited += 1\n                return DemoItem()\n", "n_tokens": 1132, "byte_len": 5579, "file_sha1": "1eda76f54c334ca3953374ab340c61005e9950a6", "start_line": 353, "end_line": 520}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_contracts.py", "rel_path": "tests/test_contracts.py", "module": "tests.test_contracts", "ext": "py", "chunk_number": 4, "symbols": ["parse_second", "test_form_contract", "test_inherited_contracts", "pre_process", "post_process", "setup_method", "test_pre_hook_keyboard_interrupt", "test_post_hook_keyboard_interrupt", "CustomFailContractPreProcess", "CustomFailContractPostProcess", "TestCustomContractPrePostProcess", "post", "test", "inherited", "method", "verbosity", "requests", "failures", "false", "pre", "process", "contract", "inherits", "demo", "kwargs", "custom", "form", "spider", "errors", "exception", "adjust_request_args", "returns_request", "returns_item", "returns_request_cb_kwargs", "returns_item_cb_kwargs", "returns_item_cb_kwargs_error_unexpected_keyword", "returns_item_cb_kwargs_error_missing_argument", "returns_dict_item", "returns_fail", "returns_dict_fail", "scrapes_item_ok", "scrapes_dict_item_ok", "scrapes_item_fail", "scrapes_dict_item_fail", "scrapes_multiple_missing_fields", "parse_no_url", "custom_form", "invalid_regex", "invalid_regex_with_valid_contract", "returns_request_meta"], "ast_kind": "class_or_type", "text": "            def parse_second(self, response):\n                self.visited += 1\n                return DemoItem()\n\n        with MockServer() as mockserver:\n            contract_doc = f\"@url {mockserver.url('/status?n=200')}\"\n\n            TestSameUrlSpider.parse_first.__doc__ = contract_doc\n            TestSameUrlSpider.parse_second.__doc__ = contract_doc\n\n            crawler = get_crawler(TestSameUrlSpider)\n            yield crawler.crawl()\n\n        assert crawler.spider.visited == 2\n\n    def test_form_contract(self):\n        spider = DemoSpider()\n        request = self.conman.from_method(spider.custom_form, self.results)\n        assert request.method == \"POST\"\n        assert isinstance(request, FormRequest)\n\n    def test_inherited_contracts(self):\n        spider = InheritsDemoSpider()\n\n        requests = self.conman.from_spider(spider, self.results)\n        assert requests\n\n\nclass CustomFailContractPreProcess(Contract):\n    name = \"test_contract\"\n\n    def pre_process(self, response):\n        raise KeyboardInterrupt(\"Pre-process exception\")\n\n\nclass CustomFailContractPostProcess(Contract):\n    name = \"test_contract\"\n\n    def post_process(self, response):\n        raise KeyboardInterrupt(\"Post-process exception\")\n\n\nclass TestCustomContractPrePostProcess:\n    def setup_method(self):\n        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n\n    def test_pre_hook_keyboard_interrupt(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n        contract = CustomFailContractPreProcess(spider.returns_request)\n        conman = ContractsManager([contract])\n\n        request = conman.from_method(spider.returns_request, self.results)\n        contract.add_pre_hook(request, self.results)\n        with pytest.raises(KeyboardInterrupt, match=\"Pre-process exception\"):\n            request.callback(response, **request.cb_kwargs)\n\n        assert not self.results.failures\n        assert not self.results.errors\n\n    def test_post_hook_keyboard_interrupt(self):\n        spider = DemoSpider()\n        response = ResponseMock()\n        contract = CustomFailContractPostProcess(spider.returns_request)\n        conman = ContractsManager([contract])\n\n        request = conman.from_method(spider.returns_request, self.results)\n        contract.add_post_hook(request, self.results)\n        with pytest.raises(KeyboardInterrupt, match=\"Post-process exception\"):\n            request.callback(response, **request.cb_kwargs)\n\n        assert not self.results.failures\n        assert not self.results.errors\n", "n_tokens": 498, "byte_len": 2546, "file_sha1": "1eda76f54c334ca3953374ab340c61005e9950a6", "start_line": 521, "end_line": 594}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_parse.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_parse.py", "rel_path": "tests/test_command_parse.py", "module": "tests.test_command_parse", "ext": "py", "chunk_number": 1, "symbols": ["setup_class", "teardown_class", "setup_method", "process_request", "parse", "parse_request_with_meta", "parse_request_with_cb_kwargs", "parse_request_without_meta", "parse_item", "process_item", "test_spider_arguments", "TestParseCommand", "AsyncDefAsyncioReturnSpider", "AsyncDefAsyncioReturnSingleElementSpider", "AsyncDefAsyncioGenLoopSpider", "AsyncDefAsyncioSpider", "AsyncDefAsyncioGenExcSpider", "CallbackSignatureDownloaderMiddleware", "MySpider", "MyGoodCrawlSpider", "MyBadCrawlSpider", "MyPipeline", "encoding", "while", "async", "def", "works", "spider", "name", "sleep", "test_request_with_meta", "test_request_with_cb_kwargs", "test_request_without_meta", "test_pipelines", "test_async_def_asyncio_parse_items_list", "test_async_def_asyncio_parse_items_single_element", "test_async_def_asyncgen_parse_loop", "test_async_def_asyncgen_parse_exc", "test_async_def_asyncio_parse", "test_parse_items", "test_parse_items_no_callback_passed", "test_wrong_callback_passed", "test_crawlspider_matching_rule_callback_set", "test_crawlspider_matching_rule_default_callback", "test_spider_with_no_rules_attribute", "test_crawlspider_missing_callback", "test_crawlspider_no_matching_rule", "test_crawlspider_not_exists_with_not_matched_url", "test_output_flag", "test_parse_add_options"], "ast_kind": "class_or_type", "text": "import argparse\nimport re\nfrom pathlib import Path\n\nfrom scrapy.commands import parse\nfrom scrapy.settings import Settings\nfrom tests.mockserver.http import MockServer\nfrom tests.test_commands import TestCommandBase\n\n\nclass TestParseCommand(TestCommandBase):\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        super().setup_method()\n        self.spider_name = \"parse_spider\"\n        (self.proj_mod_path / \"spiders\" / \"myspider.py\").write_text(\n            f\"\"\"\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.utils.test import get_from_asyncio_queue\nimport asyncio\n\n\nclass AsyncDefAsyncioReturnSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_return\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {{status}}\")\n        return [{{'id': 1}}, {{'id': 2}}]\n\nclass AsyncDefAsyncioReturnSingleElementSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_return_single_element\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.1)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {{status}}\")\n        return {{'foo': 42}}\n\nclass AsyncDefAsyncioGenLoopSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_gen_loop\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {{'foo': i}}\n        self.logger.info(f\"Got response {{response.status}}\")\n\nclass AsyncDefAsyncioSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.debug(f\"Got response {{status}}\")\n\nclass AsyncDefAsyncioGenExcSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_gen_exc\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {{'foo': i}}\n            if i > 5:\n                raise ValueError(\"Stopping the processing\")\n\nclass CallbackSignatureDownloaderMiddleware:\n    def process_request(self, request, spider):\n        from inspect import signature\n        spider.logger.debug(f\"request.callback signature: {{signature(request.callback)}}\")\n\n\nclass MySpider(scrapy.Spider):\n    name = '{self.spider_name}'\n\n    custom_settings = {{\n        \"DOWNLOADER_MIDDLEWARES\": {{\n            CallbackSignatureDownloaderMiddleware: 0,\n        }}\n    }}\n\n    def parse(self, response):\n        if getattr(self, 'test_arg', None):\n            self.logger.debug('It Works!')\n        return [scrapy.Item(), dict(foo='bar')]\n\n    def parse_request_with_meta(self, response):\n        foo = response.meta.get('foo', 'bar')\n\n        if foo == 'bar':\n            self.logger.debug('It Does Not Work :(')\n        else:\n            self.logger.debug('It Works!')\n\n    def parse_request_with_cb_kwargs(self, response, foo=None, key=None):\n        if foo == 'bar' and key == 'value':\n            self.logger.debug('It Works!')\n        else:\n            self.logger.debug('It Does Not Work :(')\n\n    def parse_request_without_meta(self, response):\n        foo = response.meta.get('foo', 'bar')\n\n        if foo == 'bar':\n            self.logger.debug('It Works!')\n        else:\n            self.logger.debug('It Does Not Work :(')\n\nclass MyGoodCrawlSpider(CrawlSpider):\n    name = 'goodcrawl{self.spider_name}'\n\n    rules = (\n        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n        Rule(LinkExtractor(allow=r'/text'), follow=True),\n    )\n\n    def parse_item(self, response):\n        return [scrapy.Item(), dict(foo='bar')]\n\n    def parse(self, response):\n        return [scrapy.Item(), dict(nomatch='default')]\n\n\nclass MyBadCrawlSpider(CrawlSpider):\n    '''Spider which doesn't define a parse_item callback while using it in a rule.'''\n    name = 'badcrawl{self.spider_name}'\n\n    rules = (\n        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n    )\n\n    def parse(self, response):\n        return [scrapy.Item(), dict(foo='bar')]\n\"\"\",\n            encoding=\"utf-8\",\n        )\n\n        (self.proj_mod_path / \"pipelines.py\").write_text(\n            \"\"\"\nimport logging\n\nclass MyPipeline:\n    component_name = 'my_pipeline'\n\n    def process_item(self, item):\n        logging.info('It Works!')\n        return item\n\"\"\",\n            encoding=\"utf-8\",\n        )\n\n        with (self.proj_mod_path / \"settings.py\").open(\"a\", encoding=\"utf-8\") as f:\n            f.write(\n                f\"\"\"\nITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n\"\"\"\n            )\n\n    def test_spider_arguments(self):\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"-a\",\n            \"test_arg=1\",\n            \"-c\",\n            \"parse\",\n            \"--verbose\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"DEBUG: It Works!\" in stderr\n", "n_tokens": 1208, "byte_len": 5273, "file_sha1": "8ceb623f6bd34e9911c9ca1fd141175d5135ec24", "start_line": 1, "end_line": 183}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_parse.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_parse.py", "rel_path": "tests/test_command_parse.py", "module": "tests.test_command_parse", "ext": "py", "chunk_number": 2, "symbols": ["test_request_with_meta", "test_request_with_cb_kwargs", "test_request_without_meta", "test_pipelines", "test_async_def_asyncio_parse_items_list", "test_async_def_asyncio_parse_items_single_element", "test_async_def_asyncgen_parse_loop", "test_async_def_asyncgen_parse_exc", "test_async_def_asyncio_parse", "test_parse_items", "test_parse_items_no_callback_passed", "test_wrong_callback_passed", "test_crawlspider_matching_rule_callback_set", "test_crawlspider_matching_rule_default_callback", "test", "crawlspider", "async", "text", "spider", "items", "works", "parse", "request", "value", "error", "dummy", "meta", "asyncdef", "asyncio", "defined", "setup_class", "teardown_class", "setup_method", "process_request", "parse_request_with_meta", "parse_request_with_cb_kwargs", "parse_request_without_meta", "parse_item", "process_item", "test_spider_arguments", "test_spider_with_no_rules_attribute", "test_crawlspider_missing_callback", "test_crawlspider_no_matching_rule", "test_crawlspider_not_exists_with_not_matched_url", "test_output_flag", "test_parse_add_options", "TestParseCommand", "AsyncDefAsyncioReturnSpider", "AsyncDefAsyncioReturnSingleElementSpider", "AsyncDefAsyncioGenLoopSpider"], "ast_kind": "function_or_method", "text": "    def test_request_with_meta(self):\n        raw_json_string = '{\"foo\" : \"baz\"}'\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"--meta\",\n            raw_json_string,\n            \"-c\",\n            \"parse_request_with_meta\",\n            \"--verbose\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"DEBUG: It Works!\" in stderr\n\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"-m\",\n            raw_json_string,\n            \"-c\",\n            \"parse_request_with_meta\",\n            \"--verbose\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"DEBUG: It Works!\" in stderr\n\n    def test_request_with_cb_kwargs(self):\n        raw_json_string = '{\"foo\" : \"bar\", \"key\": \"value\"}'\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"--cbkwargs\",\n            raw_json_string,\n            \"-c\",\n            \"parse_request_with_cb_kwargs\",\n            \"--verbose\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"DEBUG: It Works!\" in stderr\n        assert (\n            \"DEBUG: request.callback signature: (response, foo=None, key=None)\"\n            in stderr\n        )\n\n    def test_request_without_meta(self):\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"-c\",\n            \"parse_request_without_meta\",\n            \"--nolinks\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"DEBUG: It Works!\" in stderr\n\n    def test_pipelines(self):\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"--pipelines\",\n            \"-c\",\n            \"parse\",\n            \"--verbose\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"INFO: It Works!\" in stderr\n\n    def test_async_def_asyncio_parse_items_list(self):\n        _, out, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"asyncdef_asyncio_return\",\n            \"-c\",\n            \"parse\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"INFO: Got response 200\" in stderr\n        assert \"{'id': 1}\" in out\n        assert \"{'id': 2}\" in out\n\n    def test_async_def_asyncio_parse_items_single_element(self):\n        _, out, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"asyncdef_asyncio_return_single_element\",\n            \"-c\",\n            \"parse\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"INFO: Got response 200\" in stderr\n        assert \"{'foo': 42}\" in out\n\n    def test_async_def_asyncgen_parse_loop(self):\n        _, out, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"asyncdef_asyncio_gen_loop\",\n            \"-c\",\n            \"parse\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"INFO: Got response 200\" in stderr\n        for i in range(10):\n            assert f\"{{'foo': {i}}}\" in out\n\n    def test_async_def_asyncgen_parse_exc(self):\n        _, out, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"asyncdef_asyncio_gen_exc\",\n            \"-c\",\n            \"parse\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"ValueError\" in stderr\n        for i in range(7):\n            assert f\"{{'foo': {i}}}\" in out\n\n    def test_async_def_asyncio_parse(self):\n        _, _, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"asyncdef_asyncio\",\n            \"-c\",\n            \"parse\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"DEBUG: Got response 200\" in stderr\n\n    def test_parse_items(self):\n        _, out, _ = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"-c\",\n            \"parse\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"[{}, {'foo': 'bar'}]\" in out\n\n    def test_parse_items_no_callback_passed(self):\n        _, out, _ = self.proc(\n            \"parse\", \"--spider\", self.spider_name, self.mockserver.url(\"/html\")\n        )\n        assert \"[{}, {'foo': 'bar'}]\" in out\n\n    def test_wrong_callback_passed(self):\n        _, out, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"-c\",\n            \"dummy\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n        assert \"Cannot find callback\" in stderr\n\n    def test_crawlspider_matching_rule_callback_set(self):\n        \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n        _, out, _ = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"goodcrawl\" + self.spider_name,\n            \"-r\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert \"[{}, {'foo': 'bar'}]\" in out\n\n    def test_crawlspider_matching_rule_default_callback(self):\n        \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n        _, out, _ = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"goodcrawl\" + self.spider_name,\n            \"-r\",\n            self.mockserver.url(\"/text\"),\n        )\n        assert \"[{}, {'nomatch': 'default'}]\" in out\n", "n_tokens": 1214, "byte_len": 5414, "file_sha1": "8ceb623f6bd34e9911c9ca1fd141175d5135ec24", "start_line": 184, "end_line": 368}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_parse.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_parse.py", "rel_path": "tests/test_command_parse.py", "module": "tests.test_command_parse", "ext": "py", "chunk_number": 3, "symbols": ["test_spider_with_no_rules_attribute", "test_crawlspider_missing_callback", "test_crawlspider_no_matching_rule", "test_crawlspider_not_exists_with_not_matched_url", "test_output_flag", "test_parse_add_options", "encoding", "gb18030", "containing", "rules", "command", "mockserver", "path", "correct", "add", "options", "settings", "prefix", "chars", "items", "verbose", "created", "html", "nolinks", "parse", "prog", "found", "using", "successfully", "spider", "setup_class", "teardown_class", "setup_method", "process_request", "parse_request_with_meta", "parse_request_with_cb_kwargs", "parse_request_without_meta", "parse_item", "process_item", "test_spider_arguments", "test_request_with_meta", "test_request_with_cb_kwargs", "test_request_without_meta", "test_pipelines", "test_async_def_asyncio_parse_items_list", "test_async_def_asyncio_parse_items_single_element", "test_async_def_asyncgen_parse_loop", "test_async_def_asyncgen_parse_exc", "test_async_def_asyncio_parse", "test_parse_items"], "ast_kind": "function_or_method", "text": "    def test_spider_with_no_rules_attribute(self):\n        \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n        _, out, stderr = self.proc(\n            \"parse\", \"--spider\", self.spider_name, \"-r\", self.mockserver.url(\"/html\")\n        )\n        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n        assert \"No CrawlSpider rules found\" in stderr\n\n    def test_crawlspider_missing_callback(self):\n        _, out, _ = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"badcrawl\" + self.spider_name,\n            \"-r\",\n            self.mockserver.url(\"/html\"),\n        )\n        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n\n    def test_crawlspider_no_matching_rule(self):\n        \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n        _, out, stderr = self.proc(\n            \"parse\",\n            \"--spider\",\n            \"badcrawl\" + self.spider_name,\n            \"-r\",\n            self.mockserver.url(\"/enc-gb18030\"),\n        )\n        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n        assert \"Cannot find a rule that matches\" in stderr\n\n    def test_crawlspider_not_exists_with_not_matched_url(self):\n        assert self.call(\"parse\", self.mockserver.url(\"/invalid_url\")) == 0\n\n    def test_output_flag(self):\n        \"\"\"Checks if a file was created successfully having\n        correct format containing correct data in it.\n        \"\"\"\n        file_name = \"data.json\"\n        file_path = Path(self.proj_path, file_name)\n        self.proc(\n            \"parse\",\n            \"--spider\",\n            self.spider_name,\n            \"-c\",\n            \"parse\",\n            \"-o\",\n            file_name,\n            self.mockserver.url(\"/html\"),\n        )\n\n        assert file_path.exists()\n        assert file_path.is_file()\n\n        content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n        assert file_path.read_text(encoding=\"utf-8\") == content\n\n    def test_parse_add_options(self):\n        command = parse.Command()\n        command.settings = Settings()\n        parser = argparse.ArgumentParser(\n            prog=\"scrapy\",\n            formatter_class=argparse.HelpFormatter,\n            conflict_handler=\"resolve\",\n            prefix_chars=\"-\",\n        )\n        command.add_options(parser)\n        namespace = parser.parse_args(\n            [\"--verbose\", \"--nolinks\", \"-d\", \"2\", \"--spider\", self.spider_name]\n        )\n        assert namespace.nolinks\n        assert namespace.depth == 2\n        assert namespace.spider == self.spider_name\n        assert namespace.verbose\n", "n_tokens": 576, "byte_len": 2557, "file_sha1": "8ceb623f6bd34e9911c9ca1fd141175d5135ec24", "start_line": 369, "end_line": 442}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logstats.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logstats.py", "rel_path": "tests/test_logstats.py", "module": "tests.test_logstats", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_stats_calculations", "test_stats_calculations_no_time", "test_stats_calculation_no_elapsed_time", "TestLogStats", "set", "value", "pagesprev", "simulate", "stat", "start", "happens", "spider", "minute", "finish", "time", "after", "class", "test", "stats", "elapsed", "pages", "with", "spiders", "scrapy", "log", "available", "create", "closes", "closed", "values", "get", "crawler", "running", "responses", "per", "calculate", "from", "pytest", "should", "mins", "simple", "assert", "item", "scraped", "raises", "items", "when", "prate", "none"], "ast_kind": "class_or_type", "text": "from datetime import datetime\n\nimport pytest\n\nfrom scrapy.extensions.logstats import LogStats\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import SimpleSpider\n\n\nclass TestLogStats:\n    def setup_method(self):\n        self.crawler = get_crawler(SimpleSpider)\n        self.spider = self.crawler._create_spider(\"spidey\")\n        self.stats = self.crawler.stats\n\n        self.stats.set_value(\"response_received_count\", 4802)\n        self.stats.set_value(\"item_scraped_count\", 3201)\n\n    def test_stats_calculations(self):\n        logstats = LogStats.from_crawler(self.crawler)\n\n        with pytest.raises(AttributeError):\n            logstats.pagesprev\n        with pytest.raises(AttributeError):\n            logstats.itemsprev\n\n        logstats.spider_opened(self.spider)\n        assert logstats.pagesprev == 4802\n        assert logstats.itemsprev == 3201\n\n        logstats.calculate_stats()\n        assert logstats.items == 3201\n        assert logstats.pages == 4802\n        assert logstats.irate == 0.0\n        assert logstats.prate == 0.0\n        assert logstats.pagesprev == 4802\n        assert logstats.itemsprev == 3201\n\n        # Simulate what happens after a minute\n        self.stats.set_value(\"response_received_count\", 5187)\n        self.stats.set_value(\"item_scraped_count\", 3492)\n        logstats.calculate_stats()\n        assert logstats.items == 3492\n        assert logstats.pages == 5187\n        assert logstats.irate == 291.0\n        assert logstats.prate == 385.0\n        assert logstats.pagesprev == 5187\n        assert logstats.itemsprev == 3492\n\n        # Simulate when spider closes after running for 30 mins\n        self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n        self.stats.set_value(\"finish_time\", datetime.fromtimestamp(1655101972))\n        logstats.spider_closed(self.spider, \"test reason\")\n        assert self.stats.get_value(\"responses_per_minute\") == 172.9\n        assert self.stats.get_value(\"items_per_minute\") == 116.4\n\n    def test_stats_calculations_no_time(self):\n        \"\"\"The stat values should be None since the start and finish time are\n        not available.\n        \"\"\"\n        logstats = LogStats.from_crawler(self.crawler)\n        logstats.spider_closed(self.spider, \"test reason\")\n        assert self.stats.get_value(\"responses_per_minute\") is None\n        assert self.stats.get_value(\"items_per_minute\") is None\n\n    def test_stats_calculation_no_elapsed_time(self):\n        \"\"\"The stat values should be None since the elapsed time is 0.\"\"\"\n        logstats = LogStats.from_crawler(self.crawler)\n        self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n        self.stats.set_value(\"finish_time\", datetime.fromtimestamp(1655100172))\n        logstats.spider_closed(self.spider, \"test reason\")\n        assert self.stats.get_value(\"responses_per_minute\") is None\n        assert self.stats.get_value(\"items_per_minute\") is None\n", "n_tokens": 682, "byte_len": 2937, "file_sha1": "092d67dcaa5a922680ff5fb74e80b5d4e0ea26b6", "start_line": 1, "end_line": 74}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_template.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_template.py", "rel_path": "tests/test_utils_template.py", "module": "tests.test_utils_template", "ext": "py", "chunk_number": 1, "symbols": ["test_simple_render", "encoding", "utf", "utf8", "failure", "render", "templatefile", "templ", "context", "name", "spiders", "scrapy", "the", "spider", "tmp", "path", "proj", "template", "classname", "itself", "project", "test", "from", "assert", "tmpl", "unlink", "exists", "file", "utils", "simple", "import", "rendered", "read", "text", "write"], "ast_kind": "function_or_method", "text": "from scrapy.utils.template import render_templatefile\n\n\ndef test_simple_render(tmp_path):\n    context = {\"project_name\": \"proj\", \"name\": \"spi\", \"classname\": \"TheSpider\"}\n    template = \"from ${project_name}.spiders.${name} import ${classname}\"\n    rendered = \"from proj.spiders.spi import TheSpider\"\n\n    template_path = tmp_path / \"templ.py.tmpl\"\n    render_path = tmp_path / \"templ.py\"\n\n    template_path.write_text(template, encoding=\"utf8\")\n    assert template_path.is_file()  # Failure of test itself\n\n    render_templatefile(template_path, **context)\n\n    assert not template_path.exists()\n    assert render_path.read_text(encoding=\"utf8\") == rendered\n\n    render_path.unlink()\n    assert not render_path.exists()  # Failure of test itself\n", "n_tokens": 170, "byte_len": 746, "file_sha1": "6392caf63edb66acd4a987647c86ee60f568c166", "start_line": 1, "end_line": 22}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 1, "symbols": ["test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "TestRequest", "encoding", "method", "price", "different", "latin", "latin1", "xmlrpc", "passes", "callback", "about", "default", "meta", "key", "key1", "lala", "val", "val2", "https", "mock", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback", "a_function", "test_callback_and_errback_type", "test_no_callback", "test_from_curl", "test_from_curl_with_kwargs", "test_from_curl_ignore_unknown_options", "assertQueryEqual", "test_empty_formdata", "test_formdata_overrides_querystring", "test_default_encoding_bytes"], "ast_kind": "class_or_type", "text": "import json\nimport re\nimport warnings\nimport xmlrpc.client\nfrom typing import Any\nfrom unittest import mock\nfrom urllib.parse import parse_qs, unquote_to_bytes\n\nimport pytest\n\nfrom scrapy.http import (\n    FormRequest,\n    Headers,\n    HtmlResponse,\n    JsonRequest,\n    Request,\n    XmlRpcRequest,\n)\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes, to_unicode\n\n\nclass TestRequest:\n    request_class = Request\n    default_method = \"GET\"\n    default_headers: dict[bytes, list[bytes]] = {}\n    default_meta: dict[str, Any] = {}\n\n    def test_init(self):\n        # Request requires url in the __init__ method\n        with pytest.raises(TypeError):\n            self.request_class()\n\n        # url argument must be basestring\n        with pytest.raises(TypeError):\n            self.request_class(123)\n        r = self.request_class(\"http://www.example.com\")\n\n        r = self.request_class(\"http://www.example.com\")\n        assert isinstance(r.url, str)\n        assert r.url == \"http://www.example.com\"\n        assert r.method == self.default_method\n\n        assert isinstance(r.headers, Headers)\n        assert r.headers == self.default_headers\n        assert r.meta == self.default_meta\n\n        meta = {\"lala\": \"lolo\"}\n        headers = {b\"caca\": b\"coco\"}\n        r = self.request_class(\n            \"http://www.example.com\", meta=meta, headers=headers, body=\"a body\"\n        )\n\n        assert r.meta is not meta\n        assert r.meta == meta\n        assert r.headers is not headers\n        assert r.headers[b\"caca\"] == b\"coco\"\n\n    def test_url_scheme(self):\n        # This test passes by not raising any (ValueError) exception\n        self.request_class(\"http://example.org\")\n        self.request_class(\"https://example.org\")\n        self.request_class(\"s3://example.org\")\n        self.request_class(\"ftp://example.org\")\n        self.request_class(\"about:config\")\n        self.request_class(\"data:,Hello%2C%20World!\")\n\n    def test_url_no_scheme(self):\n        msg = \"Missing scheme in request url:\"\n        with pytest.raises(ValueError, match=msg):\n            self.request_class(\"foo\")\n        with pytest.raises(ValueError, match=msg):\n            self.request_class(\"/foo/\")\n        with pytest.raises(ValueError, match=msg):\n            self.request_class(\"/foo:bar\")\n\n    def test_headers(self):\n        # Different ways of setting headers attribute\n        url = \"http://www.scrapy.org\"\n        headers = {b\"Accept\": \"gzip\", b\"Custom-Header\": \"nothing to tell you\"}\n        r = self.request_class(url=url, headers=headers)\n        p = self.request_class(url=url, headers=r.headers)\n\n        assert r.headers == p.headers\n        assert r.headers is not headers\n        assert p.headers is not r.headers\n\n        # headers must not be unicode\n        h = Headers({\"key1\": \"val1\", \"key2\": \"val2\"})\n        h[\"newkey\"] = \"newval\"\n        for k, v in h.items():\n            assert isinstance(k, bytes)\n            for s in v:\n                assert isinstance(s, bytes)\n\n    def test_eq(self):\n        url = \"http://www.scrapy.org\"\n        r1 = self.request_class(url=url)\n        r2 = self.request_class(url=url)\n        assert r1 != r2\n\n        set_ = set()\n        set_.add(r1)\n        set_.add(r2)\n        assert len(set_) == 2\n\n    def test_url(self):\n        r = self.request_class(url=\"http://www.scrapy.org/path\")\n        assert r.url == \"http://www.scrapy.org/path\"\n\n    def test_url_quoting(self):\n        r = self.request_class(url=\"http://www.scrapy.org/blank%20space\")\n        assert r.url == \"http://www.scrapy.org/blank%20space\"\n        r = self.request_class(url=\"http://www.scrapy.org/blank space\")\n        assert r.url == \"http://www.scrapy.org/blank%20space\"\n\n    def test_url_encoding(self):\n        r = self.request_class(url=\"http://www.scrapy.org/price/£\")\n        assert r.url == \"http://www.scrapy.org/price/%C2%A3\"\n\n    def test_url_encoding_other(self):\n        # encoding affects only query part of URI, not path\n        # path part should always be UTF-8 encoded before percent-escaping\n        r = self.request_class(url=\"http://www.scrapy.org/price/£\", encoding=\"utf-8\")\n        assert r.url == \"http://www.scrapy.org/price/%C2%A3\"\n\n        r = self.request_class(url=\"http://www.scrapy.org/price/£\", encoding=\"latin1\")\n        assert r.url == \"http://www.scrapy.org/price/%C2%A3\"\n\n    def test_url_encoding_query(self):\n        r1 = self.request_class(url=\"http://www.scrapy.org/price/£?unit=µ\")\n        assert r1.url == \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\"\n\n        # should be same as above\n        r2 = self.request_class(\n            url=\"http://www.scrapy.org/price/£?unit=µ\", encoding=\"utf-8\"\n        )\n        assert r2.url == \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\"\n", "n_tokens": 1173, "byte_len": 4824, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 1, "end_line": 140}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 2, "symbols": ["test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "CustomRequest", "encoding", "price", "does", "containing", "latin", "latin1", "future", "possible", "acute", "dont", "filter", "ietf", "string", "https", "preserve", "make", "interpreted", "datatracker", "iris", "encoded", "isinstance", "than", "object", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback", "a_function", "test_callback_and_errback_type", "test_no_callback", "test_from_curl", "test_from_curl_with_kwargs", "test_from_curl_ignore_unknown_options"], "ast_kind": "class_or_type", "text": "    def test_url_encoding_query_latin1(self):\n        # encoding is used for encoding query-string before percent-escaping;\n        # path is still UTF-8 encoded before percent-escaping\n        r3 = self.request_class(\n            url=\"http://www.scrapy.org/price/µ?currency=£\", encoding=\"latin1\"\n        )\n        assert r3.url == \"http://www.scrapy.org/price/%C2%B5?currency=%A3\"\n\n    def test_url_encoding_nonutf8_untouched(self):\n        # percent-escaping sequences that do not match valid UTF-8 sequences\n        # should be kept untouched (just upper-cased perhaps)\n        #\n        # See https://datatracker.ietf.org/doc/html/rfc3987#section-3.2\n        #\n        # \"Conversions from URIs to IRIs MUST NOT use any character encoding\n        # other than UTF-8 in steps 3 and 4, even if it might be possible to\n        # guess from the context that another character encoding than UTF-8 was\n        # used in the URI.  For example, the URI\n        # \"http://www.example.org/r%E9sum%E9.html\" might with some guessing be\n        # interpreted to contain two e-acute characters encoded as iso-8859-1.\n        # It must not be converted to an IRI containing these e-acute\n        # characters.  Otherwise, in the future the IRI will be mapped to\n        # \"http://www.example.org/r%C3%A9sum%C3%A9.html\", which is a different\n        # URI from \"http://www.example.org/r%E9sum%E9.html\".\n        r1 = self.request_class(url=\"http://www.scrapy.org/price/%a3\")\n        assert r1.url == \"http://www.scrapy.org/price/%a3\"\n\n        r2 = self.request_class(url=\"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n        assert r2.url == \"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\"\n\n        r3 = self.request_class(url=\"http://www.scrapy.org/résumé/%a3\")\n        assert r3.url == \"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\"\n\n        r4 = self.request_class(url=\"http://www.example.org/r%E9sum%E9.html\")\n        assert r4.url == \"http://www.example.org/r%E9sum%E9.html\"\n\n    def test_body(self):\n        r1 = self.request_class(url=\"http://www.example.com/\")\n        assert r1.body == b\"\"\n\n        r2 = self.request_class(url=\"http://www.example.com/\", body=b\"\")\n        assert isinstance(r2.body, bytes)\n        assert r2.encoding == \"utf-8\"  # default encoding\n\n        r3 = self.request_class(\n            url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding=\"utf-8\"\n        )\n        assert isinstance(r3.body, bytes)\n        assert r3.body == b\"Price: \\xc2\\xa3100\"\n\n        r4 = self.request_class(\n            url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding=\"latin1\"\n        )\n        assert isinstance(r4.body, bytes)\n        assert r4.body == b\"Price: \\xa3100\"\n\n    def test_copy(self):\n        \"\"\"Test Request copy\"\"\"\n\n        def somecallback():\n            pass\n\n        r1 = self.request_class(\n            \"http://www.example.com\",\n            flags=[\"f1\", \"f2\"],\n            callback=somecallback,\n            errback=somecallback,\n        )\n        r1.meta[\"foo\"] = \"bar\"\n        r1.cb_kwargs[\"key\"] = \"value\"\n        r2 = r1.copy()\n\n        # make sure copy does not propagate callbacks\n        assert r1.callback is somecallback\n        assert r1.errback is somecallback\n        assert r2.callback is r1.callback\n        assert r2.errback is r2.errback\n\n        # make sure flags list is shallow copied\n        assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n        assert r1.flags == r2.flags\n\n        # make sure cb_kwargs dict is shallow copied\n        assert r1.cb_kwargs is not r2.cb_kwargs, (\n            \"cb_kwargs must be a shallow copy, not identical\"\n        )\n        assert r1.cb_kwargs == r2.cb_kwargs\n\n        # make sure meta dict is shallow copied\n        assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n        assert r1.meta == r2.meta\n\n        # make sure headers attribute is shallow copied\n        assert r1.headers is not r2.headers, (\n            \"headers must be a shallow copy, not identical\"\n        )\n        assert r1.headers == r2.headers\n        assert r1.encoding == r2.encoding\n        assert r1.dont_filter == r2.dont_filter\n\n        # Request.body can be identical since it's an immutable object (str)\n\n    def test_copy_inherited_classes(self):\n        \"\"\"Test Request children copies preserve their class\"\"\"\n\n        class CustomRequest(self.request_class):\n            pass\n\n        r1 = CustomRequest(\"http://www.example.com\")\n        r2 = r1.copy()\n\n        assert isinstance(r2, CustomRequest)\n", "n_tokens": 1161, "byte_len": 4531, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 141, "end_line": 253}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 3, "symbols": ["test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback", "a_function", "test_callback_and_errback_type", "test_no_callback", "post", "test", "method", "immutable", "false", "hdrs", "empty", "pass", "replace", "callback", "meta", "dont", "filter", "with", "compared", "headers", "example", "function", "body", "value", "errback", "type", "error", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_from_curl", "test_from_curl_with_kwargs", "test_from_curl_ignore_unknown_options", "assertQueryEqual"], "ast_kind": "function_or_method", "text": "    def test_replace(self):\n        \"\"\"Test Request.replace() method\"\"\"\n        r1 = self.request_class(\"http://www.example.com\", method=\"GET\")\n        hdrs = Headers(r1.headers)\n        hdrs[b\"key\"] = b\"value\"\n        r2 = r1.replace(method=\"POST\", body=\"New body\", headers=hdrs)\n        assert r1.url == r2.url\n        assert (r1.method, r2.method) == (\"GET\", \"POST\")\n        assert (r1.body, r2.body) == (b\"\", b\"New body\")\n        assert (r1.headers, r2.headers) == (self.default_headers, hdrs)\n\n        # Empty attributes (which may fail if not compared properly)\n        r3 = self.request_class(\n            \"http://www.example.com\", meta={\"a\": 1}, dont_filter=True\n        )\n        r4 = r3.replace(\n            url=\"http://www.example.com/2\", body=b\"\", meta={}, dont_filter=False\n        )\n        assert r4.url == \"http://www.example.com/2\"\n        assert r4.body == b\"\"\n        assert r4.meta == {}\n        assert r4.dont_filter is False\n\n    def test_method_always_str(self):\n        r = self.request_class(\"http://www.example.com\", method=\"POST\")\n        assert isinstance(r.method, str)\n\n    def test_immutable_attributes(self):\n        r = self.request_class(\"http://example.com\")\n        with pytest.raises(AttributeError):\n            r.url = \"http://example2.com\"\n        with pytest.raises(AttributeError):\n            r.body = \"xxx\"\n\n    def test_callback_and_errback(self):\n        def a_function():\n            pass\n\n        r1 = self.request_class(\"http://example.com\")\n        assert r1.callback is None\n        assert r1.errback is None\n\n        r2 = self.request_class(\"http://example.com\", callback=a_function)\n        assert r2.callback is a_function\n        assert r2.errback is None\n\n        r3 = self.request_class(\"http://example.com\", errback=a_function)\n        assert r3.callback is None\n        assert r3.errback is a_function\n\n        r4 = self.request_class(\n            url=\"http://example.com\",\n            callback=a_function,\n            errback=a_function,\n        )\n        assert r4.callback is a_function\n        assert r4.errback is a_function\n\n        r5 = self.request_class(\n            url=\"http://example.com\",\n            callback=NO_CALLBACK,\n            errback=NO_CALLBACK,\n        )\n        assert r5.callback is NO_CALLBACK\n        assert r5.errback is NO_CALLBACK\n\n    def test_callback_and_errback_type(self):\n        with pytest.raises(TypeError):\n            self.request_class(\"http://example.com\", callback=\"a_function\")\n        with pytest.raises(TypeError):\n            self.request_class(\"http://example.com\", errback=\"a_function\")\n        with pytest.raises(TypeError):\n            self.request_class(\n                url=\"http://example.com\",\n                callback=\"a_function\",\n                errback=\"a_function\",\n            )\n\n    def test_no_callback(self):\n        with pytest.raises(RuntimeError):\n            NO_CALLBACK()\n", "n_tokens": 660, "byte_len": 2902, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 254, "end_line": 335}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 4, "symbols": ["test_from_curl", "test_from_curl_with_kwargs", "test_from_curl_ignore_unknown_options", "assertQueryEqual", "test_empty_formdata", "TestFormRequest", "delete", "method", "from", "curl", "patch", "test", "cookies", "agent", "gauges", "unique", "khtml", "small", "custtel", "linux", "more", "chrome", "requests", "mozilla", "regarding", "webp", "khtm", "unknown", "pytest", "unicode", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "class_or_type", "text": "    def test_from_curl(self):\n        # Note: more curated tests regarding curl conversion are in\n        # `test_utils_curl.py`\n        curl_command = (\n            \"curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique\"\n            \"_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique\"\n            \"_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H\"\n            \" 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q\"\n            \"=0.9,ru;q=0.8,es;q=0.7' -H 'Upgrade-Insecure-Requests: 1' -H 'Use\"\n            \"r-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTM\"\n            \"L, like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.3202.75 S\"\n            \"afari/537.36' -H 'Content-Type: application /x-www-form-urlencode\"\n            \"d' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=\"\n            \"0.9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0\"\n            \"' -H 'Referer: http://httpbin.org/forms/post' -H 'Connection: kee\"\n            \"p-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith\"\n            \"%40example.org&size=small&topping=cheese&topping=onion&delivery=1\"\n            \"2%3A15&comments=' --compressed\"\n        )\n        r = self.request_class.from_curl(curl_command)\n        assert r.method == \"POST\"\n        assert r.url == \"http://httpbin.org/post\"\n        assert (\n            r.body == b\"custname=John+Smith&custtel=500&custemail=jsmith%40\"\n            b\"example.org&size=small&topping=cheese&topping=onion\"\n            b\"&delivery=12%3A15&comments=\"\n        )\n        assert r.cookies == {\n            \"_gauges_unique_year\": \"1\",\n            \"_gauges_unique\": \"1\",\n            \"_gauges_unique_month\": \"1\",\n            \"_gauges_unique_hour\": \"1\",\n            \"_gauges_unique_day\": \"1\",\n        }\n        assert r.headers == {\n            b\"Origin\": [b\"http://httpbin.org\"],\n            b\"Accept-Encoding\": [b\"gzip, deflate\"],\n            b\"Accept-Language\": [b\"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"],\n            b\"Upgrade-Insecure-Requests\": [b\"1\"],\n            b\"User-Agent\": [\n                b\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.\"\n                b\"36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202\"\n                b\".75 Chrome/62.0.3202.75 Safari/537.36\"\n            ],\n            b\"Content-Type\": [b\"application /x-www-form-urlencoded\"],\n            b\"Accept\": [\n                b\"text/html,application/xhtml+xml,application/xml;q=0.\"\n                b\"9,image/webp,image/apng,*/*;q=0.8\"\n            ],\n            b\"Cache-Control\": [b\"max-age=0\"],\n            b\"Referer\": [b\"http://httpbin.org/forms/post\"],\n            b\"Connection\": [b\"keep-alive\"],\n        }\n\n    def test_from_curl_with_kwargs(self):\n        r = self.request_class.from_curl(\n            'curl -X PATCH \"http://example.org\"', method=\"POST\", meta={\"key\": \"value\"}\n        )\n        assert r.method == \"POST\"\n        assert r.meta == {\"key\": \"value\"}\n\n    def test_from_curl_ignore_unknown_options(self):\n        # By default: it works and ignores the unknown options: --foo and -z\n        with warnings.catch_warnings():  # avoid warning when executing tests\n            warnings.simplefilter(\"ignore\")\n            r = self.request_class.from_curl(\n                'curl -X DELETE \"http://example.org\" --foo -z',\n            )\n            assert r.method == \"DELETE\"\n\n        # If `ignore_unknown_options` is set to `False` it raises an error with\n        # the unknown options: --foo and -z\n        with pytest.raises(ValueError, match=\"Unrecognized options:\"):\n            self.request_class.from_curl(\n                'curl -X PATCH \"http://example.org\" --foo -z',\n                ignore_unknown_options=False,\n            )\n\n\nclass TestFormRequest(TestRequest):\n    request_class = FormRequest\n\n    def assertQueryEqual(self, first, second, msg=None):\n        first = to_unicode(first).split(\"&\")\n        second = to_unicode(second).split(\"&\")\n        assert sorted(first) == sorted(second), msg\n\n    def test_empty_formdata(self):\n        r1 = self.request_class(\"http://www.example.com\", formdata={})\n        assert r1.body == b\"\"\n", "n_tokens": 1122, "byte_len": 4165, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 336, "end_line": 426}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 5, "symbols": ["test_formdata_overrides_querystring", "test_default_encoding_bytes", "test_default_encoding_textual_data", "test_default_encoding_mixed_data", "test_custom_encoding_bytes", "test_custom_encoding_textual_data", "test_multi_key_values", "test_from_response_post", "fragment", "encoding", "post", "method", "price", "test", "from", "urlencoded", "application", "default", "green", "multi", "input", "latin", "latin1", "single", "type", "custom", "maxsplit", "colours", "name", "response", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_formdata_overrides_querystring(self):\n        data = ((\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"2\"))\n        url = self.request_class(\n            \"http://www.example.com/?a=0&b=1&c=3#fragment\", method=\"GET\", formdata=data\n        ).url.split(\"#\", maxsplit=1)[0]\n        fs = _qs(self.request_class(url, method=\"GET\", formdata=data))\n        assert set(fs[b\"a\"]) == {b\"one\", b\"two\"}\n        assert fs[b\"b\"] == [b\"2\"]\n        assert fs.get(b\"c\") is None\n\n        data = {\"a\": \"1\", \"b\": \"2\"}\n        fs = _qs(\n            self.request_class(\"http://www.example.com/\", method=\"GET\", formdata=data)\n        )\n        assert fs[b\"a\"] == [b\"1\"]\n        assert fs[b\"b\"] == [b\"2\"]\n\n    def test_default_encoding_bytes(self):\n        # using default encoding (utf-8)\n        data = {b\"one\": b\"two\", b\"price\": b\"\\xc2\\xa3 100\"}\n        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n        assert r2.method == \"POST\"\n        assert r2.encoding == \"utf-8\"\n        self.assertQueryEqual(r2.body, b\"price=%C2%A3+100&one=two\")\n        assert r2.headers[b\"Content-Type\"] == b\"application/x-www-form-urlencoded\"\n\n    def test_default_encoding_textual_data(self):\n        # using default encoding (utf-8)\n        data = {\"µ one\": \"two\", \"price\": \"£ 100\"}\n        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n        assert r2.method == \"POST\"\n        assert r2.encoding == \"utf-8\"\n        self.assertQueryEqual(r2.body, b\"price=%C2%A3+100&%C2%B5+one=two\")\n        assert r2.headers[b\"Content-Type\"] == b\"application/x-www-form-urlencoded\"\n\n    def test_default_encoding_mixed_data(self):\n        # using default encoding (utf-8)\n        data = {\"\\u00b5one\": b\"two\", b\"price\\xc2\\xa3\": \"\\u00a3 100\"}\n        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n        assert r2.method == \"POST\"\n        assert r2.encoding == \"utf-8\"\n        self.assertQueryEqual(r2.body, b\"%C2%B5one=two&price%C2%A3=%C2%A3+100\")\n        assert r2.headers[b\"Content-Type\"] == b\"application/x-www-form-urlencoded\"\n\n    def test_custom_encoding_bytes(self):\n        data = {b\"\\xb5 one\": b\"two\", b\"price\": b\"\\xa3 100\"}\n        r2 = self.request_class(\n            \"http://www.example.com\", formdata=data, encoding=\"latin1\"\n        )\n        assert r2.method == \"POST\"\n        assert r2.encoding == \"latin1\"\n        self.assertQueryEqual(r2.body, b\"price=%A3+100&%B5+one=two\")\n        assert r2.headers[b\"Content-Type\"] == b\"application/x-www-form-urlencoded\"\n\n    def test_custom_encoding_textual_data(self):\n        data = {\"price\": \"£ 100\"}\n        r3 = self.request_class(\n            \"http://www.example.com\", formdata=data, encoding=\"latin1\"\n        )\n        assert r3.encoding == \"latin1\"\n        assert r3.body == b\"price=%A3+100\"\n\n    def test_multi_key_values(self):\n        # using multiples values for a single key\n        data = {\"price\": \"\\xa3 100\", \"colours\": [\"red\", \"blue\", \"green\"]}\n        r3 = self.request_class(\"http://www.example.com\", formdata=data)\n        self.assertQueryEqual(\n            r3.body, b\"colours=red&colours=blue&colours=green&price=%C2%A3+100\"\n        )\n\n    def test_from_response_post(self):\n        response = _buildresponse(\n            b\"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        assert req.method == \"POST\"\n        assert req.headers[b\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req)\n        assert set(fs[b\"test\"]) == {b\"val1\", b\"val2\"}\n        assert set(fs[b\"one\"]) == {b\"two\", b\"three\"}\n        assert fs[b\"test2\"] == [b\"xxx\"]\n        assert fs[b\"six\"] == [b\"seven\"]\n", "n_tokens": 1135, "byte_len": 4040, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 427, "end_line": 518}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 6, "symbols": ["test_from_response_post_nonascii_bytes_utf8", "test_from_response_post_nonascii_bytes_latin1", "test_from_response_post_nonascii_unicode", "test_from_response_duplicate_form_key", "test_from_response_override_duplicate_form_key", "test_from_response_extra_headers", "encoding", "post", "method", "gzip", "application", "input", "latin", "latin1", "name", "from", "response", "test", "hostname", "action", "hidden", "val", "val2", "headers", "example", "deflate", "value", "urlparse", "cached", "three", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_post_nonascii_bytes_utf8(self):\n        response = _buildresponse(\n            b\"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val1\">\n            <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx \\xc2\\xb5\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        assert req.method == \"POST\"\n        assert req.headers[b\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req, to_unicode=True)\n        assert set(fs[\"test £\"]) == {\"val1\", \"val2\"}\n        assert set(fs[\"one\"]) == {\"two\", \"three\"}\n        assert fs[\"test2\"] == [\"xxx µ\"]\n        assert fs[\"six\"] == [\"seven\"]\n\n    def test_from_response_post_nonascii_bytes_latin1(self):\n        response = _buildresponse(\n            b\"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test \\xa3\" value=\"val1\">\n            <input type=\"hidden\" name=\"test \\xa3\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx \\xb5\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n            encoding=\"latin1\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        assert req.method == \"POST\"\n        assert req.headers[b\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req, to_unicode=True, encoding=\"latin1\")\n        assert set(fs[\"test £\"]) == {\"val1\", \"val2\"}\n        assert set(fs[\"one\"]) == {\"two\", \"three\"}\n        assert fs[\"test2\"] == [\"xxx µ\"]\n        assert fs[\"six\"] == [\"seven\"]\n\n    def test_from_response_post_nonascii_unicode(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test £\" value=\"val1\">\n            <input type=\"hidden\" name=\"test £\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx µ\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        assert req.method == \"POST\"\n        assert req.headers[b\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req, to_unicode=True)\n        assert set(fs[\"test £\"]) == {\"val1\", \"val2\"}\n        assert set(fs[\"one\"]) == {\"two\", \"three\"}\n        assert fs[\"test2\"] == [\"xxx µ\"]\n        assert fs[\"six\"] == [\"seven\"]\n\n    def test_from_response_duplicate_form_key(self):\n        response = _buildresponse(\"<form></form>\", url=\"http://www.example.com\")\n        req = self.request_class.from_response(\n            response=response,\n            method=\"GET\",\n            formdata=((\"foo\", \"bar\"), (\"foo\", \"baz\")),\n        )\n        assert urlparse_cached(req).hostname == \"www.example.com\"\n        assert urlparse_cached(req).query == \"foo=bar&foo=baz\"\n\n    def test_from_response_override_duplicate_form_key(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formdata=((\"two\", \"2\"), (\"two\", \"4\"))\n        )\n        fs = _qs(req)\n        assert fs[b\"one\"] == [b\"1\"]\n        assert fs[b\"two\"] == [b\"2\", b\"4\"]\n\n    def test_from_response_extra_headers(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response=response,\n            formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"},\n            headers={\"Accept-Encoding\": \"gzip,deflate\"},\n        )\n        assert req.method == \"POST\"\n        assert req.headers[\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.headers[\"Accept-Encoding\"] == b\"gzip,deflate\"\n", "n_tokens": 1152, "byte_len": 4657, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 519, "end_line": 626}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 7, "symbols": ["test_from_response_get", "test_from_response_override_params", "test_from_response_drop_params", "test_from_response_override_method", "test_from_response_override_url", "test_from_response_case_insensitive", "test_from_response_submit_first_clickable", "get", "clickable", "post", "method", "image", "test", "from", "xpath", "input", "clickable1", "absolute", "name", "response", "hostname", "form", "request", "action", "hidden", "path", "clickable2", "val", "val2", "example", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_get(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        r1 = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n        assert r1.method == \"GET\"\n        assert urlparse_cached(r1).hostname == \"www.example.com\"\n        assert urlparse_cached(r1).path == \"/this/get.php\"\n        fs = _qs(r1)\n        assert set(fs[b\"test\"]) == {b\"val1\", b\"val2\"}\n        assert set(fs[b\"one\"]) == {b\"two\", b\"three\"}\n        assert fs[b\"test2\"] == [b\"xxx\"]\n        assert fs[b\"six\"] == [b\"seven\"]\n\n    def test_from_response_override_params(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"two\": \"2\"})\n        fs = _qs(req)\n        assert fs[b\"one\"] == [b\"1\"]\n        assert fs[b\"two\"] == [b\"2\"]\n\n    def test_from_response_drop_params(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"two\": None})\n        fs = _qs(req)\n        assert fs[b\"one\"] == [b\"1\"]\n        assert b\"two\" not in fs\n\n    def test_from_response_override_method(self):\n        response = _buildresponse(\n            \"\"\"<html><body>\n            <form action=\"/app\"></form>\n            </body></html>\"\"\"\n        )\n        request = FormRequest.from_response(response)\n        assert request.method == \"GET\"\n        request = FormRequest.from_response(response, method=\"POST\")\n        assert request.method == \"POST\"\n\n    def test_from_response_override_url(self):\n        response = _buildresponse(\n            \"\"\"<html><body>\n            <form action=\"/app\"></form>\n            </body></html>\"\"\"\n        )\n        request = FormRequest.from_response(response)\n        assert request.url == \"http://example.com/app\"\n        request = FormRequest.from_response(response, url=\"http://foo.bar/absolute\")\n        assert request.url == \"http://foo.bar/absolute\"\n        request = FormRequest.from_response(response, url=\"/relative\")\n        assert request.url == \"http://example.com/relative\"\n\n    def test_from_response_case_insensitive(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"SuBmIt\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"iMaGe\" name=\"i1\" src=\"http://my.image.org/1.jpg\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response)\n        fs = _qs(req)\n        assert fs[b\"clickable1\"] == [b\"clicked1\"]\n        assert b\"i1\" not in fs, fs  # xpath in _get_inputs()\n        assert b\"clickable2\" not in fs, fs  # xpath in _get_clickable()\n\n    def test_from_response_submit_first_clickable(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"two\": \"2\"})\n        fs = _qs(req)\n        assert fs[b\"clickable1\"] == [b\"clicked1\"]\n        assert b\"clickable2\" not in fs, fs\n        assert fs[b\"one\"] == [b\"1\"]\n        assert fs[b\"two\"] == [b\"2\"]\n", "n_tokens": 1027, "byte_len": 4102, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 627, "end_line": 725}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 8, "symbols": ["test_from_response_submit_not_first_clickable", "test_from_response_dont_submit_image_as_input", "test_from_response_dont_submit_reset_as_input", "test_from_response_clickdata_does_not_ignore_image", "test_from_response_multiple_clickdata", "test_from_response_unicode_clickdata", "test_from_response_unicode_clickdata_latin1", "encoding", "test", "from", "method", "price", "resetme", "text", "image", "u00a5", "input", "clickable", "clickable1", "poundsign", "latin", "latin1", "name", "response", "yensign", "action", "hidden", "clickable2", "u20ac", "value", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_submit_not_first_clickable(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"two\": \"2\"}, clickdata={\"name\": \"clickable2\"}\n        )\n        fs = _qs(req)\n        assert fs[b\"clickable2\"] == [b\"clicked2\"]\n        assert b\"clickable1\" not in fs, fs\n        assert fs[b\"one\"] == [b\"1\"]\n        assert fs[b\"two\"] == [b\"2\"]\n\n    def test_from_response_dont_submit_image_as_input(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"hidden\" name=\"i1\" value=\"i1v\">\n            <input type=\"image\" name=\"i2\" src=\"http://my.image.org/1.jpg\">\n            <input type=\"submit\" name=\"i3\" value=\"i3v\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, dont_click=True)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"i1v\"]}\n\n    def test_from_response_dont_submit_reset_as_input(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"hidden\" name=\"i1\" value=\"i1v\">\n            <input type=\"text\" name=\"i2\" value=\"i2v\">\n            <input type=\"reset\" name=\"resetme\">\n            <input type=\"submit\" name=\"i3\" value=\"i3v\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, dont_click=True)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"i1v\"], b\"i2\": [b\"i2v\"]}\n\n    def test_from_response_clickdata_does_not_ignore_image(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"text\" name=\"i1\" value=\"i1v\">\n            <input id=\"image\" name=\"i2\" type=\"image\" value=\"i2v\" alt=\"Login\" src=\"http://my.image.org/1.jpg\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"i1v\"], b\"i2\": [b\"i2v\"]}\n\n    def test_from_response_multiple_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked1\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked2\">\n            <input type=\"hidden\" name=\"one\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"two\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, clickdata={\"name\": \"clickable\", \"value\": \"clicked2\"}\n        )\n        fs = _qs(req)\n        assert fs[b\"clickable\"] == [b\"clicked2\"]\n        assert fs[b\"one\"] == [b\"clicked1\"]\n        assert fs[b\"two\"] == [b\"clicked2\"]\n\n    def test_from_response_unicode_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"price in \\u00a3\" value=\"\\u00a3 1000\">\n            <input type=\"submit\" name=\"price in \\u20ac\" value=\"\\u20ac 2000\">\n            <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n            <input type=\"hidden\" name=\"eurosign\" value=\"\\u20ac\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, clickdata={\"name\": \"price in \\u00a3\"}\n        )\n        fs = _qs(req, to_unicode=True)\n        assert fs[\"price in \\u00a3\"]\n\n    def test_from_response_unicode_clickdata_latin1(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"price in \\u00a3\" value=\"\\u00a3 1000\">\n            <input type=\"submit\" name=\"price in \\u00a5\" value=\"\\u00a5 2000\">\n            <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n            <input type=\"hidden\" name=\"yensign\" value=\"\\u00a5\">\n            </form>\"\"\",\n            encoding=\"latin1\",\n        )\n        req = self.request_class.from_response(\n            response, clickdata={\"name\": \"price in \\u00a5\"}\n        )\n        fs = _qs(req, to_unicode=True, encoding=\"latin1\")\n        assert fs[\"price in \\u00a5\"]\n", "n_tokens": 1171, "byte_len": 4285, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 726, "end_line": 827}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#9", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 9, "symbols": ["test_from_response_multiple_forms_clickdata", "test_from_response_override_clickable", "test_from_response_dont_click", "test_from_response_ambiguous_clickdata", "test_from_response_non_matching_clickdata", "test_from_response_nr_index_clickdata", "test_from_response_invalid_nr_index_clickdata", "test_from_response_errors_noform", "test_from_response_invalid_html5", "test_from_response_errors_formnumber", "formname", "method", "clickme", "form", "form1", "text", "test", "from", "doctype", "input", "clickable", "clickable1", "field", "field1", "criteria", "value", "error", "name", "test2", "response", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_multiple_forms_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form name=\"form1\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"field1\" value=\"value1\">\n            </form>\n            <form name=\"form2\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked2\">\n            <input type=\"hidden\" name=\"field2\" value=\"value2\">\n            </form>\n            \"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formname=\"form2\", clickdata={\"name\": \"clickable\"}\n        )\n        fs = _qs(req)\n        assert fs[b\"clickable\"] == [b\"clicked2\"]\n        assert fs[b\"field2\"] == [b\"value2\"]\n        assert b\"field1\" not in fs, fs\n\n    def test_from_response_override_clickable(self):\n        response = _buildresponse(\n            \"\"\"<form><input type=\"submit\" name=\"clickme\" value=\"one\"> </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"clickme\": \"two\"}, clickdata={\"name\": \"clickme\"}\n        )\n        fs = _qs(req)\n        assert fs[b\"clickme\"] == [b\"two\"]\n\n    def test_from_response_dont_click(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, dont_click=True)\n        fs = _qs(r1)\n        assert b\"clickable1\" not in fs, fs\n        assert b\"clickable2\" not in fs, fs\n\n    def test_from_response_ambiguous_clickdata(self):\n        response = _buildresponse(\n            \"\"\"\n            <form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        with pytest.raises(\n            ValueError,\n            match=\"Multiple elements found .* matching the criteria in clickdata\",\n        ):\n            self.request_class.from_response(response, clickdata={\"type\": \"submit\"})\n\n    def test_from_response_non_matching_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n            </form>\"\"\"\n        )\n        with pytest.raises(\n            ValueError, match=\"No clickable element matching clickdata:\"\n        ):\n            self.request_class.from_response(\n                response, clickdata={\"nonexistent\": \"notme\"}\n            )\n\n    def test_from_response_nr_index_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\n            \"\"\"\n        )\n        req = self.request_class.from_response(response, clickdata={\"nr\": 1})\n        fs = _qs(req)\n        assert b\"clickable2\" in fs\n        assert b\"clickable1\" not in fs\n\n    def test_from_response_invalid_nr_index_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n            </form>\n            \"\"\"\n        )\n        with pytest.raises(\n            ValueError, match=\"No clickable element matching clickdata:\"\n        ):\n            self.request_class.from_response(response, clickdata={\"nr\": 1})\n\n    def test_from_response_errors_noform(self):\n        response = _buildresponse(\"\"\"<html></html>\"\"\")\n        with pytest.raises(ValueError, match=\"No <form> element found in\"):\n            self.request_class.from_response(response)\n\n    def test_from_response_invalid_html5(self):\n        response = _buildresponse(\n            \"\"\"<!DOCTYPE html><body></html><form>\"\"\"\n            \"\"\"<input type=\"text\" name=\"foo\" value=\"xxx\">\"\"\"\n            \"\"\"</form></body></html>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"bar\": \"buz\"})\n        fs = _qs(req)\n        assert fs == {b\"foo\": [b\"xxx\"], b\"bar\": [b\"buz\"]}\n\n    def test_from_response_errors_formnumber(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\"\n        )\n        with pytest.raises(IndexError):\n            self.request_class.from_response(response, formnumber=1)\n", "n_tokens": 1135, "byte_len": 4858, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 828, "end_line": 951}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#10", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 10, "symbols": ["test_from_response_noformname", "test_from_response_formname_exists", "test_from_response_formname_nonexistent", "test_from_response_formname_errors_formnumber", "test_from_response_formid_exists", "test_from_response_formname_nonexistent_fallback_formid", "test_from_response_formid_nonexistent", "formname", "post", "method", "form", "form1", "application", "test", "from", "input", "name", "response", "with", "action", "hidden", "headers", "value", "four", "three", "formnumber", "buildresponse", "pytest", "assert", "raises", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_noformname(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formdata={\"two\": \"3\"})\n        assert r1.method == \"POST\"\n        assert r1.headers[\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        fs = _qs(r1)\n        assert fs == {b\"one\": [b\"1\"], b\"two\": [b\"3\"]}\n\n    def test_from_response_formname_exists(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formname=\"form2\")\n        assert r1.method == \"POST\"\n        fs = _qs(r1)\n        assert fs == {b\"four\": [b\"4\"], b\"three\": [b\"3\"]}\n\n    def test_from_response_formname_nonexistent(self):\n        response = _buildresponse(\n            \"\"\"<form name=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formname=\"form3\")\n        assert r1.method == \"POST\"\n        fs = _qs(r1)\n        assert fs == {b\"one\": [b\"1\"]}\n\n    def test_from_response_formname_errors_formnumber(self):\n        response = _buildresponse(\n            \"\"\"<form name=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        with pytest.raises(IndexError):\n            self.request_class.from_response(response, formname=\"form3\", formnumber=2)\n\n    def test_from_response_formid_exists(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form id=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formid=\"form2\")\n        assert r1.method == \"POST\"\n        fs = _qs(r1)\n        assert fs == {b\"four\": [b\"4\"], b\"three\": [b\"3\"]}\n\n    def test_from_response_formname_nonexistent_fallback_formid(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(\n            response, formname=\"form3\", formid=\"form2\"\n        )\n        assert r1.method == \"POST\"\n        fs = _qs(r1)\n        assert fs == {b\"four\": [b\"4\"], b\"three\": [b\"3\"]}\n\n    def test_from_response_formid_nonexistent(self):\n        response = _buildresponse(\n            \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form id=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formid=\"form3\")\n        assert r1.method == \"POST\"\n        fs = _qs(r1)\n        assert fs == {b\"one\": [b\"1\"]}\n", "n_tokens": 1099, "byte_len": 4229, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 952, "end_line": 1054}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#11", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 11, "symbols": ["test_from_response_formid_errors_formnumber", "test_from_response_select", "test_from_response_radio", "test_from_response_checkbox", "test_from_response_input_text", "test_from_response_input_hidden", "post", "checked", "method", "form", "form1", "text", "i4v2", "select", "input", "i4v1", "test", "from", "i5v1", "radio", "name", "response", "with", "action", "hidden", "option", "value", "multiple", "formnumber", "buildresponse", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_formid_errors_formnumber(self):\n        response = _buildresponse(\n            \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        with pytest.raises(IndexError):\n            self.request_class.from_response(response, formid=\"form3\", formnumber=2)\n\n    def test_from_response_select(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <select name=\"i1\">\n                <option value=\"i1v1\">option 1</option>\n                <option value=\"i1v2\" selected>option 2</option>\n            </select>\n            <select name=\"i2\">\n                <option value=\"i2v1\">option 1</option>\n                <option value=\"i2v2\">option 2</option>\n            </select>\n            <select>\n                <option value=\"i3v1\">option 1</option>\n                <option value=\"i3v2\">option 2</option>\n            </select>\n            <select name=\"i4\" multiple>\n                <option value=\"i4v1\">option 1</option>\n                <option value=\"i4v2\" selected>option 2</option>\n                <option value=\"i4v3\" selected>option 3</option>\n            </select>\n            <select name=\"i5\" multiple>\n                <option value=\"i5v1\">option 1</option>\n                <option value=\"i5v2\">option 2</option>\n            </select>\n            <select name=\"i6\"></select>\n            <select name=\"i7\"/>\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req, to_unicode=True)\n        assert fs == {\"i1\": [\"i1v2\"], \"i2\": [\"i2v1\"], \"i4\": [\"i4v2\", \"i4v3\"]}\n\n    def test_from_response_radio(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"radio\" name=\"i1\" value=\"i1v1\">\n            <input type=\"radio\" name=\"i1\" value=\"iv2\" checked>\n            <input type=\"radio\" name=\"i2\" checked>\n            <input type=\"radio\" name=\"i2\">\n            <input type=\"radio\" name=\"i3\" value=\"i3v1\">\n            <input type=\"radio\" name=\"i3\">\n            <input type=\"radio\" value=\"i4v1\">\n            <input type=\"radio\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"iv2\"], b\"i2\": [b\"on\"]}\n\n    def test_from_response_checkbox(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"checkbox\" name=\"i1\" value=\"i1v1\">\n            <input type=\"checkbox\" name=\"i1\" value=\"iv2\" checked>\n            <input type=\"checkbox\" name=\"i2\" checked>\n            <input type=\"checkbox\" name=\"i2\">\n            <input type=\"checkbox\" name=\"i3\" value=\"i3v1\">\n            <input type=\"checkbox\" name=\"i3\">\n            <input type=\"checkbox\" value=\"i4v1\">\n            <input type=\"checkbox\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"iv2\"], b\"i2\": [b\"on\"]}\n\n    def test_from_response_input_text(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"text\" name=\"i1\" value=\"i1v1\">\n            <input type=\"text\" name=\"i2\">\n            <input type=\"text\" value=\"i3v1\">\n            <input type=\"text\">\n            <input name=\"i4\" value=\"i4v1\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"i1v1\"], b\"i2\": [b\"\"], b\"i4\": [b\"i4v1\"]}\n\n    def test_from_response_input_hidden(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"hidden\" name=\"i1\" value=\"i1v1\">\n            <input type=\"hidden\" name=\"i2\">\n            <input type=\"hidden\" value=\"i3v1\">\n            <input type=\"hidden\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"i1v1\"], b\"i2\": [b\"\"]}\n", "n_tokens": 1126, "byte_len": 4051, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 1055, "end_line": 1159}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#12", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 12, "symbols": ["test_from_response_input_textarea", "test_from_response_descendants", "test_from_response_xpath", "test_from_response_unicode_xpath", "test_from_response_button_submit", "test_from_response_button_notype", "checked", "post", "method", "button", "submit", "text", "application", "i4v2", "select", "xpath", "input", "formxpath", "test", "test1", "value", "error", "radio", "submit1", "name", "from", "test2", "response", "with", "action", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_input_textarea(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <textarea name=\"i1\">i1v</textarea>\n            <textarea name=\"i2\"></textarea>\n            <textarea name=\"i3\"/>\n            <textarea>i4v</textarea>\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        assert fs == {b\"i1\": [b\"i1v\"], b\"i2\": [b\"\"], b\"i3\": [b\"\"]}\n\n    def test_from_response_descendants(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <div>\n              <fieldset>\n                <input type=\"text\" name=\"i1\">\n                <select name=\"i2\">\n                    <option value=\"v1\" selected>\n                </select>\n              </fieldset>\n              <input type=\"radio\" name=\"i3\" value=\"i3v2\" checked>\n              <input type=\"checkbox\" name=\"i4\" value=\"i4v2\" checked>\n              <textarea name=\"i5\"></textarea>\n              <input type=\"hidden\" name=\"h1\" value=\"h1v\">\n              </div>\n            <input type=\"hidden\" name=\"h2\" value=\"h2v\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        assert set(fs) == {b\"h2\", b\"i2\", b\"i1\", b\"i3\", b\"h1\", b\"i5\", b\"i4\"}\n\n    def test_from_response_xpath(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form action=\"post2.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(\n            response, formxpath=\"//form[@action='post.php']\"\n        )\n        fs = _qs(r1)\n        assert fs[b\"one\"] == [b\"1\"]\n\n        r1 = self.request_class.from_response(\n            response, formxpath=\"//form/input[@name='four']\"\n        )\n        fs = _qs(r1)\n        assert fs[b\"three\"] == [b\"3\"]\n\n        with pytest.raises(ValueError, match=\"No <form> element found with\"):\n            self.request_class.from_response(\n                response, formxpath=\"//form/input[@name='abc']\"\n            )\n\n    def test_from_response_unicode_xpath(self):\n        response = _buildresponse(b'<form name=\"\\xd1\\x8a\"></form>')\n        r = self.request_class.from_response(\n            response, formxpath=\"//form[@name='\\u044a']\"\n        )\n        fs = _qs(r)\n        assert not fs\n\n        xpath = \"//form[@name='\\u03b1']\"\n        with pytest.raises(ValueError, match=re.escape(xpath)):\n            self.request_class.from_response(response, formxpath=xpath)\n\n    def test_from_response_button_submit(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <button type=\"submit\" name=\"button1\" value=\"submit1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        assert req.method == \"POST\"\n        assert req.headers[\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req)\n        assert fs[b\"test1\"] == [b\"val1\"]\n        assert fs[b\"test2\"] == [b\"val2\"]\n        assert fs[b\"button1\"] == [b\"submit1\"]\n\n    def test_from_response_button_notype(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <button name=\"button1\" value=\"submit1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        assert req.method == \"POST\"\n        assert req.headers[\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req)\n        assert fs[b\"test1\"] == [b\"val1\"]\n        assert fs[b\"test2\"] == [b\"val2\"]\n        assert fs[b\"button1\"] == [b\"submit1\"]\n", "n_tokens": 1106, "byte_len": 4352, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 1160, "end_line": 1270}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#13", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 13, "symbols": ["test_from_response_submit_novalue", "test_from_response_button_novalue", "test_html_base_form_action", "test_spaces_in_action", "test_from_response_css", "test_from_response_valid_form_methods", "test_form_response_with_invalid_formdata_type_error", "test_form_response_with_custom_invalid_formdata_value_error", "button", "method", "append", "name", "val", "val2", "four", "three", "pytest", "href", "button1", "inducing", "type", "html", "post", "post2", "http", "test", "form", "found", "match", "submit", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_from_response_submit_novalue(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <input type=\"submit\" name=\"button1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        assert req.method == \"POST\"\n        assert req.headers[\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req)\n        assert fs[b\"test1\"] == [b\"val1\"]\n        assert fs[b\"test2\"] == [b\"val2\"]\n        assert fs[b\"button1\"] == [b\"\"]\n\n    def test_from_response_button_novalue(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <button type=\"submit\" name=\"button1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        assert req.method == \"POST\"\n        assert req.headers[\"Content-type\"] == b\"application/x-www-form-urlencoded\"\n        assert req.url == \"http://www.example.com/this/post.php\"\n        fs = _qs(req)\n        assert fs[b\"test1\"] == [b\"val1\"]\n        assert fs[b\"test2\"] == [b\"val2\"]\n        assert fs[b\"button1\"] == [b\"\"]\n\n    def test_html_base_form_action(self):\n        response = _buildresponse(\n            \"\"\"\n            <html>\n                <head>\n                    <base href=\" http://b.com/\">\n                </head>\n                <body>\n                    <form action=\"test_form\">\n                    </form>\n                </body>\n            </html>\n            \"\"\",\n            url=\"http://a.com/\",\n        )\n        req = self.request_class.from_response(response)\n        assert req.url == \"http://b.com/test_form\"\n\n    def test_spaces_in_action(self):\n        resp = _buildresponse('<body><form action=\" path\\n\"></form></body>')\n        req = self.request_class.from_response(resp)\n        assert req.url == \"http://example.com/path\"\n\n    def test_from_response_css(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form action=\"post2.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(\n            response, formcss=\"form[action='post.php']\"\n        )\n        fs = _qs(r1)\n        assert fs[b\"one\"] == [b\"1\"]\n\n        r1 = self.request_class.from_response(response, formcss=\"input[name='four']\")\n        fs = _qs(r1)\n        assert fs[b\"three\"] == [b\"3\"]\n\n        with pytest.raises(ValueError, match=\"No <form> element found with\"):\n            self.request_class.from_response(response, formcss=\"input[name='abc']\")\n\n    def test_from_response_valid_form_methods(self):\n        form_methods = [\n            [method, method] for method in self.request_class.valid_form_methods\n        ]\n        form_methods.append([\"UNKNOWN\", \"GET\"])\n\n        for method, expected in form_methods:\n            response = _buildresponse(\n                f'<form action=\"post.php\" method=\"{method}\">'\n                '<input type=\"hidden\" name=\"one\" value=\"1\">'\n                \"</form>\"\n            )\n            r = self.request_class.from_response(response)\n            assert r.method == expected\n\n    def test_form_response_with_invalid_formdata_type_error(self):\n        \"\"\"Test that a ValueError is raised for non-iterable and non-dict formdata input\"\"\"\n        response = _buildresponse(\n            \"\"\"<html><body>\n            <form action=\"/submit\" method=\"post\">\n                <input type=\"text\" name=\"test\" value=\"value\">\n            </form>\n            </body></html>\"\"\"\n        )\n        with pytest.raises(\n            ValueError, match=\"formdata should be a dict or iterable of tuples\"\n        ):\n            FormRequest.from_response(response, formdata=123)\n\n    def test_form_response_with_custom_invalid_formdata_value_error(self):\n        \"\"\"Test that a ValueError is raised for fault-inducing iterable formdata input\"\"\"\n        response = _buildresponse(\n            \"\"\"<html><body>\n                <form action=\"/submit\" method=\"post\">\n                    <input type=\"text\" name=\"test\" value=\"value\">\n                </form>\n            </body></html>\"\"\"\n        )\n\n        with pytest.raises(\n            ValueError, match=\"formdata should be a dict or iterable of tuples\"\n        ):\n            FormRequest.from_response(response, formdata=(\"a\",))\n", "n_tokens": 1138, "byte_len": 5009, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 1271, "end_line": 1397}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#14", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 14, "symbols": ["test_get_form_with_xpath_no_form_parent", "_buildresponse", "_qs", "_test_request", "test_xmlrpc_dumps", "test_latin1", "test_data", "test_data_method", "test_body_data", "test_empty_body_data", "TestXmlRpcRequest", "TestJsonRequest", "encoding", "method", "selects", "dumps", "formxpath", "latin", "latin1", "xmlrpc", "name", "dont", "filter", "passed", "passing", "rpc", "rpc2", "username", "default", "pytest", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "class_or_type", "text": "    def test_get_form_with_xpath_no_form_parent(self):\n        \"\"\"Test that _get_from raised a ValueError when an XPath selects an element\n        not nested within a <form> and no <form> parent is found\"\"\"\n        response = _buildresponse(\n            \"\"\"<html><body>\n                <div id=\"outside-form\">\n                    <p>This paragraph is not inside a form.</p>\n                </div>\n                <form action=\"/submit\" method=\"post\">\n                    <input type=\"text\" name=\"inside-form\" value=\"\">\n                </form>\n            </body></html>\"\"\"\n        )\n\n        with pytest.raises(ValueError, match=\"No <form> element found with\"):\n            FormRequest.from_response(response, formxpath='//div[@id=\"outside-form\"]/p')\n\n\ndef _buildresponse(body, **kwargs):\n    kwargs.setdefault(\"body\", body)\n    kwargs.setdefault(\"url\", \"http://example.com\")\n    kwargs.setdefault(\"encoding\", \"utf-8\")\n    return HtmlResponse(**kwargs)\n\n\ndef _qs(req, encoding=\"utf-8\", to_unicode=False):\n    qs = req.body if req.method == \"POST\" else req.url.partition(\"?\")[2]\n    uqs = unquote_to_bytes(qs)\n    if to_unicode:\n        uqs = uqs.decode(encoding)\n    return parse_qs(uqs, True)\n\n\nclass TestXmlRpcRequest(TestRequest):\n    request_class = XmlRpcRequest\n    default_method = \"POST\"\n    default_headers = {b\"Content-Type\": [b\"text/xml\"]}\n\n    def _test_request(self, **kwargs):\n        r = self.request_class(\"http://scrapytest.org/rpc2\", **kwargs)\n        assert r.headers[b\"Content-Type\"] == b\"text/xml\"\n        assert r.body == to_bytes(\n            xmlrpc.client.dumps(**kwargs), encoding=kwargs.get(\"encoding\", \"utf-8\")\n        )\n        assert r.method == \"POST\"\n        assert r.encoding == kwargs.get(\"encoding\", \"utf-8\")\n        assert r.dont_filter\n\n    def test_xmlrpc_dumps(self):\n        self._test_request(params=(\"value\",))\n        self._test_request(params=(\"username\", \"password\"), methodname=\"login\")\n        self._test_request(params=(\"response\",), methodresponse=\"login\")\n        self._test_request(params=(\"pas£\",), encoding=\"utf-8\")\n        self._test_request(params=(None,), allow_none=1)\n        with pytest.raises(TypeError):\n            self._test_request()\n        with pytest.raises(TypeError):\n            self._test_request(params=(None,))\n\n    def test_latin1(self):\n        self._test_request(params=(\"pas£\",), encoding=\"latin1\")\n\n\nclass TestJsonRequest(TestRequest):\n    request_class = JsonRequest\n    default_method = \"GET\"\n    default_headers = {\n        b\"Content-Type\": [b\"application/json\"],\n        b\"Accept\": [b\"application/json, text/javascript, */*; q=0.01\"],\n    }\n\n    def test_data(self):\n        r1 = self.request_class(url=\"http://www.example.com/\")\n        assert r1.body == b\"\"\n\n        body = b\"body\"\n        r2 = self.request_class(url=\"http://www.example.com/\", body=body)\n        assert r2.body == body\n\n        data = {\n            \"name\": \"value\",\n        }\n        r3 = self.request_class(url=\"http://www.example.com/\", data=data)\n        assert r3.body == to_bytes(json.dumps(data))\n\n        # empty data\n        r4 = self.request_class(url=\"http://www.example.com/\", data=[])\n        assert r4.body == to_bytes(json.dumps([]))\n\n    def test_data_method(self):\n        # data is not passed\n        r1 = self.request_class(url=\"http://www.example.com/\")\n        assert r1.method == \"GET\"\n\n        body = b\"body\"\n        r2 = self.request_class(url=\"http://www.example.com/\", body=body)\n        assert r2.method == \"GET\"\n\n        data = {\n            \"name\": \"value\",\n        }\n        r3 = self.request_class(url=\"http://www.example.com/\", data=data)\n        assert r3.method == \"POST\"\n\n        # method passed explicitly\n        r4 = self.request_class(url=\"http://www.example.com/\", data=data, method=\"GET\")\n        assert r4.method == \"GET\"\n\n        r5 = self.request_class(url=\"http://www.example.com/\", data=[])\n        assert r5.method == \"POST\"\n\n    def test_body_data(self):\n        \"\"\"passing both body and data should result a warning\"\"\"\n        body = b\"body\"\n        data = {\n            \"name\": \"value\",\n        }\n        with warnings.catch_warnings(record=True) as _warnings:\n            r5 = self.request_class(url=\"http://www.example.com/\", body=body, data=data)\n            assert r5.body == body\n            assert r5.method == \"GET\"\n            assert len(_warnings) == 1\n            assert \"data will be ignored\" in str(_warnings[0].message)\n\n    def test_empty_body_data(self):\n        \"\"\"passing any body value and data should result a warning\"\"\"\n        data = {\n            \"name\": \"value\",\n        }\n        with warnings.catch_warnings(record=True) as _warnings:\n            r6 = self.request_class(url=\"http://www.example.com/\", body=b\"\", data=data)\n            assert r6.body == b\"\"\n            assert r6.method == \"GET\"\n            assert len(_warnings) == 1\n            assert \"data will be ignored\" in str(_warnings[0].message)\n", "n_tokens": 1163, "byte_len": 4932, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 1398, "end_line": 1533}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py#15", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_request.py", "rel_path": "tests/test_http_request.py", "module": "tests.test_http_request", "ext": "py", "chunk_number": 15, "symbols": ["test_body_none_data", "test_body_data_none", "test_dumps_sort_keys", "test_dumps_kwargs", "test_replace_data", "test_replace_sort_keys", "test_replace_dumps_kwargs", "test_replacement_both_body_and_data_warns", "post", "test", "dumps", "method", "ensure", "ascii", "will", "data", "data2", "patch", "allow", "nan", "replace", "name", "mock", "ignored", "sort", "keys", "with", "json", "warning", "data1", "test_init", "test_url_scheme", "test_url_no_scheme", "test_headers", "test_eq", "test_url", "test_url_quoting", "test_url_encoding", "test_url_encoding_other", "test_url_encoding_query", "test_url_encoding_query_latin1", "test_url_encoding_nonutf8_untouched", "test_body", "test_copy", "somecallback", "test_copy_inherited_classes", "test_replace", "test_method_always_str", "test_immutable_attributes", "test_callback_and_errback"], "ast_kind": "function_or_method", "text": "    def test_body_none_data(self):\n        data = {\n            \"name\": \"value\",\n        }\n        with warnings.catch_warnings(record=True) as _warnings:\n            r7 = self.request_class(url=\"http://www.example.com/\", body=None, data=data)\n            assert r7.body == to_bytes(json.dumps(data))\n            assert r7.method == \"POST\"\n            assert len(_warnings) == 0\n\n    def test_body_data_none(self):\n        with warnings.catch_warnings(record=True) as _warnings:\n            r8 = self.request_class(url=\"http://www.example.com/\", body=None, data=None)\n            assert r8.method == \"GET\"\n            assert len(_warnings) == 0\n\n    def test_dumps_sort_keys(self):\n        \"\"\"Test that sort_keys=True is passed to json.dumps by default\"\"\"\n        data = {\n            \"name\": \"value\",\n        }\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            self.request_class(url=\"http://www.example.com/\", data=data)\n            kwargs = mock_dumps.call_args[1]\n            assert kwargs[\"sort_keys\"] is True\n\n    def test_dumps_kwargs(self):\n        \"\"\"Test that dumps_kwargs are passed to json.dumps\"\"\"\n        data = {\n            \"name\": \"value\",\n        }\n        dumps_kwargs = {\n            \"ensure_ascii\": True,\n            \"allow_nan\": True,\n        }\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            self.request_class(\n                url=\"http://www.example.com/\", data=data, dumps_kwargs=dumps_kwargs\n            )\n            kwargs = mock_dumps.call_args[1]\n            assert kwargs[\"ensure_ascii\"] is True\n            assert kwargs[\"allow_nan\"] is True\n\n    def test_replace_data(self):\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        r1 = self.request_class(url=\"http://www.example.com/\", data=data1)\n        r2 = r1.replace(data=data2)\n        assert r2.body == to_bytes(json.dumps(data2))\n\n    def test_replace_sort_keys(self):\n        \"\"\"Test that replace provides sort_keys=True to json.dumps\"\"\"\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        r1 = self.request_class(url=\"http://www.example.com/\", data=data1)\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            r1.replace(data=data2)\n            kwargs = mock_dumps.call_args[1]\n            assert kwargs[\"sort_keys\"] is True\n\n    def test_replace_dumps_kwargs(self):\n        \"\"\"Test that dumps_kwargs are provided to json.dumps when replace is called\"\"\"\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        dumps_kwargs = {\n            \"ensure_ascii\": True,\n            \"allow_nan\": True,\n        }\n        r1 = self.request_class(\n            url=\"http://www.example.com/\", data=data1, dumps_kwargs=dumps_kwargs\n        )\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            r1.replace(data=data2)\n            kwargs = mock_dumps.call_args[1]\n            assert kwargs[\"ensure_ascii\"] is True\n            assert kwargs[\"allow_nan\"] is True\n\n    def test_replacement_both_body_and_data_warns(self):\n        \"\"\"Test that we get a warning if both body and data are passed\"\"\"\n        body1 = None\n        body2 = b\"body\"\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        r1 = self.request_class(url=\"http://www.example.com/\", data=data1, body=body1)\n\n        with warnings.catch_warnings(record=True) as _warnings:\n            r1.replace(data=data2, body=body2)\n            assert \"Both body and data passed. data will be ignored\" in str(\n                _warnings[0].message\n            )\n", "n_tokens": 880, "byte_len": 3801, "file_sha1": "ca6f7ba9e3a322a4aa7c573b7dfc5cea7c89bc63", "start_line": 1534, "end_line": 1640}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_genspider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_genspider.py", "rel_path": "tests/test_command_genspider.py", "module": "tests.test_command_genspider", "ext": "py", "chunk_number": 1, "symbols": ["test_arguments", "test_template", "test_list", "test_dump", "test_same_name_as_project", "test_same_filename_as_existing_spider", "TestGenspiderCommand", "encoding", "bool", "tplname", "stat", "spider", "name", "domain", "spiders", "future", "test", "arguments", "created", "https", "path", "pytest", "file", "contents", "truncate", "command", "none", "script", "modify", "time", "test_url", "test_template_start_urls", "test_generate_standalone_spider", "test_same_name_as_existing_file", "TestGenspiderStandaloneCommand", "were", "find", "group", "http", "generate", "expected", "seek", "argument", "annotations", "renamed", "class", "example", "csvfeed", "spname", "list"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\nimport pytest\n\nfrom tests.test_commands import TestCommandBase, TestProjectBase\n\n\nclass TestGenspiderCommand(TestCommandBase):\n    def test_arguments(self):\n        # only pass one argument. spider script shouldn't be created\n        assert self.call(\"genspider\", \"test_name\") == 2\n        assert not Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n        # pass two arguments <name> <domain>. spider script should be created\n        assert self.call(\"genspider\", \"test_name\", \"test.com\") == 0\n        assert Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n\n    @pytest.mark.parametrize(\n        \"tplname\",\n        [\n            \"basic\",\n            \"crawl\",\n            \"xmlfeed\",\n            \"csvfeed\",\n        ],\n    )\n    def test_template(self, tplname: str) -> None:\n        args = [f\"--template={tplname}\"] if tplname else []\n        spname = \"test_spider\"\n        spmodule = f\"{self.project_name}.spiders.{spname}\"\n        p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n        assert (\n            f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\"\n            in out\n        )\n        assert Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").exists()\n        modify_time_before = (\n            Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n        )\n        p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n        assert f\"Spider {spname!r} already exists in module\" in out\n        modify_time_after = (\n            Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n        )\n        assert modify_time_after == modify_time_before\n\n    def test_list(self):\n        assert self.call(\"genspider\", \"--list\") == 0\n\n    def test_dump(self):\n        assert self.call(\"genspider\", \"--dump=basic\") == 0\n        assert self.call(\"genspider\", \"-d\", \"basic\") == 0\n\n    def test_same_name_as_project(self):\n        assert self.call(\"genspider\", self.project_name) == 2\n        assert not Path(\n            self.proj_mod_path, \"spiders\", f\"{self.project_name}.py\"\n        ).exists()\n\n    @pytest.mark.parametrize(\"force\", [True, False])\n    def test_same_filename_as_existing_spider(self, force: bool) -> None:\n        file_name = \"example\"\n        file_path = Path(self.proj_mod_path, \"spiders\", f\"{file_name}.py\")\n        assert self.call(\"genspider\", file_name, \"example.com\") == 0\n        assert file_path.exists()\n\n        # change name of spider but not its file name\n        with file_path.open(\"r+\", encoding=\"utf-8\") as spider_file:\n            file_data = spider_file.read()\n            file_data = file_data.replace('name = \"example\"', 'name = \"renamed\"')\n            spider_file.seek(0)\n            spider_file.write(file_data)\n            spider_file.truncate()\n        modify_time_before = file_path.stat().st_mtime\n        file_contents_before = file_data\n\n        if force:\n            p, out, err = self.proc(\"genspider\", \"--force\", file_name, \"example.com\")\n            assert (\n                f\"Created spider {file_name!r} using template 'basic' in module\" in out\n            )\n            modify_time_after = file_path.stat().st_mtime\n            assert modify_time_after != modify_time_before\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            assert file_contents_after != file_contents_before\n        else:\n            p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n            assert f\"{file_path.resolve()} already exists\" in out\n            modify_time_after = file_path.stat().st_mtime\n            assert modify_time_after == modify_time_before\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            assert file_contents_after == file_contents_before\n\n    @pytest.mark.parametrize(\n        (\"url\", \"domain\"),\n        [\n            (\"test.com\", \"test.com\"),\n            (\"https://test.com\", \"test.com\"),\n        ],\n    )", "n_tokens": 972, "byte_len": 4043, "file_sha1": "438259739ebf278d619352d05f5feb73c5e86b94", "start_line": 1, "end_line": 102}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_genspider.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_genspider.py", "rel_path": "tests/test_command_genspider.py", "module": "tests.test_command_genspider", "ext": "py", "chunk_number": 2, "symbols": ["test_url", "test_template_start_urls", "test_generate_standalone_spider", "test_same_name_as_existing_file", "TestGenspiderStandaloneCommand", "encoding", "test", "url", "expected", "temp", "path", "false", "allowed", "domains", "bool", "stat", "contents", "spider", "were", "domain", "genspider", "mark", "class", "parametrize", "file", "call", "spiders", "xmlfeed", "created", "https", "test_arguments", "test_template", "test_list", "test_dump", "test_same_name_as_project", "test_same_filename_as_existing_spider", "TestGenspiderCommand", "tplname", "name", "future", "arguments", "find", "pytest", "truncate", "command", "none", "group", "script", "http", "generate"], "ast_kind": "class_or_type", "text": "    def test_url(self, url: str, domain: str) -> None:\n        assert self.call(\"genspider\", \"--force\", \"test_name\", url) == 0\n        m = self.find_in_file(\n            self.proj_mod_path / \"spiders\" / \"test_name.py\",\n            r\"allowed_domains\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n        )\n        assert m is not None\n        assert m.group(1) == domain\n        m = self.find_in_file(\n            self.proj_mod_path / \"spiders\" / \"test_name.py\",\n            r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n        )\n        assert m is not None\n        assert m.group(1) == f\"https://{domain}\"\n\n    @pytest.mark.parametrize(\n        (\"url\", \"expected\", \"template\"),\n        [\n            # basic\n            (\"https://test.com\", \"https://test.com\", \"basic\"),\n            (\"http://test.com\", \"http://test.com\", \"basic\"),\n            (\"http://test.com/other/path\", \"http://test.com/other/path\", \"basic\"),\n            (\"test.com/other/path\", \"https://test.com/other/path\", \"basic\"),\n            # crawl\n            (\"https://test.com\", \"https://test.com\", \"crawl\"),\n            (\"http://test.com\", \"http://test.com\", \"crawl\"),\n            (\"http://test.com/other/path\", \"http://test.com/other/path\", \"crawl\"),\n            (\"test.com/other/path\", \"https://test.com/other/path\", \"crawl\"),\n            (\"test.com\", \"https://test.com\", \"crawl\"),\n            # xmlfeed\n            (\"https://test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"),\n            (\"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"xmlfeed\"),\n            (\"test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"),\n            # csvfeed\n            (\"https://test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"),\n            (\"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"csvfeed\"),\n            (\"test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"),\n        ],\n    )\n    def test_template_start_urls(self, url: str, expected: str, template: str) -> None:\n        assert self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url) == 0\n        m = self.find_in_file(\n            self.proj_mod_path / \"spiders\" / \"test_name.py\",\n            r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n        )\n        assert m is not None\n        assert m.group(1) == expected\n\n\nclass TestGenspiderStandaloneCommand(TestProjectBase):\n    def test_generate_standalone_spider(self):\n        self.call(\"genspider\", \"example\", \"example.com\")\n        assert Path(self.temp_path, \"example.py\").exists()\n\n    @pytest.mark.parametrize(\"force\", [True, False])\n    def test_same_name_as_existing_file(self, force: bool) -> None:\n        file_name = \"example\"\n        file_path = Path(self.temp_path, file_name + \".py\")\n        p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n        assert f\"Created spider {file_name!r} using template 'basic' \" in out\n        assert file_path.exists()\n        modify_time_before = file_path.stat().st_mtime\n        file_contents_before = file_path.read_text(encoding=\"utf-8\")\n\n        if force:\n            # use different template to ensure contents were changed\n            p, out, err = self.proc(\n                \"genspider\", \"--force\", \"-t\", \"crawl\", file_name, \"example.com\"\n            )\n            assert f\"Created spider {file_name!r} using template 'crawl' \" in out\n            modify_time_after = file_path.stat().st_mtime\n            assert modify_time_after != modify_time_before\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            assert file_contents_after != file_contents_before\n        else:\n            p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n            assert (\n                f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\"\n                in out\n            )\n            modify_time_after = file_path.stat().st_mtime\n            assert modify_time_after == modify_time_before\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            assert file_contents_after == file_contents_before\n", "n_tokens": 993, "byte_len": 4041, "file_sha1": "438259739ebf278d619352d05f5feb73c5e86b94", "start_line": 103, "end_line": 187}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_datatypes.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_datatypes.py", "rel_path": "tests/test_utils_datatypes.py", "module": "tests.test_utils_datatypes", "ext": "py", "chunk_number": 1, "symbols": ["dict_class", "test_init_dict", "test_init_pair_sequence", "test_init_mapping", "__init__", "__getitem__", "__iter__", "__len__", "test_init_mutable_mapping", "__setitem__", "__delitem__", "test_caseless", "test_delete", "test_getdefault", "test_setdefault", "test_fromkeys", "test_contains", "test_pop", "test_normkey", "_normkey", "test_normvalue", "TestCaseInsensitiveDictBase", "MyMapping", "MyMutableMapping", "MyDict", "dict", "class", "normkey", "instance", "caseless", "_normvalue", "test_copy", "test_repr", "test_iter", "test_deprecation_message", "test_list", "test_range", "test_range_step", "test_string_seq", "test_stringset_seq", "test_set", "test_cache_with_limit", "test_cache_without_limit", "test_cache_non_weak_referenceable_objects", "TestCaseInsensitiveDict", "TestCaselessDict", "TestSequenceExclude", "TestLocalCache", "TestLocalWeakReferencedCache", "takes"], "ast_kind": "class_or_type", "text": "import copy\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterator, Mapping, MutableMapping\n\nimport pytest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request\nfrom scrapy.utils.datatypes import (\n    CaseInsensitiveDict,\n    CaselessDict,\n    LocalCache,\n    LocalWeakReferencedCache,\n    SequenceExclude,\n)\nfrom scrapy.utils.python import garbage_collect\n\n\nclass TestCaseInsensitiveDictBase(ABC):\n    @property\n    @abstractmethod\n    def dict_class(self) -> type[MutableMapping]:\n        raise NotImplementedError\n\n    def test_init_dict(self):\n        seq = {\"red\": 1, \"black\": 3}\n        d = self.dict_class(seq)\n        assert d[\"red\"] == 1\n        assert d[\"black\"] == 3\n\n    def test_init_pair_sequence(self):\n        seq = ((\"red\", 1), (\"black\", 3))\n        d = self.dict_class(seq)\n        assert d[\"red\"] == 1\n        assert d[\"black\"] == 3\n\n    def test_init_mapping(self):\n        class MyMapping(Mapping):\n            def __init__(self, **kwargs):\n                self._d = kwargs\n\n            def __getitem__(self, key):\n                return self._d[key]\n\n            def __iter__(self):\n                return iter(self._d)\n\n            def __len__(self):\n                return len(self._d)\n\n        seq = MyMapping(red=1, black=3)\n        d = self.dict_class(seq)\n        assert d[\"red\"] == 1\n        assert d[\"black\"] == 3\n\n    def test_init_mutable_mapping(self):\n        class MyMutableMapping(MutableMapping):\n            def __init__(self, **kwargs):\n                self._d = kwargs\n\n            def __getitem__(self, key):\n                return self._d[key]\n\n            def __setitem__(self, key, value):\n                self._d[key] = value\n\n            def __delitem__(self, key):\n                del self._d[key]\n\n            def __iter__(self):\n                return iter(self._d)\n\n            def __len__(self):\n                return len(self._d)\n\n        seq = MyMutableMapping(red=1, black=3)\n        d = self.dict_class(seq)\n        assert d[\"red\"] == 1\n        assert d[\"black\"] == 3\n\n    def test_caseless(self):\n        d = self.dict_class()\n        d[\"key_Lower\"] = 1\n        assert d[\"KEy_loWer\"] == 1\n        assert d.get(\"KEy_loWer\") == 1\n\n        d[\"KEY_LOWER\"] = 3\n        assert d[\"key_Lower\"] == 3\n        assert d.get(\"key_Lower\") == 3\n\n    def test_delete(self):\n        d = self.dict_class({\"key_lower\": 1})\n        del d[\"key_LOWER\"]\n        with pytest.raises(KeyError):\n            d[\"key_LOWER\"]\n        with pytest.raises(KeyError):\n            d[\"key_lower\"]\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_getdefault(self):\n        d = CaselessDict()\n        assert d.get(\"c\", 5) == 5\n        d[\"c\"] = 10\n        assert d.get(\"c\", 5) == 10\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_setdefault(self):\n        d = CaselessDict({\"a\": 1, \"b\": 2})\n\n        r = d.setdefault(\"A\", 5)\n        assert r == 1\n        assert d[\"A\"] == 1\n\n        r = d.setdefault(\"c\", 5)\n        assert r == 5\n        assert d[\"C\"] == 5\n\n    def test_fromkeys(self):\n        keys = (\"a\", \"b\")\n\n        d = self.dict_class.fromkeys(keys)\n        assert d[\"A\"] is None\n        assert d[\"B\"] is None\n\n        d = self.dict_class.fromkeys(keys, 1)\n        assert d[\"A\"] == 1\n        assert d[\"B\"] == 1\n\n        instance = self.dict_class()\n        d = instance.fromkeys(keys)\n        assert d[\"A\"] is None\n        assert d[\"B\"] is None\n\n        d = instance.fromkeys(keys, 1)\n        assert d[\"A\"] == 1\n        assert d[\"B\"] == 1\n\n    def test_contains(self):\n        d = self.dict_class()\n        d[\"a\"] = 1\n        assert \"A\" in d\n\n    def test_pop(self):\n        d = self.dict_class()\n        d[\"a\"] = 1\n        assert d.pop(\"A\") == 1\n        with pytest.raises(KeyError):\n            d.pop(\"A\")\n\n    def test_normkey(self):\n        class MyDict(self.dict_class):\n            def _normkey(self, key):\n                return key.title()\n\n            normkey = _normkey  # deprecated CaselessDict class\n\n        d = MyDict()\n        d[\"key-one\"] = 2\n        assert list(d.keys()) == [\"Key-One\"]\n\n    def test_normvalue(self):\n        class MyDict(self.dict_class):", "n_tokens": 1107, "byte_len": 4272, "file_sha1": "384b05bbda9e928050efda6fb1a2b895c62e8165", "start_line": 1, "end_line": 163}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_datatypes.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_datatypes.py", "rel_path": "tests/test_utils_datatypes.py", "module": "tests.test_utils_datatypes", "ext": "py", "chunk_number": 2, "symbols": ["_normvalue", "test_copy", "test_repr", "test_iter", "test_deprecation_message", "test_list", "test_range", "test_range_step", "test_string_seq", "test_stringset_seq", "test_set", "test_cache_with_limit", "TestCaseInsensitiveDict", "TestCaselessDict", "TestSequenceExclude", "TestLocalCache", "dict", "class", "test", "caseless", "inclusion", "deprecated", "pytest", "isinstance", "case", "insensitive", "header", "header1", "heade", "none", "dict_class", "test_init_dict", "test_init_pair_sequence", "test_init_mapping", "__init__", "__getitem__", "__iter__", "__len__", "test_init_mutable_mapping", "__setitem__", "__delitem__", "test_caseless", "test_delete", "test_getdefault", "test_setdefault", "test_fromkeys", "test_contains", "test_pop", "test_normkey", "_normkey"], "ast_kind": "class_or_type", "text": "            def _normvalue(self, value):\n                if value is not None:\n                    return value + 1\n                return None\n\n            normvalue = _normvalue  # deprecated CaselessDict class\n\n        d = MyDict({\"key\": 1})\n        assert d[\"key\"] == 2\n        assert d.get(\"key\") == 2\n\n        d = MyDict()\n        d[\"key\"] = 1\n        assert d[\"key\"] == 2\n        assert d.get(\"key\") == 2\n\n        d = MyDict()\n        d.setdefault(\"key\", 1)\n        assert d[\"key\"] == 2\n        assert d.get(\"key\") == 2\n\n        d = MyDict()\n        d.update({\"key\": 1})\n        assert d[\"key\"] == 2\n        assert d.get(\"key\") == 2\n\n        d = MyDict.fromkeys((\"key\",), 1)\n        assert d[\"key\"] == 2\n        assert d.get(\"key\") == 2\n\n    def test_copy(self):\n        h1 = self.dict_class({\"header1\": \"value\"})\n        h2 = copy.copy(h1)\n        assert isinstance(h2, self.dict_class)\n        assert h1 == h2\n        assert h1.get(\"header1\") == h2.get(\"header1\")\n        assert h1.get(\"header1\") == h2.get(\"HEADER1\")\n        h3 = h1.copy()\n        assert isinstance(h3, self.dict_class)\n        assert h1 == h3\n        assert h1.get(\"header1\") == h3.get(\"header1\")\n        assert h1.get(\"header1\") == h3.get(\"HEADER1\")\n\n\nclass TestCaseInsensitiveDict(TestCaseInsensitiveDictBase):\n    dict_class = CaseInsensitiveDict\n\n    def test_repr(self):\n        d1 = self.dict_class({\"foo\": \"bar\"})\n        assert repr(d1) == \"<CaseInsensitiveDict: {'foo': 'bar'}>\"\n        d2 = self.dict_class({\"AsDf\": \"QwErTy\", \"FoO\": \"bAr\"})\n        assert repr(d2) == \"<CaseInsensitiveDict: {'AsDf': 'QwErTy', 'FoO': 'bAr'}>\"\n\n    def test_iter(self):\n        d = self.dict_class({\"AsDf\": \"QwErTy\", \"FoO\": \"bAr\"})\n        iterkeys = iter(d)\n        assert isinstance(iterkeys, Iterator)\n        assert list(iterkeys) == [\"AsDf\", \"FoO\"]\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestCaselessDict(TestCaseInsensitiveDictBase):\n    dict_class = CaselessDict\n\n    def test_deprecation_message(self):\n        with warnings.catch_warnings(record=True) as caught:\n            warnings.filterwarnings(\"always\", category=ScrapyDeprecationWarning)\n            self.dict_class({\"foo\": \"bar\"})\n\n            assert len(caught) == 1\n            assert issubclass(caught[0].category, ScrapyDeprecationWarning)\n            assert (\n                str(caught[0].message)\n                == \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\"\n            )\n\n\nclass TestSequenceExclude:\n    def test_list(self):\n        seq = [1, 2, 3]\n        d = SequenceExclude(seq)\n        assert 0 in d\n        assert 4 in d\n        assert 2 not in d\n\n    def test_range(self):\n        seq = range(10, 20)\n        d = SequenceExclude(seq)\n        assert 5 in d\n        assert 20 in d\n        assert 15 not in d\n\n    def test_range_step(self):\n        seq = range(10, 20, 3)\n        d = SequenceExclude(seq)\n        are_not_in = [v for v in range(10, 20, 3) if v in d]\n        assert are_not_in == []\n\n        are_not_in = [v for v in range(10, 20) if v in d]\n        assert are_not_in == [11, 12, 14, 15, 17, 18]\n\n    def test_string_seq(self):\n        seq = \"cde\"\n        d = SequenceExclude(seq)\n        chars = \"\".join(v for v in \"abcdefg\" if v in d)\n        assert chars == \"abfg\"\n\n    def test_stringset_seq(self):\n        seq = set(\"cde\")\n        d = SequenceExclude(seq)\n        chars = \"\".join(v for v in \"abcdefg\" if v in d)\n        assert chars == \"abfg\"\n\n    def test_set(self):\n        \"\"\"Anything that is not in the supplied sequence will evaluate as 'in' the container.\"\"\"\n        seq = {-3, \"test\", 1.1}\n        d = SequenceExclude(seq)\n        assert 0 in d\n        assert \"foo\" in d\n        assert 3.14 in d\n        assert set(\"bar\") in d\n\n        # supplied sequence is a set, so checking for list (non)inclusion fails\n        with pytest.raises(TypeError):\n            [\"a\", \"b\", \"c\"] in d  # noqa: B015\n\n        for v in [-3, \"test\", 1.1]:\n            assert v not in d\n\n\nclass TestLocalCache:\n    def test_cache_with_limit(self):\n        cache = LocalCache(limit=2)\n        cache[\"a\"] = 1\n        cache[\"b\"] = 2\n        cache[\"c\"] = 3\n        assert len(cache) == 2\n        assert \"a\" not in cache\n        assert \"b\" in cache\n        assert \"c\" in cache\n        assert cache[\"b\"] == 2\n        assert cache[\"c\"] == 3\n", "n_tokens": 1232, "byte_len": 4425, "file_sha1": "384b05bbda9e928050efda6fb1a2b895c62e8165", "start_line": 164, "end_line": 307}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_datatypes.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_datatypes.py", "rel_path": "tests/test_utils_datatypes.py", "module": "tests.test_utils_datatypes", "ext": "py", "chunk_number": 3, "symbols": ["test_cache_without_limit", "test_cache_with_limit", "test_cache_non_weak_referenceable_objects", "TestLocalWeakReferencedCache", "takes", "cache", "delete", "refs", "garbage", "collect", "local", "append", "objects", "reference", "half", "test", "class", "last", "reflected", "pypy", "https", "example", "pylint", "make", "variable", "references", "list", "loop", "assert", "weak", "dict_class", "test_init_dict", "test_init_pair_sequence", "test_init_mapping", "__init__", "__getitem__", "__iter__", "__len__", "test_init_mutable_mapping", "__setitem__", "__delitem__", "test_caseless", "test_delete", "test_getdefault", "test_setdefault", "test_fromkeys", "test_contains", "test_pop", "test_normkey", "_normkey"], "ast_kind": "class_or_type", "text": "    def test_cache_without_limit(self):\n        maximum = 10**4\n        cache = LocalCache()\n        for x in range(maximum):\n            cache[str(x)] = x\n        assert len(cache) == maximum\n        for x in range(maximum):\n            assert str(x) in cache\n            assert cache[str(x)] == x\n\n\nclass TestLocalWeakReferencedCache:\n    def test_cache_with_limit(self):\n        cache = LocalWeakReferencedCache(limit=2)\n        r1 = Request(\"https://example.org\")\n        r2 = Request(\"https://example.com\")\n        r3 = Request(\"https://example.net\")\n        cache[r1] = 1\n        cache[r2] = 2\n        cache[r3] = 3\n        assert len(cache) == 2\n        assert r1 not in cache\n        assert r2 in cache\n        assert r3 in cache\n        assert cache[r1] is None\n        assert cache[r2] == 2\n        assert cache[r3] == 3\n        del r2\n\n        # PyPy takes longer to collect dead references\n        garbage_collect()\n\n        assert len(cache) == 1\n\n    def test_cache_non_weak_referenceable_objects(self):\n        cache = LocalWeakReferencedCache()\n        k1 = None\n        k2 = 1\n        k3 = [1, 2, 3]\n        cache[k1] = 1\n        cache[k2] = 2\n        cache[k3] = 3\n        assert k1 not in cache\n        assert k2 not in cache\n        assert k3 not in cache\n        assert len(cache) == 0\n\n    def test_cache_without_limit(self):\n        maximum = 10**4\n        cache = LocalWeakReferencedCache()\n        refs = []\n        for x in range(maximum):\n            refs.append(Request(f\"https://example.org/{x}\"))\n            cache[refs[-1]] = x\n        assert len(cache) == maximum\n        for i, r in enumerate(refs):\n            assert r in cache\n            assert cache[r] == i\n        del r  # delete reference to the last object in the list  # pylint: disable=undefined-loop-variable\n\n        # delete half of the objects, make sure that is reflected in the cache\n        for _ in range(maximum // 2):\n            refs.pop()\n\n        # PyPy takes longer to collect dead references\n        garbage_collect()\n\n        assert len(cache) == maximum // 2\n        for i, r in enumerate(refs):\n            assert r in cache\n            assert cache[r] == i\n", "n_tokens": 552, "byte_len": 2170, "file_sha1": "384b05bbda9e928050efda6fb1a2b895c62e8165", "start_line": 308, "end_line": 379}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py", "rel_path": "tests/test_engine.py", "module": "tests.test_engine", "ext": "py", "chunk_number": 1, "symbols": ["parse", "parse_item", "from_crawler", "spider_idle", "__init__", "geturl", "getpath", "MyItem", "AttrsItem", "DataClassItem", "MySpider", "DupeFilterSpider", "DictItemsSpider", "AttrsItemsSpider", "DataClassItemsSpider", "ItemZeroDivisionErrorSpider", "ChangeCloseReasonSpider", "CrawlerRun", "get", "testdata", "price", "async", "signal", "mock", "multiline", "custom", "reason", "spider", "name", "deferred", "item_error", "item_scraped", "headers_received", "bytes_received", "request_scheduled", "request_reached", "request_dropped", "response_downloaded", "record_signal", "_assert_visited_urls", "_assert_scheduled_requests", "_assert_dropped_requests", "_assert_downloaded_responses", "_assert_items_error", "_assert_scraped_items", "_assert_headers_received", "_assert_bytes_received", "_assert_signals_caught", "test_close_without_downloader", "test_start_already_running_exception"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport re\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom logging import DEBUG\nfrom typing import TYPE_CHECKING, cast\nfrom unittest.mock import Mock, call\nfrom urllib.parse import urlparse\n\nimport attr\nimport pytest\nfrom itemadapter import ItemAdapter\nfrom pydispatch import dispatcher\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy import signals\nfrom scrapy.core.engine import ExecutionEngine, _Slot\nfrom scrapy.core.scheduler import BaseScheduler\nfrom scrapy.exceptions import CloseSpider, IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import (\n    _schedule_coro,\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.signal import disconnect_all\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests import get_testdata\n\nif TYPE_CHECKING:\n    from scrapy.core.scheduler import Scheduler\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import MemoryStatsCollector\n    from tests.mockserver.http import MockServer\n\n\nclass MyItem(Item):\n    name = Field()\n    url = Field()\n    price = Field()\n\n\n@attr.s\nclass AttrsItem:\n    name = attr.ib(default=\"\")\n    url = attr.ib(default=\"\")\n    price = attr.ib(default=0)\n\n\n@dataclass\nclass DataClassItem:\n    name: str = \"\"\n    url: str = \"\"\n    price: int = 0\n\n\nclass MySpider(Spider):\n    name = \"scrapytest.org\"\n\n    itemurl_re = re.compile(r\"item\\d+.html\")\n    name_re = re.compile(r\"<h1>(.*?)</h1>\", re.MULTILINE)\n    price_re = re.compile(r\">Price: \\$(.*?)<\", re.MULTILINE)\n\n    item_cls: type = MyItem\n\n    def parse(self, response):\n        xlink = LinkExtractor()\n        itemre = re.compile(self.itemurl_re)\n        for link in xlink.extract_links(response):\n            if itemre.search(link.url):\n                yield Request(url=link.url, callback=self.parse_item)\n\n    def parse_item(self, response):\n        adapter = ItemAdapter(self.item_cls())\n        m = self.name_re.search(response.text)\n        if m:\n            adapter[\"name\"] = m.group(1)\n        adapter[\"url\"] = response.url\n        m = self.price_re.search(response.text)\n        if m:\n            adapter[\"price\"] = m.group(1)\n        return adapter.item\n\n\nclass DupeFilterSpider(MySpider):\n    async def start(self):\n        for url in self.start_urls:\n            yield Request(url)  # no dont_filter=True\n\n\nclass DictItemsSpider(MySpider):\n    item_cls = dict\n\n\nclass AttrsItemsSpider(MySpider):\n    item_cls = AttrsItem\n\n\nclass DataClassItemsSpider(MySpider):\n    item_cls = DataClassItem\n\n\nclass ItemZeroDivisionErrorSpider(MySpider):\n    custom_settings = {\n        \"ITEM_PIPELINES\": {\n            \"tests.pipelines.ProcessWithZeroDivisionErrorPipeline\": 300,\n        }\n    }\n\n\nclass ChangeCloseReasonSpider(MySpider):\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        crawler.signals.connect(spider.spider_idle, signals.spider_idle)\n        return spider\n\n    def spider_idle(self):\n        raise CloseSpider(reason=\"custom_reason\")\n\n\nclass CrawlerRun:\n    \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n\n    def __init__(self, spider_class):\n        self.respplug = []\n        self.reqplug = []\n        self.reqdropped = []\n        self.reqreached = []\n        self.itemerror = []\n        self.itemresp = []\n        self.headers = {}\n        self.bytes = defaultdict(list)\n        self.signals_caught = {}\n        self.spider_class = spider_class\n\n    async def run(self, mockserver: MockServer) -> None:\n        self.mockserver = mockserver\n\n        start_urls = [\n            self.geturl(\"/static/\"),\n            self.geturl(\"/redirect\"),\n            self.geturl(\"/redirect\"),  # duplicate\n            self.geturl(\"/numbers\"),\n        ]\n\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                dispatcher.connect(self.record_signal, signal)\n\n        self.crawler = get_crawler(self.spider_class)\n        self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n        self.crawler.signals.connect(self.item_error, signals.item_error)\n        self.crawler.signals.connect(self.headers_received, signals.headers_received)\n        self.crawler.signals.connect(self.bytes_received, signals.bytes_received)\n        self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n        self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n        self.crawler.signals.connect(\n            self.request_reached, signals.request_reached_downloader\n        )\n        self.crawler.signals.connect(\n            self.response_downloaded, signals.response_downloaded\n        )\n        self.crawler.crawl(start_urls=start_urls)\n\n        self.deferred: defer.Deferred[None] = defer.Deferred()\n        dispatcher.connect(self.stop, signals.engine_stopped)\n        await maybe_deferred_to_future(self.deferred)\n\n    async def stop(self):\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                disconnect_all(signal)\n        self.deferred.callback(None)\n        await self.crawler.stop_async()\n\n    def geturl(self, path: str) -> str:\n        return self.mockserver.url(path)\n\n    def getpath(self, url):\n        u = urlparse(url)\n        return u.path\n", "n_tokens": 1239, "byte_len": 5714, "file_sha1": "fd77b9f6e8ed9e483d804e4aa5a7703a34665df8", "start_line": 1, "end_line": 195}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py", "rel_path": "tests/test_engine.py", "module": "tests.test_engine", "ext": "py", "chunk_number": 2, "symbols": ["item_error", "item_scraped", "headers_received", "bytes_received", "request_scheduled", "request_reached", "request_dropped", "response_downloaded", "record_signal", "_assert_visited_urls", "_assert_scheduled_requests", "_assert_dropped_requests", "_assert_downloaded_responses", "_assert_items_error", "_assert_scraped_items", "_assert_headers_received", "TestEngineBase", "price", "append", "signal", "name", "record", "urls", "visited", "item", "none", "html", "adapter", "server", "assert", "parse", "parse_item", "from_crawler", "spider_idle", "__init__", "geturl", "getpath", "_assert_bytes_received", "_assert_signals_caught", "test_close_without_downloader", "test_start_already_running_exception", "test_start_request_processing_exception", "fingerprint", "test_short_timeout", "engine", "test_request_scheduled_signal", "enqueue_request", "signal_handler", "crawler", "MyItem"], "ast_kind": "class_or_type", "text": "    def item_error(self, item, response, spider, failure):\n        self.itemerror.append((item, response, spider, failure))\n\n    def item_scraped(self, item, spider, response):\n        self.itemresp.append((item, response))\n\n    def headers_received(self, headers, body_length, request, spider):\n        self.headers[request] = headers\n\n    def bytes_received(self, data, request, spider):\n        self.bytes[request].append(data)\n\n    def request_scheduled(self, request, spider):\n        self.reqplug.append((request, spider))\n\n    def request_reached(self, request, spider):\n        self.reqreached.append((request, spider))\n\n    def request_dropped(self, request, spider):\n        self.reqdropped.append((request, spider))\n\n    def response_downloaded(self, response, spider):\n        self.respplug.append((response, spider))\n\n    def record_signal(self, *args, **kwargs):\n        \"\"\"Record a signal and its parameters\"\"\"\n        signalargs = kwargs.copy()\n        sig = signalargs.pop(\"signal\")\n        signalargs.pop(\"sender\", None)\n        self.signals_caught[sig] = signalargs\n\n\nclass TestEngineBase:\n    @staticmethod\n    def _assert_visited_urls(run: CrawlerRun) -> None:\n        must_be_visited = [\n            \"/static/\",\n            \"/redirect\",\n            \"/redirected\",\n            \"/static/item1.html\",\n            \"/static/item2.html\",\n            \"/static/item999.html\",\n        ]\n        urls_visited = {rp[0].url for rp in run.respplug}\n        urls_expected = {run.geturl(p) for p in must_be_visited}\n        assert urls_expected <= urls_visited, (\n            f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n        )\n\n    @staticmethod\n    def _assert_scheduled_requests(run: CrawlerRun, count: int) -> None:\n        assert len(run.reqplug) == count\n\n        paths_expected = [\n            \"/static/item999.html\",\n            \"/static/item2.html\",\n            \"/static/item1.html\",\n        ]\n\n        urls_requested = {rq[0].url for rq in run.reqplug}\n        urls_expected = {run.geturl(p) for p in paths_expected}\n        assert urls_expected <= urls_requested\n        scheduled_requests_count = len(run.reqplug)\n        dropped_requests_count = len(run.reqdropped)\n        responses_count = len(run.respplug)\n        assert scheduled_requests_count == dropped_requests_count + responses_count\n        assert len(run.reqreached) == responses_count\n\n    @staticmethod\n    def _assert_dropped_requests(run: CrawlerRun) -> None:\n        assert len(run.reqdropped) == 1\n\n    @staticmethod\n    def _assert_downloaded_responses(run: CrawlerRun, count: int) -> None:\n        # response tests\n        assert len(run.respplug) == count\n        assert len(run.reqreached) == count\n\n        for response, _ in run.respplug:\n            if run.getpath(response.url) == \"/static/item999.html\":\n                assert response.status == 404\n            if run.getpath(response.url) == \"/redirect\":\n                assert response.status == 302\n\n    @staticmethod\n    def _assert_items_error(run: CrawlerRun) -> None:\n        assert len(run.itemerror) == 2\n        for item, response, spider, failure in run.itemerror:\n            assert failure.value.__class__ is ZeroDivisionError\n            assert spider == run.crawler.spider\n\n            assert item[\"url\"] == response.url\n            if \"item1.html\" in item[\"url\"]:\n                assert item[\"name\"] == \"Item 1 name\"\n                assert item[\"price\"] == \"100\"\n            if \"item2.html\" in item[\"url\"]:\n                assert item[\"name\"] == \"Item 2 name\"\n                assert item[\"price\"] == \"200\"\n\n    @staticmethod\n    def _assert_scraped_items(run: CrawlerRun) -> None:\n        assert len(run.itemresp) == 2\n        for item, response in run.itemresp:\n            item = ItemAdapter(item)\n            assert item[\"url\"] == response.url\n            if \"item1.html\" in item[\"url\"]:\n                assert item[\"name\"] == \"Item 1 name\"\n                assert item[\"price\"] == \"100\"\n            if \"item2.html\" in item[\"url\"]:\n                assert item[\"name\"] == \"Item 2 name\"\n                assert item[\"price\"] == \"200\"\n\n    @staticmethod\n    def _assert_headers_received(run: CrawlerRun) -> None:\n        for headers in run.headers.values():\n            assert b\"Server\" in headers\n            assert b\"TwistedWeb\" in headers[b\"Server\"]\n            assert b\"Date\" in headers\n            assert b\"Content-Type\" in headers\n", "n_tokens": 1005, "byte_len": 4415, "file_sha1": "fd77b9f6e8ed9e483d804e4aa5a7703a34665df8", "start_line": 196, "end_line": 315}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py", "rel_path": "tests/test_engine.py", "module": "tests.test_engine", "ext": "py", "chunk_number": 3, "symbols": ["_assert_bytes_received", "_assert_signals_caught", "test_close_without_downloader", "__init__", "test_start_already_running_exception", "TestEngine", "CustomException", "BadDownloader", "downloader", "get", "testdata", "such", "async", "signal", "were", "custom", "reason", "deferred", "from", "joined", "data", "elif", "mockserver", "numbers", "crawler", "pytest", "order", "equiv", "href", "test", "parse", "parse_item", "from_crawler", "spider_idle", "geturl", "getpath", "item_error", "item_scraped", "headers_received", "bytes_received", "request_scheduled", "request_reached", "request_dropped", "response_downloaded", "record_signal", "_assert_visited_urls", "_assert_scheduled_requests", "_assert_dropped_requests", "_assert_downloaded_responses", "_assert_items_error"], "ast_kind": "class_or_type", "text": "    @staticmethod\n    def _assert_bytes_received(run: CrawlerRun) -> None:\n        assert len(run.bytes) == 9\n        for request, data in run.bytes.items():\n            joined_data = b\"\".join(data)\n            if run.getpath(request.url) == \"/static/\":\n                assert joined_data == get_testdata(\"test_site\", \"index.html\")\n            elif run.getpath(request.url) == \"/static/item1.html\":\n                assert joined_data == get_testdata(\"test_site\", \"item1.html\")\n            elif run.getpath(request.url) == \"/static/item2.html\":\n                assert joined_data == get_testdata(\"test_site\", \"item2.html\")\n            elif run.getpath(request.url) == \"/redirected\":\n                assert joined_data == b\"Redirected here\"\n            elif run.getpath(request.url) == \"/redirect\":\n                assert (\n                    joined_data == b\"\\n<html>\\n\"\n                    b\"    <head>\\n\"\n                    b'        <meta http-equiv=\"refresh\" content=\"0;URL=/redirected\">\\n'\n                    b\"    </head>\\n\"\n                    b'    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n'\n                    b'    <a href=\"/redirected\">click here</a>\\n'\n                    b\"    </body>\\n\"\n                    b\"</html>\\n\"\n                )\n            elif run.getpath(request.url) == \"/static/item999.html\":\n                assert (\n                    joined_data == b\"\\n<html>\\n\"\n                    b\"  <head><title>404 - No Such Resource</title></head>\\n\"\n                    b\"  <body>\\n\"\n                    b\"    <h1>No Such Resource</h1>\\n\"\n                    b\"    <p>File not found.</p>\\n\"\n                    b\"  </body>\\n\"\n                    b\"</html>\\n\"\n                )\n            elif run.getpath(request.url) == \"/numbers\":\n                # signal was fired multiple times\n                assert len(data) > 1\n                # bytes were received in order\n                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n                assert joined_data == b\"\".join(numbers)\n\n    @staticmethod\n    def _assert_signals_caught(run: CrawlerRun) -> None:\n        assert signals.engine_started in run.signals_caught\n        assert signals.engine_stopped in run.signals_caught\n        assert signals.spider_opened in run.signals_caught\n        assert signals.spider_idle in run.signals_caught\n        assert signals.spider_closed in run.signals_caught\n        assert signals.headers_received in run.signals_caught\n\n        assert {\"spider\": run.crawler.spider} == run.signals_caught[\n            signals.spider_opened\n        ]\n        assert {\"spider\": run.crawler.spider} == run.signals_caught[signals.spider_idle]\n        assert {\n            \"spider\": run.crawler.spider,\n            \"reason\": \"finished\",\n        } == run.signals_caught[signals.spider_closed]\n\n\nclass TestEngine(TestEngineBase):\n    @deferred_f_from_coro_f\n    async def test_crawler(self, mockserver: MockServer) -> None:\n        for spider in (\n            MySpider,\n            DictItemsSpider,\n            AttrsItemsSpider,\n            DataClassItemsSpider,\n        ):\n            run = CrawlerRun(spider)\n            await run.run(mockserver)\n            self._assert_visited_urls(run)\n            self._assert_scheduled_requests(run, count=9)\n            self._assert_downloaded_responses(run, count=9)\n            self._assert_scraped_items(run)\n            self._assert_signals_caught(run)\n            self._assert_bytes_received(run)\n\n    @deferred_f_from_coro_f\n    async def test_crawler_dupefilter(self, mockserver: MockServer) -> None:\n        run = CrawlerRun(DupeFilterSpider)\n        await run.run(mockserver)\n        self._assert_scheduled_requests(run, count=8)\n        self._assert_dropped_requests(run)\n\n    @deferred_f_from_coro_f\n    async def test_crawler_itemerror(self, mockserver: MockServer) -> None:\n        run = CrawlerRun(ItemZeroDivisionErrorSpider)\n        await run.run(mockserver)\n        self._assert_items_error(run)\n\n    @deferred_f_from_coro_f\n    async def test_crawler_change_close_reason_on_idle(\n        self, mockserver: MockServer\n    ) -> None:\n        run = CrawlerRun(ChangeCloseReasonSpider)\n        await run.run(mockserver)\n        assert {\n            \"spider\": run.crawler.spider,\n            \"reason\": \"custom_reason\",\n        } == run.signals_caught[signals.spider_closed]\n\n    @deferred_f_from_coro_f\n    async def test_close_downloader(self):\n        e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n        await e.close_async()\n\n    def test_close_without_downloader(self):\n        class CustomException(Exception):\n            pass\n\n        class BadDownloader:\n            def __init__(self, crawler):\n                raise CustomException\n\n        with pytest.raises(CustomException):\n            ExecutionEngine(\n                get_crawler(MySpider, {\"DOWNLOADER\": BadDownloader}), lambda _: None\n            )\n\n    @inlineCallbacks\n    def test_start_already_running_exception(self):\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        e = ExecutionEngine(crawler, lambda _: None)\n        yield deferred_from_coro(e.open_spider_async())\n        _schedule_coro(e.start_async())\n        with pytest.raises(RuntimeError, match=\"Engine already running\"):\n            yield deferred_from_coro(e.start_async())\n        yield deferred_from_coro(e.stop_async())\n", "n_tokens": 1200, "byte_len": 5394, "file_sha1": "fd77b9f6e8ed9e483d804e4aa5a7703a34665df8", "start_line": 316, "end_line": 446}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py", "rel_path": "tests/test_engine.py", "module": "tests.test_engine", "ext": "py", "chunk_number": 4, "symbols": ["test_start_request_processing_exception", "fingerprint", "test_short_timeout", "engine", "test_request_scheduled_signal", "__init__", "enqueue_request", "BadRequestFingerprinter", "SimpleSpider", "TestEngineDownloadAsync", "TestEngineDownload", "TestScheduler", "test", "while", "async", "bool", "first", "append", "final", "response", "mock", "spider", "name", "deferred", "from", "error", "command", "succeed", "get", "crawler", "parse", "parse_item", "from_crawler", "spider_idle", "geturl", "getpath", "item_error", "item_scraped", "headers_received", "bytes_received", "request_scheduled", "request_reached", "request_dropped", "response_downloaded", "record_signal", "_assert_visited_urls", "_assert_scheduled_requests", "_assert_dropped_requests", "_assert_downloaded_responses", "_assert_items_error"], "ast_kind": "class_or_type", "text": "    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_start_already_running_exception_asyncio(self):\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        e = ExecutionEngine(crawler, lambda _: None)\n        await e.open_spider_async()\n        with pytest.raises(RuntimeError, match=\"Engine already running\"):\n            await asyncio.gather(e.start_async(), e.start_async())\n        await e.stop_async()\n\n    @inlineCallbacks\n    def test_start_request_processing_exception(self):\n        class BadRequestFingerprinter:\n            def fingerprint(self, request):\n                raise ValueError  # to make Scheduler.enqueue_request() fail\n\n        class SimpleSpider(Spider):\n            name = \"simple\"\n\n            async def start(self):\n                yield Request(\"data:,\")\n\n        crawler = get_crawler(\n            SimpleSpider, {\"REQUEST_FINGERPRINTER_CLASS\": BadRequestFingerprinter}\n        )\n        with LogCapture() as log:\n            yield crawler.crawl()\n        assert \"Error while processing requests from start()\" in str(log)\n        assert \"Spider closed (shutdown)\" in str(log)\n\n    def test_short_timeout(self):\n        args = (\n            sys.executable,\n            \"-m\",\n            \"scrapy.cmdline\",\n            \"fetch\",\n            \"-s\",\n            \"CLOSESPIDER_TIMEOUT=0.001\",\n            \"-s\",\n            \"LOG_LEVEL=DEBUG\",\n            \"http://toscrape.com\",\n        )\n        p = subprocess.Popen(\n            args,\n            stderr=subprocess.PIPE,\n        )\n\n        try:\n            _, stderr = p.communicate(timeout=15)\n        except subprocess.TimeoutExpired:\n            p.kill()\n            p.communicate()\n            pytest.fail(\"Command took too much time to complete\")\n\n        stderr_str = stderr.decode(\"utf-8\")\n        assert \"AttributeError\" not in stderr_str, stderr_str\n        assert \"AssertionError\" not in stderr_str, stderr_str\n\n\nclass TestEngineDownloadAsync:\n    \"\"\"Test cases for ExecutionEngine.download_async().\"\"\"\n\n    @pytest.fixture\n    def engine(self) -> ExecutionEngine:\n        crawler = get_crawler(MySpider)\n        engine = ExecutionEngine(crawler, lambda _: None)\n        engine.downloader.close()\n        engine.downloader = Mock()\n        engine._slot = Mock()\n        engine._slot.inprogress = set()\n        return engine\n\n    @staticmethod\n    async def _download(engine: ExecutionEngine, request: Request) -> Response:\n        return await engine.download_async(request)\n\n    @deferred_f_from_coro_f\n    async def test_download_async_success(self, engine):\n        \"\"\"Test basic successful async download of a request.\"\"\"\n        request = Request(\"http://example.com\")\n        response = Response(\"http://example.com\", body=b\"test body\")\n        engine.spider = Mock()\n        engine.downloader.fetch.return_value = defer.succeed(response)\n        engine._slot.add_request = Mock()\n        engine._slot.remove_request = Mock()\n\n        result = await self._download(engine, request)\n        assert result == response\n        engine._slot.add_request.assert_called_once_with(request)\n        engine._slot.remove_request.assert_called_once_with(request)\n        engine.downloader.fetch.assert_called_once_with(request)\n\n    @deferred_f_from_coro_f\n    async def test_download_async_redirect(self, engine):\n        \"\"\"Test async download with a redirect request.\"\"\"\n        original_request = Request(\"http://example.com\")\n        redirect_request = Request(\"http://example.com/redirect\")\n        final_response = Response(\"http://example.com/redirect\", body=b\"redirected\")\n\n        # First call returns redirect request, second call returns final response\n        engine.downloader.fetch.side_effect = [\n            defer.succeed(redirect_request),\n            defer.succeed(final_response),\n        ]\n        engine.spider = Mock()\n        engine._slot.add_request = Mock()\n        engine._slot.remove_request = Mock()\n\n        result = await self._download(engine, original_request)\n        assert result == final_response\n        assert engine.downloader.fetch.call_count == 2\n        engine._slot.add_request.assert_has_calls(\n            [call(original_request), call(redirect_request)]\n        )\n        engine._slot.remove_request.assert_has_calls(\n            [call(original_request), call(redirect_request)]\n        )\n\n    @deferred_f_from_coro_f\n    async def test_download_async_no_spider(self, engine):\n        \"\"\"Test async download attempt when no spider is available.\"\"\"\n        request = Request(\"http://example.com\")\n        engine.spider = None\n        with pytest.raises(RuntimeError, match=\"No open spider to crawl:\"):\n            await self._download(engine, request)\n\n    @deferred_f_from_coro_f\n    async def test_download_async_failure(self, engine):\n        \"\"\"Test async download when the downloader raises an exception.\"\"\"\n        request = Request(\"http://example.com\")\n        error = RuntimeError(\"Download failed\")\n        engine.spider = Mock()\n        engine.downloader.fetch.return_value = defer.fail(error)\n        engine._slot.add_request = Mock()\n        engine._slot.remove_request = Mock()\n\n        with pytest.raises(RuntimeError, match=\"Download failed\"):\n            await self._download(engine, request)\n        engine._slot.add_request.assert_called_once_with(request)\n        engine._slot.remove_request.assert_called_once_with(request)\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestEngineDownload(TestEngineDownloadAsync):\n    \"\"\"Test cases for ExecutionEngine.download().\"\"\"\n\n    @staticmethod\n    async def _download(engine: ExecutionEngine, request: Request) -> Response:\n        return await maybe_deferred_to_future(engine.download(request))\n\n\ndef test_request_scheduled_signal(caplog):\n    class TestScheduler(BaseScheduler):\n        def __init__(self):\n            self.enqueued = []\n\n        def enqueue_request(self, request: Request) -> bool:\n            self.enqueued.append(request)\n            return True\n", "n_tokens": 1212, "byte_len": 6068, "file_sha1": "fd77b9f6e8ed9e483d804e4aa5a7703a34665df8", "start_line": 447, "end_line": 607}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine.py", "rel_path": "tests/test_engine.py", "module": "tests.test_engine", "ext": "py", "chunk_number": 5, "symbols": ["signal_handler", "crawler", "TestEngineCloseSpider", "disconnect", "test", "slot", "signal", "handler", "async", "mock", "spider", "deferred", "from", "scheduler", "sending", "error", "https", "get", "pytest", "execution", "engine", "during", "schedule", "request", "stop", "none", "fixture", "reason", "runtime", "fail", "parse", "parse_item", "from_crawler", "spider_idle", "__init__", "geturl", "getpath", "item_error", "item_scraped", "headers_received", "bytes_received", "request_scheduled", "request_reached", "request_dropped", "response_downloaded", "record_signal", "_assert_visited_urls", "_assert_scheduled_requests", "_assert_dropped_requests", "_assert_downloaded_responses"], "ast_kind": "class_or_type", "text": "    def signal_handler(request: Request, spider: Spider) -> None:\n        if \"drop\" in request.url:\n            raise IgnoreRequest\n\n    crawler = get_crawler(MySpider)\n    engine = ExecutionEngine(crawler, lambda _: None)\n    engine.downloader._slot_gc_loop.stop()\n    scheduler = TestScheduler()\n\n    async def start():\n        return\n        yield\n\n    engine._start = start()\n    engine._slot = _Slot(False, Mock(), scheduler)\n    crawler.signals.connect(signal_handler, signals.request_scheduled)\n    keep_request = Request(\"https://keep.example\")\n    engine._schedule_request(keep_request)\n    drop_request = Request(\"https://drop.example\")\n    caplog.set_level(DEBUG)\n    engine._schedule_request(drop_request)\n    assert scheduler.enqueued == [keep_request], (\n        f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n    )\n    crawler.signals.disconnect(signal_handler, signals.request_scheduled)\n\n\nclass TestEngineCloseSpider:\n    \"\"\"Tests for exception handling coverage during close_spider_async().\"\"\"\n\n    @pytest.fixture\n    def crawler(self) -> Crawler:\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        return crawler\n\n    @deferred_f_from_coro_f\n    async def test_no_slot(self, crawler: Crawler) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        slot = engine._slot\n        engine._slot = None\n        with pytest.raises(RuntimeError, match=\"Engine slot not assigned\"):\n            await engine.close_spider_async()\n        # close it correctly\n        engine._slot = slot\n        await engine.close_spider_async()\n\n    @deferred_f_from_coro_f\n    async def test_no_spider(self, crawler: Crawler) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        with pytest.raises(RuntimeError, match=\"Spider not opened\"):\n            await engine.close_spider_async()\n\n    @deferred_f_from_coro_f\n    async def test_exception_slot(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        assert engine._slot\n        del engine._slot.heartbeat\n        await engine.close_spider_async()\n        assert \"Slot close failure\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_exception_downloader(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        del engine.downloader.slots\n        await engine.close_spider_async()\n        assert \"Downloader close failure\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_exception_scraper(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        engine.scraper.slot = None\n        await engine.close_spider_async()\n        assert \"Scraper close failure\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_exception_scheduler(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        assert engine._slot\n        del cast(\"Scheduler\", engine._slot.scheduler).dqs\n        await engine.close_spider_async()\n        assert \"Scheduler close failure\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_exception_signal(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        signal_manager = engine.signals\n        del engine.signals\n        await engine.close_spider_async()\n        assert \"Error while sending spider_close signal\" in caplog.text\n        # send the spider_closed signal to close various components\n        await signal_manager.send_catch_log_async(\n            signal=signals.spider_closed,\n            spider=engine.spider,\n            reason=\"cancelled\",\n        )\n\n    @deferred_f_from_coro_f\n    async def test_exception_stats(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: None)\n        await engine.open_spider_async()\n        del cast(\"MemoryStatsCollector\", crawler.stats).spider_stats\n        await engine.close_spider_async()\n        assert \"Stats close failure\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_exception_callback(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        engine = ExecutionEngine(crawler, lambda _: defer.fail(ValueError()))\n        await engine.open_spider_async()\n        await engine.close_spider_async()\n        assert \"Error running spider_closed_callback\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_exception_async_callback(\n        self, crawler: Crawler, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        async def cb(_):\n            raise ValueError\n\n        engine = ExecutionEngine(crawler, cb)\n        await engine.open_spider_async()\n        await engine.close_spider_async()\n        assert \"Error running spider_closed_callback\" in caplog.text\n", "n_tokens": 1184, "byte_len": 5362, "file_sha1": "fd77b9f6e8ed9e483d804e4aa5a7703a34665df8", "start_line": 608, "end_line": 751}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_selector.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_selector.py", "rel_path": "tests/test_selector.py", "module": "tests.test_selector", "ext": "py", "chunk_number": 1, "symbols": ["test_simple_selection", "test_root_base_url", "test_flavor_detection", "test_http_header_encoding_precedence", "test_badly_encoded_body", "test_weakref_slots", "test_selector_bad_args", "TestSelector", "TestJMESPath", "encoding", "does", "xpath", "selector", "name", "doesn", "rules", "test", "simple", "slots", "pytest", "equiv", "jmes", "isinstance", "encode", "html", "type", "reason", "parse", "http", "match", "test_json_has_html", "test_html_has_json", "test_jmestpath_with_re", "test_jmespath_not_available", "json", "string", "sometimes", "none", "check", "utf", "utf8", "number", "packaging", "jmespath", "referenceable", "concat", "class", "meta", "symbol", "resp"], "ast_kind": "class_or_type", "text": "import weakref\n\nimport parsel\nimport pytest\nfrom packaging import version\n\nfrom scrapy.http import HtmlResponse, TextResponse, XmlResponse\nfrom scrapy.selector import Selector\n\nPARSEL_VERSION = version.parse(getattr(parsel, \"__version__\", \"0.0\"))\nPARSEL_18_PLUS = PARSEL_VERSION >= version.parse(\"1.8.0\")\n\n\nclass TestSelector:\n    def test_simple_selection(self):\n        \"\"\"Simple selector tests\"\"\"\n        body = b\"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n        response = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        sel = Selector(response)\n\n        xl = sel.xpath(\"//input\")\n        assert len(xl) == 2\n        for x in xl:\n            assert isinstance(x, Selector)\n\n        assert sel.xpath(\"//input\").getall() == [x.get() for x in sel.xpath(\"//input\")]\n        assert [x.get() for x in sel.xpath(\"//input[@name='a']/@name\")] == [\"a\"]\n        assert [\n            x.get()\n            for x in sel.xpath(\n                \"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\"\n            )\n        ] == [\"12.0\"]\n        assert sel.xpath(\"concat('xpath', 'rules')\").getall() == [\"xpathrules\"]\n        assert [\n            x.get()\n            for x in sel.xpath(\n                \"concat(//input[@name='a']/@value, //input[@name='b']/@value)\"\n            )\n        ] == [\"12\"]\n\n    def test_root_base_url(self):\n        body = b'<html><form action=\"/path\"><input name=\"a\" /></form></html>'\n        url = \"http://example.com\"\n        response = TextResponse(url=url, body=body, encoding=\"utf-8\")\n        sel = Selector(response)\n        assert url == sel.root.base\n\n    def test_flavor_detection(self):\n        text = b'<div><img src=\"a.jpg\"><p>Hello</div>'\n        sel = Selector(XmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n        assert sel.type == \"xml\"\n        assert sel.xpath(\"//div\").getall() == [\n            '<div><img src=\"a.jpg\"><p>Hello</p></img></div>'\n        ]\n\n        sel = Selector(HtmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n        assert sel.type == \"html\"\n        assert sel.xpath(\"//div\").getall() == [\n            '<div><img src=\"a.jpg\"><p>Hello</p></div>'\n        ]\n\n    def test_http_header_encoding_precedence(self):\n        # '\\xa3'     = pound symbol in unicode\n        # '\\xc2\\xa3' = pound symbol in utf-8\n        # '\\xa3'     = pound symbol in latin-1 (iso-8859-1)\n\n        meta = (\n            '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">'\n        )\n        head = f\"<head>{meta}</head>\"\n        body_content = '<span id=\"blank\">\\xa3</span>'\n        body = f\"<body>{body_content}</body>\"\n        html = f\"<html>{head}{body}</html>\"\n        encoding = \"utf-8\"\n        html_utf8 = html.encode(encoding)\n\n        headers = {\"Content-Type\": [\"text/html; charset=utf-8\"]}\n        response = HtmlResponse(\n            url=\"http://example.com\", headers=headers, body=html_utf8\n        )\n        x = Selector(response)\n        assert x.xpath(\"//span[@id='blank']/text()\").getall() == [\"\\xa3\"]\n\n    def test_badly_encoded_body(self):\n        # \\xe9 alone isn't valid utf8 sequence\n        r1 = TextResponse(\n            \"http://www.example.com\",\n            body=b\"<html><p>an Jos\\xe9 de</p><html>\",\n            encoding=\"utf-8\",\n        )\n        Selector(r1).xpath(\"//text()\").getall()\n\n    def test_weakref_slots(self):\n        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n        x = Selector(text=\"\")\n        weakref.ref(x)\n        assert not hasattr(x, \"__dict__\"), (\n            f\"{x.__class__.__name__} does not use __slots__\"\n        )\n\n    def test_selector_bad_args(self):\n        with pytest.raises(ValueError, match=\"received both response and text\"):\n            Selector(TextResponse(url=\"http://example.com\", body=b\"\"), text=\"\")\n\n\n@pytest.mark.skipif(not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\")\nclass TestJMESPath:", "n_tokens": 1004, "byte_len": 3942, "file_sha1": "882b7e76c2a79e67dda5dbe62b385c7100b80ead", "start_line": 1, "end_line": 108}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_selector.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_selector.py", "rel_path": "tests/test_selector.py", "module": "tests.test_selector", "ext": "py", "chunk_number": 2, "symbols": ["test_json_has_html", "test_html_has_json", "test_jmestpath_with_re", "test_jmespath_not_available", "encoding", "text", "response", "getall", "user", "jmespath", "xpath", "supports", "test", "name", "mark", "parse", "plus", "skipif", "wrapper", "json", "with", "resp", "total", "example", "body", "value", "first", "unavailable", "returned", "jmestpath", "test_simple_selection", "test_root_base_url", "test_flavor_detection", "test_http_header_encoding_precedence", "test_badly_encoded_body", "test_weakref_slots", "test_selector_bad_args", "TestSelector", "TestJMESPath", "does", "selector", "doesn", "rules", "simple", "slots", "string", "pytest", "equiv", "jmes", "isinstance"], "ast_kind": "function_or_method", "text": "    def test_json_has_html(self) -> None:\n        \"\"\"Sometimes the information is returned in a json wrapper\"\"\"\n\n        body = \"\"\"\n        {\n            \"content\": [\n                {\n                    \"name\": \"A\",\n                    \"value\": \"a\"\n                },\n                {\n                    \"name\": {\n                        \"age\": 18\n                    },\n                    \"value\": \"b\"\n                },\n                {\n                    \"name\": \"C\",\n                    \"value\": \"c\"\n                },\n                {\n                    \"name\": \"<a>D</a>\",\n                    \"value\": \"<div>d</div>\"\n                }\n            ],\n            \"html\": \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\"\n        }\n        \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        assert (\n            resp.jmespath(\"html\").get()\n            == \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\"\n        )\n        assert resp.jmespath(\"html\").xpath(\"//div/a/text()\").getall() == [\"a\", \"b\", \"d\"]\n        assert resp.jmespath(\"html\").css(\"div > b\").getall() == [\"<b>f</b>\"]\n        assert resp.jmespath(\"content\").jmespath(\"name.age\").get() == \"18\"\n\n    def test_html_has_json(self) -> None:\n        body = \"\"\"\n        <div>\n            <h1>Information</h1>\n            <content>\n            {\n              \"user\": [\n                        {\n                                  \"name\": \"A\",\n                                  \"age\": 18\n                        },\n                        {\n                                  \"name\": \"B\",\n                                  \"age\": 32\n                        },\n                        {\n                                  \"name\": \"C\",\n                                  \"age\": 22\n                        },\n                        {\n                                  \"name\": \"D\",\n                                  \"age\": 25\n                        }\n              ],\n              \"total\": 4,\n              \"status\": \"ok\"\n            }\n            </content>\n        </div>\n        \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        assert resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").getall() == [\n            \"A\",\n            \"B\",\n            \"C\",\n            \"D\",\n        ]\n        assert resp.xpath(\"//div/content\").jmespath(\"user[*].name\").getall() == [\n            \"A\",\n            \"B\",\n            \"C\",\n            \"D\",\n        ]\n        assert resp.xpath(\"//div/content\").jmespath(\"total\").get() == \"4\"\n\n    def test_jmestpath_with_re(self) -> None:\n        body = \"\"\"\n            <div>\n                <h1>Information</h1>\n                <content>\n                {\n                  \"user\": [\n                            {\n                                      \"name\": \"A\",\n                                      \"age\": 18\n                            },\n                            {\n                                      \"name\": \"B\",\n                                      \"age\": 32\n                            },\n                            {\n                                      \"name\": \"C\",\n                                      \"age\": 22\n                            },\n                            {\n                                      \"name\": \"D\",\n                                      \"age\": 25\n                            }\n                  ],\n                  \"total\": 4,\n                  \"status\": \"ok\"\n                }\n                </content>\n            </div>\n            \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        assert resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").re(\n            r\"(\\w+)\"\n        ) == [\"A\", \"B\", \"C\", \"D\"]\n        assert resp.xpath(\"//div/content\").jmespath(\"user[*].name\").re(r\"(\\w+)\") == [\n            \"A\",\n            \"B\",\n            \"C\",\n            \"D\",\n        ]\n\n        assert resp.xpath(\"//div/content\").jmespath(\"unavailable\").re(r\"(\\d+)\") == []\n\n        assert (\n            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re_first(r\"(\\d+)\")\n            is None\n        )\n\n        assert resp.xpath(\"//div/content\").jmespath(\"user[*].age.to_string(@)\").re(\n            r\"(\\d+)\"\n        ) == [\"18\", \"32\", \"22\", \"25\"]\n\n\n@pytest.mark.skipif(PARSEL_18_PLUS, reason=\"parsel >= 1.8 supports jmespath\")\ndef test_jmespath_not_available() -> None:\n    body = \"\"\"\n    {\n        \"website\": {\"name\": \"Example\"}\n    }\n    \"\"\"\n    resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n    with pytest.raises(AttributeError):\n        resp.jmespath(\"website.name\").get()\n", "n_tokens": 1012, "byte_len": 4688, "file_sha1": "882b7e76c2a79e67dda5dbe62b385c7100b80ead", "start_line": 109, "end_line": 254}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py", "rel_path": "tests/test_spidermiddleware.py", "module": "tests.test_spidermiddleware", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "scrape_func", "process_spider_input", "process_spider_output", "process_spider_exception", "_construct_mw_setting", "_callback", "TestSpiderMiddleware", "TestProcessSpiderInputInvalidOutput", "InvalidProcessSpiderInputMiddleware", "TestProcessSpiderOutputInvalidOutput", "InvalidProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionInvalidOutput", "InvalidProcessSpiderOutputExceptionMiddleware", "RaiseExceptionProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionReRaise", "ProcessSpiderExceptionReturnNoneMiddleware", "TestBaseAsyncSpiderMiddleware", "failure", "method", "async", "test", "process", "call", "later", "case", "spider", "deferred", "from", "spiders", "process_spider_output_async", "crawler", "mwman", "test_simple_mw", "test_async_mw", "test_universal_mw", "test_universal_mw_no_sync", "test_universal_mw_both_sync", "test_universal_mw_both_async", "ProcessSpiderOutputSimpleMiddleware", "ProcessSpiderOutputAsyncGenMiddleware", "ProcessSpiderOutputUniversalMiddleware", "ProcessSpiderExceptionSimpleIterableMiddleware", "ProcessSpiderExceptionAsyncIteratorMiddleware", "TestProcessSpiderOutputSimple", "TestProcessSpiderOutputAsyncGen", "ProcessSpiderOutputNonIterableMiddleware", "ProcessSpiderOutputCoroutineMiddleware", "TestProcessSpiderOutputInvalidResult", "ProcessStartSimpleMiddleware"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterator, Iterable\nfrom inspect import isasyncgen\nfrom typing import TYPE_CHECKING, Any\nfrom unittest import mock\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\n\nfrom scrapy.core.spidermw import SpiderMiddlewareManager\nfrom scrapy.exceptions import ScrapyDeprecationWarning, _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from twisted.python.failure import Failure\n\n    from scrapy.crawler import Crawler\n\n\nclass TestSpiderMiddleware:\n    def setup_method(self):\n        self.request = Request(\"http://example.com/index.html\")\n        self.response = Response(self.request.url, request=self.request)\n        self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}})\n        self.crawler.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n\n    async def _scrape_response(self) -> Any:\n        \"\"\"Execute spider mw manager's scrape_response_async method and return the result.\n        Raise exception in case of failure.\n        \"\"\"\n\n        def scrape_func(\n            response: Response | Failure, request: Request\n        ) -> defer.Deferred[Iterable[Any]]:\n            it = mock.MagicMock()\n            return defer.succeed(it)\n\n        return await self.mwman.scrape_response_async(\n            scrape_func, self.response, self.request\n        )\n\n\nclass TestProcessSpiderInputInvalidOutput(TestSpiderMiddleware):\n    \"\"\"Invalid return value for process_spider_input method\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_invalid_process_spider_input(self):\n        class InvalidProcessSpiderInputMiddleware:\n            def process_spider_input(self, response):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessSpiderInputMiddleware())\n        with pytest.raises(_InvalidOutput):\n            await self._scrape_response()\n\n\nclass TestProcessSpiderOutputInvalidOutput(TestSpiderMiddleware):\n    \"\"\"Invalid return value for process_spider_output method\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_invalid_process_spider_output(self):\n        class InvalidProcessSpiderOutputMiddleware:\n            def process_spider_output(self, response, result):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessSpiderOutputMiddleware())\n        with pytest.raises(_InvalidOutput):\n            await self._scrape_response()\n\n\nclass TestProcessSpiderExceptionInvalidOutput(TestSpiderMiddleware):\n    \"\"\"Invalid return value for process_spider_exception method\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_invalid_process_spider_exception(self):\n        class InvalidProcessSpiderOutputExceptionMiddleware:\n            def process_spider_exception(self, response, exception):\n                return 1\n\n        class RaiseExceptionProcessSpiderOutputMiddleware:\n            def process_spider_output(self, response, result):\n                raise RuntimeError\n\n        self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n        with pytest.raises(_InvalidOutput):\n            await self._scrape_response()\n\n\nclass TestProcessSpiderExceptionReRaise(TestSpiderMiddleware):\n    \"\"\"Re raise the exception by returning None\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_process_spider_exception_return_none(self):\n        class ProcessSpiderExceptionReturnNoneMiddleware:\n            def process_spider_exception(self, response, exception):\n                return None\n\n        class RaiseExceptionProcessSpiderOutputMiddleware:\n            def process_spider_output(self, response, result):\n                1 / 0\n\n        self.mwman._add_middleware(ProcessSpiderExceptionReturnNoneMiddleware())\n        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n        with pytest.raises(ZeroDivisionError):\n            await self._scrape_response()\n\n\nclass TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n    \"\"\"Helpers for testing sync, async and mixed middlewares.\n\n    Should work for process_spider_output and, when it's supported, process_start.\n    \"\"\"\n\n    ITEM_TYPE: type | tuple\n    RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects\n\n    @staticmethod\n    def _construct_mw_setting(\n        *mw_classes: type[Any], start_index: int | None = None\n    ) -> dict[type[Any], int]:\n        if start_index is None:\n            start_index = 10\n        return {i: c for c, i in enumerate(mw_classes, start=start_index)}\n\n    def _callback(self) -> Any:\n        yield {\"foo\": 1}\n        yield {\"foo\": 2}\n        yield {\"foo\": 3}\n\n    async def _scrape_func(\n        self, response: Response | Failure, request: Request\n    ) -> Iterable[Any] | AsyncIterator[Any]:\n        return self._callback()\n\n    async def _get_middleware_result(\n        self, *mw_classes: type[Any], start_index: int | None = None\n    ) -> Any:\n        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n        self.crawler = get_crawler(\n            Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n        )\n        self.crawler.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n        return await self.mwman.scrape_response_async(\n            self._scrape_func, self.response, self.request\n        )\n", "n_tokens": 1206, "byte_len": 5764, "file_sha1": "7ef5935fb8c8204eafb518e15886e16990a8f08a", "start_line": 1, "end_line": 156}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py", "rel_path": "tests/test_spidermiddleware.py", "module": "tests.test_spidermiddleware", "ext": "py", "chunk_number": 2, "symbols": ["process_spider_output", "process_spider_exception", "ProcessSpiderOutputSimpleMiddleware", "ProcessSpiderOutputAsyncGenMiddleware", "ProcessSpiderOutputUniversalMiddleware", "ProcessSpiderExceptionSimpleIterableMiddleware", "ProcessSpiderExceptionAsyncIteratorMiddleware", "TestProcessSpiderOutputSimple", "TestProcessSpiderOutputAsyncGen", "async", "process", "spider", "bool", "call", "later", "test", "simple", "asyncgen", "generator", "deferred", "from", "doesn", "universal", "ite", "type", "upgrade", "isinstance", "none", "resul", "count", "setup_method", "scrape_func", "process_spider_input", "_construct_mw_setting", "_callback", "process_spider_output_async", "crawler", "mwman", "test_simple_mw", "test_async_mw", "test_universal_mw", "test_universal_mw_no_sync", "test_universal_mw_both_sync", "test_universal_mw_both_async", "TestSpiderMiddleware", "TestProcessSpiderInputInvalidOutput", "InvalidProcessSpiderInputMiddleware", "TestProcessSpiderOutputInvalidOutput", "InvalidProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionInvalidOutput"], "ast_kind": "class_or_type", "text": "    async def _test_simple_base(\n        self,\n        *mw_classes: type[Any],\n        downgrade: bool = False,\n        start_index: int | None = None,\n    ) -> None:\n        with LogCapture() as log:\n            result = await self._get_middleware_result(\n                *mw_classes, start_index=start_index\n            )\n        assert isinstance(result, Iterable)\n        result_list = list(result)\n        assert len(result_list) == self.RESULT_COUNT\n        assert isinstance(result_list[0], self.ITEM_TYPE)\n        assert (\"downgraded to a non-async\" in str(log)) == downgrade\n        assert (\"doesn't support asynchronous spider output\" in str(log)) == (\n            ProcessSpiderOutputSimpleMiddleware in mw_classes\n        )\n\n    async def _test_asyncgen_base(\n        self,\n        *mw_classes: type[Any],\n        downgrade: bool = False,\n        start_index: int | None = None,\n    ) -> None:\n        with LogCapture() as log:\n            result = await self._get_middleware_result(\n                *mw_classes, start_index=start_index\n            )\n        assert isinstance(result, AsyncIterator)\n        result_list = await collect_asyncgen(result)\n        assert len(result_list) == self.RESULT_COUNT\n        assert isinstance(result_list[0], self.ITEM_TYPE)\n        assert (\"downgraded to a non-async\" in str(log)) == downgrade\n\n\nclass ProcessSpiderOutputSimpleMiddleware:\n    def process_spider_output(self, response, result):\n        yield from result\n\n\nclass ProcessSpiderOutputAsyncGenMiddleware:\n    async def process_spider_output(self, response, result):\n        async for r in result:\n            yield r\n\n\nclass ProcessSpiderOutputUniversalMiddleware:\n    def process_spider_output(self, response, result):\n        yield from result\n\n    async def process_spider_output_async(self, response, result):\n        async for r in result:\n            yield r\n\n\nclass ProcessSpiderExceptionSimpleIterableMiddleware:\n    def process_spider_exception(self, response, exception):\n        yield {\"foo\": 1}\n        yield {\"foo\": 2}\n        yield {\"foo\": 3}\n\n\nclass ProcessSpiderExceptionAsyncIteratorMiddleware:\n    async def process_spider_exception(self, response, exception):\n        yield {\"foo\": 1}\n        d = defer.Deferred()\n        call_later(0, d.callback, None)\n        await maybe_deferred_to_future(d)\n        yield {\"foo\": 2}\n        yield {\"foo\": 3}\n\n\nclass TestProcessSpiderOutputSimple(TestBaseAsyncSpiderMiddleware):\n    \"\"\"process_spider_output tests for simple callbacks\"\"\"\n\n    ITEM_TYPE = dict\n    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n\n    @deferred_f_from_coro_f\n    async def test_simple(self):\n        \"\"\"Simple mw\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE)\n\n    @deferred_f_from_coro_f\n    async def test_asyncgen(self):\n        \"\"\"Asyncgen mw; upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN)\n\n    @deferred_f_from_coro_f\n    async def test_simple_asyncgen(self):\n        \"\"\"Simple mw -> asyncgen mw; upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_SIMPLE)\n\n    @deferred_f_from_coro_f\n    async def test_asyncgen_simple(self):\n        \"\"\"Asyncgen mw -> simple mw; upgrade then downgrade\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, self.MW_ASYNCGEN, downgrade=True)\n\n    @deferred_f_from_coro_f\n    async def test_universal(self):\n        \"\"\"Universal mw\"\"\"\n        await self._test_simple_base(self.MW_UNIVERSAL)\n\n    @deferred_f_from_coro_f\n    async def test_universal_simple(self):\n        \"\"\"Universal mw -> simple mw\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL)\n\n    @deferred_f_from_coro_f\n    async def test_simple_universal(self):\n        \"\"\"Simple mw -> universal mw\"\"\"\n        await self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE)\n\n    @deferred_f_from_coro_f\n    async def test_universal_asyncgen(self):\n        \"\"\"Universal mw -> asyncgen mw; upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_UNIVERSAL)\n\n    @deferred_f_from_coro_f\n    async def test_asyncgen_universal(self):\n        \"\"\"Asyncgen mw -> universal mw; upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_UNIVERSAL, self.MW_ASYNCGEN)\n\n\nclass TestProcessSpiderOutputAsyncGen(TestProcessSpiderOutputSimple):\n    \"\"\"process_spider_output tests for async generator callbacks\"\"\"\n\n    async def _callback(self) -> Any:\n        for item in super()._callback():\n            yield item\n\n    @deferred_f_from_coro_f\n    async def test_simple(self):\n        \"\"\"Simple mw; downgrade\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, downgrade=True)\n\n    @deferred_f_from_coro_f\n    async def test_simple_asyncgen(self):\n        \"\"\"Simple mw -> asyncgen mw; downgrade then upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_SIMPLE, downgrade=True)\n\n    @deferred_f_from_coro_f\n    async def test_universal(self):\n        \"\"\"Universal mw\"\"\"\n        await self._test_asyncgen_base(self.MW_UNIVERSAL)\n", "n_tokens": 1181, "byte_len": 5103, "file_sha1": "7ef5935fb8c8204eafb518e15886e16990a8f08a", "start_line": 157, "end_line": 305}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py", "rel_path": "tests/test_spidermiddleware.py", "module": "tests.test_spidermiddleware", "ext": "py", "chunk_number": 3, "symbols": ["process_spider_output", "process_spider_output_async", "crawler", "mwman", "test_simple_mw", "test_async_mw", "test_universal_mw", "test_universal_mw_no_sync", "test_universal_mw_both_sync", "ProcessSpiderOutputNonIterableMiddleware", "ProcessSpiderOutputCoroutineMiddleware", "TestProcessSpiderOutputInvalidResult", "ProcessStartSimpleMiddleware", "TestProcessStartSimple", "TestSpider", "UniversalMiddlewareNoSync", "UniversalMiddlewareBothSync", "UniversalMiddlewareBothAsync", "TestUniversalMiddlewareManager", "async", "process", "spider", "generator", "test", "universal", "name", "deferred", "from", "dont", "filter", "setup_method", "scrape_func", "process_spider_input", "process_spider_exception", "_construct_mw_setting", "_callback", "test_universal_mw_both_async", "TestSpiderMiddleware", "TestProcessSpiderInputInvalidOutput", "InvalidProcessSpiderInputMiddleware", "TestProcessSpiderOutputInvalidOutput", "InvalidProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionInvalidOutput", "InvalidProcessSpiderOutputExceptionMiddleware", "RaiseExceptionProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionReRaise", "ProcessSpiderExceptionReturnNoneMiddleware", "TestBaseAsyncSpiderMiddleware", "ProcessSpiderOutputSimpleMiddleware", "ProcessSpiderOutputAsyncGenMiddleware"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_universal_simple(self):\n        \"\"\"Universal mw -> simple mw; downgrade\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL, downgrade=True)\n\n    @deferred_f_from_coro_f\n    async def test_simple_universal(self):\n        \"\"\"Simple mw -> universal mw; downgrade\"\"\"\n        await self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE, downgrade=True)\n\n\nclass ProcessSpiderOutputNonIterableMiddleware:\n    def process_spider_output(self, response, result):\n        return\n\n\nclass ProcessSpiderOutputCoroutineMiddleware:\n    async def process_spider_output(self, response, result):\n        return result\n\n\nclass TestProcessSpiderOutputInvalidResult(TestBaseAsyncSpiderMiddleware):\n    @deferred_f_from_coro_f\n    async def test_non_iterable(self):\n        with pytest.raises(\n            _InvalidOutput,\n            match=r\"\\.process_spider_output must return an iterable, got <class 'NoneType'>\",\n        ):\n            await self._get_middleware_result(ProcessSpiderOutputNonIterableMiddleware)\n\n    @deferred_f_from_coro_f\n    async def test_coroutine(self):\n        with pytest.raises(\n            _InvalidOutput,\n            match=r\"\\.process_spider_output must be an asynchronous generator\",\n        ):\n            await self._get_middleware_result(ProcessSpiderOutputCoroutineMiddleware)\n\n\nclass ProcessStartSimpleMiddleware:\n    async def process_start(self, start):\n        async for item_or_request in start:\n            yield item_or_request\n\n\nclass TestProcessStartSimple(TestBaseAsyncSpiderMiddleware):\n    \"\"\"process_start tests for simple start\"\"\"\n\n    ITEM_TYPE = (Request, dict)\n    MW_SIMPLE = ProcessStartSimpleMiddleware\n\n    async def _get_processed_start(\n        self, *mw_classes: type[Any]\n    ) -> AsyncIterator[Any] | None:\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                for i in range(2):\n                    yield Request(f\"https://example.com/{i}\", dont_filter=True)\n                yield {\"name\": \"test item\"}\n\n        setting = self._construct_mw_setting(*mw_classes)\n        self.crawler = get_crawler(\n            TestSpider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n        )\n        self.crawler.spider = self.crawler._create_spider()\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n        return await self.mwman.process_start()\n\n    @deferred_f_from_coro_f\n    async def test_simple(self):\n        \"\"\"Simple mw\"\"\"\n        start = await self._get_processed_start(self.MW_SIMPLE)\n        assert isasyncgen(start)\n        start_list = await collect_asyncgen(start)\n        assert len(start_list) == self.RESULT_COUNT\n        assert isinstance(start_list[0], self.ITEM_TYPE)\n\n\nclass UniversalMiddlewareNoSync:\n    async def process_spider_output_async(self, response, result):\n        yield\n\n\nclass UniversalMiddlewareBothSync:\n    def process_spider_output(self, response, result):\n        yield\n\n    def process_spider_output_async(self, response, result):\n        yield\n\n\nclass UniversalMiddlewareBothAsync:\n    async def process_spider_output(self, response, result):\n        yield\n\n    async def process_spider_output_async(self, response, result):\n        yield\n\n\nclass TestUniversalMiddlewareManager:\n    @pytest.fixture\n    def crawler(self) -> Crawler:\n        return get_crawler(Spider)\n\n    @pytest.fixture\n    def mwman(self, crawler: Crawler) -> SpiderMiddlewareManager:\n        return SpiderMiddlewareManager.from_crawler(crawler)\n\n    def test_simple_mw(self, mwman: SpiderMiddlewareManager) -> None:\n        mw = ProcessSpiderOutputSimpleMiddleware()\n        mwman._add_middleware(mw)\n        assert (\n            mwman.methods[\"process_spider_output\"][0] == mw.process_spider_output  # pylint: disable=comparison-with-callable\n        )\n\n    def test_async_mw(self, mwman: SpiderMiddlewareManager) -> None:\n        mw = ProcessSpiderOutputAsyncGenMiddleware()\n        mwman._add_middleware(mw)\n        assert (\n            mwman.methods[\"process_spider_output\"][0] == mw.process_spider_output  # pylint: disable=comparison-with-callable\n        )\n\n    def test_universal_mw(self, mwman: SpiderMiddlewareManager) -> None:\n        mw = ProcessSpiderOutputUniversalMiddleware()\n        mwman._add_middleware(mw)\n        assert mwman.methods[\"process_spider_output\"][0] == (\n            mw.process_spider_output,\n            mw.process_spider_output_async,\n        )\n\n    def test_universal_mw_no_sync(\n        self, mwman: SpiderMiddlewareManager, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        mwman._add_middleware(UniversalMiddlewareNoSync())\n        assert (\n            \"UniversalMiddlewareNoSync has process_spider_output_async\"\n            \" without process_spider_output\" in caplog.text\n        )\n        assert mwman.methods[\"process_spider_output\"][0] is None\n\n    def test_universal_mw_both_sync(\n        self, mwman: SpiderMiddlewareManager, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        mw = UniversalMiddlewareBothSync()\n        mwman._add_middleware(mw)\n        assert (\n            \"UniversalMiddlewareBothSync.process_spider_output_async \"\n            \"is not an async generator function\" in caplog.text\n        )\n        assert (\n            mwman.methods[\"process_spider_output\"][0] == mw.process_spider_output  # pylint: disable=comparison-with-callable\n        )\n", "n_tokens": 1182, "byte_len": 5431, "file_sha1": "7ef5935fb8c8204eafb518e15886e16990a8f08a", "start_line": 306, "end_line": 460}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py", "rel_path": "tests/test_spidermiddleware.py", "module": "tests.test_spidermiddleware", "ext": "py", "chunk_number": 4, "symbols": ["test_universal_mw_both_async", "_callback", "TestBuiltinMiddlewareSimple", "TestBuiltinMiddlewareAsyncGen", "TestProcessSpiderException", "async", "process", "spider", "test", "builtin", "simple", "exc", "downgrade", "generator", "asyncgen", "deferred", "from", "universal", "invalid", "output", "spide", "middlewares", "ite", "type", "upgrade", "get", "crawler", "pytest", "none", "methods", "setup_method", "scrape_func", "process_spider_input", "process_spider_output", "process_spider_exception", "_construct_mw_setting", "process_spider_output_async", "mwman", "test_simple_mw", "test_async_mw", "test_universal_mw", "test_universal_mw_no_sync", "test_universal_mw_both_sync", "TestSpiderMiddleware", "TestProcessSpiderInputInvalidOutput", "InvalidProcessSpiderInputMiddleware", "TestProcessSpiderOutputInvalidOutput", "InvalidProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionInvalidOutput", "InvalidProcessSpiderOutputExceptionMiddleware"], "ast_kind": "class_or_type", "text": "    def test_universal_mw_both_async(\n        self, mwman: SpiderMiddlewareManager, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        mwman._add_middleware(UniversalMiddlewareBothAsync())\n        assert (\n            \"UniversalMiddlewareBothAsync.process_spider_output \"\n            \"is an async generator function while process_spider_output_async exists\"\n            in caplog.text\n        )\n        assert mwman.methods[\"process_spider_output\"][0] is None\n\n\nclass TestBuiltinMiddlewareSimple(TestBaseAsyncSpiderMiddleware):\n    ITEM_TYPE = dict\n    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n\n    async def _get_middleware_result(\n        self, *mw_classes: type[Any], start_index: int | None = None\n    ) -> Any:\n        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n        self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES\": setting})\n        self.crawler.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n        return await self.mwman.scrape_response_async(\n            self._scrape_func, self.response, self.request\n        )\n\n    @deferred_f_from_coro_f\n    async def test_just_builtin(self):\n        await self._test_simple_base()\n\n    @deferred_f_from_coro_f\n    async def test_builtin_simple(self):\n        await self._test_simple_base(self.MW_SIMPLE, start_index=1000)\n\n    @deferred_f_from_coro_f\n    async def test_builtin_async(self):\n        \"\"\"Upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)\n\n    @deferred_f_from_coro_f\n    async def test_builtin_universal(self):\n        await self._test_simple_base(self.MW_UNIVERSAL, start_index=1000)\n\n    @deferred_f_from_coro_f\n    async def test_simple_builtin(self):\n        await self._test_simple_base(self.MW_SIMPLE)\n\n    @deferred_f_from_coro_f\n    async def test_async_builtin(self):\n        \"\"\"Upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN)\n\n    @deferred_f_from_coro_f\n    async def test_universal_builtin(self):\n        await self._test_simple_base(self.MW_UNIVERSAL)\n\n\nclass TestBuiltinMiddlewareAsyncGen(TestBuiltinMiddlewareSimple):\n    async def _callback(self) -> Any:\n        for item in super()._callback():\n            yield item\n\n    @deferred_f_from_coro_f\n    async def test_just_builtin(self):\n        await self._test_asyncgen_base()\n\n    @deferred_f_from_coro_f\n    async def test_builtin_simple(self):\n        \"\"\"Downgrade\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, downgrade=True, start_index=1000)\n\n    @deferred_f_from_coro_f\n    async def test_builtin_async(self):\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)\n\n    @deferred_f_from_coro_f\n    async def test_builtin_universal(self):\n        await self._test_asyncgen_base(self.MW_UNIVERSAL, start_index=1000)\n\n    @deferred_f_from_coro_f\n    async def test_simple_builtin(self):\n        \"\"\"Downgrade\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, downgrade=True)\n\n    @deferred_f_from_coro_f\n    async def test_async_builtin(self):\n        await self._test_asyncgen_base(self.MW_ASYNCGEN)\n\n    @deferred_f_from_coro_f\n    async def test_universal_builtin(self):\n        await self._test_asyncgen_base(self.MW_UNIVERSAL)\n\n\nclass TestProcessSpiderException(TestBaseAsyncSpiderMiddleware):\n    ITEM_TYPE = dict\n    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n    MW_EXC_SIMPLE = ProcessSpiderExceptionSimpleIterableMiddleware\n    MW_EXC_ASYNCGEN = ProcessSpiderExceptionAsyncIteratorMiddleware\n\n    def _callback(self) -> Any:\n        1 / 0\n\n    async def _test_asyncgen_nodowngrade(self, *mw_classes: type[Any]) -> None:\n        with pytest.raises(\n            _InvalidOutput, match=\"Async iterable returned from .+ cannot be downgraded\"\n        ):\n            await self._get_middleware_result(*mw_classes)\n\n    @deferred_f_from_coro_f\n    async def test_exc_simple(self):\n        \"\"\"Simple exc mw\"\"\"\n        await self._test_simple_base(self.MW_EXC_SIMPLE)\n\n    @deferred_f_from_coro_f\n    async def test_exc_async(self):\n        \"\"\"Async exc mw\"\"\"\n        await self._test_asyncgen_base(self.MW_EXC_ASYNCGEN)\n\n    @deferred_f_from_coro_f\n    async def test_exc_simple_simple(self):\n        \"\"\"Simple exc mw -> simple output mw\"\"\"\n        await self._test_simple_base(self.MW_SIMPLE, self.MW_EXC_SIMPLE)\n\n    @deferred_f_from_coro_f\n    async def test_exc_async_async(self):\n        \"\"\"Async exc mw -> async output mw\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_ASYNCGEN)\n\n    @deferred_f_from_coro_f\n    async def test_exc_simple_async(self):\n        \"\"\"Simple exc mw -> async output mw; upgrade\"\"\"\n        await self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_SIMPLE)\n", "n_tokens": 1231, "byte_len": 4999, "file_sha1": "7ef5935fb8c8204eafb518e15886e16990a8f08a", "start_line": 461, "end_line": 598}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware.py", "rel_path": "tests/test_spidermiddleware.py", "module": "tests.test_spidermiddleware", "ext": "py", "chunk_number": 5, "symbols": ["process_spider_input", "process_spider_output", "process_spider_exception", "TestDeprecatedSpiderArg", "DeprecatedSpiderArgMiddleware", "mwman", "async", "await", "result", "requires", "argument", "spider", "scrapy", "deprecation", "warns", "exception", "supported", "return", "test", "asyncgen", "simple", "cannot", "exc", "deferred", "from", "deprecated", "class", "work", "process", "output", "setup_method", "scrape_func", "_construct_mw_setting", "_callback", "process_spider_output_async", "crawler", "test_simple_mw", "test_async_mw", "test_universal_mw", "test_universal_mw_no_sync", "test_universal_mw_both_sync", "test_universal_mw_both_async", "TestSpiderMiddleware", "TestProcessSpiderInputInvalidOutput", "InvalidProcessSpiderInputMiddleware", "TestProcessSpiderOutputInvalidOutput", "InvalidProcessSpiderOutputMiddleware", "TestProcessSpiderExceptionInvalidOutput", "InvalidProcessSpiderOutputExceptionMiddleware", "RaiseExceptionProcessSpiderOutputMiddleware"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_exc_async_simple(self):\n        \"\"\"Async exc mw -> simple output mw; cannot work as downgrading is not supported\"\"\"\n        await self._test_asyncgen_nodowngrade(self.MW_SIMPLE, self.MW_EXC_ASYNCGEN)\n\n\nclass TestDeprecatedSpiderArg(TestSpiderMiddleware):\n    @deferred_f_from_coro_f\n    async def test_deprecated_spider_arg(self):\n        class DeprecatedSpiderArgMiddleware:\n            def process_spider_input(self, response, spider):\n                return None\n\n            def process_spider_output(self, response, result, spider):\n                1 / 0\n\n            def process_spider_exception(self, response, exception, spider):\n                return []\n\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"process_spider_input\\(\\) requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"process_spider_output\\(\\) requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"process_spider_exception\\(\\) requires a spider argument\",\n            ),\n        ):\n            self.mwman._add_middleware(DeprecatedSpiderArgMiddleware())\n        await self._scrape_response()\n", "n_tokens": 278, "byte_len": 1336, "file_sha1": "7ef5935fb8c8204eafb518e15886e16990a8f08a", "start_line": 599, "end_line": 634}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scrapy__getattr__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scrapy__getattr__.py", "rel_path": "tests/test_scrapy__getattr__.py", "module": "tests.test_scrapy__getattr__", "ext": "py", "chunk_number": 1, "symbols": ["test_deprecated_twisted_version", "test_deprecated_concurrent_requests_per_ip_attribute", "test", "deprecated", "module", "warns", "twisted", "name", "concurren", "request", "instead", "with", "scrapy", "record", "pylint", "version", "warnings", "plc0415", "true", "noqa", "from", "settings", "default", "assert", "tuple", "isinstance", "attribute", "none", "catch", "import", "disable", "message", "args"], "ast_kind": "function_or_method", "text": "import warnings\n\n\ndef test_deprecated_twisted_version():\n    with warnings.catch_warnings(record=True) as warns:\n        from scrapy import (  # noqa: PLC0415  # pylint: disable=no-name-in-module\n            twisted_version,\n        )\n\n        assert twisted_version is not None\n        assert isinstance(twisted_version, tuple)\n        assert (\n            \"The scrapy.twisted_version attribute is deprecated, use twisted.version instead\"\n            in warns[0].message.args\n        )\n\n\ndef test_deprecated_concurrent_requests_per_ip_attribute():\n    with warnings.catch_warnings(record=True) as warns:\n        from scrapy.settings.default_settings import (  # noqa: PLC0415\n            CONCURRENT_REQUESTS_PER_IP,\n        )\n\n        assert CONCURRENT_REQUESTS_PER_IP is not None\n        assert isinstance(CONCURRENT_REQUESTS_PER_IP, int)\n        assert (\n            \"The scrapy.settings.default_settings.CONCURRENT_REQUESTS_PER_IP attribute is deprecated, use scrapy.settings.default_settings.CONCURRENT_REQUESTS_PER_DOMAIN instead.\"\n            in warns[0].message.args\n        )\n", "n_tokens": 214, "byte_len": 1085, "file_sha1": "43018e217a7a82e95a51c1c56a667438fb601a54", "start_line": 1, "end_line": 30}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http11.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http11.py", "rel_path": "tests/test_downloader_handler_twisted_http11.py", "module": "tests.test_downloader_handler_twisted_http11", "ext": "py", "chunk_number": 1, "symbols": ["download_handler_cls", "settings_dict", "HTTP11DownloadHandlerMixin", "TestHttp11", "TestHttps11", "TestSimpleHttps", "TestHttps11WrongHostname", "TestHttps11InvalidDNSId", "TestHttps11InvalidDNSPattern", "TestHttps11CustomCiphers", "TestHttp11WithCrawler", "TestHttp11Proxy", "test", "https", "downloader", "settings", "dict", "core", "pass", "http", "http11", "typing", "htt", "download", "property", "return", "annotations", "class", "simple", "scrapy", "future", "typ", "checking", "from", "handler", "none", "handlers", "type", "import", "self", "default", "tests"], "ast_kind": "class_or_type", "text": "\"\"\"Tests for scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\nfrom tests.test_downloader_handlers_http_base import (\n    TestHttp11Base,\n    TestHttpProxyBase,\n    TestHttps11Base,\n    TestHttpsCustomCiphersBase,\n    TestHttpsInvalidDNSIdBase,\n    TestHttpsInvalidDNSPatternBase,\n    TestHttpsWrongHostnameBase,\n    TestHttpWithCrawlerBase,\n    TestSimpleHttpsBase,\n)\n\nif TYPE_CHECKING:\n    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n\n\nclass HTTP11DownloadHandlerMixin:\n    @property\n    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n        return HTTP11DownloadHandler\n\n\nclass TestHttp11(HTTP11DownloadHandlerMixin, TestHttp11Base):\n    pass\n\n\nclass TestHttps11(HTTP11DownloadHandlerMixin, TestHttps11Base):\n    pass\n\n\nclass TestSimpleHttps(HTTP11DownloadHandlerMixin, TestSimpleHttpsBase):\n    pass\n\n\nclass TestHttps11WrongHostname(HTTP11DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n    pass\n\n\nclass TestHttps11InvalidDNSId(HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n    pass\n\n\nclass TestHttps11InvalidDNSPattern(\n    HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase\n):\n    pass\n\n\nclass TestHttps11CustomCiphers(HTTP11DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n    pass\n\n\nclass TestHttp11WithCrawler(TestHttpWithCrawlerBase):\n    @property\n    def settings_dict(self) -> dict[str, Any] | None:\n        return None  # default handler settings\n\n\nclass TestHttp11Proxy(HTTP11DownloadHandlerMixin, TestHttpProxyBase):\n    pass\n", "n_tokens": 376, "byte_len": 1672, "file_sha1": "2506ea8faf7e90df18cf52ad1d49bd4d44272bfd", "start_line": 1, "end_line": 68}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_stats.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_stats.py", "rel_path": "tests/test_stats.py", "module": "tests.test_stats", "ext": "py", "chunk_number": 1, "symbols": ["crawler", "spider", "test_core_stats_default_stats_collector", "test_core_stats_dummy_stats_collector", "test_collector", "test_dummy_collector", "test_deprecated_spider_arg", "TestCoreStatsExtension", "TestStatsCollector", "stats", "inc", "value", "mock", "test", "test3", "deprecated", "spiders", "core", "future", "typ", "checking", "get", "pytest", "collector", "passing", "anything", "none", "fixture", "return", "finish", "reason", "elapsed", "time", "datetime", "default", "match", "argument", "typing", "annotations", "class", "create", "item", "dropped", "from", "corestats", "dummy", "exceptions", "response", "received", "self"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.extensions.corestats import CoreStats\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import DummyStatsCollector, StatsCollector\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider)\n\n\n@pytest.fixture\ndef spider(crawler: Crawler) -> Spider:\n    return crawler._create_spider(\"foo\")\n\n\nclass TestCoreStatsExtension:\n    @mock.patch(\"scrapy.extensions.corestats.datetime\")\n    def test_core_stats_default_stats_collector(\n        self, mock_datetime: mock.Mock, crawler: Crawler, spider: Spider\n    ) -> None:\n        fixed_datetime = datetime(2019, 12, 1, 11, 38)\n        mock_datetime.now = mock.Mock(return_value=fixed_datetime)\n        crawler.stats = StatsCollector(crawler)\n        ext = CoreStats.from_crawler(crawler)\n        ext.spider_opened(spider)\n        ext.item_scraped({}, spider)\n        ext.response_received(spider)\n        ext.item_dropped({}, spider, ZeroDivisionError())\n        ext.spider_closed(spider, \"finished\")\n        assert ext.stats._stats == {\n            \"start_time\": fixed_datetime,\n            \"finish_time\": fixed_datetime,\n            \"item_scraped_count\": 1,\n            \"response_received_count\": 1,\n            \"item_dropped_count\": 1,\n            \"item_dropped_reasons_count/ZeroDivisionError\": 1,\n            \"finish_reason\": \"finished\",\n            \"elapsed_time_seconds\": 0.0,\n        }\n\n    def test_core_stats_dummy_stats_collector(\n        self, crawler: Crawler, spider: Spider\n    ) -> None:\n        crawler.stats = DummyStatsCollector(crawler)\n        ext = CoreStats.from_crawler(crawler)\n        ext.spider_opened(spider)\n        ext.item_scraped({}, spider)\n        ext.response_received(spider)\n        ext.item_dropped({}, spider, ZeroDivisionError())\n        ext.spider_closed(spider, \"finished\")\n        assert ext.stats._stats == {}\n\n\nclass TestStatsCollector:\n    def test_collector(self, crawler: Crawler) -> None:\n        stats = StatsCollector(crawler)\n        assert stats.get_stats() == {}\n        assert stats.get_value(\"anything\") is None\n        assert stats.get_value(\"anything\", \"default\") == \"default\"\n        stats.set_value(\"test\", \"value\")\n        assert stats.get_stats() == {\"test\": \"value\"}\n        stats.set_value(\"test2\", 23)\n        assert stats.get_stats() == {\"test\": \"value\", \"test2\": 23}\n        assert stats.get_value(\"test2\") == 23\n        stats.inc_value(\"test2\")\n        assert stats.get_value(\"test2\") == 24\n        stats.inc_value(\"test2\", 6)\n        assert stats.get_value(\"test2\") == 30\n        stats.max_value(\"test2\", 6)\n        assert stats.get_value(\"test2\") == 30\n        stats.max_value(\"test2\", 40)\n        assert stats.get_value(\"test2\") == 40\n        stats.max_value(\"test3\", 1)\n        assert stats.get_value(\"test3\") == 1\n        stats.min_value(\"test2\", 60)\n        assert stats.get_value(\"test2\") == 40\n        stats.min_value(\"test2\", 35)\n        assert stats.get_value(\"test2\") == 35\n        stats.min_value(\"test4\", 7)\n        assert stats.get_value(\"test4\") == 7\n\n    def test_dummy_collector(self, crawler: Crawler) -> None:\n        stats = DummyStatsCollector(crawler)\n        assert stats.get_stats() == {}\n        assert stats.get_value(\"anything\") is None\n        assert stats.get_value(\"anything\", \"default\") == \"default\"\n        stats.set_value(\"test\", \"value\")\n        stats.inc_value(\"v1\")\n        stats.max_value(\"v2\", 100)\n        stats.min_value(\"v3\", 100)\n        stats.open_spider()\n        stats.set_value(\"test\", \"value\")\n        assert stats.get_stats() == {}\n\n    def test_deprecated_spider_arg(self, crawler: Crawler, spider: Spider) -> None:\n        stats = StatsCollector(crawler)\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"Passing a 'spider' argument to StatsCollector.set_value\\(\\) is deprecated\",\n        ):\n            stats.set_value(\"test\", \"value\", spider=spider)\n        assert stats.get_stats() == {\"test\": \"value\"}\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"Passing a 'spider' argument to StatsCollector.get_stats\\(\\) is deprecated\",\n        ):\n            assert stats.get_stats(spider) == {\"test\": \"value\"}\n", "n_tokens": 1073, "byte_len": 4473, "file_sha1": "8fbe7df061b50d0b2a0c114dc0332acd58e4a7fa", "start_line": 1, "end_line": 121}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "Base", "TestLinkExtractorBase", "get", "testdata", "test", "extract", "sample", "sample3", "lib", "w3lib", "lxml", "link", "urls", "page", "url", "future", "lxmlhtml", "pytest", "google", "href", "canonicalize", "test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url", "test_encoded_url_in_restricted_xpath", "test_ignored_extensions", "test_process_value", "process_value", "test_base_url_with_restrict_xpaths", "test_attrs", "test_tags", "test_tags_attrs", "test_xhtml"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport pickle\nimport re\n\nimport pytest\nfrom packaging.version import Version\nfrom w3lib import __version__ as w3lib_version\n\nfrom scrapy.http import HtmlResponse, XmlResponse\nfrom scrapy.link import Link\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\nfrom tests import get_testdata\n\n\n# a hack to skip base class tests in pytest\nclass Base:\n    class TestLinkExtractorBase:\n        extractor_cls: type | None = None\n\n        def setup_method(self):\n            body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n            self.response = HtmlResponse(url=\"http://example.com/index\", body=body)\n\n        def test_urls_type(self):\n            \"\"\"Test that the resulting urls are str objects\"\"\"\n            lx = self.extractor_cls()\n            assert all(\n                isinstance(link.url, str) for link in lx.extract_links(self.response)\n            )\n\n        def test_extract_all_links(self):\n            lx = self.extractor_cls()\n            page4_url = \"http://example.com/page%204.html\"\n\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html#foo\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n                Link(url=\"http://www.google.com/something\", text=\"\"),\n                Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n                Link(url=page4_url, text=\"href with whitespaces\"),\n            ]\n\n        def test_extract_filter_allow(self):\n            lx = self.extractor_cls(allow=(\"sample\",))\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html#foo\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n            ]\n\n        def test_extract_filter_allow_with_duplicates(self):\n            lx = self.extractor_cls(allow=(\"sample\",), unique=False)\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html\",\n                    text=\"sample 3 repetition\",\n                ),\n                Link(\n                    url=\"http://example.com/sample3.html\",\n                    text=\"sample 3 repetition\",\n                ),\n                Link(\n                    url=\"http://example.com/sample3.html#foo\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n            ]\n\n        def test_extract_filter_allow_with_duplicates_canonicalize(self):\n            lx = self.extractor_cls(allow=(\"sample\",), unique=False, canonicalize=True)\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html\",\n                    text=\"sample 3 repetition\",\n                ),\n                Link(\n                    url=\"http://example.com/sample3.html\",\n                    text=\"sample 3 repetition\",\n                ),\n                Link(\n                    url=\"http://example.com/sample3.html\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n            ]\n\n        def test_extract_filter_allow_no_duplicates_canonicalize(self):\n            lx = self.extractor_cls(allow=(\"sample\",), unique=True, canonicalize=True)\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n            ]\n\n        def test_extract_filter_allow_and_deny(self):\n            lx = self.extractor_cls(allow=(\"sample\",), deny=(\"3\",))\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n            ]\n\n        def test_extract_filter_allowed_domains(self):\n            lx = self.extractor_cls(allow_domains=(\"google.com\",))\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://www.google.com/something\", text=\"\"),\n            ]\n", "n_tokens": 1050, "byte_len": 5230, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 1, "end_line": 121}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 2, "symbols": ["test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "encoding", "sample", "sample3", "nofollow", "nofollow2", "about", "situations", "google", "href", "windows", "test", "matches", "item", "html", "follow", "http", "class", "choose", "blah", "example", "restrict", "sample1", "allow", "domains", "somepage", "setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url", "test_encoded_url_in_restricted_xpath", "test_ignored_extensions", "test_process_value", "process_value", "test_base_url_with_restrict_xpaths"], "ast_kind": "function_or_method", "text": "        def test_extraction_using_single_values(self):\n            \"\"\"Test the extractor's behaviour among different situations\"\"\"\n\n            lx = self.extractor_cls(allow=\"sample\")\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html#foo\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n            ]\n\n            lx = self.extractor_cls(allow=\"sample\", deny=\"3\")\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n            ]\n\n            lx = self.extractor_cls(allow_domains=\"google.com\")\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://www.google.com/something\", text=\"\"),\n            ]\n\n            lx = self.extractor_cls(deny_domains=\"example.com\")\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://www.google.com/something\", text=\"\"),\n            ]\n\n        def test_nofollow(self):\n            \"\"\"Test the extractor's behaviour for links with rel='nofollow'\"\"\"\n\n            html = b\"\"\"<html><head><title>Page title</title></head>\n            <body>\n            <div class='links'>\n            <p><a href=\"/about.html\">About us</a></p>\n            </div>\n            <div>\n            <p><a href=\"/follow.html\">Follow this link</a></p>\n            </div>\n            <div>\n            <p><a href=\"/nofollow.html\" rel=\"nofollow\">Dont follow this one</a></p>\n            </div>\n            <div>\n            <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n            </div>\n            <div>\n            <p><a href=\"http://google.com/something\" rel=\"external nofollow\">External link not to follow</a></p>\n            </div>\n            </body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.org/about.html\", text=\"About us\"),\n                Link(url=\"http://example.org/follow.html\", text=\"Follow this link\"),\n                Link(\n                    url=\"http://example.org/nofollow.html\",\n                    text=\"Dont follow this one\",\n                    nofollow=True,\n                ),\n                Link(\n                    url=\"http://example.org/nofollow2.html\",\n                    text=\"Choose to follow or not\",\n                ),\n                Link(\n                    url=\"http://google.com/something\",\n                    text=\"External link not to follow\",\n                    nofollow=True,\n                ),\n            ]\n\n        def test_matches(self):\n            url1 = \"http://lotsofstuff.com/stuff1/index\"\n            url2 = \"http://evenmorestuff.com/uglystuff/index\"\n\n            lx = self.extractor_cls(allow=(r\"stuff1\",))\n            assert lx.matches(url1)\n            assert not lx.matches(url2)\n\n            lx = self.extractor_cls(deny=(r\"uglystuff\",))\n            assert lx.matches(url1)\n            assert not lx.matches(url2)\n\n            lx = self.extractor_cls(allow_domains=(\"evenmorestuff.com\",))\n            assert not lx.matches(url1)\n            assert lx.matches(url2)\n\n            lx = self.extractor_cls(deny_domains=(\"lotsofstuff.com\",))\n            assert not lx.matches(url1)\n            assert lx.matches(url2)\n\n            lx = self.extractor_cls(\n                allow=[\"blah1\"],\n                deny=[\"blah2\"],\n                allow_domains=[\"blah1.com\"],\n                deny_domains=[\"blah2.com\"],\n            )\n            assert lx.matches(\"http://blah1.com/blah1\")\n            assert not lx.matches(\"http://blah1.com/blah2\")\n            assert not lx.matches(\"http://blah2.com/blah1\")\n            assert not lx.matches(\"http://blah2.com/blah2\")\n\n        def test_restrict_xpaths(self):\n            lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n            ]\n\n        def test_restrict_xpaths_encoding(self):\n            \"\"\"Test restrict_xpaths with encodings\"\"\"\n            html = b\"\"\"<html><head><title>Page title</title></head>\n            <body><p><a href=\"item/12.html\">Item 12</a></p>\n            <div class='links'>\n            <p><a href=\"/about.html\">About us\\xa3</a></p>\n            </div>\n            <div>\n            <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n            </div>\n            </body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://example.org/somepage/index.html\",\n                body=html,\n                encoding=\"windows-1252\",\n            )\n\n            lx = self.extractor_cls(restrict_xpaths=\"//div[@class='links']\")\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.org/about.html\", text=\"About us\\xa3\")\n            ]\n", "n_tokens": 1192, "byte_len": 5464, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 122, "end_line": 254}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 3, "symbols": ["test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url", "test_encoded_url_in_restricted_xpath", "test_ignored_extensions", "test_process_value", "process_value", "encoding", "gb18030", "sample", "sample3", "test", "restrict", "about", "cause", "false", "encoded", "href", "deny", "extensions", "windows", "denied", "none", "nofollow", "html", "group", "http", "setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "test_base_url_with_restrict_xpaths", "test_attrs", "test_tags", "test_tags_attrs", "test_xhtml", "test_link_wrong_href"], "ast_kind": "function_or_method", "text": "        def test_restrict_xpaths_with_html_entities(self):\n            html = b'<html><body><p><a href=\"/&hearts;/you?c=&euro;\">text</a></p></body></html>'\n            response = HtmlResponse(\n                \"http://example.org/somepage/index.html\",\n                body=html,\n                encoding=\"iso8859-15\",\n            )\n            links = self.extractor_cls(restrict_xpaths=\"//p\").extract_links(response)\n            assert links == [\n                Link(url=\"http://example.org/%E2%99%A5/you?c=%A4\", text=\"text\")\n            ]\n\n        def test_restrict_xpaths_concat_in_handle_data(self):\n            \"\"\"html entities cause SGMLParser to call handle_data hook twice\"\"\"\n            body = b\"\"\"<html><body><div><a href=\"/foo\">&gt;\\xbe\\xa9&lt;\\xb6\\xab</a></body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org\", body=body, encoding=\"gb18030\")\n            lx = self.extractor_cls(restrict_xpaths=\"//div\")\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://example.org/foo\",\n                    text=\">\\u4eac<\\u4e1c\",\n                    fragment=\"\",\n                    nofollow=False,\n                )\n            ]\n\n        def test_restrict_css(self):\n            lx = self.extractor_cls(restrict_css=(\"#subwrapper a\",))\n            assert lx.extract_links(self.response) == [\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\")\n            ]\n\n        def test_restrict_css_and_restrict_xpaths_together(self):\n            lx = self.extractor_cls(\n                restrict_xpaths=('//div[@id=\"subwrapper\"]',),\n                restrict_css=(\"#subwrapper + a\",),\n            )\n            assert list(lx.extract_links(self.response)) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n            ]\n\n        def test_area_tag_with_unicode_present(self):\n            body = b\"\"\"<html><body>\\xbe\\xa9<map><area href=\"http://example.org/foo\" /></map></body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org\", body=body, encoding=\"utf-8\")\n            lx = self.extractor_cls()\n            lx.extract_links(response)\n            lx.extract_links(response)\n            lx.extract_links(response)\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://example.org/foo\",\n                    text=\"\",\n                    fragment=\"\",\n                    nofollow=False,\n                )\n            ]\n\n        def test_encoded_url(self):\n            body = b\"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://known.fm/AC%2FDC/\", body=body, encoding=\"utf8\"\n            )\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://known.fm/AC%2FDC/?page=2\",\n                    text=\"BinB\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n            ]\n\n        def test_encoded_url_in_restricted_xpath(self):\n            body = b\"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://known.fm/AC%2FDC/\", body=body, encoding=\"utf8\"\n            )\n            lx = self.extractor_cls(restrict_xpaths=\"//div\")\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://known.fm/AC%2FDC/?page=2\",\n                    text=\"BinB\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n            ]\n\n        def test_ignored_extensions(self):\n            # jpg is ignored by default\n            html = b\"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n            response = HtmlResponse(\"http://example.org/\", body=html)\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.org/page.html\", text=\"asd\"),\n            ]\n\n            # override denied extensions\n            lx = self.extractor_cls(deny_extensions=[\"html\"])\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.org/photo.jpg\"),\n            ]\n\n        def test_process_value(self):\n            \"\"\"Test restrict_xpaths with encodings\"\"\"\n            html = b\"\"\"\n<a href=\"javascript:goToPage('../other/page.html','photo','width=600,height=540,scrollbars'); return false\">Text</a>\n<a href=\"/about.html\">About us</a>\n            \"\"\"\n            response = HtmlResponse(\n                \"http://example.org/somepage/index.html\",\n                body=html,\n                encoding=\"windows-1252\",\n            )\n\n            def process_value(value):\n                m = re.search(r\"javascript:goToPage\\('(.*?)'\", value)\n                return m.group(1) if m else None\n\n            lx = self.extractor_cls(process_value=process_value)\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.org/other/page.html\", text=\"Text\")\n            ]\n", "n_tokens": 1141, "byte_len": 5253, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 255, "end_line": 379}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 4, "symbols": ["test_base_url_with_restrict_xpaths", "test_attrs", "test_tags", "test_tags_attrs", "fragment", "test", "attrs", "text", "false", "sample", "sample3", "inner", "tags", "base", "item", "link", "innertag", "with", "page", "url", "example", "head", "body", "sample1", "repetition", "index", "somepage", "extract", "links", "sample2", "setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url"], "ast_kind": "function_or_method", "text": "        def test_base_url_with_restrict_xpaths(self):\n            html = b\"\"\"<html><head><title>Page title</title><base href=\"http://otherdomain.com/base/\" /></head>\n            <body><p><a href=\"item/12.html\">Item 12</a></p>\n            </body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n            lx = self.extractor_cls(restrict_xpaths=\"//p\")\n            assert lx.extract_links(response) == [\n                Link(url=\"http://otherdomain.com/base/item/12.html\", text=\"Item 12\")\n            ]\n\n        def test_attrs(self):\n            lx = self.extractor_cls(attrs=\"href\")\n            page4_url = \"http://example.com/page%204.html\"\n\n            assert lx.extract_links(self.response) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html#foo\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n                Link(url=\"http://www.google.com/something\", text=\"\"),\n                Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n                Link(url=page4_url, text=\"href with whitespaces\"),\n            ]\n\n            lx = self.extractor_cls(\n                attrs=(\"href\", \"src\"), tags=(\"a\", \"area\", \"img\"), deny_extensions=()\n            )\n            assert lx.extract_links(self.response) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample2.jpg\", text=\"\"),\n                Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                Link(\n                    url=\"http://example.com/sample3.html#foo\",\n                    text=\"sample 3 repetition with fragment\",\n                ),\n                Link(url=\"http://www.google.com/something\", text=\"\"),\n                Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n                Link(url=page4_url, text=\"href with whitespaces\"),\n            ]\n\n            lx = self.extractor_cls(attrs=None)\n            assert lx.extract_links(self.response) == []\n\n        def test_tags(self):\n            html = (\n                b'<html><area href=\"sample1.html\"></area>'\n                b'<a href=\"sample2.html\">sample 2</a><img src=\"sample2.jpg\"/></html>'\n            )\n            response = HtmlResponse(\"http://example.com/index.html\", body=html)\n\n            lx = self.extractor_cls(tags=None)\n            assert lx.extract_links(response) == []\n\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n            ]\n\n            lx = self.extractor_cls(tags=\"area\")\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.com/sample1.html\", text=\"\"),\n            ]\n\n            lx = self.extractor_cls(tags=\"a\")\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n            ]\n\n            lx = self.extractor_cls(\n                tags=(\"a\", \"img\"), attrs=(\"href\", \"src\"), deny_extensions=()\n            )\n            assert lx.extract_links(response) == [\n                Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                Link(url=\"http://example.com/sample2.jpg\", text=\"\"),\n            ]\n\n        def test_tags_attrs(self):\n            html = b\"\"\"\n            <html><body>\n            <div id=\"item1\" data-url=\"get?id=1\"><a href=\"#\">Item 1</a></div>\n            <div id=\"item2\" data-url=\"get?id=2\"><a href=\"#\">Item 2</a></div>\n            </body></html>\n            \"\"\"\n            response = HtmlResponse(\"http://example.com/index.html\", body=html)\n\n            lx = self.extractor_cls(tags=\"div\", attrs=\"data-url\")\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://example.com/get?id=1\",\n                    text=\"Item 1\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.com/get?id=2\",\n                    text=\"Item 2\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n            ]\n\n            lx = self.extractor_cls(tags=(\"div\",), attrs=(\"data-url\",))\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://example.com/get?id=1\",\n                    text=\"Item 1\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.com/get?id=2\",\n                    text=\"Item 2\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n            ]\n", "n_tokens": 1091, "byte_len": 5165, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 380, "end_line": 501}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 5, "symbols": ["test_xhtml", "test_link_wrong_href", "test_ftp_links", "test_pickle_extractor", "fragment", "encoding", "utf", "utf8", "dumps", "about", "text", "false", "loads", "xhtml", "doctype", "nofollow", "nofollow2", "test", "strict", "class", "link", "choose", "links", "blah", "example", "head", "body", "item", "item3", "xmlns", "setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url"], "ast_kind": "function_or_method", "text": "        def test_xhtml(self):\n            xhtml = b\"\"\"\n    <?xml version=\"1.0\"?>\n    <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n    <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n    <head>\n        <title>XHTML document title</title>\n    </head>\n    <body>\n        <div class='links'>\n        <p><a href=\"/about.html\">About us</a></p>\n        </div>\n        <div>\n        <p><a href=\"/follow.html\">Follow this link</a></p>\n        </div>\n        <div>\n        <p><a href=\"/nofollow.html\" rel=\"nofollow\">Dont follow this one</a></p>\n        </div>\n        <div>\n        <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n        </div>\n        <div>\n        <p><a href=\"http://google.com/something\" rel=\"external nofollow\">External link not to follow</a></p>\n        </div>\n    </body>\n    </html>\n            \"\"\"\n\n            response = HtmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://example.com/about.html\",\n                    text=\"About us\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.com/follow.html\",\n                    text=\"Follow this link\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.com/nofollow.html\",\n                    text=\"Dont follow this one\",\n                    fragment=\"\",\n                    nofollow=True,\n                ),\n                Link(\n                    url=\"http://example.com/nofollow2.html\",\n                    text=\"Choose to follow or not\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://google.com/something\",\n                    text=\"External link not to follow\",\n                    nofollow=True,\n                ),\n            ]\n\n            response = XmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"http://example.com/about.html\",\n                    text=\"About us\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.com/follow.html\",\n                    text=\"Follow this link\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.com/nofollow.html\",\n                    text=\"Dont follow this one\",\n                    fragment=\"\",\n                    nofollow=True,\n                ),\n                Link(\n                    url=\"http://example.com/nofollow2.html\",\n                    text=\"Choose to follow or not\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://google.com/something\",\n                    text=\"External link not to follow\",\n                    nofollow=True,\n                ),\n            ]\n\n        def test_link_wrong_href(self):\n            html = b\"\"\"\n            <a href=\"http://example.org/item1.html\">Item 1</a>\n            <a href=\"http://[example.org/item2.html\">Item 2</a>\n            <a href=\"http://example.org/item3.html\">Item 3</a>\n            \"\"\"\n            response = HtmlResponse(\"http://example.org/index.html\", body=html)\n            lx = self.extractor_cls()\n            assert list(lx.extract_links(response)) == [\n                Link(\n                    url=\"http://example.org/item1.html\",\n                    text=\"Item 1\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.org/item3.html\",\n                    text=\"Item 3\",\n                    nofollow=False,\n                ),\n            ]\n\n        def test_ftp_links(self):\n            body = b\"\"\"\n            <html><body>\n            <div><a href=\"ftp://www.external.com/\">An Item</a></div>\n            </body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://www.example.com/index.html\", body=body, encoding=\"utf8\"\n            )\n            lx = self.extractor_cls()\n            assert lx.extract_links(response) == [\n                Link(\n                    url=\"ftp://www.external.com/\",\n                    text=\"An Item\",\n                    fragment=\"\",\n                    nofollow=False,\n                ),\n            ]\n\n        def test_pickle_extractor(self):\n            lx = self.extractor_cls()\n            assert isinstance(pickle.loads(pickle.dumps(lx)), self.extractor_cls)\n", "n_tokens": 991, "byte_len": 4967, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 502, "end_line": 643}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 6, "symbols": ["test_link_extractor_aggregation", "test_link_wrong_href", "TestLxmlLinkExtractor", "used", "underlying", "implementation", "aggregates", "test", "text", "false", "lxml", "link", "class", "base", "when", "like", "actual", "links", "https", "example", "once", "body", "item", "item3", "index", "extract", "restrict", "css", "true", "restrictions", "setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url"], "ast_kind": "class_or_type", "text": "        def test_link_extractor_aggregation(self):\n            \"\"\"When a parameter like restrict_css is used, the underlying\n            implementation calls its internal link extractor once per selector\n            matching the specified restrictions, and then aggregates the\n            extracted links.\n\n            Test that aggregation respects the unique and canonicalize\n            parameters.\n            \"\"\"\n            # unique=True (default), canonicalize=False (default)\n            lx = self.extractor_cls(restrict_css=(\"div\",))\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            assert actual == [\n                Link(url=\"https://example.com/a\", text=\"a1\"),\n                Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                Link(url=\"https://example.com/b?b=2&a=1\", text=\"b2\"),\n            ]\n\n            # unique=True (default), canonicalize=True\n            lx = self.extractor_cls(restrict_css=(\"div\",), canonicalize=True)\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            assert actual == [\n                Link(url=\"https://example.com/a\", text=\"a1\"),\n                Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n            ]\n\n            # unique=False, canonicalize=False (default)\n            lx = self.extractor_cls(restrict_css=(\"div\",), unique=False)\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            assert actual == [\n                Link(url=\"https://example.com/a\", text=\"a1\"),\n                Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                Link(url=\"https://example.com/a\", text=\"a2\"),\n                Link(url=\"https://example.com/b?b=2&a=1\", text=\"b2\"),\n            ]\n\n            # unique=False, canonicalize=True\n            lx = self.extractor_cls(\n                restrict_css=(\"div\",), unique=False, canonicalize=True\n            )\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            assert actual == [\n                Link(url=\"https://example.com/a\", text=\"a1\"),\n                Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                Link(url=\"https://example.com/a\", text=\"a2\"),\n                Link(url=\"https://example.com/b?a=1&b=2\", text=\"b2\"),\n            ]\n\n\nclass TestLxmlLinkExtractor(Base.TestLinkExtractorBase):\n    extractor_cls = LxmlLinkExtractor\n\n    def test_link_wrong_href(self):\n        html = b\"\"\"\n        <a href=\"http://example.org/item1.html\">Item 1</a>\n        <a href=\"http://[example.org/item2.html\">Item 2</a>\n        <a href=\"http://example.org/item3.html\">Item 3</a>\n        \"\"\"\n        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n        lx = self.extractor_cls()\n        assert list(lx.extract_links(response)) == [\n            Link(url=\"http://example.org/item1.html\", text=\"Item 1\", nofollow=False),\n            Link(url=\"http://example.org/item3.html\", text=\"Item 3\", nofollow=False),\n        ]\n", "n_tokens": 1062, "byte_len": 4640, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 644, "end_line": 760}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_linkextractors.py", "rel_path": "tests/test_linkextractors.py", "module": "tests.test_linkextractors", "ext": "py", "chunk_number": 7, "symbols": ["test_link_restrict_text", "test_skip_bad_links", "test_link_allowed_is_false_with_empty_url", "test_link_allowed_is_false_with_bad_url_prefix", "test_link_allowed_is_false_with_missing_url_prefix", "text", "false", "unique", "inclusion", "test", "link", "complain", "lib", "w3lib", "lxml", "mark", "before", "skipif", "about", "version", "invalid", "port", "example", "regex", "allowed", "body", "value", "item", "item3", "index", "setup_method", "test_urls_type", "test_extract_all_links", "test_extract_filter_allow", "test_extract_filter_allow_with_duplicates", "test_extract_filter_allow_with_duplicates_canonicalize", "test_extract_filter_allow_no_duplicates_canonicalize", "test_extract_filter_allow_and_deny", "test_extract_filter_allowed_domains", "test_extraction_using_single_values", "test_nofollow", "test_matches", "test_restrict_xpaths", "test_restrict_xpaths_encoding", "test_restrict_xpaths_with_html_entities", "test_restrict_xpaths_concat_in_handle_data", "test_restrict_css", "test_restrict_css_and_restrict_xpaths_together", "test_area_tag_with_unicode_present", "test_encoded_url"], "ast_kind": "function_or_method", "text": "    def test_link_restrict_text(self):\n        html = b\"\"\"\n        <a href=\"http://example.org/item1.html\">Pic of a cat</a>\n        <a href=\"http://example.org/item2.html\">Pic of a dog</a>\n        <a href=\"http://example.org/item3.html\">Pic of a cow</a>\n        \"\"\"\n        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n        # Simple text inclusion test\n        lx = self.extractor_cls(restrict_text=\"dog\")\n        assert list(lx.extract_links(response)) == [\n            Link(\n                url=\"http://example.org/item2.html\",\n                text=\"Pic of a dog\",\n                nofollow=False,\n            ),\n        ]\n        # Unique regex test\n        lx = self.extractor_cls(restrict_text=r\"of.*dog\")\n        assert list(lx.extract_links(response)) == [\n            Link(\n                url=\"http://example.org/item2.html\",\n                text=\"Pic of a dog\",\n                nofollow=False,\n            ),\n        ]\n        # Multiple regex test\n        lx = self.extractor_cls(restrict_text=[r\"of.*dog\", r\"of.*cat\"])\n        assert list(lx.extract_links(response)) == [\n            Link(\n                url=\"http://example.org/item1.html\",\n                text=\"Pic of a cat\",\n                nofollow=False,\n            ),\n            Link(\n                url=\"http://example.org/item2.html\",\n                text=\"Pic of a dog\",\n                nofollow=False,\n            ),\n        ]\n\n    @pytest.mark.skipif(\n        Version(w3lib_version) < Version(\"2.0.0\"),\n        reason=(\n            \"Before w3lib 2.0.0, w3lib.url.safe_url_string would not complain \"\n            \"about an invalid port value.\"\n        ),\n    )\n    def test_skip_bad_links(self):\n        html = b\"\"\"\n        <a href=\"http://example.org:non-port\">Why would you do this?</a>\n        <a href=\"http://example.org/item2.html\">Good Link</a>\n        <a href=\"http://example.org/item3.html\">Good Link 2</a>\n        \"\"\"\n        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n        lx = self.extractor_cls()\n        assert list(lx.extract_links(response)) == [\n            Link(\n                url=\"http://example.org/item2.html\",\n                text=\"Good Link\",\n                nofollow=False,\n            ),\n            Link(\n                url=\"http://example.org/item3.html\",\n                text=\"Good Link 2\",\n                nofollow=False,\n            ),\n        ]\n\n    def test_link_allowed_is_false_with_empty_url(self):\n        bad_link = Link(\"\")\n        assert not LxmlLinkExtractor()._link_allowed(bad_link)\n\n    def test_link_allowed_is_false_with_bad_url_prefix(self):\n        bad_link = Link(\"htp://should_be_http.example\")\n        assert not LxmlLinkExtractor()._link_allowed(bad_link)\n\n    def test_link_allowed_is_false_with_missing_url_prefix(self):\n        bad_link = Link(\"should_have_prefix.example\")\n        assert not LxmlLinkExtractor()._link_allowed(bad_link)\n", "n_tokens": 659, "byte_len": 2923, "file_sha1": "8002923595671efcb1e3e1830f29f27c92ea7dee", "start_line": 761, "end_line": 840}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_runspider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_runspider.py", "rel_path": "tests/test_command_runspider.py", "module": "tests.test_command_runspider", "ext": "py", "chunk_number": 1, "symbols": ["_create_file", "runspider", "get_log", "test_runspider", "test_run_fail_spider", "test_run_good_spider", "test_runspider_log_level", "test_runspider_default_reactor", "test_runspider_dnscache_disabled", "parse", "test_runspider_log_short_names", "test_runspider_no_spider_found", "test_runspider_file_not_found", "test_runspider_unable_to_load", "test_start_errors", "TestRunSpiderCommand", "MySpider", "BadSpider", "encoding", "async", "oops", "test", "works", "log", "log1", "spider", "name", "run", "load", "spiders", "test_asyncio_enabled_true", "test_asyncio_enabled_default", "test_asyncio_enabled_false", "test_custom_asyncio_loop_enabled_true", "test_custom_asyncio_loop_enabled_false", "test_output", "test_overwrite_output", "test_output_and_overwrite_output", "test_output_stdout", "test_absolute_path_linux", "test_absolute_path_windows", "test_args_change_settings", "from_crawler", "TestWindowsRunSpiderCommand", "dumps", "first", "line", "output", "output1", "custom"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport inspect\nimport platform\nimport sys\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory, mkdtemp\nfrom typing import TYPE_CHECKING\n\nimport pytest\n\nfrom tests.test_commands import TestCommandBase\nfrom tests.test_crawler import ExceptionSpider, NoRequestsSpider\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n\n\nclass TestRunSpiderCommand(TestCommandBase):\n    spider_filename = \"myspider.py\"\n\n    debug_log_spider = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug(\"It Works!\")\n        return\n        yield\n\"\"\"\n\n    badspider = \"\"\"\nimport scrapy\n\nclass BadSpider(scrapy.Spider):\n    name = \"bad\"\n    async def start(self):\n        raise Exception(\"oops!\")\n        yield\n        \"\"\"\n\n    @contextmanager\n    def _create_file(self, content: str, name: str | None = None) -> Iterator[str]:\n        with TemporaryDirectory() as tmpdir:\n            if name:\n                fname = Path(tmpdir, name).resolve()\n            else:\n                fname = Path(tmpdir, self.spider_filename).resolve()\n            fname.write_text(content, encoding=\"utf-8\")\n            yield str(fname)\n\n    def runspider(self, code, name=None, args=()):\n        with self._create_file(code, name) as fname:\n            return self.proc(\"runspider\", fname, *args)\n\n    def get_log(self, code, name=None, args=()):\n        _, _, stderr = self.runspider(code, name, args=args)\n        return stderr\n\n    def test_runspider(self):\n        log = self.get_log(self.debug_log_spider)\n        assert \"DEBUG: It Works!\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"INFO: Spider closed (finished)\" in log\n\n    def test_run_fail_spider(self):\n        proc, _, _ = self.runspider(\n            \"import scrapy\\n\" + inspect.getsource(ExceptionSpider)\n        )\n        ret = proc.returncode\n        assert ret != 0\n\n    def test_run_good_spider(self):\n        proc, _, _ = self.runspider(\n            \"import scrapy\\n\" + inspect.getsource(NoRequestsSpider)\n        )\n        ret = proc.returncode\n        assert ret == 0\n\n    def test_runspider_log_level(self):\n        log = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_LEVEL=INFO\"))\n        assert \"DEBUG: It Works!\" not in log\n        assert \"INFO: Spider opened\" in log\n\n    def test_runspider_default_reactor(self):\n        log = self.get_log(self.debug_log_spider, args=(\"-s\", \"TWISTED_REACTOR=\"))\n        assert \"DEBUG: It Works!\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            not in log\n        )\n        assert \"INFO: Spider opened\" in log\n        assert \"INFO: Closing spider (finished)\" in log\n        assert \"INFO: Spider closed (finished)\" in log\n\n    def test_runspider_dnscache_disabled(self):\n        # see https://github.com/scrapy/scrapy/issues/2811\n        # The spider below should not be able to connect to localhost:12345,\n        # which is intended,\n        # but this should not be because of DNS lookup error\n        # assumption: localhost will resolve in all cases (true?)\n        dnscache_spider = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n    start_urls = ['http://localhost:12345']\n\n    custom_settings = {\n        \"ROBOTSTXT_OBEY\": False,\n        \"RETRY_ENABLED\": False,\n    }\n\n    def parse(self, response):\n        return {'test': 'value'}\n\"\"\"\n        log = self.get_log(dnscache_spider, args=(\"-s\", \"DNSCACHE_ENABLED=False\"))\n        assert \"DNSLookupError\" not in log\n        assert \"INFO: Spider opened\" in log\n\n    def test_runspider_log_short_names(self):\n        log1 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=1\"))\n        assert \"[myspider] DEBUG: It Works!\" in log1\n        assert \"[scrapy]\" in log1\n        assert \"[scrapy.core.engine]\" not in log1\n\n        log2 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=0\"))\n        assert \"[myspider] DEBUG: It Works!\" in log2\n        assert \"[scrapy]\" not in log2\n        assert \"[scrapy.core.engine]\" in log2\n\n    def test_runspider_no_spider_found(self):\n        log = self.get_log(\"from scrapy.spiders import Spider\\n\")\n        assert \"No spider found in file\" in log\n\n    def test_runspider_file_not_found(self):\n        _, _, log = self.proc(\"runspider\", \"some_non_existent_file\")\n        assert \"File not found: some_non_existent_file\" in log\n\n    def test_runspider_unable_to_load(self):\n        log = self.get_log(\"\", name=\"myspider.txt\")\n        assert \"Unable to load\" in log\n\n    def test_start_errors(self):\n        log = self.get_log(self.badspider, name=\"badspider.py\")\n        assert \"start\" in log\n        assert \"badspider.py\" in log, log\n", "n_tokens": 1201, "byte_len": 4912, "file_sha1": "192a4421366eca6d9094475fab6cdc7a6839c5b2", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_runspider.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_runspider.py", "rel_path": "tests/test_command_runspider.py", "module": "tests.test_command_runspider", "ext": "py", "chunk_number": 2, "symbols": ["test_asyncio_enabled_true", "test_asyncio_enabled_default", "test_asyncio_enabled_false", "test_custom_asyncio_loop_enabled_true", "test_custom_asyncio_loop_enabled_false", "test_output", "test_overwrite_output", "test_output_and_overwrite_output", "test_output_stdout", "test_absolute_path_linux", "parse", "MySpider", "encoding", "dumps", "async", "first", "line", "output", "output1", "test", "custom", "spider", "name", "linux", "world", "path", "pytest", "debug", "settings", "system", "_create_file", "runspider", "get_log", "test_runspider", "test_run_fail_spider", "test_run_good_spider", "test_runspider_log_level", "test_runspider_default_reactor", "test_runspider_dnscache_disabled", "test_runspider_log_short_names", "test_runspider_no_spider_found", "test_runspider_file_not_found", "test_runspider_unable_to_load", "test_start_errors", "test_absolute_path_windows", "test_args_change_settings", "from_crawler", "TestRunSpiderCommand", "BadSpider", "TestWindowsRunSpiderCommand"], "ast_kind": "class_or_type", "text": "    def test_asyncio_enabled_true(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\n                \"-s\",\n                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            ],\n        )\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    def test_asyncio_enabled_default(self):\n        log = self.get_log(self.debug_log_spider, args=[])\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    def test_asyncio_enabled_false(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\"-s\", \"TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor\"],\n        )\n        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            not in log\n        )\n\n    @pytest.mark.requires_uvloop\n    def test_custom_asyncio_loop_enabled_true(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\n                \"-s\",\n                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                \"-s\",\n                \"ASYNCIO_EVENT_LOOP=uvloop.Loop\",\n            ],\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" in log\n\n    def test_custom_asyncio_loop_enabled_false(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\n                \"-s\",\n                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            ],\n        )\n        if sys.platform != \"win32\":\n            loop = asyncio.new_event_loop()\n        else:\n            loop = asyncio.SelectorEventLoop()\n        assert (\n            f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\"\n            in log\n        )\n\n    def test_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n        return\n        yield\n\"\"\"\n        args = [\"-o\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\" in log\n\n    def test_overwrite_output(self):\n        spider_code = \"\"\"\nimport json\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug(\n            'FEEDS: {}'.format(\n                json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n            )\n        )\n        return\n        yield\n\"\"\"\n        Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n        args = [\"-O\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}'\n            in log\n        )\n        with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n            first_line = f2.readline()\n        assert first_line != \"not empty\"\n\n    def test_output_and_overwrite_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        return\n        yield\n\"\"\"\n        args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            \"error: Please use only one of -o/--output and -O/--overwrite-output\" in log\n        )\n\n    def test_output_stdout(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n        return\n        yield\n\"\"\"\n        args = [\"-o\", \"-:json\"]\n        log = self.get_log(spider_code, args=args)\n        assert \"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\" in log\n\n    @pytest.mark.skipif(platform.system() == \"Windows\", reason=\"Linux only\")\n    def test_absolute_path_linux(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    start_urls = [\"data:,\"]\n\n    def parse(self, response):\n        yield {\"hello\": \"world\"}\n        \"\"\"\n        temp_dir = mkdtemp()\n\n        args = [\"-o\", f\"{temp_dir}/output1.json:json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output1.json\"\n            in log\n        )\n\n        args = [\"-o\", f\"{temp_dir}/output2.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output2.json\"\n            in log\n        )\n", "n_tokens": 1193, "byte_len": 5024, "file_sha1": "192a4421366eca6d9094475fab6cdc7a6839c5b2", "start_line": 156, "end_line": 322}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_runspider.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_runspider.py", "rel_path": "tests/test_command_runspider.py", "module": "tests.test_command_runspider", "ext": "py", "chunk_number": 3, "symbols": ["test_absolute_path_windows", "parse", "test_args_change_settings", "from_crawler", "test_start_errors", "test_runspider_unable_to_load", "MySpider", "TestWindowsRunSpiderCommand", "badspider", "async", "output", "output1", "mkdtemp", "spider", "windows", "return", "name", "mark", "getint", "skipif", "class", "test", "run", "filename", "world", "json", "runspider", "scrapy", "logger", "files", "_create_file", "get_log", "test_runspider", "test_run_fail_spider", "test_run_good_spider", "test_runspider_log_level", "test_runspider_default_reactor", "test_runspider_dnscache_disabled", "test_runspider_log_short_names", "test_runspider_no_spider_found", "test_runspider_file_not_found", "test_asyncio_enabled_true", "test_asyncio_enabled_default", "test_asyncio_enabled_false", "test_custom_asyncio_loop_enabled_true", "test_custom_asyncio_loop_enabled_false", "test_output", "test_overwrite_output", "test_output_and_overwrite_output", "test_output_stdout"], "ast_kind": "class_or_type", "text": "    @pytest.mark.skipif(platform.system() != \"Windows\", reason=\"Windows only\")\n    def test_absolute_path_windows(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    start_urls = [\"data:,\"]\n\n    def parse(self, response):\n        yield {\"hello\": \"world\"}\n        \"\"\"\n        temp_dir = mkdtemp()\n\n        args = [\"-o\", f\"{temp_dir}\\\\output1.json:json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output1.json\"\n            in log\n        )\n\n        args = [\"-o\", f\"{temp_dir}\\\\output2.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output2.json\"\n            in log\n        )\n\n    def test_args_change_settings(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n        return spider\n\n    async def start(self):\n        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n        return\n        yield\n\"\"\"\n        args = [\"-a\", \"foo=42\"]\n        log = self.get_log(spider_code, args=args)\n        assert \"Spider closed (finished)\" in log\n        assert \"The value of FOO is 42\" in log\n\n\n@pytest.mark.skipif(\n    platform.system() != \"Windows\", reason=\"Windows required for .pyw files\"\n)\nclass TestWindowsRunSpiderCommand(TestRunSpiderCommand):\n    spider_filename = \"myspider.pyw\"\n\n    def test_start_errors(self):\n        log = self.get_log(self.badspider, name=\"badspider.pyw\")\n        assert \"start\" in log\n        assert \"badspider.pyw\" in log\n\n    def test_runspider_unable_to_load(self):\n        pytest.skip(\"Already Tested in 'RunSpiderCommandTest'\")\n", "n_tokens": 505, "byte_len": 2008, "file_sha1": "192a4421366eca6d9094475fab6cdc7a6839c5b2", "start_line": 323, "end_line": 389}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py", "rel_path": "tests/test_loader.py", "module": "tests.test_loader", "ext": "py", "chunk_number": 1, "symbols": ["processor_with_args", "test_add_value_on_unknown_field", "test_load_item_using_default_loader", "test_load_item_using_custom_loader", "test_keep_single_value", "test_keep_list", "test_add_value_singlevalue_singlevalue", "test_add_value_singlevalue_list", "test_add_value_list_singlevalue", "test_add_value_list_list", "test_get_output_value_singlevalue", "test_get_output_value_list", "NameItem", "SummaryItem", "NestedItem", "AttrsNameItem", "NameDataClass", "NameItemLoader", "NestedItemLoader", "ProcessorItemLoader", "DefaultedItemLoader", "TestBasicItemLoader", "InitializationTestMixin", "test", "keep", "summary", "take", "first", "after", "selector", "test_values_single", "test_values_list", "test_avoid_reprocessing_with_initial_values_single", "test_avoid_reprocessing_with_initial_values_list", "test_avoid_reprocessing_without_initial_values_single", "test_avoid_reprocessing_without_initial_values_list", "test_output_processor", "__init__", "test_init_method", "test_init_method_errors", "test_init_method_with_selector", "test_init_method_with_selector_css", "test_init_method_with_base_response", "test_init_method_with_response", "test_init_method_with_response_css", "test_add_xpath_re", "test_replace_xpath", "test_get_xpath", "test_replace_xpath_multi_fields", "test_replace_xpath_re"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport dataclasses\n\nimport attr\nimport pytest\nfrom itemadapter import ItemAdapter\nfrom itemloaders.processors import Compose, Identity, MapCompose, TakeFirst\n\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.loader import ItemLoader\nfrom scrapy.selector import Selector\n\n\n# test items\nclass NameItem(Item):\n    name = Field()\n\n\nclass SummaryItem(NameItem):\n    url = Field()\n    summary = Field()\n\n\nclass NestedItem(Item):\n    name = Field()\n    name_div = Field()\n    name_value = Field()\n\n    url = Field()\n    image = Field()\n\n\n@attr.s\nclass AttrsNameItem:\n    name = attr.ib(default=\"\")\n\n\n@dataclasses.dataclass\nclass NameDataClass:\n    name: list = dataclasses.field(default_factory=list)\n\n\n# test item loaders\nclass NameItemLoader(ItemLoader):\n    default_item_class = SummaryItem\n\n\nclass NestedItemLoader(ItemLoader):\n    default_item_class = NestedItem\n\n\nclass ProcessorItemLoader(NameItemLoader):\n    name_in = MapCompose(lambda v: v.title())\n\n\nclass DefaultedItemLoader(NameItemLoader):\n    default_input_processor = MapCompose(lambda v: v[:-1])\n\n\n# test processors\ndef processor_with_args(value, other=None, loader_context=None):\n    if \"key\" in loader_context:\n        return loader_context[\"key\"]\n    return value\n\n\nclass TestBasicItemLoader:\n    def test_add_value_on_unknown_field(self):\n        il = ProcessorItemLoader()\n        with pytest.raises(KeyError):\n            il.add_value(\"wrong_field\", [\"lala\", \"lolo\"])\n\n    def test_load_item_using_default_loader(self):\n        i = SummaryItem()\n        i[\"summary\"] = \"lala\"\n        il = ItemLoader(item=i)\n        il.add_value(\"name\", \"marta\")\n        item = il.load_item()\n        assert item is i\n        assert item[\"summary\"] == [\"lala\"]\n        assert item[\"name\"] == [\"marta\"]\n\n    def test_load_item_using_custom_loader(self):\n        il = ProcessorItemLoader()\n        il.add_value(\"name\", \"marta\")\n        item = il.load_item()\n        assert item[\"name\"] == [\"Marta\"]\n\n\nclass InitializationTestMixin:\n    item_class: type | None = None\n\n    def test_keep_single_value(self):\n        \"\"\"Loaded item should contain values from the initial item\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\"]}\n\n    def test_keep_list(self):\n        \"\"\"Loaded item should contain values from the initial item\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\", \"bar\"]}\n\n    def test_add_value_singlevalue_singlevalue(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", \"bar\")\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\", \"bar\"]}\n\n    def test_add_value_singlevalue_list(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", [\"item\", \"loader\"])\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\", \"item\", \"loader\"]}\n\n    def test_add_value_list_singlevalue(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", \"qwerty\")\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\", \"bar\", \"qwerty\"]}\n\n    def test_add_value_list_list(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", [\"item\", \"loader\"])\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\n            \"name\": [\"foo\", \"bar\", \"item\", \"loader\"]\n        }\n\n    def test_get_output_value_singlevalue(self):\n        \"\"\"Getting output value must not remove value from item\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        assert il.get_output_value(\"name\") == [\"foo\"]\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\"]}\n\n    def test_get_output_value_list(self):\n        \"\"\"Getting output value must not remove value from item\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        assert il.get_output_value(\"name\") == [\"foo\", \"bar\"]\n        loaded_item = il.load_item()\n        assert isinstance(loaded_item, self.item_class)\n        assert ItemAdapter(loaded_item).asdict() == {\"name\": [\"foo\", \"bar\"]}\n", "n_tokens": 1225, "byte_len": 5444, "file_sha1": "c8b0b7fce3df54e4f2bb76ce89c6fec6200d269b", "start_line": 1, "end_line": 166}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py", "rel_path": "tests/test_loader.py", "module": "tests.test_loader", "ext": "py", "chunk_number": 2, "symbols": ["test_values_single", "test_values_list", "test_avoid_reprocessing_with_initial_values_single", "test_avoid_reprocessing_with_initial_values_list", "test_avoid_reprocessing_without_initial_values_single", "test_avoid_reprocessing_without_initial_values_list", "test_output_processor", "__init__", "test_init_method", "test_init_method_errors", "test_init_method_with_selector", "test_init_method_with_selector_css", "test_init_method_with_base_response", "TestInitializationFromDict", "TestInitializationFromItem", "TestInitializationFromAttrsItem", "TestInitializationFromDataClass", "BaseNoInputReprocessingLoader", "NoInputReprocessingItem", "NoInputReprocessingItemLoader", "TestNoInputReprocessingFromItem", "TestOutputProcessorItem", "TempItem", "TempLoader", "TestSelectortemLoader", "encoding", "take", "first", "after", "name", "processor_with_args", "test_add_value_on_unknown_field", "test_load_item_using_default_loader", "test_load_item_using_custom_loader", "test_keep_single_value", "test_keep_list", "test_add_value_singlevalue_singlevalue", "test_add_value_singlevalue_list", "test_add_value_list_singlevalue", "test_add_value_list_list", "test_get_output_value_singlevalue", "test_get_output_value_list", "test_init_method_with_response", "test_init_method_with_response_css", "test_add_xpath_re", "test_replace_xpath", "test_get_xpath", "test_replace_xpath_multi_fields", "test_replace_xpath_re", "test_add_css_re"], "ast_kind": "class_or_type", "text": "    def test_values_single(self):\n        \"\"\"Values from initial item must be added to loader._values\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        assert il._values.get(\"name\") == [\"foo\"]\n\n    def test_values_list(self):\n        \"\"\"Values from initial item must be added to loader._values\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        assert il._values.get(\"name\") == [\"foo\", \"bar\"]\n\n\nclass TestInitializationFromDict(InitializationTestMixin):\n    item_class = dict\n\n\nclass TestInitializationFromItem(InitializationTestMixin):\n    item_class = NameItem\n\n\nclass TestInitializationFromAttrsItem(InitializationTestMixin):\n    item_class = AttrsNameItem\n\n\nclass TestInitializationFromDataClass(InitializationTestMixin):\n    item_class = NameDataClass\n\n\nclass BaseNoInputReprocessingLoader(ItemLoader):\n    title_in = MapCompose(str.upper)\n    title_out = TakeFirst()\n\n\nclass NoInputReprocessingItem(Item):\n    title = Field()\n\n\nclass NoInputReprocessingItemLoader(BaseNoInputReprocessingLoader):\n    default_item_class = NoInputReprocessingItem\n\n\nclass TestNoInputReprocessingFromItem:\n    \"\"\"\n    Loaders initialized from loaded items must not reprocess fields (Item instances)\n    \"\"\"\n\n    def test_avoid_reprocessing_with_initial_values_single(self):\n        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title=\"foo\"))\n        il_loaded = il.load_item()\n        assert il_loaded == {\"title\": \"foo\"}\n        assert NoInputReprocessingItemLoader(item=il_loaded).load_item() == {\n            \"title\": \"foo\"\n        }\n\n    def test_avoid_reprocessing_with_initial_values_list(self):\n        il = NoInputReprocessingItemLoader(\n            item=NoInputReprocessingItem(title=[\"foo\", \"bar\"])\n        )\n        il_loaded = il.load_item()\n        assert il_loaded == {\"title\": \"foo\"}\n        assert NoInputReprocessingItemLoader(item=il_loaded).load_item() == {\n            \"title\": \"foo\"\n        }\n\n    def test_avoid_reprocessing_without_initial_values_single(self):\n        il = NoInputReprocessingItemLoader()\n        il.add_value(\"title\", \"FOO\")\n        il_loaded = il.load_item()\n        assert il_loaded == {\"title\": \"FOO\"}\n        assert NoInputReprocessingItemLoader(item=il_loaded).load_item() == {\n            \"title\": \"FOO\"\n        }\n\n    def test_avoid_reprocessing_without_initial_values_list(self):\n        il = NoInputReprocessingItemLoader()\n        il.add_value(\"title\", [\"foo\", \"bar\"])\n        il_loaded = il.load_item()\n        assert il_loaded == {\"title\": \"FOO\"}\n        assert NoInputReprocessingItemLoader(item=il_loaded).load_item() == {\n            \"title\": \"FOO\"\n        }\n\n\nclass TestOutputProcessorItem:\n    def test_output_processor(self):\n        class TempItem(Item):\n            temp = Field()\n\n            def __init__(self, *args, **kwargs):\n                super().__init__(self, *args, **kwargs)\n                self.setdefault(\"temp\", 0.3)\n\n        class TempLoader(ItemLoader):\n            default_item_class = TempItem\n            default_input_processor = Identity()\n            default_output_processor = Compose(TakeFirst())\n\n        loader = TempLoader()\n        item = loader.load_item()\n        assert isinstance(item, TempItem)\n        assert dict(item) == {\"temp\": 0.3}\n\n\nclass TestSelectortemLoader:\n    response = HtmlResponse(\n        url=\"\",\n        encoding=\"utf-8\",\n        body=b\"\"\"\n    <html>\n    <body>\n    <div id=\"id\">marta</div>\n    <p>paragraph</p>\n    <a href=\"http://www.scrapy.org\">homepage</a>\n    <img src=\"/images/logo.png\" width=\"244\" height=\"65\" alt=\"Scrapy\">\n    </body>\n    </html>\n    \"\"\",\n    )\n\n    def test_init_method(self):\n        l = ProcessorItemLoader()\n        assert l.selector is None\n\n    def test_init_method_errors(self):\n        l = ProcessorItemLoader()\n        with pytest.raises(RuntimeError):\n            l.add_xpath(\"url\", \"//a/@href\")\n        with pytest.raises(RuntimeError):\n            l.replace_xpath(\"url\", \"//a/@href\")\n        with pytest.raises(RuntimeError):\n            l.get_xpath(\"//a/@href\")\n        with pytest.raises(RuntimeError):\n            l.add_css(\"name\", \"#name::text\")\n        with pytest.raises(RuntimeError):\n            l.replace_css(\"name\", \"#name::text\")\n        with pytest.raises(RuntimeError):\n            l.get_css(\"#name::text\")\n\n    def test_init_method_with_selector(self):\n        sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n        l = ProcessorItemLoader(selector=sel)\n        assert l.selector is sel\n\n        l.add_xpath(\"name\", \"//div/text()\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n\n    def test_init_method_with_selector_css(self):\n        sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n        l = ProcessorItemLoader(selector=sel)\n        assert l.selector is sel\n\n        l.add_css(\"name\", \"div::text\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n\n    def test_init_method_with_base_response(self):\n        \"\"\"Selector should be None after initialization\"\"\"\n        response = Response(\"https://scrapy.org\")\n        l = ProcessorItemLoader(response=response)\n        assert l.selector is None\n", "n_tokens": 1181, "byte_len": 5229, "file_sha1": "c8b0b7fce3df54e4f2bb76ce89c6fec6200d269b", "start_line": 167, "end_line": 327}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py", "rel_path": "tests/test_loader.py", "module": "tests.test_loader", "ext": "py", "chunk_number": 3, "symbols": ["test_init_method_with_response", "test_init_method_with_response_css", "test_add_xpath_re", "test_replace_xpath", "test_get_xpath", "test_replace_xpath_multi_fields", "test_replace_xpath_re", "test_add_css_re", "test_replace_css", "test_get_css", "test_replace_css_multi_fields", "test", "replace", "text", "images", "xpath", "take", "first", "paragraph", "add", "name", "accumulating", "lambda", "marta", "attr", "scrapy", "get", "css", "href", "output", "processor_with_args", "test_add_value_on_unknown_field", "test_load_item_using_default_loader", "test_load_item_using_custom_loader", "test_keep_single_value", "test_keep_list", "test_add_value_singlevalue_singlevalue", "test_add_value_singlevalue_list", "test_add_value_list_singlevalue", "test_add_value_list_list", "test_get_output_value_singlevalue", "test_get_output_value_list", "test_values_single", "test_values_list", "test_avoid_reprocessing_with_initial_values_single", "test_avoid_reprocessing_with_initial_values_list", "test_avoid_reprocessing_without_initial_values_single", "test_avoid_reprocessing_without_initial_values_list", "test_output_processor", "__init__"], "ast_kind": "function_or_method", "text": "    def test_init_method_with_response(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.selector\n\n        l.add_xpath(\"name\", \"//div/text()\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n\n    def test_init_method_with_response_css(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.selector\n\n        l.add_css(\"name\", \"div::text\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n\n        l.add_css(\"url\", \"a::attr(href)\")\n        assert l.get_output_value(\"url\") == [\"http://www.scrapy.org\"]\n\n        # combining/accumulating CSS selectors and XPath expressions\n        l.add_xpath(\"name\", \"//div/text()\")\n        assert l.get_output_value(\"name\") == [\"Marta\", \"Marta\"]\n\n        l.add_xpath(\"url\", \"//img/@src\")\n        assert l.get_output_value(\"url\") == [\n            \"http://www.scrapy.org\",\n            \"/images/logo.png\",\n        ]\n\n    def test_add_xpath_re(self):\n        l = ProcessorItemLoader(response=self.response)\n        l.add_xpath(\"name\", \"//div/text()\", re=\"ma\")\n        assert l.get_output_value(\"name\") == [\"Ma\"]\n\n    def test_replace_xpath(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.selector\n        l.add_xpath(\"name\", \"//div/text()\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n        l.replace_xpath(\"name\", \"//p/text()\")\n        assert l.get_output_value(\"name\") == [\"Paragraph\"]\n\n        l.replace_xpath(\"name\", [\"//p/text()\", \"//div/text()\"])\n        assert l.get_output_value(\"name\") == [\"Paragraph\", \"Marta\"]\n\n    def test_get_xpath(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.get_xpath(\"//p/text()\") == [\"paragraph\"]\n        assert l.get_xpath(\"//p/text()\", TakeFirst()) == \"paragraph\"\n        assert l.get_xpath(\"//p/text()\", TakeFirst(), re=\"pa\") == \"pa\"\n\n        assert l.get_xpath([\"//p/text()\", \"//div/text()\"]) == [\"paragraph\", \"marta\"]\n\n    def test_replace_xpath_multi_fields(self):\n        l = ProcessorItemLoader(response=self.response)\n        l.add_xpath(None, \"//div/text()\", TakeFirst(), lambda x: {\"name\": x})\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n        l.replace_xpath(None, \"//p/text()\", TakeFirst(), lambda x: {\"name\": x})\n        assert l.get_output_value(\"name\") == [\"Paragraph\"]\n\n    def test_replace_xpath_re(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.selector\n        l.add_xpath(\"name\", \"//div/text()\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n        l.replace_xpath(\"name\", \"//div/text()\", re=\"ma\")\n        assert l.get_output_value(\"name\") == [\"Ma\"]\n\n    def test_add_css_re(self):\n        l = ProcessorItemLoader(response=self.response)\n        l.add_css(\"name\", \"div::text\", re=\"ma\")\n        assert l.get_output_value(\"name\") == [\"Ma\"]\n\n        l.add_css(\"url\", \"a::attr(href)\", re=\"http://(.+)\")\n        assert l.get_output_value(\"url\") == [\"www.scrapy.org\"]\n\n    def test_replace_css(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.selector\n        l.add_css(\"name\", \"div::text\")\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n        l.replace_css(\"name\", \"p::text\")\n        assert l.get_output_value(\"name\") == [\"Paragraph\"]\n\n        l.replace_css(\"name\", [\"p::text\", \"div::text\"])\n        assert l.get_output_value(\"name\") == [\"Paragraph\", \"Marta\"]\n\n        l.add_css(\"url\", \"a::attr(href)\", re=\"http://(.+)\")\n        assert l.get_output_value(\"url\") == [\"www.scrapy.org\"]\n        l.replace_css(\"url\", \"img::attr(src)\")\n        assert l.get_output_value(\"url\") == [\"/images/logo.png\"]\n\n    def test_get_css(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.get_css(\"p::text\") == [\"paragraph\"]\n        assert l.get_css(\"p::text\", TakeFirst()) == \"paragraph\"\n        assert l.get_css(\"p::text\", TakeFirst(), re=\"pa\") == \"pa\"\n\n        assert l.get_css([\"p::text\", \"div::text\"]) == [\"paragraph\", \"marta\"]\n        assert l.get_css([\"a::attr(href)\", \"img::attr(src)\"]) == [\n            \"http://www.scrapy.org\",\n            \"/images/logo.png\",\n        ]\n\n    def test_replace_css_multi_fields(self):\n        l = ProcessorItemLoader(response=self.response)\n        l.add_css(None, \"div::text\", TakeFirst(), lambda x: {\"name\": x})\n        assert l.get_output_value(\"name\") == [\"Marta\"]\n        l.replace_css(None, \"p::text\", TakeFirst(), lambda x: {\"name\": x})\n        assert l.get_output_value(\"name\") == [\"Paragraph\"]\n\n        l.add_css(None, \"a::attr(href)\", TakeFirst(), lambda x: {\"url\": x})\n        assert l.get_output_value(\"url\") == [\"http://www.scrapy.org\"]\n        l.replace_css(None, \"img::attr(src)\", TakeFirst(), lambda x: {\"url\": x})\n        assert l.get_output_value(\"url\") == [\"/images/logo.png\"]\n", "n_tokens": 1171, "byte_len": 4769, "file_sha1": "c8b0b7fce3df54e4f2bb76ce89c6fec6200d269b", "start_line": 328, "end_line": 441}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_loader.py", "rel_path": "tests/test_loader.py", "module": "tests.test_loader", "ext": "py", "chunk_number": 4, "symbols": ["test_replace_css_re", "test_nested_xpath", "test_nested_css", "test_nested_replace", "test_nested_ordering", "test_nested_load_item", "function_processor_strip", "function_processor_upper", "test_processor_defined_in_item", "TestSubselectorLoader", "FunctionProcessorItem", "FunctionProcessorItemLoader", "TestFunctionProcessor", "encoding", "getall", "qwerty", "function", "processor", "text", "test", "nested", "image", "asdf", "images", "add", "value", "upper", "xpath", "replace", "output", "processor_with_args", "test_add_value_on_unknown_field", "test_load_item_using_default_loader", "test_load_item_using_custom_loader", "test_keep_single_value", "test_keep_list", "test_add_value_singlevalue_singlevalue", "test_add_value_singlevalue_list", "test_add_value_list_singlevalue", "test_add_value_list_list", "test_get_output_value_singlevalue", "test_get_output_value_list", "test_values_single", "test_values_list", "test_avoid_reprocessing_with_initial_values_single", "test_avoid_reprocessing_with_initial_values_list", "test_avoid_reprocessing_without_initial_values_single", "test_avoid_reprocessing_without_initial_values_list", "test_output_processor", "__init__"], "ast_kind": "class_or_type", "text": "    def test_replace_css_re(self):\n        l = ProcessorItemLoader(response=self.response)\n        assert l.selector\n        l.add_css(\"url\", \"a::attr(href)\")\n        assert l.get_output_value(\"url\") == [\"http://www.scrapy.org\"]\n        l.replace_css(\"url\", \"a::attr(href)\", re=r\"http://www\\.(.+)\")\n        assert l.get_output_value(\"url\") == [\"scrapy.org\"]\n\n\nclass TestSubselectorLoader:\n    response = HtmlResponse(\n        url=\"\",\n        encoding=\"utf-8\",\n        body=b\"\"\"\n    <html>\n    <body>\n    <header>\n      <div id=\"id\">marta</div>\n      <p>paragraph</p>\n    </header>\n    <footer class=\"footer\">\n      <a href=\"http://www.scrapy.org\">homepage</a>\n      <img src=\"/images/logo.png\" width=\"244\" height=\"65\" alt=\"Scrapy\">\n    </footer>\n    </body>\n    </html>\n    \"\"\",\n    )\n\n    def test_nested_xpath(self):\n        l = NestedItemLoader(response=self.response)\n\n        nl = l.nested_xpath(\"//header\")\n        nl.add_xpath(\"name\", \"div/text()\")\n        nl.add_css(\"name_div\", \"#id\")\n        nl.add_value(\"name_value\", nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n\n        assert l.get_output_value(\"name\") == [\"marta\"]\n        assert l.get_output_value(\"name_div\") == ['<div id=\"id\">marta</div>']\n        assert l.get_output_value(\"name_value\") == [\"marta\"]\n\n        assert l.get_output_value(\"name\") == nl.get_output_value(\"name\")\n        assert l.get_output_value(\"name_div\") == nl.get_output_value(\"name_div\")\n        assert l.get_output_value(\"name_value\") == nl.get_output_value(\"name_value\")\n\n    def test_nested_css(self):\n        l = NestedItemLoader(response=self.response)\n        nl = l.nested_css(\"header\")\n        nl.add_xpath(\"name\", \"div/text()\")\n        nl.add_css(\"name_div\", \"#id\")\n        nl.add_value(\"name_value\", nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n\n        assert l.get_output_value(\"name\") == [\"marta\"]\n        assert l.get_output_value(\"name_div\") == ['<div id=\"id\">marta</div>']\n        assert l.get_output_value(\"name_value\") == [\"marta\"]\n\n        assert l.get_output_value(\"name\") == nl.get_output_value(\"name\")\n        assert l.get_output_value(\"name_div\") == nl.get_output_value(\"name_div\")\n        assert l.get_output_value(\"name_value\") == nl.get_output_value(\"name_value\")\n\n    def test_nested_replace(self):\n        l = NestedItemLoader(response=self.response)\n        nl1 = l.nested_xpath(\"//footer\")\n        nl2 = nl1.nested_xpath(\"a\")\n\n        l.add_xpath(\"url\", \"//footer/a/@href\")\n        assert l.get_output_value(\"url\") == [\"http://www.scrapy.org\"]\n        nl1.replace_xpath(\"url\", \"img/@src\")\n        assert l.get_output_value(\"url\") == [\"/images/logo.png\"]\n        nl2.replace_xpath(\"url\", \"@href\")\n        assert l.get_output_value(\"url\") == [\"http://www.scrapy.org\"]\n\n    def test_nested_ordering(self):\n        l = NestedItemLoader(response=self.response)\n        nl1 = l.nested_xpath(\"//footer\")\n        nl2 = nl1.nested_xpath(\"a\")\n\n        nl1.add_xpath(\"url\", \"img/@src\")\n        l.add_xpath(\"url\", \"//footer/a/@href\")\n        nl2.add_xpath(\"url\", \"text()\")\n        l.add_xpath(\"url\", \"//footer/a/@href\")\n\n        assert l.get_output_value(\"url\") == [\n            \"/images/logo.png\",\n            \"http://www.scrapy.org\",\n            \"homepage\",\n            \"http://www.scrapy.org\",\n        ]\n\n    def test_nested_load_item(self):\n        l = NestedItemLoader(response=self.response)\n        nl1 = l.nested_xpath(\"//footer\")\n        nl2 = nl1.nested_xpath(\"img\")\n\n        l.add_xpath(\"name\", \"//header/div/text()\")\n        nl1.add_xpath(\"url\", \"a/@href\")\n        nl2.add_xpath(\"image\", \"@src\")\n\n        item = l.load_item()\n\n        assert item is l.item\n        assert item is nl1.item\n        assert item is nl2.item\n\n        assert item[\"name\"] == [\"marta\"]\n        assert item[\"url\"] == [\"http://www.scrapy.org\"]\n        assert item[\"image\"] == [\"/images/logo.png\"]\n\n\n# Functions as processors\n\n\ndef function_processor_strip(iterable):\n    return [x.strip() for x in iterable]\n\n\ndef function_processor_upper(iterable):\n    return [x.upper() for x in iterable]\n\n\nclass FunctionProcessorItem(Item):\n    foo = Field(\n        input_processor=function_processor_strip,\n        output_processor=function_processor_upper,\n    )\n\n\nclass FunctionProcessorItemLoader(ItemLoader):\n    default_item_class = FunctionProcessorItem\n\n\nclass TestFunctionProcessor:\n    def test_processor_defined_in_item(self):\n        lo = FunctionProcessorItemLoader()\n        lo.add_value(\"foo\", \"  bar  \")\n        lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n        assert dict(lo.load_item()) == {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]}\n", "n_tokens": 1152, "byte_len": 4596, "file_sha1": "c8b0b7fce3df54e4f2bb76ce89c6fec6200d269b", "start_line": 442, "end_line": 579}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_middleware.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_middleware.py", "rel_path": "tests/test_middleware.py", "module": "tests.test_middleware", "ext": "py", "chunk_number": 1, "symbols": ["open_spider", "close_spider", "process", "__init__", "_get_mwlist_from_settings", "_add_middleware", "crawler", "test_init", "test_methods", "test_enabled", "test_enabled_from_settings", "test_no_crawler", "M1", "M2", "M3", "MOff", "MyMiddlewareManager", "mwman", "hasattr", "test", "init", "pass", "argument", "open", "spider", "append", "scrapy", "deprecation", "with", "warns", "self", "typing", "return", "middleware", "annotations", "class", "classes", "close", "get", "mwlist", "ignore", "attr", "not", "configured", "future", "typ", "checking", "classmethod", "enabled", "middlewares"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport pytest\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nclass M1:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n    def process(self, response, request):\n        pass\n\n\nclass M2:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n\nclass M3:\n    def process(self, response, request):\n        pass\n\n\nclass MOff:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n    def __init__(self):\n        raise NotConfigured(\"foo\")\n\n\nclass MyMiddlewareManager(MiddlewareManager):\n    component_name = \"my\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings):\n        return [M1, MOff, M3]\n\n    def _add_middleware(self, mw):\n        if hasattr(mw, \"open_spider\"):\n            self.methods[\"open_spider\"].append(mw.open_spider)\n        if hasattr(mw, \"close_spider\"):\n            self.methods[\"close_spider\"].appendleft(mw.close_spider)\n        if hasattr(mw, \"process\"):\n            self.methods[\"process\"].append(mw.process)\n\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider)\n\n\ndef test_init(crawler: Crawler) -> None:\n    m1, m2, m3 = M1(), M2(), M3()\n    mwman = MyMiddlewareManager(m1, m2, m3, crawler=crawler)\n    assert list(mwman.methods[\"open_spider\"]) == [m1.open_spider, m2.open_spider]\n    assert list(mwman.methods[\"close_spider\"]) == [m2.close_spider, m1.close_spider]\n    assert list(mwman.methods[\"process\"]) == [m1.process, m3.process]\n    assert mwman.crawler == crawler\n\n\ndef test_methods(crawler: Crawler) -> None:\n    mwman = MyMiddlewareManager(M1(), M2(), M3(), crawler=crawler)\n    assert [x.__self__.__class__ for x in mwman.methods[\"open_spider\"]] == [M1, M2]  # type: ignore[union-attr]\n    assert [x.__self__.__class__ for x in mwman.methods[\"close_spider\"]] == [M2, M1]  # type: ignore[union-attr]\n    assert [x.__self__.__class__ for x in mwman.methods[\"process\"]] == [M1, M3]  # type: ignore[union-attr]\n\n\ndef test_enabled(crawler: Crawler) -> None:\n    m1, m2, m3 = M1(), M2(), M3()\n    mwman = MyMiddlewareManager(m1, m2, m3, crawler=crawler)\n    assert mwman.middlewares == (m1, m2, m3)\n\n\ndef test_enabled_from_settings(crawler: Crawler) -> None:\n    crawler = get_crawler()\n    mwman = MyMiddlewareManager.from_crawler(crawler)\n    classes = [x.__class__ for x in mwman.middlewares]\n    assert classes == [M1, M3]\n    assert mwman.crawler == crawler\n\n\ndef test_no_crawler() -> None:\n    m1, m2, m3 = M1(), M2(), M3()\n    with pytest.warns(\n        ScrapyDeprecationWarning, match=\"was called without the crawler argument\"\n    ):\n        mwman = MyMiddlewareManager(m1, m2, m3)\n    assert mwman.middlewares == (m1, m2, m3)\n    assert mwman.crawler is None\n", "n_tokens": 849, "byte_len": 3040, "file_sha1": "f4a7fbc429436fe0b5fb0b661e60a55f2dacbb9b", "start_line": 1, "end_line": 110}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http10.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http10.py", "rel_path": "tests/test_downloader_handler_twisted_http10.py", "module": "tests.test_downloader_handler_twisted_http10", "ext": "py", "chunk_number": 1, "symbols": ["download_handler_cls", "HTTP10DownloadHandlerMixin", "TestHttp10", "TestHttps10", "TestHttp10Proxy", "secure", "downloader", "method", "async", "protocol", "core", "http", "http10", "await", "test", "scrapy", "deprecation", "case", "implemented", "typing", "download", "property", "return", "htt", "annotations", "deferred", "from", "class", "mark", "handler", "ignore", "future", "typ", "checking", "defer", "mockserver", "host", "request", "true", "pytest", "filterwarnings", "assert", "mock", "server", "exceptions", "none", "handlers", "https", "utils", "type"], "ast_kind": "class_or_type", "text": "\"\"\"Tests for scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport pytest\n\nfrom scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\nfrom scrapy.http import Request\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom tests.test_downloader_handlers_http_base import (\n    TestHttpBase,\n    TestHttpProxyBase,\n    download_request,\n)\n\nif TYPE_CHECKING:\n    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n    from tests.mockserver.http import MockServer\n\n\nclass HTTP10DownloadHandlerMixin:\n    @property\n    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n        return HTTP10DownloadHandler\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestHttp10(HTTP10DownloadHandlerMixin, TestHttpBase):\n    \"\"\"HTTP 1.0 test case\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_protocol(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/host\", is_secure=self.is_secure), method=\"GET\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.protocol == \"HTTP/1.0\"\n\n\nclass TestHttps10(TestHttp10):\n    is_secure = True\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestHttp10Proxy(HTTP10DownloadHandlerMixin, TestHttpProxyBase):\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy_https_timeout(self):\n        pytest.skip(\"Not implemented\")\n\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy_without_http_scheme(self):\n        pytest.skip(\"Not implemented\")\n", "n_tokens": 378, "byte_len": 1763, "file_sha1": "a225e36be6e80b80686c66876dfd7cb32c0a56f2", "start_line": 1, "end_line": 57}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py", "rel_path": "tests/test_http2_client_protocol.py", "module": "tests.test_http2_client_protocol", "ext": "py", "chunk_number": 1, "symbols": ["generate_random_string", "make_html_body", "parse", "render_GET", "make_response", "render_POST", "_delayed_render", "DummySpider", "Data", "GetDataHtmlSmall", "GetDataHtmlLarge", "PostDataJsonMixin", "PostDataJsonSmall", "PostDataJsonLarge", "Dataloss", "NoContentLengthHeader", "TimeoutResponse", "QueryParams", "RequestHeaders", "dumps", "content", "length", "protocol", "generate", "random", "loads", "private", "certificate", "get", "all", "make_request_dfd", "site", "client_certificate", "get_url", "test_invalid_negotiated_protocol", "test_cancel_request", "test_received_dataloss_response", "test_inactive_stream", "assert_inactive_stream", "test_connection_timeout", "TestHttps2ClientProtocol", "method", "async", "send", "case", "connection", "spider", "client", "factory", "decoded"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport json\nimport random\nimport re\nimport string\nfrom ipaddress import IPv4Address\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, cast\nfrom unittest import mock\nfrom urllib.parse import urlencode\n\nimport pytest\nfrom pytest_twisted import async_yield_fixture\nfrom twisted.internet.defer import (\n    CancelledError,\n    Deferred,\n    DeferredList,\n    inlineCallbacks,\n)\nfrom twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\nfrom twisted.internet.error import TimeoutError as TxTimeoutError\nfrom twisted.internet.ssl import Certificate, PrivateCertificate, optionsForClientTLS\nfrom twisted.web.client import URI, ResponseFailed\nfrom twisted.web.http import H2_ENABLED\nfrom twisted.web.http import Request as TxRequest\nfrom twisted.web.server import NOT_DONE_YET, Site\nfrom twisted.web.static import File\n\nfrom scrapy.http import JsonRequest, Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n)\nfrom tests.mockserver.http_resources import LeafResource, Status\nfrom tests.mockserver.utils import ssl_context_factory\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Coroutine, Generator\n\n    from scrapy.core.http2.protocol import H2ClientProtocol\n\n\npytestmark = pytest.mark.skipif(\n    not H2_ENABLED, reason=\"HTTP/2 support in Twisted is not enabled\"\n)\n\n\ndef generate_random_string(size: int) -> str:\n    return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=size))\n\n\ndef make_html_body(val: str) -> bytes:\n    response = f\"\"\"<html>\n<h1>Hello from HTTP2<h1>\n<p>{val}</p>\n</html>\"\"\"\n    return bytes(response, \"utf-8\")\n\n\nclass DummySpider(Spider):\n    name = \"dummy\"\n    start_urls: list = []\n\n    def parse(self, response):\n        print(response)\n\n\nclass Data:\n    SMALL_SIZE = 1024  # 1 KB\n    LARGE_SIZE = 1024**2  # 1 MB\n\n    STR_SMALL = generate_random_string(SMALL_SIZE)\n    STR_LARGE = generate_random_string(LARGE_SIZE)\n\n    EXTRA_SMALL = generate_random_string(1024 * 15)\n    EXTRA_LARGE = generate_random_string((1024**2) * 15)\n\n    HTML_SMALL = make_html_body(STR_SMALL)\n    HTML_LARGE = make_html_body(STR_LARGE)\n\n    JSON_SMALL = {\"data\": STR_SMALL}\n    JSON_LARGE = {\"data\": STR_LARGE}\n\n    DATALOSS = b\"Dataloss Content\"\n    NO_CONTENT_LENGTH = b\"This response do not have any content-length header\"\n\n\nclass GetDataHtmlSmall(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"text/html; charset=UTF-8\")\n        return Data.HTML_SMALL\n\n\nclass GetDataHtmlLarge(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"text/html; charset=UTF-8\")\n        return Data.HTML_LARGE\n\n\nclass PostDataJsonMixin:\n    @staticmethod\n    def make_response(request: TxRequest, extra_data: str) -> bytes:\n        assert request.content is not None\n        response = {\n            \"request-headers\": {},\n            \"request-body\": json.loads(request.content.read()),\n            \"extra-data\": extra_data,\n        }\n        for k, v in request.requestHeaders.getAllRawHeaders():\n            response[\"request-headers\"][str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n        response_bytes = bytes(json.dumps(response), \"utf-8\")\n        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n        return response_bytes\n\n\nclass PostDataJsonSmall(LeafResource, PostDataJsonMixin):\n    def render_POST(self, request: TxRequest):\n        return self.make_response(request, Data.EXTRA_SMALL)\n\n\nclass PostDataJsonLarge(LeafResource, PostDataJsonMixin):\n    def render_POST(self, request: TxRequest):\n        return self.make_response(request, Data.EXTRA_LARGE)\n\n\nclass Dataloss(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(b\"Content-Length\", b\"1024\")\n        self.deferRequest(request, 0, self._delayed_render, request)\n        return NOT_DONE_YET\n\n    @staticmethod\n    def _delayed_render(request: TxRequest):\n        request.write(Data.DATALOSS)\n        request.finish()\n\n\nclass NoContentLengthHeader(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.requestHeaders.removeHeader(\"Content-Length\")\n        self.deferRequest(request, 0, self._delayed_render, request)\n        return NOT_DONE_YET\n\n    @staticmethod\n    def _delayed_render(request: TxRequest):\n        request.write(Data.NO_CONTENT_LENGTH)\n        request.finish()\n\n\nclass TimeoutResponse(LeafResource):\n    def render_GET(self, request: TxRequest):\n        return NOT_DONE_YET\n\n\nclass QueryParams(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n\n        query_params: dict[str, str] = {}\n        assert request.args is not None\n        for k, v in request.args.items():\n            query_params[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n        return bytes(json.dumps(query_params), \"utf-8\")\n\n\nclass RequestHeaders(LeafResource):\n    \"\"\"Sends all the headers received as a response\"\"\"\n", "n_tokens": 1204, "byte_len": 5296, "file_sha1": "96799316872b6b66d7cab2968dde1e69493b5318", "start_line": 1, "end_line": 176}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py", "rel_path": "tests/test_http2_client_protocol.py", "module": "tests.test_http2_client_protocol", "ext": "py", "chunk_number": 2, "symbols": ["render_GET", "make_request_dfd", "site", "client_certificate", "get_url", "TestHttps2ClientProtocol", "encoding", "client", "factory", "dumps", "content", "length", "async", "protocol", "starting", "private", "certificate", "append", "get", "all", "file", "small", "coroutine", "deferred", "from", "https", "large", "path", "port", "interface", "generate_random_string", "make_html_body", "parse", "make_response", "render_POST", "_delayed_render", "test_invalid_negotiated_protocol", "test_cancel_request", "test_received_dataloss_response", "test_inactive_stream", "assert_inactive_stream", "test_connection_timeout", "DummySpider", "Data", "GetDataHtmlSmall", "GetDataHtmlLarge", "PostDataJsonMixin", "PostDataJsonSmall", "PostDataJsonLarge", "Dataloss"], "ast_kind": "class_or_type", "text": "    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n        headers = {}\n        for k, v in request.requestHeaders.getAllRawHeaders():\n            headers[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n        return bytes(json.dumps(headers), \"utf-8\")\n\n\ndef make_request_dfd(client: H2ClientProtocol, request: Request) -> Deferred[Response]:\n    return client.request(request, DummySpider())\n\n\nasync def make_request(client: H2ClientProtocol, request: Request) -> Response:\n    return await maybe_deferred_to_future(make_request_dfd(client, request))\n\n\nclass TestHttps2ClientProtocol:\n    scheme = \"https\"\n    host = \"localhost\"\n    key_file = Path(__file__).parent / \"keys\" / \"localhost.key\"\n    certificate_file = Path(__file__).parent / \"keys\" / \"localhost.crt\"\n\n    @pytest.fixture\n    def site(self, tmp_path):\n        r = File(str(tmp_path))\n        r.putChild(b\"get-data-html-small\", GetDataHtmlSmall())\n        r.putChild(b\"get-data-html-large\", GetDataHtmlLarge())\n\n        r.putChild(b\"post-data-json-small\", PostDataJsonSmall())\n        r.putChild(b\"post-data-json-large\", PostDataJsonLarge())\n\n        r.putChild(b\"dataloss\", Dataloss())\n        r.putChild(b\"no-content-length-header\", NoContentLengthHeader())\n        r.putChild(b\"status\", Status())\n        r.putChild(b\"query-params\", QueryParams())\n        r.putChild(b\"timeout\", TimeoutResponse())\n        r.putChild(b\"request-headers\", RequestHeaders())\n        return Site(r, timeout=None)\n\n    @async_yield_fixture\n    async def server_port(self, site: Site) -> AsyncGenerator[int]:\n        from twisted.internet import reactor\n\n        context_factory = ssl_context_factory(\n            str(self.key_file), str(self.certificate_file)\n        )\n        server_endpoint = SSL4ServerEndpoint(\n            reactor, 0, context_factory, interface=self.host\n        )\n        server = await server_endpoint.listen(site)\n\n        yield server.getHost().port\n\n        await server.stopListening()\n\n    @pytest.fixture\n    def client_certificate(self) -> PrivateCertificate:\n        pem = self.key_file.read_text(\n            encoding=\"utf-8\"\n        ) + self.certificate_file.read_text(encoding=\"utf-8\")\n        return PrivateCertificate.loadPEM(pem)\n\n    @async_yield_fixture\n    async def client(\n        self, server_port: int, client_certificate: PrivateCertificate\n    ) -> AsyncGenerator[H2ClientProtocol]:\n        from twisted.internet import reactor\n\n        from scrapy.core.http2.protocol import H2ClientFactory  # noqa: PLC0415\n\n        client_options = optionsForClientTLS(\n            hostname=self.host,\n            trustRoot=client_certificate,\n            acceptableProtocols=[b\"h2\"],\n        )\n        uri = URI.fromBytes(bytes(self.get_url(server_port, \"/\"), \"utf-8\"))\n        h2_client_factory = H2ClientFactory(uri, Settings(), Deferred())\n        client_endpoint = SSL4ClientEndpoint(\n            reactor, self.host, server_port, client_options\n        )\n        client = await client_endpoint.connect(h2_client_factory)\n\n        yield client\n\n        if client.connected:\n            client.transport.loseConnection()\n            client.transport.abortConnection()\n\n    def get_url(self, portno: int, path: str) -> str:\n        \"\"\"\n        :param path: Should have / at the starting compulsorily if not empty\n        :return: Complete url\n        \"\"\"\n        assert len(path) > 0\n        assert path[0] == \"/\" or path[0] == \"&\"\n        return f\"{self.scheme}://{self.host}:{portno}{path}\"\n\n    @staticmethod\n    async def _check_repeat(\n        get_coro: Callable[[], Coroutine[Any, Any, None]], count: int\n    ) -> None:\n        d_list = []\n        for _ in range(count):\n            d = deferred_from_coro(get_coro())\n            d_list.append(d)\n\n        await maybe_deferred_to_future(DeferredList(d_list, fireOnOneErrback=True))\n\n    async def _check_GET(\n        self,\n        client: H2ClientProtocol,\n        request: Request,\n        expected_body: bytes,\n        expected_status: int,\n    ) -> None:\n        response = await make_request(client, request)\n        assert response.status == expected_status\n        assert response.body == expected_body\n        assert response.request == request\n\n        content_length_header = response.headers.get(\"Content-Length\")\n        assert content_length_header is not None\n        content_length = int(content_length_header)\n        assert len(response.body) == content_length\n\n    @deferred_f_from_coro_f\n    async def test_GET_small_body(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = Request(self.get_url(server_port, \"/get-data-html-small\"))\n        await self._check_GET(client, request, Data.HTML_SMALL, 200)\n\n    @deferred_f_from_coro_f\n    async def test_GET_large_body(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = Request(self.get_url(server_port, \"/get-data-html-large\"))\n        await self._check_GET(client, request, Data.HTML_LARGE, 200)\n\n    async def _check_GET_x10(\n        self,\n        client: H2ClientProtocol,\n        request: Request,\n        expected_body: bytes,\n        expected_status: int,\n    ) -> None:\n        async def get_coro() -> None:\n            await self._check_GET(client, request, expected_body, expected_status)\n\n        await self._check_repeat(get_coro, 10)\n", "n_tokens": 1225, "byte_len": 5445, "file_sha1": "96799316872b6b66d7cab2968dde1e69493b5318", "start_line": 177, "end_line": 329}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py", "rel_path": "tests/test_http2_client_protocol.py", "module": "tests.test_http2_client_protocol", "ext": "py", "chunk_number": 3, "symbols": ["test_invalid_negotiated_protocol", "test_cancel_request", "parse", "method", "content", "length", "async", "protocol", "loads", "test", "large", "were", "pos", "small", "deferred", "from", "mock", "client", "sent", "pytest", "extr", "items", "get", "coro", "encoding", "check", "repeat", "none", "return", "value", "generate_random_string", "make_html_body", "render_GET", "make_response", "render_POST", "_delayed_render", "make_request_dfd", "site", "client_certificate", "get_url", "test_received_dataloss_response", "test_inactive_stream", "assert_inactive_stream", "test_connection_timeout", "DummySpider", "Data", "GetDataHtmlSmall", "GetDataHtmlLarge", "PostDataJsonMixin", "PostDataJsonSmall"], "ast_kind": "function_or_method", "text": "    @deferred_f_from_coro_f\n    async def test_GET_small_body_x10(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        await self._check_GET_x10(\n            client,\n            Request(self.get_url(server_port, \"/get-data-html-small\")),\n            Data.HTML_SMALL,\n            200,\n        )\n\n    @deferred_f_from_coro_f\n    async def test_GET_large_body_x10(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        await self._check_GET_x10(\n            client,\n            Request(self.get_url(server_port, \"/get-data-html-large\")),\n            Data.HTML_LARGE,\n            200,\n        )\n\n    @staticmethod\n    async def _check_POST_json(\n        client: H2ClientProtocol,\n        request: Request,\n        expected_request_body: dict[str, str],\n        expected_extra_data: str,\n        expected_status: int,\n    ) -> None:\n        response = await make_request(client, request)\n\n        assert response.status == expected_status\n        assert response.request == request\n\n        content_length_header = response.headers.get(\"Content-Length\")\n        assert content_length_header is not None\n        content_length = int(content_length_header)\n        assert len(response.body) == content_length\n\n        # Parse the body\n        content_encoding_header = response.headers[b\"Content-Encoding\"]\n        assert content_encoding_header is not None\n        content_encoding = str(content_encoding_header, \"utf-8\")\n        body = json.loads(str(response.body, content_encoding))\n        assert \"request-body\" in body\n        assert \"extra-data\" in body\n        assert \"request-headers\" in body\n\n        request_body = body[\"request-body\"]\n        assert request_body == expected_request_body\n\n        extra_data = body[\"extra-data\"]\n        assert extra_data == expected_extra_data\n\n        # Check if headers were sent successfully\n        request_headers = body[\"request-headers\"]\n        for k, v in request.headers.items():\n            k_str = str(k, \"utf-8\")\n            assert k_str in request_headers\n            assert request_headers[k_str] == str(v[0], \"utf-8\")\n\n    @deferred_f_from_coro_f\n    async def test_POST_small_json(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = JsonRequest(\n            url=self.get_url(server_port, \"/post-data-json-small\"),\n            method=\"POST\",\n            data=Data.JSON_SMALL,\n        )\n        await self._check_POST_json(\n            client, request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200\n        )\n\n    @deferred_f_from_coro_f\n    async def test_POST_large_json(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = JsonRequest(\n            url=self.get_url(server_port, \"/post-data-json-large\"),\n            method=\"POST\",\n            data=Data.JSON_LARGE,\n        )\n        await self._check_POST_json(\n            client, request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200\n        )\n\n    async def _check_POST_json_x10(self, *args, **kwargs):\n        async def get_coro() -> None:\n            await self._check_POST_json(*args, **kwargs)\n\n        await self._check_repeat(get_coro, 10)\n\n    @deferred_f_from_coro_f\n    async def test_POST_small_json_x10(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = JsonRequest(\n            url=self.get_url(server_port, \"/post-data-json-small\"),\n            method=\"POST\",\n            data=Data.JSON_SMALL,\n        )\n        await self._check_POST_json_x10(\n            client, request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200\n        )\n\n    @deferred_f_from_coro_f\n    async def test_POST_large_json_x10(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = JsonRequest(\n            url=self.get_url(server_port, \"/post-data-json-large\"),\n            method=\"POST\",\n            data=Data.JSON_LARGE,\n        )\n        await self._check_POST_json_x10(\n            client, request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200\n        )\n\n    @inlineCallbacks\n    def test_invalid_negotiated_protocol(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> Generator[Deferred[Any], Any, None]:\n        with mock.patch(\n            \"scrapy.core.http2.protocol.PROTOCOL_NAME\", return_value=b\"not-h2\"\n        ):\n            request = Request(url=self.get_url(server_port, \"/status?n=200\"))\n            with pytest.raises(ResponseFailed):\n                yield make_request_dfd(client, request)\n\n    @inlineCallbacks\n    def test_cancel_request(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> Generator[Deferred[Any], Any, None]:\n        request = Request(url=self.get_url(server_port, \"/get-data-html-large\"))\n        d = make_request_dfd(client, request)\n        d.cancel()\n        response = cast(\"Response\", (yield d))\n        assert response.status == 499\n        assert response.request == request\n", "n_tokens": 1104, "byte_len": 4926, "file_sha1": "96799316872b6b66d7cab2968dde1e69493b5318", "start_line": 330, "end_line": 471}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py", "rel_path": "tests/test_http2_client_protocol.py", "module": "tests.test_http2_client_protocol", "ext": "py", "chunk_number": 4, "symbols": ["test_received_dataloss_response", "test_inactive_stream", "async", "send", "case", "connection", "stream", "small", "cancelling", "deferred", "from", "test", "log", "sending", "level", "more", "length", "large", "client", "protocol", "dataloss", "pytest", "isinstance", "get", "coro", "inactive", "than", "max", "check", "repeat", "generate_random_string", "make_html_body", "parse", "render_GET", "make_response", "render_POST", "_delayed_render", "make_request_dfd", "site", "client_certificate", "get_url", "test_invalid_negotiated_protocol", "test_cancel_request", "assert_inactive_stream", "test_connection_timeout", "DummySpider", "Data", "GetDataHtmlSmall", "GetDataHtmlLarge", "PostDataJsonMixin"], "ast_kind": "function_or_method", "text": "    @deferred_f_from_coro_f\n    async def test_download_maxsize_exceeded(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = Request(\n            url=self.get_url(server_port, \"/get-data-html-large\"),\n            meta={\"download_maxsize\": 1000},\n        )\n        with pytest.raises(CancelledError) as exc_info:\n            await make_request(client, request)\n        error_pattern = re.compile(\n            rf\"Cancelling download of {request.url}: received response \"\n            rf\"size \\(\\d*\\) larger than download max size \\(1000\\)\"\n        )\n        assert len(re.findall(error_pattern, str(exc_info.value))) == 1\n\n    @inlineCallbacks\n    def test_received_dataloss_response(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> Generator[Deferred[Any], Any, None]:\n        \"\"\"In case when value of Header Content-Length != len(Received Data)\n        ProtocolError is raised\"\"\"\n        from h2.exceptions import InvalidBodyLengthError  # noqa: PLC0415\n\n        request = Request(url=self.get_url(server_port, \"/dataloss\"))\n        with pytest.raises(ResponseFailed) as exc_info:\n            yield make_request_dfd(client, request)\n        assert len(exc_info.value.reasons) > 0\n        assert any(\n            isinstance(error, InvalidBodyLengthError)\n            for error in exc_info.value.reasons\n        )\n\n    @deferred_f_from_coro_f\n    async def test_missing_content_length_header(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = Request(url=self.get_url(server_port, \"/no-content-length-header\"))\n        response = await make_request(client, request)\n        assert response.status == 200\n        assert response.body == Data.NO_CONTENT_LENGTH\n        assert response.request == request\n        assert \"Content-Length\" not in response.headers\n\n    async def _check_log_warnsize(\n        self,\n        client: H2ClientProtocol,\n        request: Request,\n        warn_pattern: re.Pattern[str],\n        expected_body: bytes,\n        caplog: pytest.LogCaptureFixture,\n    ) -> None:\n        with caplog.at_level(\"WARNING\", \"scrapy.core.http2.stream\"):\n            response = await make_request(client, request)\n        assert response.status == 200\n        assert response.request == request\n        assert response.body == expected_body\n\n        # Check the warning is raised only once for this request\n        assert len(re.findall(warn_pattern, caplog.text)) == 1\n\n    @deferred_f_from_coro_f\n    async def test_log_expected_warnsize(\n        self,\n        server_port: int,\n        client: H2ClientProtocol,\n        caplog: pytest.LogCaptureFixture,\n    ) -> None:\n        request = Request(\n            url=self.get_url(server_port, \"/get-data-html-large\"),\n            meta={\"download_warnsize\": 1000},\n        )\n        warn_pattern = re.compile(\n            rf\"Expected response size \\(\\d*\\) larger than \"\n            rf\"download warn size \\(1000\\) in request {request}\"\n        )\n\n        await self._check_log_warnsize(\n            client, request, warn_pattern, Data.HTML_LARGE, caplog\n        )\n\n    @deferred_f_from_coro_f\n    async def test_log_received_warnsize(\n        self,\n        server_port: int,\n        client: H2ClientProtocol,\n        caplog: pytest.LogCaptureFixture,\n    ) -> None:\n        request = Request(\n            url=self.get_url(server_port, \"/no-content-length-header\"),\n            meta={\"download_warnsize\": 10},\n        )\n        warn_pattern = re.compile(\n            rf\"Received more \\(\\d*\\) bytes than download \"\n            rf\"warn size \\(10\\) in request {request}\"\n        )\n\n        await self._check_log_warnsize(\n            client, request, warn_pattern, Data.NO_CONTENT_LENGTH, caplog\n        )\n\n    @deferred_f_from_coro_f\n    async def test_max_concurrent_streams(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        \"\"\"Send 500 requests at one to check if we can handle\n        very large number of request.\n        \"\"\"\n\n        async def get_coro() -> None:\n            await self._check_GET(\n                client,\n                Request(self.get_url(server_port, \"/get-data-html-small\")),\n                Data.HTML_SMALL,\n                200,\n            )\n\n        await self._check_repeat(get_coro, 500)\n\n    @inlineCallbacks\n    def test_inactive_stream(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> Generator[Deferred[Any], Any, None]:\n        \"\"\"Here we send 110 requests considering the MAX_CONCURRENT_STREAMS\n        by default is 100. After sending the first 100 requests we close the\n        connection.\"\"\"\n        d_list = []\n", "n_tokens": 1033, "byte_len": 4648, "file_sha1": "96799316872b6b66d7cab2968dde1e69493b5318", "start_line": 472, "end_line": 599}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py", "rel_path": "tests/test_http2_client_protocol.py", "module": "tests.test_http2_client_protocol", "ext": "py", "chunk_number": 5, "symbols": ["assert_inactive_stream", "async", "generate", "random", "send", "loads", "append", "private", "certificate", "connection", "stream", "small", "deferred", "from", "https", "test", "query", "port", "save", "notlocalhostdomain", "client", "protocol", "failed", "check", "invalid", "pytest", "close", "notlocalhost", "isinstance", "encoding", "generate_random_string", "make_html_body", "parse", "render_GET", "make_response", "render_POST", "_delayed_render", "make_request_dfd", "site", "client_certificate", "get_url", "test_invalid_negotiated_protocol", "test_cancel_request", "test_received_dataloss_response", "test_inactive_stream", "test_connection_timeout", "DummySpider", "Data", "GetDataHtmlSmall", "GetDataHtmlLarge"], "ast_kind": "function_or_method", "text": "        def assert_inactive_stream(failure):\n            assert failure.check(ResponseFailed) is not None\n\n            from scrapy.core.http2.stream import InactiveStreamClosed  # noqa: PLC0415\n\n            assert any(\n                isinstance(e, InactiveStreamClosed) for e in failure.value.reasons\n            )\n\n        # Send 100 request (we do not check the result)\n        for _ in range(100):\n            d = make_request_dfd(\n                client, Request(self.get_url(server_port, \"/get-data-html-small\"))\n            )\n            d.addBoth(lambda _: None)\n            d_list.append(d)\n\n        # Now send 10 extra request and save the response deferred in a list\n        for _ in range(10):\n            d = make_request_dfd(\n                client, Request(self.get_url(server_port, \"/get-data-html-small\"))\n            )\n            d.addCallback(lambda _: pytest.fail(\"This request should have failed\"))\n            d.addErrback(assert_inactive_stream)\n            d_list.append(d)\n\n        # Close the connection now to fire all the extra 10 requests errback\n        # with InactiveStreamClosed\n        assert client.transport\n        client.transport.loseConnection()\n\n        yield DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)\n\n    @deferred_f_from_coro_f\n    async def test_invalid_request_type(self, client: H2ClientProtocol):\n        with pytest.raises(TypeError):\n            await make_request(client, \"https://InvalidDataTypePassed.com\")  # type: ignore[arg-type]\n\n    @deferred_f_from_coro_f\n    async def test_query_parameters(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        params = {\n            \"a\": generate_random_string(20),\n            \"b\": generate_random_string(20),\n            \"c\": generate_random_string(20),\n            \"d\": generate_random_string(20),\n        }\n        request = Request(\n            self.get_url(server_port, f\"/query-params?{urlencode(params)}\")\n        )\n        response = await make_request(client, request)\n        content_encoding_header = response.headers[b\"Content-Encoding\"]\n        assert content_encoding_header is not None\n        content_encoding = str(content_encoding_header, \"utf-8\")\n        data = json.loads(str(response.body, content_encoding))\n        assert data == params\n\n    @deferred_f_from_coro_f\n    async def test_status_codes(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        for status in [200, 404]:\n            request = Request(self.get_url(server_port, f\"/status?n={status}\"))\n            response = await make_request(client, request)\n            assert response.status == status\n\n    @deferred_f_from_coro_f\n    async def test_response_has_correct_certificate_ip_address(\n        self,\n        server_port: int,\n        client: H2ClientProtocol,\n        client_certificate: PrivateCertificate,\n    ) -> None:\n        request = Request(self.get_url(server_port, \"/status?n=200\"))\n        response = await make_request(client, request)\n        assert response.request == request\n        assert isinstance(response.certificate, Certificate)\n        assert response.certificate.original is not None\n        assert response.certificate.getIssuer() == client_certificate.getIssuer()\n        assert response.certificate.getPublicKey().matches(\n            client_certificate.getPublicKey()\n        )\n        assert isinstance(response.ip_address, IPv4Address)\n        assert str(response.ip_address) == \"127.0.0.1\"\n\n    @staticmethod\n    async def _check_invalid_netloc(client: H2ClientProtocol, url: str) -> None:\n        from scrapy.core.http2.stream import InvalidHostname  # noqa: PLC0415\n\n        request = Request(url)\n        with pytest.raises(InvalidHostname) as exc_info:\n            await make_request(client, request)\n        error_msg = str(exc_info.value)\n        assert \"localhost\" in error_msg\n        assert \"127.0.0.1\" in error_msg\n        assert str(request) in error_msg\n\n    @deferred_f_from_coro_f\n    async def test_invalid_hostname(self, client: H2ClientProtocol) -> None:\n        await self._check_invalid_netloc(\n            client, \"https://notlocalhost.notlocalhostdomain\"\n        )\n\n    @deferred_f_from_coro_f\n    async def test_invalid_host_port(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        port = server_port + 1\n        await self._check_invalid_netloc(client, f\"https://127.0.0.1:{port}\")\n\n    @deferred_f_from_coro_f\n    async def test_connection_stays_with_invalid_requests(\n        self, server_port: int, client: H2ClientProtocol\n    ):\n        await maybe_deferred_to_future(self.test_invalid_hostname(client))\n        await maybe_deferred_to_future(self.test_invalid_host_port(server_port, client))\n        await maybe_deferred_to_future(self.test_GET_small_body(server_port, client))\n        await maybe_deferred_to_future(self.test_POST_small_json(server_port, client))\n", "n_tokens": 1070, "byte_len": 4920, "file_sha1": "96799316872b6b66d7cab2968dde1e69493b5318", "start_line": 600, "end_line": 719}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http2_client_protocol.py", "rel_path": "tests/test_http2_client_protocol.py", "module": "tests.test_http2_client_protocol", "ext": "py", "chunk_number": 6, "symbols": ["test_connection_timeout", "status", "protocol", "async", "core", "reasons", "loads", "await", "idle", "server", "port", "make", "request", "set", "timeout", "http", "http2", "response", "failed", "connection", "dict", "break", "deferred", "from", "decoded", "test", "with", "json", "scrapy", "more", "generate_random_string", "make_html_body", "parse", "render_GET", "make_response", "render_POST", "_delayed_render", "make_request_dfd", "site", "client_certificate", "get_url", "test_invalid_negotiated_protocol", "test_cancel_request", "test_received_dataloss_response", "test_inactive_stream", "assert_inactive_stream", "DummySpider", "Data", "GetDataHtmlSmall", "GetDataHtmlLarge"], "ast_kind": "function_or_method", "text": "    @inlineCallbacks\n    def test_connection_timeout(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> Generator[Deferred[Any], Any, None]:\n        request = Request(self.get_url(server_port, \"/timeout\"))\n\n        # Update the timer to 1s to test connection timeout\n        client.setTimeout(1)\n\n        with pytest.raises(ResponseFailed) as exc_info:\n            yield make_request_dfd(client, request)\n\n        for err in exc_info.value.reasons:\n            from scrapy.core.http2.protocol import H2ClientProtocol  # noqa: PLC0415\n\n            if isinstance(err, TxTimeoutError):\n                assert (\n                    f\"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s\"\n                    in str(err)\n                )\n                break\n        else:\n            pytest.fail(\"No TimeoutError raised.\")\n\n    @deferred_f_from_coro_f\n    async def test_request_headers_received(\n        self, server_port: int, client: H2ClientProtocol\n    ) -> None:\n        request = Request(\n            self.get_url(server_port, \"/request-headers\"),\n            headers={\"header-1\": \"header value 1\", \"header-2\": \"header value 2\"},\n        )\n        response = await make_request(client, request)\n        assert response.status == 200\n        assert response.request == request\n\n        response_headers = json.loads(str(response.body, \"utf-8\"))\n        assert isinstance(response_headers, dict)\n        for k, v in request.headers.items():\n            k_decoded, v_decoded = str(k, \"utf-8\"), str(v[0], \"utf-8\")\n            assert k_decoded in response_headers\n            assert v_decoded == response_headers[k_decoded]\n", "n_tokens": 363, "byte_len": 1654, "file_sha1": "96799316872b6b66d7cab2968dde1e69493b5318", "start_line": 720, "end_line": 762}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py", "rel_path": "tests/test_utils_iterators.py", "module": "tests.test_utils_iterators", "ext": "py", "chunk_number": 1, "symbols": ["xmliter", "test_xmliter", "test_xmliter_unusual_node", "test_xmliter_unicode", "TestXmliterBase", "encoding", "get", "testdata", "text", "response", "name", "iterators", "getall", "matchmenot", "xml", "schema", "xpath", "scrapy", "deprecation", "instance", "test", "taken", "matchme", "typing", "github", "namespace", "type", "selector", "args", "annotations", "test_xmliter_text", "test_xmliter_namespaces", "test_xmliter_namespaced_nodename", "test_xmliter_namespaced_nodename_missing", "test_xmliter_exception", "test_xmliter_objtype_exception", "test_xmliter_encoding", "test_deprecation", "test_xmliter_iterate_namespace", "test_xmliter_namespaces_prefix", "test_csviter_defaults", "test_csviter_delimiter", "test_csviter_quotechar", "test_csviter_wrong_quotechar", "test_csviter_delimiter_binary_response_assume_utf8_encoding", "test_csviter_headers", "test_csviter_falserow", "test_csviter_exception", "test_csviter_encoding", "test_body_or_str"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response, TextResponse, XmlResponse\nfrom scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml\nfrom tests import get_testdata\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n\n    from scrapy import Selector\n\n\nclass TestXmliterBase(ABC):\n    @abstractmethod\n    def xmliter(\n        self, obj: Response | str | bytes, nodename: str, *args: Any\n    ) -> Iterator[Selector]:\n        raise NotImplementedError\n\n    def test_xmliter(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                      xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n              <product id=\"001\">\n                <type>Type 1</type>\n                <name>Name 1</name>\n              </product>\n              <product id=\"002\">\n                <type>Type 2</type>\n                <name>Name 2</name>\n              </product>\n            </products>\n        \"\"\"\n\n        response = XmlResponse(url=\"http://example.com\", body=body)\n        attrs = [\n            (\n                x.attrib[\"id\"],\n                x.xpath(\"name/text()\").getall(),\n                x.xpath(\"./type/text()\").getall(),\n            )\n            for x in self.xmliter(response, \"product\")\n        ]\n\n        assert attrs == [\n            (\"001\", [\"Name 1\"], [\"Type 1\"]),\n            (\"002\", [\"Name 2\"], [\"Type 2\"]),\n        ]\n\n    def test_xmliter_unusual_node(self):\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <root>\n                <matchme...></matchme...>\n                <matchmenot></matchmenot>\n            </root>\n        \"\"\"\n        response = XmlResponse(url=\"http://example.com\", body=body)\n        nodenames = [\n            e.xpath(\"name()\").getall() for e in self.xmliter(response, \"matchme...\")\n        ]\n        assert nodenames == [[\"matchme...\"]]\n\n    def test_xmliter_unicode(self):\n        # example taken from https://github.com/scrapy/scrapy/issues/1665\n        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <þingflokkar>\n               <þingflokkur id=\"26\">\n                  <heiti />\n                  <skammstafanir>\n                     <stuttskammstöfun>-</stuttskammstöfun>\n                     <löngskammstöfun />\n                  </skammstafanir>\n                  <tímabil>\n                     <fyrstaþing>80</fyrstaþing>\n                  </tímabil>\n               </þingflokkur>\n               <þingflokkur id=\"21\">\n                  <heiti>Alþýðubandalag</heiti>\n                  <skammstafanir>\n                     <stuttskammstöfun>Ab</stuttskammstöfun>\n                     <löngskammstöfun>Alþb.</löngskammstöfun>\n                  </skammstafanir>\n                  <tímabil>\n                     <fyrstaþing>76</fyrstaþing>\n                     <síðastaþing>123</síðastaþing>\n                  </tímabil>\n               </þingflokkur>\n               <þingflokkur id=\"27\">\n                  <heiti>Alþýðuflokkur</heiti>\n                  <skammstafanir>\n                     <stuttskammstöfun>A</stuttskammstöfun>\n                     <löngskammstöfun>Alþfl.</löngskammstöfun>\n                  </skammstafanir>\n                  <tímabil>\n                     <fyrstaþing>27</fyrstaþing>\n                     <síðastaþing>120</síðastaþing>\n                  </tímabil>\n               </þingflokkur>\n            </þingflokkar>\"\"\"\n\n        for r in (\n            # with bytes\n            XmlResponse(url=\"http://example.com\", body=body.encode(\"utf-8\")),\n            # Unicode body needs encoding information\n            XmlResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\"),\n        ):\n            attrs = [\n                (\n                    x.attrib[\"id\"],\n                    x.xpath(\"./skammstafanir/stuttskammstöfun/text()\").getall(),\n                    x.xpath(\"./tímabil/fyrstaþing/text()\").getall(),\n                )\n                for x in self.xmliter(r, \"þingflokkur\")\n            ]\n\n            assert attrs == [\n                (\"26\", [\"-\"], [\"80\"]),\n                (\"21\", [\"Ab\"], [\"76\"]),\n                (\"27\", [\"A\"], [\"27\"]),\n            ]\n", "n_tokens": 1135, "byte_len": 4419, "file_sha1": "9b303df850544c33f492b297935ed20e91bbeedb", "start_line": 1, "end_line": 128}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py", "rel_path": "tests/test_utils_iterators.py", "module": "tests.test_utils_iterators", "ext": "py", "chunk_number": 2, "symbols": ["test_xmliter_text", "test_xmliter_namespaces", "test_xmliter_namespaced_nodename", "test_xmliter_namespaced_nodename_missing", "test_xmliter_exception", "test_xmliter_objtype_exception", "encoding", "test", "xmliter", "channel", "getall", "price", "text", "images", "xpath", "image", "link", "dummy", "company", "item", "mydummycompany", "products", "with", "nothing", "next", "body", "xmlns", "type", "error", "base", "test_xmliter", "test_xmliter_unusual_node", "test_xmliter_unicode", "test_xmliter_encoding", "test_deprecation", "test_xmliter_iterate_namespace", "test_xmliter_namespaces_prefix", "test_csviter_defaults", "test_csviter_delimiter", "test_csviter_quotechar", "test_csviter_wrong_quotechar", "test_csviter_delimiter_binary_response_assume_utf8_encoding", "test_csviter_headers", "test_csviter_falserow", "test_csviter_exception", "test_csviter_encoding", "test_body_or_str", "_assert_type_and_value", "TestXmliterBase", "TestXmliter"], "ast_kind": "function_or_method", "text": "    def test_xmliter_text(self):\n        body = (\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n            \"<products><product>one</product><product>two</product></products>\"\n        )\n\n        assert [x.xpath(\"text()\").getall() for x in self.xmliter(body, \"product\")] == [\n            [\"one\"],\n            [\"two\"],\n        ]\n\n    def test_xmliter_namespaces(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n                    <g:id>ITEM_1</g:id>\n                    <g:price>400</g:price>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"item\")\n        node = next(my_iter)\n        node.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\n        assert node.xpath(\"title/text()\").getall() == [\"Item 1\"]\n        assert node.xpath(\"description/text()\").getall() == [\"This is item 1\"]\n        assert node.xpath(\"link/text()\").getall() == [\n            \"http://www.mydummycompany.com/items/1\"\n        ]\n        assert node.xpath(\"g:image_link/text()\").getall() == [\n            \"http://www.mydummycompany.com/images/item1.jpg\"\n        ]\n        assert node.xpath(\"g:id/text()\").getall() == [\"ITEM_1\"]\n        assert node.xpath(\"g:price/text()\").getall() == [\"400\"]\n        assert node.xpath(\"image_link/text()\").getall() == []\n        assert node.xpath(\"id/text()\").getall() == []\n        assert node.xpath(\"price/text()\").getall() == []\n\n    def test_xmliter_namespaced_nodename(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n                    <g:id>ITEM_1</g:id>\n                    <g:price>400</g:price>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"g:image_link\")\n        node = next(my_iter)\n        node.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\n        assert node.xpath(\"text()\").extract() == [\n            \"http://www.mydummycompany.com/images/item1.jpg\"\n        ]\n\n    def test_xmliter_namespaced_nodename_missing(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n                    <g:id>ITEM_1</g:id>\n                    <g:price>400</g:price>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"g:link_image\")\n        with pytest.raises(StopIteration):\n            next(my_iter)\n\n    def test_xmliter_exception(self):\n        body = (\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n            \"<products><product>one</product><product>two</product></products>\"\n        )\n\n        my_iter = self.xmliter(body, \"product\")\n        next(my_iter)\n        next(my_iter)\n        with pytest.raises(StopIteration):\n            next(my_iter)\n\n    def test_xmliter_objtype_exception(self):\n        i = self.xmliter(42, \"product\")\n        with pytest.raises(TypeError):\n            next(i)\n", "n_tokens": 1165, "byte_len": 4914, "file_sha1": "9b303df850544c33f492b297935ed20e91bbeedb", "start_line": 129, "end_line": 244}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py", "rel_path": "tests/test_utils_iterators.py", "module": "tests.test_utils_iterators", "ext": "py", "chunk_number": 3, "symbols": ["test_xmliter_encoding", "xmliter", "test_deprecation", "test_xmliter_iterate_namespace", "test_xmliter_namespaces_prefix", "test_xmliter_objtype_exception", "TestXmliter", "TestLxmlXmliter", "TestUtilsCsv", "encoding", "xpath", "table", "selector", "name", "length", "schools", "w3schools", "namespace", "iter", "pytest", "bananas", "u015f", "google", "items", "item", "turkish", "u011e", "test", "http", "deprecation", "test_xmliter", "test_xmliter_unusual_node", "test_xmliter_unicode", "test_xmliter_text", "test_xmliter_namespaces", "test_xmliter_namespaced_nodename", "test_xmliter_namespaced_nodename_missing", "test_xmliter_exception", "test_csviter_defaults", "test_csviter_delimiter", "test_csviter_quotechar", "test_csviter_wrong_quotechar", "test_csviter_delimiter_binary_response_assume_utf8_encoding", "test_csviter_headers", "test_csviter_falserow", "test_csviter_exception", "test_csviter_encoding", "test_body_or_str", "_assert_type_and_value", "TestXmliterBase"], "ast_kind": "class_or_type", "text": "    def test_xmliter_encoding(self):\n        body = (\n            b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'\n            b\"<xml>\\n\"\n            b\"    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n\"\n            b\"</xml>\\n\\n\"\n        )\n        response = XmlResponse(\"http://www.example.com\", body=body)\n        assert (\n            next(self.xmliter(response, \"item\")).get()\n            == \"<item>Some Turkish Characters \\xd6\\xc7\\u015e\\u0130\\u011e\\xdc \\xfc\\u011f\\u0131\\u015f\\xe7\\xf6</item>\"\n        )\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestXmliter(TestXmliterBase):\n    def xmliter(\n        self, obj: Response | str | bytes, nodename: str, *args: Any\n    ) -> Iterator[Selector]:\n        return xmliter(obj, nodename)\n\n    def test_deprecation(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <products>\n              <product></product>\n            </products>\n        \"\"\"\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=\"xmliter\",\n        ):\n            next(self.xmliter(body, \"product\"))\n\n\nclass TestLxmlXmliter(TestXmliterBase):\n    def xmliter(\n        self, obj: Response | str | bytes, nodename: str, *args: Any\n    ) -> Iterator[Selector]:\n        return xmliter_lxml(obj, nodename, *args)\n\n    def test_xmliter_iterate_namespace(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <image_link>http://www.mydummycompany.com/images/item1.jpg</image_link>\n                    <image_link>http://www.mydummycompany.com/images/item2.jpg</image_link>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n\n        no_namespace_iter = self.xmliter(response, \"image_link\")\n        assert len(list(no_namespace_iter)) == 0\n\n        namespace_iter = self.xmliter(\n            response, \"image_link\", \"http://base.google.com/ns/1.0\"\n        )\n        node = next(namespace_iter)\n        assert node.xpath(\"text()\").getall() == [\n            \"http://www.mydummycompany.com/images/item1.jpg\"\n        ]\n        node = next(namespace_iter)\n        assert node.xpath(\"text()\").getall() == [\n            \"http://www.mydummycompany.com/images/item2.jpg\"\n        ]\n\n    def test_xmliter_namespaces_prefix(self):\n        body = b\"\"\"\n        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n        <root>\n            <h:table xmlns:h=\"http://www.w3.org/TR/html4/\">\n              <h:tr>\n                <h:td>Apples</h:td>\n                <h:td>Bananas</h:td>\n              </h:tr>\n            </h:table>\n\n            <f:table xmlns:f=\"http://www.w3schools.com/furniture\">\n              <f:name>African Coffee Table</f:name>\n              <f:width>80</f:width>\n              <f:length>120</f:length>\n            </f:table>\n\n        </root>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"table\", \"http://www.w3.org/TR/html4/\", \"h\")\n\n        node = next(my_iter)\n        assert len(node.xpath(\"h:tr/h:td\").getall()) == 2\n        assert node.xpath(\"h:tr/h:td[1]/text()\").getall() == [\"Apples\"]\n        assert node.xpath(\"h:tr/h:td[2]/text()\").getall() == [\"Bananas\"]\n\n        my_iter = self.xmliter(\n            response, \"table\", \"http://www.w3schools.com/furniture\", \"f\"\n        )\n\n        node = next(my_iter)\n        assert node.xpath(\"f:name/text()\").getall() == [\"African Coffee Table\"]\n\n    def test_xmliter_objtype_exception(self):\n        i = self.xmliter(42, \"product\")\n        with pytest.raises(TypeError):\n            next(i)\n\n\nclass TestUtilsCsv:", "n_tokens": 1081, "byte_len": 4230, "file_sha1": "9b303df850544c33f492b297935ed20e91bbeedb", "start_line": 245, "end_line": 361}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py", "rel_path": "tests/test_utils_iterators.py", "module": "tests.test_utils_iterators", "ext": "py", "chunk_number": 4, "symbols": ["test_csviter_defaults", "test_csviter_delimiter", "test_csviter_quotechar", "test_csviter_wrong_quotechar", "test_csviter_delimiter_binary_response_assume_utf8_encoding", "test_csviter_headers", "test", "csviter", "get", "testdata", "text", "response", "autocasting", "sample", "sample3", "result", "foobar", "splitlines", "u203d", "name", "check", "replace", "delimiter", "csv", "csv1", "quotechar", "xedc", "like", "explicit", "stinkin", "xmliter", "test_xmliter", "test_xmliter_unusual_node", "test_xmliter_unicode", "test_xmliter_text", "test_xmliter_namespaces", "test_xmliter_namespaced_nodename", "test_xmliter_namespaced_nodename_missing", "test_xmliter_exception", "test_xmliter_objtype_exception", "test_xmliter_encoding", "test_deprecation", "test_xmliter_iterate_namespace", "test_xmliter_namespaces_prefix", "test_csviter_falserow", "test_csviter_exception", "test_csviter_encoding", "test_body_or_str", "_assert_type_and_value", "TestXmliterBase"], "ast_kind": "function_or_method", "text": "    def test_csviter_defaults(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response)\n\n        result = list(csv)\n        assert result == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n\n        # explicit type check cuz' we no like stinkin' autocasting! yarrr\n        for result_row in result:\n            assert all(isinstance(k, str) for k in result_row)\n            assert all(isinstance(v, str) for v in result_row.values())\n\n    def test_csviter_delimiter(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\").replace(b\",\", b\"\\t\")\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response, delimiter=\"\\t\")\n\n        assert list(csv) == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n\n    def test_csviter_quotechar(self):\n        body1 = get_testdata(\"feeds\", \"feed-sample6.csv\")\n        body2 = get_testdata(\"feeds\", \"feed-sample6.csv\").replace(b\",\", b\"|\")\n\n        response1 = TextResponse(url=\"http://example.com/\", body=body1)\n        csv1 = csviter(response1, quotechar=\"'\")\n\n        assert list(csv1) == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n\n        response2 = TextResponse(url=\"http://example.com/\", body=body2)\n        csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n\n        assert list(csv2) == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n\n    def test_csviter_wrong_quotechar(self):\n        body = get_testdata(\"feeds\", \"feed-sample6.csv\")\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response)\n\n        assert list(csv) == [\n            {\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n            {\n                \"'id'\": \"2\",\n                \"'name'\": \"'unicode'\",\n                \"'value'\": \"'\\xfan\\xedc\\xf3d\\xe9\\u203d'\",\n            },\n            {\"'id'\": \"'3'\", \"'name'\": \"'multi'\", \"'value'\": \"'foo\"},\n            {\"'id'\": \"4\", \"'name'\": \"'empty'\", \"'value'\": \"\"},\n        ]\n\n    def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\").replace(b\",\", b\"\\t\")\n        response = Response(url=\"http://example.com/\", body=body)\n        csv = csviter(response, delimiter=\"\\t\")\n\n        assert list(csv) == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n\n    def test_csviter_headers(self):\n        sample = get_testdata(\"feeds\", \"feed-sample3.csv\").splitlines()\n        headers, body = sample[0].split(b\",\"), b\"\\n\".join(sample[1:])\n\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response, headers=[h.decode(\"utf-8\") for h in headers])\n\n        assert list(csv) == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n", "n_tokens": 1181, "byte_len": 4146, "file_sha1": "9b303df850544c33f492b297935ed20e91bbeedb", "start_line": 362, "end_line": 457}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_iterators.py", "rel_path": "tests/test_utils_iterators.py", "module": "tests.test_utils_iterators", "ext": "py", "chunk_number": 5, "symbols": ["test_csviter_falserow", "test_csviter_exception", "test_csviter_encoding", "test_body_or_str", "_assert_type_and_value", "TestBodyOrStr", "encoding", "utf", "utf8", "get", "testdata", "text", "response", "expected", "u2569", "false", "sample", "sample3", "cp852", "latin", "latin1", "foobar", "test", "csviter", "u203d", "name", "mark", "body", "class", "parametrize", "xmliter", "test_xmliter", "test_xmliter_unusual_node", "test_xmliter_unicode", "test_xmliter_text", "test_xmliter_namespaces", "test_xmliter_namespaced_nodename", "test_xmliter_namespaced_nodename_missing", "test_xmliter_exception", "test_xmliter_objtype_exception", "test_xmliter_encoding", "test_deprecation", "test_xmliter_iterate_namespace", "test_xmliter_namespaces_prefix", "test_csviter_defaults", "test_csviter_delimiter", "test_csviter_quotechar", "test_csviter_wrong_quotechar", "test_csviter_delimiter_binary_response_assume_utf8_encoding", "test_csviter_headers"], "ast_kind": "class_or_type", "text": "    def test_csviter_falserow(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n        body = b\"\\n\".join((body, b\"a,b\", b\"a,b,c,d\"))\n\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response)\n\n        assert list(csv) == [\n            {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n            {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n            {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n            {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n        ]\n\n    def test_csviter_exception(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        my_iter = csviter(response)\n        next(my_iter)\n        next(my_iter)\n        next(my_iter)\n        next(my_iter)\n        with pytest.raises(StopIteration):\n            next(my_iter)\n\n    def test_csviter_encoding(self):\n        body1 = get_testdata(\"feeds\", \"feed-sample4.csv\")\n        body2 = get_testdata(\"feeds\", \"feed-sample5.csv\")\n\n        response = TextResponse(\n            url=\"http://example.com/\", body=body1, encoding=\"latin1\"\n        )\n        csv = csviter(response)\n        assert list(csv) == [\n            {\"id\": \"1\", \"name\": \"latin1\", \"value\": \"test\"},\n            {\"id\": \"2\", \"name\": \"something\", \"value\": \"\\xf1\\xe1\\xe9\\xf3\"},\n        ]\n\n        response = TextResponse(url=\"http://example.com/\", body=body2, encoding=\"cp852\")\n        csv = csviter(response)\n        assert list(csv) == [\n            {\"id\": \"1\", \"name\": \"cp852\", \"value\": \"test\"},\n            {\n                \"id\": \"2\",\n                \"name\": \"something\",\n                \"value\": \"\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557\",\n            },\n        ]\n\n\nclass TestBodyOrStr:\n    bbody = b\"utf8-body\"\n    ubody = bbody.decode(\"utf8\")\n\n    @pytest.mark.parametrize(\n        \"obj\",\n        [\n            bbody,\n            ubody,\n            TextResponse(url=\"http://example.org/\", body=bbody, encoding=\"utf-8\"),\n            Response(url=\"http://example.org/\", body=bbody),\n        ],\n    )\n    def test_body_or_str(self, obj: Response | str | bytes) -> None:\n        r1 = _body_or_str(obj)\n        self._assert_type_and_value(r1, self.ubody, obj)\n        r2 = _body_or_str(obj, unicode=True)\n        self._assert_type_and_value(r2, self.ubody, obj)\n        r3 = _body_or_str(obj, unicode=False)\n        self._assert_type_and_value(r3, self.bbody, obj)\n        assert type(r1) is type(r2)\n        assert type(r1) is not type(r3)\n\n    @staticmethod\n    def _assert_type_and_value(\n        a: str | bytes, b: str | bytes, obj: Response | str | bytes\n    ) -> None:\n        assert type(a) is type(b), f\"Got {type(a)}, expected {type(b)} for {obj!r}\"\n        assert a == b\n", "n_tokens": 761, "byte_len": 2786, "file_sha1": "9b303df850544c33f492b297935ed20e91bbeedb", "start_line": 458, "end_line": 538}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_dependencies.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_dependencies.py", "rel_path": "tests/test_dependencies.py", "module": "tests.test_dependencies", "ext": "py", "chunk_number": 1, "symbols": ["test_pinned_twisted_version", "TestScrapyUtils", "pinned", "pattern", "file", "twisted", "github", "class", "when", "with", "scrapy", "environ", "https", "issuecomment", "pathlib", "config", "parser", "path", "within", "version", "deps", "running", "make", "pull", "test", "pytest", "from", "parent", "assert", "sure", "none", "tox", "environment", "import", "short", "scrap", "self", "configparser", "tests", "skip", "match", "that", "search", "dependencies", "read"], "ast_kind": "class_or_type", "text": "import os\nimport re\nfrom configparser import ConfigParser\nfrom pathlib import Path\n\nimport pytest\nfrom twisted import version as twisted_version\n\n\nclass TestScrapyUtils:\n    def test_pinned_twisted_version(self):\n        \"\"\"When running tests within a Tox environment with pinned\n        dependencies, make sure that the version of Twisted is the pinned\n        version.\n\n        See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011\n        \"\"\"\n        if not os.environ.get(\"_SCRAPY_PINNED\", None):\n            pytest.skip(\"Not in a pinned environment\")\n\n        tox_config_file_path = Path(__file__).parent / \"..\" / \"tox.ini\"\n        config_parser = ConfigParser()\n        config_parser.read(tox_config_file_path)\n        pattern = r\"Twisted==([\\d.]+)\"\n        match = re.search(pattern, config_parser[\"pinned\"][\"deps\"])\n        pinned_twisted_version_string = match[1]\n\n        assert twisted_version.short() == pinned_twisted_version_string\n", "n_tokens": 215, "byte_len": 962, "file_sha1": "29d2e9959587e66f98ae7681a437a9b6ac0d7f62", "start_line": 1, "end_line": 29}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_mail.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_mail.py", "rel_path": "tests/test_mail.py", "module": "tests.test_mail", "ext": "py", "chunk_number": 1, "symbols": ["test_send", "test_send_single_values_to_and_cc", "test_send_html", "test_send_attach", "_catch_mail_sent", "test_send_utf8", "test_send_attach_utf8", "test_create_sender_factory_with_host", "TestMailSender", "charset", "text", "seek", "false", "factory", "attach", "internet", "test", "send", "attachment", "testhost", "mail", "smtphost", "plain", "twisted", "mimetype", "payload", "email", "type", "context", "mailsender", "subject", "class", "attachs", "get", "scrapy", "defer", "body", "build", "protocol", "decode", "sslverify", "create", "true", "debug", "from", "list", "smtp", "assert", "kwargs", "multipart"], "ast_kind": "class_or_type", "text": "from email.charset import Charset\nfrom io import BytesIO\n\nfrom twisted.internet import defer\nfrom twisted.internet._sslverify import ClientTLSOptions\n\nfrom scrapy.mail import MailSender\n\n\nclass TestMailSender:\n    def test_send(self):\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=\"subject\",\n            body=\"body\",\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n\n        assert self.catched_msg[\"to\"] == [\"test@scrapy.org\"]\n        assert self.catched_msg[\"subject\"] == \"subject\"\n        assert self.catched_msg[\"body\"] == \"body\"\n\n        msg = self.catched_msg[\"msg\"]\n        assert msg[\"to\"] == \"test@scrapy.org\"\n        assert msg[\"subject\"] == \"subject\"\n        assert msg.get_payload() == \"body\"\n        assert msg.get(\"Content-Type\") == \"text/plain\"\n\n    def test_send_single_values_to_and_cc(self):\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=\"test@scrapy.org\",\n            subject=\"subject\",\n            body=\"body\",\n            cc=\"test@scrapy.org\",\n            _callback=self._catch_mail_sent,\n        )\n\n    def test_send_html(self):\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=\"subject\",\n            body=\"<p>body</p>\",\n            mimetype=\"text/html\",\n            _callback=self._catch_mail_sent,\n        )\n\n        msg = self.catched_msg[\"msg\"]\n        assert msg.get_payload() == \"<p>body</p>\"\n        assert msg.get(\"Content-Type\") == \"text/html\"\n\n    def test_send_attach(self):\n        attach = BytesIO()\n        attach.write(b\"content\")\n        attach.seek(0)\n        attachs = [(\"attachment\", \"text/plain\", attach)]\n\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=\"subject\",\n            body=\"body\",\n            attachs=attachs,\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n        assert self.catched_msg[\"to\"] == [\"test@scrapy.org\"]\n        assert self.catched_msg[\"subject\"] == \"subject\"\n        assert self.catched_msg[\"body\"] == \"body\"\n\n        msg = self.catched_msg[\"msg\"]\n        assert msg[\"to\"] == \"test@scrapy.org\"\n        assert msg[\"subject\"] == \"subject\"\n\n        payload = msg.get_payload()\n        assert isinstance(payload, list)\n        assert len(payload) == 2\n\n        text, attach = payload\n        assert text.get_payload(decode=True) == b\"body\"\n        assert text.get_charset() == Charset(\"us-ascii\")\n        assert attach.get_payload(decode=True) == b\"content\"\n\n    def _catch_mail_sent(self, **kwargs):\n        self.catched_msg = {**kwargs}\n\n    def test_send_utf8(self):\n        subject = \"sübjèçt\"\n        body = \"bödÿ-àéïöñß\"\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=subject,\n            body=body,\n            charset=\"utf-8\",\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n        assert self.catched_msg[\"subject\"] == subject\n        assert self.catched_msg[\"body\"] == body\n\n        msg = self.catched_msg[\"msg\"]\n        assert msg[\"subject\"] == subject\n        assert msg.get_payload(decode=True).decode(\"utf-8\") == body\n        assert msg.get_charset() == Charset(\"utf-8\")\n        assert msg.get(\"Content-Type\") == 'text/plain; charset=\"utf-8\"'\n\n    def test_send_attach_utf8(self):\n        subject = \"sübjèçt\"\n        body = \"bödÿ-àéïöñß\"\n        attach = BytesIO()\n        attach.write(body.encode(\"utf-8\"))\n        attach.seek(0)\n        attachs = [(\"attachment\", \"text/plain\", attach)]\n\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=subject,\n            body=body,\n            attachs=attachs,\n            charset=\"utf-8\",\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n        assert self.catched_msg[\"subject\"] == subject\n        assert self.catched_msg[\"body\"] == body\n\n        msg = self.catched_msg[\"msg\"]\n        assert msg[\"subject\"] == subject\n        assert msg.get_charset() == Charset(\"utf-8\")\n        assert msg.get(\"Content-Type\") == 'multipart/mixed; charset=\"utf-8\"'\n\n        payload = msg.get_payload()\n        assert isinstance(payload, list)\n        assert len(payload) == 2\n\n        text, attach = payload\n        assert text.get_payload(decode=True).decode(\"utf-8\") == body\n        assert text.get_charset() == Charset(\"utf-8\")\n        assert attach.get_payload(decode=True).decode(\"utf-8\") == body\n\n    def test_create_sender_factory_with_host(self):\n        mailsender = MailSender(debug=False, smtphost=\"smtp.testhost.com\")\n\n        factory = mailsender._create_sender_factory(\n            to_addrs=[\"test@scrapy.org\"], msg=\"test\", d=defer.Deferred()\n        )\n\n        context = factory.buildProtocol(\"test@scrapy.org\").context\n        assert isinstance(context, ClientTLSOptions)\n", "n_tokens": 1193, "byte_len": 5095, "file_sha1": "60afe53c10e61f6191bd714e4477fdcdaf86e0d0", "start_line": 1, "end_line": 159}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_poet.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_poet.py", "rel_path": "tests/test_poet.py", "module": "tests.test_poet", "ext": "py", "chunk_number": 1, "symbols": ["test_callbacks", "poet", "resolved", "sitemap", "spider", "typing", "annotations", "work", "crawl", "parse", "spiders", "scrapy", "get", "type", "make", "stack", "making", "from", "parts", "needed", "sure", "csv", "feed", "callback", "callbacks", "import", "test", "xml", "that", "tests", "abstract"], "ast_kind": "function_or_method", "text": "\"\"\"Tests that make sure parts needed for the scrapy-poet stack work.\"\"\"\n\nfrom typing import get_type_hints\n\nfrom scrapy import Spider\nfrom scrapy.spiders import CrawlSpider, CSVFeedSpider, SitemapSpider, XMLFeedSpider\n\n\ndef test_callbacks():\n    \"\"\"Making sure annotations on all non-abstract callbacks can be resolved.\"\"\"\n\n    for cb in [\n        Spider._parse,\n        CrawlSpider._parse,\n        CrawlSpider._callback,\n        XMLFeedSpider._parse,\n        CSVFeedSpider._parse,\n        SitemapSpider._parse_sitemap,\n    ]:\n        get_type_hints(cb)\n", "n_tokens": 125, "byte_len": 554, "file_sha1": "cc78e780c7a4d260161349aa1ae42cb68e0064aa", "start_line": 1, "end_line": 21}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_signals.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_signals.py", "rel_path": "tests/test_signals.py", "module": "tests.test_signals", "ext": "py", "chunk_number": 1, "symbols": ["parse", "track_call", "setup_class", "teardown_class", "setup_method", "test_simple_pipeline", "ItemSpider", "TestMain", "TestMockServer", "itemspider", "async", "item", "spider", "await", "internet", "teardown", "class", "append", "test", "scheduler", "exit", "simple", "twisted", "return", "name", "scraped", "deferred", "from", "maybe", "meta", "mark", "only", "asyncio", "scrapy", "defer", "classmethod", "mockserver", "get", "crawler", "index", "connect", "enter", "yield", "pytest", "assert", "signals", "request", "items", "mock", "server"], "ast_kind": "class_or_type", "text": "import pytest\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\nfrom tests.mockserver.http import MockServer\n\n\nclass ItemSpider(Spider):\n    name = \"itemspider\"\n\n    async def start(self):\n        for index in range(10):\n            yield Request(\n                self.mockserver.url(f\"/status?n=200&id={index}\"), meta={\"index\": index}\n            )\n\n    def parse(self, response):\n        return {\"index\": response.meta[\"index\"]}\n\n\nclass TestMain:\n    @deferred_f_from_coro_f\n    async def test_scheduler_empty(self):\n        crawler = get_crawler()\n        calls = []\n\n        def track_call():\n            calls.append(object())\n\n        crawler.signals.connect(track_call, signals.scheduler_empty)\n        await maybe_deferred_to_future(crawler.crawl())\n        assert len(calls) >= 1\n\n\nclass TestMockServer:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        self.items = []\n\n    async def _on_item_scraped(self, item):\n        item = await get_from_asyncio_queue(item)\n        self.items.append(item)\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_simple_pipeline(self):\n        crawler = get_crawler(ItemSpider)\n        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(self.items) == 10\n        for index in range(10):\n            assert {\"index\": index} in self.items\n", "n_tokens": 397, "byte_len": 1780, "file_sha1": "6093e6610bb3e26e54d58fd0973719232d917537", "start_line": 1, "end_line": 63}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_log.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_log.py", "rel_path": "tests/test_utils_log.py", "module": "tests.test_utils_log", "ext": "py", "chunk_number": 1, "symbols": ["test_failure", "test_non_failure", "setup_method", "test_top_level_logger", "test_children_logger", "test_overlapping_name_logger", "test_different_name_logger", "crawler", "logger", "test_init", "test_accepted_level", "test_filtered_out_level", "test_redirect", "test_spider_logger_adapter_process", "log_stream", "spider", "test_debug_logging", "TestFailureToExcInfo", "TestTopLevelFormatter", "TestLogCounterHandler", "TestStreamLogger", "TestLogging", "failure", "expected", "extra", "test", "logging", "spiders", "future", "typ", "test_info_logging", "test_warning_logging", "test_error_logging", "test_critical_logging", "test_overwrite_spider_extra", "TestLoggingWithExtra", "loads", "important", "info", "checking", "regex", "pattern", "get", "log", "pytest", "debug", "settings", "critical", "children", "add"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport json\nimport logging\nimport re\nimport sys\nfrom io import StringIO\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.python.failure import Failure\n\nfrom scrapy.utils.log import (\n    LogCounterHandler,\n    SpiderLoggerAdapter,\n    StreamLogger,\n    TopLevelFormatter,\n    failure_to_exc_info,\n)\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import LogSpider\n\nif TYPE_CHECKING:\n    from collections.abc import Generator, Mapping, MutableMapping\n\n    from scrapy.crawler import Crawler\n\n\nclass TestFailureToExcInfo:\n    def test_failure(self):\n        try:\n            0 / 0\n        except ZeroDivisionError:\n            exc_info = sys.exc_info()\n            failure = Failure()\n\n        assert exc_info == failure_to_exc_info(failure)\n\n    def test_non_failure(self):\n        assert failure_to_exc_info(\"test\") is None\n\n\nclass TestTopLevelFormatter:\n    def setup_method(self):\n        self.handler = LogCapture()\n        self.handler.addFilter(TopLevelFormatter([\"test\"]))\n\n    def test_top_level_logger(self):\n        logger = logging.getLogger(\"test\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"test\", \"WARNING\", \"test log msg\"))\n\n    def test_children_logger(self):\n        logger = logging.getLogger(\"test.test1\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"test\", \"WARNING\", \"test log msg\"))\n\n    def test_overlapping_name_logger(self):\n        logger = logging.getLogger(\"test2\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"test2\", \"WARNING\", \"test log msg\"))\n\n    def test_different_name_logger(self):\n        logger = logging.getLogger(\"different\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"different\", \"WARNING\", \"test log msg\"))\n\n\nclass TestLogCounterHandler:\n    @pytest.fixture\n    def crawler(self) -> Crawler:\n        settings = {\"LOG_LEVEL\": \"WARNING\"}\n        return get_crawler(settings_dict=settings)\n\n    @pytest.fixture\n    def logger(self, crawler: Crawler) -> Generator[logging.Logger]:\n        logger = logging.getLogger(\"test\")\n        logger.setLevel(logging.NOTSET)\n        logger.propagate = False\n        handler = LogCounterHandler(crawler)\n        logger.addHandler(handler)\n\n        yield logger\n\n        logger.propagate = True\n        logger.removeHandler(handler)\n\n    def test_init(self, crawler: Crawler, logger: logging.Logger) -> None:\n        assert crawler.stats\n        assert crawler.stats.get_value(\"log_count/DEBUG\") is None\n        assert crawler.stats.get_value(\"log_count/INFO\") is None\n        assert crawler.stats.get_value(\"log_count/WARNING\") is None\n        assert crawler.stats.get_value(\"log_count/ERROR\") is None\n        assert crawler.stats.get_value(\"log_count/CRITICAL\") is None\n\n    def test_accepted_level(self, crawler: Crawler, logger: logging.Logger) -> None:\n        logger.error(\"test log msg\")\n        assert crawler.stats\n        assert crawler.stats.get_value(\"log_count/ERROR\") == 1\n\n    def test_filtered_out_level(self, crawler: Crawler, logger: logging.Logger) -> None:\n        logger.debug(\"test log msg\")\n        assert crawler.stats\n        assert crawler.stats.get_value(\"log_count/INFO\") is None\n\n\nclass TestStreamLogger:\n    def test_redirect(self):\n        logger = logging.getLogger(\"test\")\n        logger.setLevel(logging.WARNING)\n        old_stdout = sys.stdout\n        sys.stdout = StreamLogger(logger, logging.ERROR)\n\n        with LogCapture() as log:\n            print(\"test log msg\")\n        log.check((\"test\", \"ERROR\", \"test log msg\"))\n\n        sys.stdout = old_stdout\n\n\n@pytest.mark.parametrize(\n    (\"base_extra\", \"log_extra\", \"expected_extra\"),\n    [\n        (\n            {\"spider\": \"test\"},\n            {\"extra\": {\"log_extra\": \"info\"}},\n            {\"extra\": {\"log_extra\": \"info\", \"spider\": \"test\"}},\n        ),\n        (\n            {\"spider\": \"test\"},\n            {\"extra\": None},\n            {\"extra\": {\"spider\": \"test\"}},\n        ),\n        (\n            {\"spider\": \"test\"},\n            {\"extra\": {\"spider\": \"test2\"}},\n            {\"extra\": {\"spider\": \"test\"}},\n        ),\n    ],\n)\ndef test_spider_logger_adapter_process(\n    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: dict\n) -> None:\n    logger = logging.getLogger(\"test\")\n    spider_logger_adapter = SpiderLoggerAdapter(logger, base_extra)\n\n    log_message = \"test_log_message\"\n    result_message, result_kwargs = spider_logger_adapter.process(\n        log_message, log_extra\n    )\n\n    assert result_message == log_message\n    assert result_kwargs == expected_extra\n\n\nclass TestLogging:\n    @pytest.fixture\n    def log_stream(self) -> StringIO:\n        return StringIO()\n\n    @pytest.fixture\n    def spider(self) -> LogSpider:\n        return LogSpider()\n\n    @pytest.fixture(autouse=True)\n    def logger(self, log_stream: StringIO) -> Generator[logging.Logger]:\n        handler = logging.StreamHandler(log_stream)\n        logger = logging.getLogger(\"log_spider\")\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n\n        yield logger\n\n        logger.removeHandler(handler)\n\n    def test_debug_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Foo message\"\n        spider.log_debug(log_message)\n        log_contents = log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n", "n_tokens": 1191, "byte_len": 5555, "file_sha1": "52fff651a0a2f8c2b3d4d90d4e1ea6cb4bd0b950", "start_line": 1, "end_line": 187}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_log.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_log.py", "rel_path": "tests/test_utils_log.py", "module": "tests.test_utils_log", "ext": "py", "chunk_number": 2, "symbols": ["test_info_logging", "test_warning_logging", "test_error_logging", "test_critical_logging", "log_stream", "spider", "logger", "test_debug_logging", "test_overwrite_spider_extra", "TestLoggingWithExtra", "test", "critical", "warning", "formatter", "levelname", "loads", "compile", "info", "important", "extra", "return", "getvalue", "logging", "autouse", "class", "log", "contents", "json", "stream", "handler", "test_failure", "test_non_failure", "setup_method", "test_top_level_logger", "test_children_logger", "test_overlapping_name_logger", "test_different_name_logger", "crawler", "test_init", "test_accepted_level", "test_filtered_out_level", "test_redirect", "test_spider_logger_adapter_process", "TestFailureToExcInfo", "TestTopLevelFormatter", "TestLogCounterHandler", "TestStreamLogger", "TestLogging", "failure", "expected"], "ast_kind": "class_or_type", "text": "    def test_info_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Bar message\"\n        spider.log_info(log_message)\n        log_contents = log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_warning_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Baz message\"\n        spider.log_warning(log_message)\n        log_contents = log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_error_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Foo bar message\"\n        spider.log_error(log_message)\n        log_contents = log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_critical_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Foo bar baz message\"\n        spider.log_critical(log_message)\n        log_contents = log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n\nclass TestLoggingWithExtra:\n    regex_pattern = re.compile(r\"^<LogSpider\\s'log_spider'\\sat\\s[^>]+>$\")\n\n    @pytest.fixture\n    def log_stream(self) -> StringIO:\n        return StringIO()\n\n    @pytest.fixture\n    def spider(self) -> LogSpider:\n        return LogSpider()\n\n    @pytest.fixture(autouse=True)\n    def logger(self, log_stream: StringIO) -> Generator[logging.Logger]:\n        handler = logging.StreamHandler(log_stream)\n        formatter = logging.Formatter(\n            '{\"levelname\": \"%(levelname)s\", \"message\": \"%(message)s\", \"spider\": \"%(spider)s\", \"important_info\": \"%(important_info)s\"}'\n        )\n        handler.setFormatter(formatter)\n        logger = logging.getLogger(\"log_spider\")\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n\n        yield logger\n\n        logger.removeHandler(handler)\n\n    def test_debug_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Foo message\"\n        extra = {\"important_info\": \"foo\"}\n        spider.log_debug(log_message, extra)\n        log_contents_str = log_stream.getvalue()\n        log_contents = json.loads(log_contents_str)\n\n        assert log_contents[\"levelname\"] == \"DEBUG\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_info_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Bar message\"\n        extra = {\"important_info\": \"bar\"}\n        spider.log_info(log_message, extra)\n        log_contents_str = log_stream.getvalue()\n        log_contents = json.loads(log_contents_str)\n\n        assert log_contents[\"levelname\"] == \"INFO\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_warning_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Baz message\"\n        extra = {\"important_info\": \"baz\"}\n        spider.log_warning(log_message, extra)\n        log_contents_str = log_stream.getvalue()\n        log_contents = json.loads(log_contents_str)\n\n        assert log_contents[\"levelname\"] == \"WARNING\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_error_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Foo bar message\"\n        extra = {\"important_info\": \"foo bar\"}\n        spider.log_error(log_message, extra)\n        log_contents_str = log_stream.getvalue()\n        log_contents = json.loads(log_contents_str)\n\n        assert log_contents[\"levelname\"] == \"ERROR\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_critical_logging(self, log_stream: StringIO, spider: LogSpider) -> None:\n        log_message = \"Foo bar baz message\"\n        extra = {\"important_info\": \"foo bar baz\"}\n        spider.log_critical(log_message, extra)\n        log_contents_str = log_stream.getvalue()\n        log_contents = json.loads(log_contents_str)\n\n        assert log_contents[\"levelname\"] == \"CRITICAL\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_overwrite_spider_extra(\n        self, log_stream: StringIO, spider: LogSpider\n    ) -> None:\n        log_message = \"Foo message\"\n        extra = {\"important_info\": \"foo\", \"spider\": \"shouldn't change\"}\n        spider.log_error(log_message, extra)\n        log_contents_str = log_stream.getvalue()\n        log_contents = json.loads(log_contents_str)\n\n        assert log_contents[\"levelname\"] == \"ERROR\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n", "n_tokens": 1117, "byte_len": 5276, "file_sha1": "52fff651a0a2f8c2b3d4d90d4e1ea6cb4bd0b950", "start_line": 188, "end_line": 316}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_link.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_link.py", "rel_path": "tests/test_link.py", "module": "tests.test_link", "ext": "py", "chunk_number": 1, "symbols": ["_assert_same_links", "_assert_different_links", "test_eq_and_hash", "test_repr", "test_bytes_url", "TestLink", "fragment", "used", "repr", "text", "false", "class", "link", "eval", "with", "scrapy", "example", "test", "bytes", "pylint", "hash", "other", "assert", "same", "type", "error", "true", "pytest", "from", "link2", "and", "something", "raises", "nofollow", "import", "self", "http", "disable", "different", "test2", "link1"], "ast_kind": "class_or_type", "text": "import pytest\n\nfrom scrapy.link import Link\n\n\nclass TestLink:\n    def _assert_same_links(self, link1, link2):\n        assert link1 == link2\n        assert hash(link1) == hash(link2)\n\n    def _assert_different_links(self, link1, link2):\n        assert link1 != link2\n        assert hash(link1) != hash(link2)\n\n    def test_eq_and_hash(self):\n        l1 = Link(\"http://www.example.com\")\n        l2 = Link(\"http://www.example.com/other\")\n        l3 = Link(\"http://www.example.com\")\n\n        self._assert_same_links(l1, l1)\n        self._assert_different_links(l1, l2)\n        self._assert_same_links(l1, l3)\n\n        l4 = Link(\"http://www.example.com\", text=\"test\")\n        l5 = Link(\"http://www.example.com\", text=\"test2\")\n        l6 = Link(\"http://www.example.com\", text=\"test\")\n\n        self._assert_same_links(l4, l4)\n        self._assert_different_links(l4, l5)\n        self._assert_same_links(l4, l6)\n\n        l7 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n        )\n        l8 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n        )\n        l9 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n        )\n        l10 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"other\", nofollow=False\n        )\n        self._assert_same_links(l7, l8)\n        self._assert_different_links(l7, l9)\n        self._assert_different_links(l7, l10)\n\n    def test_repr(self):\n        l1 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n        )\n        l2 = eval(repr(l1))  # pylint: disable=eval-used\n        self._assert_same_links(l1, l2)\n\n    def test_bytes_url(self):\n        with pytest.raises(TypeError):\n            Link(b\"http://www.example.com/\\xc2\\xa3\")\n", "n_tokens": 486, "byte_len": 1862, "file_sha1": "49bb3458968a11eee3f3c9e07b86fd2819ffee5e", "start_line": 1, "end_line": 58}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_urllength.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_urllength.py", "rel_path": "tests/test_spidermiddleware_urllength.py", "module": "tests.test_spidermiddleware_urllength", "ext": "py", "chunk_number": 1, "symbols": ["crawler", "stats", "mw", "process_spider_output", "test_middleware_works", "test_logging", "urllength", "statscollectors", "text", "ignoring", "typing", "return", "reqs", "test", "logging", "spider", "middleware", "annotations", "url", "length", "with", "level", "spiders", "scrapy", "future", "typ", "checking", "log", "capture", "get", "from", "urllengt", "limit", "pytest", "collector", "list", "scrapytest", "assert", "maxlength", "request", "long", "spidermiddlewares", "none", "fixture", "ignored", "info", "utils", "import", "http", "link"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nfrom logging import INFO\nfrom typing import TYPE_CHECKING\n\nimport pytest\n\nfrom scrapy.http import Request, Response\nfrom scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nmaxlength = 25\nresponse = Response(\"http://scrapytest.org\")\nshort_url_req = Request(\"http://scrapytest.org/\")\nlong_url_req = Request(\"http://scrapytest.org/this_is_a_long_url\")\nreqs: list[Request] = [short_url_req, long_url_req]\n\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider, {\"URLLENGTH_LIMIT\": maxlength})\n\n\n@pytest.fixture\ndef stats(crawler: Crawler) -> StatsCollector:\n    assert crawler.stats is not None\n    return crawler.stats\n\n\n@pytest.fixture\ndef mw(crawler: Crawler) -> UrlLengthMiddleware:\n    return UrlLengthMiddleware.from_crawler(crawler)\n\n\ndef process_spider_output(mw: UrlLengthMiddleware) -> list[Request]:\n    return list(mw.process_spider_output(response, reqs))\n\n\ndef test_middleware_works(mw: UrlLengthMiddleware) -> None:\n    assert process_spider_output(mw) == [short_url_req]\n\n\ndef test_logging(\n    stats: StatsCollector, mw: UrlLengthMiddleware, caplog: pytest.LogCaptureFixture\n) -> None:\n    with caplog.at_level(INFO):\n        process_spider_output(mw)\n    ric = stats.get_value(\"urllength/request_ignored_count\")\n    assert ric == 1\n    assert f\"Ignoring link (url length > {maxlength})\" in caplog.text\n", "n_tokens": 377, "byte_len": 1572, "file_sha1": "e22dc0c31bf2b25c9839ffeac879a0d5c53001a8", "start_line": 1, "end_line": 57}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_closespider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_closespider.py", "rel_path": "tests/test_closespider.py", "module": "tests.test_closespider", "ext": "py", "chunk_number": 1, "symbols": ["setup_class", "teardown_class", "test_closespider_itemcount", "test_closespider_pagecount", "test_closespider_pagecount_no_item", "test_closespider_pagecount_no_item_with_pagecount", "test_closespider_errorcount", "test_closespider_timeout", "test_closespider_timeout_no_item", "TestCloseSpider", "max", "requests", "items", "errorcount", "item", "spider", "internet", "teardown", "class", "slow", "closespide", "itemcount", "closespider", "exit", "test", "timeout", "twisted", "close", "reason", "pagecount", "pagecoun", "name", "meta", "spiders", "scrapy", "timeou", "defer", "total", "classmethod", "mockserver", "exceptions", "get", "crawler", "enter", "yield", "from", "assert", "exception", "cls", "scraped"], "ast_kind": "class_or_type", "text": "from twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import (\n    ErrorSpider,\n    FollowAllSpider,\n    ItemSpider,\n    MaxItemsAndRequestsSpider,\n    SlowSpider,\n)\n\n\nclass TestCloseSpider:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @inlineCallbacks\n    def test_closespider_itemcount(self):\n        close_on = 5\n        crawler = get_crawler(ItemSpider, {\"CLOSESPIDER_ITEMCOUNT\": close_on})\n        yield crawler.crawl(mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_itemcount\"\n        itemcount = crawler.stats.get_value(\"item_scraped_count\")\n        assert itemcount >= close_on\n\n    @inlineCallbacks\n    def test_closespider_pagecount(self):\n        close_on = 5\n        crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_PAGECOUNT\": close_on})\n        yield crawler.crawl(mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_pagecount\"\n        pagecount = crawler.stats.get_value(\"response_received_count\")\n        assert pagecount >= close_on\n\n    @inlineCallbacks\n    def test_closespider_pagecount_no_item(self):\n        close_on = 5\n        max_items = 5\n        max_requests = close_on + max_items\n        crawler = get_crawler(\n            MaxItemsAndRequestsSpider,\n            {\n                \"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on,\n            },\n        )\n        yield crawler.crawl(\n            max_items=max_items, max_requests=max_requests, mockserver=self.mockserver\n        )\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_pagecount_no_item\"\n        pagecount = crawler.stats.get_value(\"response_received_count\")\n        itemcount = crawler.stats.get_value(\"item_scraped_count\")\n        assert pagecount <= close_on + itemcount\n\n    @inlineCallbacks\n    def test_closespider_pagecount_no_item_with_pagecount(self):\n        close_on_pagecount_no_item = 5\n        close_on_pagecount = 20\n        crawler = get_crawler(\n            FollowAllSpider,\n            {\n                \"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on_pagecount_no_item,\n                \"CLOSESPIDER_PAGECOUNT\": close_on_pagecount,\n            },\n        )\n        yield crawler.crawl(mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_pagecount_no_item\"\n        pagecount = crawler.stats.get_value(\"response_received_count\")\n        assert pagecount < close_on_pagecount\n\n    @inlineCallbacks\n    def test_closespider_errorcount(self):\n        close_on = 5\n        crawler = get_crawler(ErrorSpider, {\"CLOSESPIDER_ERRORCOUNT\": close_on})\n        yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_errorcount\"\n        key = f\"spider_exceptions/{crawler.spider.exception_cls.__name__}\"\n        errorcount = crawler.stats.get_value(key)\n        assert crawler.stats.get_value(\"spider_exceptions/count\") >= close_on\n        assert errorcount >= close_on\n\n    @inlineCallbacks\n    def test_closespider_timeout(self):\n        close_on = 0.1\n        crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_TIMEOUT\": close_on})\n        yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_timeout\"\n        total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n        assert total_seconds >= close_on\n\n    @inlineCallbacks\n    def test_closespider_timeout_no_item(self):\n        timeout = 1\n        crawler = get_crawler(SlowSpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n        yield crawler.crawl(n=3, mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        assert reason == \"closespider_timeout_no_item\"\n        total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n        assert total_seconds >= timeout\n", "n_tokens": 960, "byte_len": 4283, "file_sha1": "34eae3e63e6243de4d430ea125e414beecedcd16", "start_line": 1, "end_line": 112}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http2.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http2.py", "rel_path": "tests/test_downloader_handler_twisted_http2.py", "module": "tests.test_downloader_handler_twisted_http2", "ext": "py", "chunk_number": 1, "symbols": ["download_handler_cls", "check", "test_download_cause_data_loss", "test_download_allow_data_loss", "test_download_allow_data_loss_via_setting", "H2DownloadHandlerMixin", "TestHttps2", "method", "async", "protocol", "htt", "datalos", "mock", "works", "connection", "after", "stream", "spider", "data", "received", "deferred", "from", "enabled", "download", "handler", "spiders", "future", "typ", "checking", "https", "settings_dict", "TestHttps2WrongHostname", "TestHttps2InvalidDNSId", "TestHttps2InvalidDNSPattern", "TestHttps2CustomCiphers", "TestHttp2WithCrawler", "TestHttps2Proxy", "loads", "case", "sending", "mockserver", "bad", "content", "pytest", "test", "request", "request2", "response", "response2", "process"], "ast_kind": "class_or_type", "text": "\"\"\"Tests for scrapy.core.downloader.handlers.http2.H2DownloadHandler.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom typing import TYPE_CHECKING, Any\nfrom unittest import mock\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer, error\nfrom twisted.web.error import SchemeNotSupported\nfrom twisted.web.http import H2_ENABLED\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom tests.test_downloader_handlers_http_base import (\n    TestHttpProxyBase,\n    TestHttps11Base,\n    TestHttpsCustomCiphersBase,\n    TestHttpsInvalidDNSIdBase,\n    TestHttpsInvalidDNSPatternBase,\n    TestHttpsWrongHostnameBase,\n    TestHttpWithCrawlerBase,\n    download_request,\n)\n\nif TYPE_CHECKING:\n    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n    from tests.mockserver.http import MockServer\n    from tests.mockserver.proxy_echo import ProxyEchoMockServer\n\n\npytestmark = pytest.mark.skipif(\n    not H2_ENABLED, reason=\"HTTP/2 support in Twisted is not enabled\"\n)\n\n\nclass H2DownloadHandlerMixin:\n    @property\n    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n        # the import can fail when H2_ENABLED is False\n        from scrapy.core.downloader.handlers.http2 import (  # noqa: PLC0415\n            H2DownloadHandler,\n        )\n\n        return H2DownloadHandler\n\n\nclass TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n    HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n\n    @deferred_f_from_coro_f\n    async def test_protocol(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/host\", is_secure=self.is_secure), method=\"GET\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.protocol == \"h2\"\n\n    @deferred_f_from_coro_f\n    async def test_download_with_maxsize_very_large_file(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        from twisted.internet import reactor\n\n        with mock.patch(\"scrapy.core.http2.stream.logger\") as logger:\n            request = Request(\n                mockserver.url(\"/largechunkedfile\", is_secure=self.is_secure)\n            )\n\n            def check(logger: mock.Mock) -> None:\n                logger.error.assert_called_once_with(mock.ANY)\n\n            with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n                await download_request(\n                    download_handler, request, Spider(\"foo\", download_maxsize=1500)\n                )\n\n            # As the error message is logged in the dataReceived callback, we\n            # have to give a bit of time to the reactor to process the queue\n            # after closing the connection.\n            d: defer.Deferred[mock.Mock] = defer.Deferred()\n            d.addCallback(check)\n            reactor.callLater(0.1, d.callback, logger)\n            await maybe_deferred_to_future(d)\n\n    @deferred_f_from_coro_f\n    async def test_unsupported_scheme(\n        self, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\"ftp://unsupported.scheme\")\n        with pytest.raises(SchemeNotSupported):\n            await download_request(download_handler, request)\n\n    def test_download_cause_data_loss(self) -> None:  # type: ignore[override]\n        pytest.skip(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_allow_data_loss(self) -> None:  # type: ignore[override]\n        pytest.skip(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_allow_data_loss_via_setting(self) -> None:  # type: ignore[override]\n        pytest.skip(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    @deferred_f_from_coro_f\n    async def test_concurrent_requests_same_domain(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request1 = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n        response1 = await download_request(download_handler, request1)\n        assert response1.body == b\"Works\"\n\n        request2 = Request(\n            mockserver.url(\"/echo\", is_secure=self.is_secure), method=\"POST\"\n        )\n        response2 = await download_request(download_handler, request2)\n        assert response2.headers[\"Content-Length\"] == b\"79\"\n\n    @pytest.mark.xfail(reason=\"https://github.com/python-hyper/h2/issues/1247\")\n    @deferred_f_from_coro_f\n    async def test_connect_request(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/file\", is_secure=self.is_secure), method=\"CONNECT\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.body == b\"\"\n\n    @deferred_f_from_coro_f\n    async def test_custom_content_length_good(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/contentlength\", is_secure=self.is_secure))\n        custom_content_length = str(len(request.body))\n        request.headers[\"Content-Length\"] = custom_content_length\n        response = await download_request(download_handler, request)\n        assert response.text == custom_content_length\n", "n_tokens": 1163, "byte_len": 5401, "file_sha1": "8af5a42b827745f54d3503a348a0afe58337aa50", "start_line": 1, "end_line": 142}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http2.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_http2.py", "rel_path": "tests/test_downloader_handler_twisted_http2.py", "module": "tests.test_downloader_handler_twisted_http2", "ext": "py", "chunk_number": 2, "symbols": ["settings_dict", "TestHttps2WrongHostname", "TestHttps2InvalidDNSId", "TestHttps2InvalidDNSPattern", "TestHttps2CustomCiphers", "TestHttp2WithCrawler", "TestHttps2Proxy", "async", "loads", "case", "stream", "deferred", "from", "download", "handler", "sending", "https", "mockserver", "bad", "content", "pytest", "test", "downloa", "handlers", "none", "proxy", "echo", "http", "header", "secure", "download_handler_cls", "check", "test_download_cause_data_loss", "test_download_allow_data_loss", "test_download_allow_data_loss_via_setting", "H2DownloadHandlerMixin", "TestHttps2", "method", "protocol", "htt", "datalos", "mock", "works", "connection", "after", "spider", "data", "received", "enabled", "spiders"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_custom_content_length_bad(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/contentlength\", is_secure=self.is_secure))\n        actual_content_length = str(len(request.body))\n        bad_content_length = str(len(request.body) + 1)\n        request.headers[\"Content-Length\"] = bad_content_length\n        with LogCapture() as log:\n            response = await download_request(download_handler, request)\n        assert response.text == actual_content_length\n        log.check_present(\n            (\n                \"scrapy.core.http2.stream\",\n                \"WARNING\",\n                f\"Ignoring bad Content-Length header \"\n                f\"{bad_content_length!r} of request {request}, sending \"\n                f\"{actual_content_length!r} instead\",\n            )\n        )\n\n    @deferred_f_from_coro_f\n    async def test_duplicate_header(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/echo\", is_secure=self.is_secure))\n        header, value1, value2 = \"Custom-Header\", \"foo\", \"bar\"\n        request.headers.appendlist(header, value1)\n        request.headers.appendlist(header, value2)\n        response = await download_request(download_handler, request)\n        assert json.loads(response.text)[\"headers\"][header] == [value1, value2]\n\n\nclass TestHttps2WrongHostname(H2DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n    pass\n\n\nclass TestHttps2InvalidDNSId(H2DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n    pass\n\n\nclass TestHttps2InvalidDNSPattern(\n    H2DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase\n):\n    pass\n\n\nclass TestHttps2CustomCiphers(H2DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n    pass\n\n\nclass TestHttp2WithCrawler(TestHttpWithCrawlerBase):\n    \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n\n    @property\n    def settings_dict(self) -> dict[str, Any] | None:\n        return {\n            \"DOWNLOAD_HANDLERS\": {\n                \"https\": \"scrapy.core.downloader.handlers.http2.H2DownloadHandler\"\n            }\n        }\n\n    is_secure = True\n\n\nclass TestHttps2Proxy(H2DownloadHandlerMixin, TestHttpProxyBase):\n    is_secure = True\n    expected_http_proxy_request_body = b\"/\"\n\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy_https_timeout(\n        self,\n        proxy_mockserver: ProxyEchoMockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        with pytest.raises(NotImplementedError):\n            await maybe_deferred_to_future(\n                super().test_download_with_proxy_https_timeout(\n                    proxy_mockserver, download_handler\n                )\n            )\n\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy_without_http_scheme(\n        self,\n        proxy_mockserver: ProxyEchoMockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        with pytest.raises(SchemeNotSupported):\n            await maybe_deferred_to_future(\n                super().test_download_with_proxy_without_http_scheme(\n                    proxy_mockserver, download_handler\n                )\n            )\n", "n_tokens": 690, "byte_len": 3250, "file_sha1": "8af5a42b827745f54d3503a348a0afe58337aa50", "start_line": 143, "end_line": 237}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 1, "symbols": ["_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "TestCookiesMiddleware", "bool", "decoded", "cookies", "test", "setting", "latin", "latin1", "name", "domain", "string", "get", "crawler", "pytest", "cookie", "str", "isinstance", "items", "test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header", "test_request_cookies_encoding", "test_request_headers_cookie_encoding", "test_invalid_cookies", "test_primitive_type_cookies", "_test_cookie_redirect", "test_cookie_redirect_same_domain", "test_cookie_redirect_same_domain_forcing_get", "test_cookie_redirect_different_domain", "test_cookie_redirect_different_domain_forcing_get", "_test_cookie_header_redirect", "test_cookie_header_redirect_same_domain", "test_cookie_header_redirect_same_domain_forcing_get"], "ast_kind": "class_or_type", "text": "import logging\n\nimport pytest\nfrom testfixtures import LogCapture\n\nfrom scrapy.downloadermiddlewares.cookies import CookiesMiddleware\nfrom scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\nfrom scrapy.downloadermiddlewares.redirect import RedirectMiddleware\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\nUNSET = object()\n\n\ndef _cookie_to_set_cookie_value(cookie):\n    \"\"\"Given a cookie defined as a dictionary with name and value keys, and\n    optional path and domain keys, return the equivalent string that can be\n    associated to a ``Set-Cookie`` header.\"\"\"\n    decoded = {}\n    for key in (\"name\", \"value\", \"path\", \"domain\"):\n        if cookie.get(key) is None:\n            if key in (\"name\", \"value\"):\n                return None\n            continue\n        if isinstance(cookie[key], (bool, float, int, str)):\n            decoded[key] = str(cookie[key])\n        else:\n            try:\n                decoded[key] = cookie[key].decode(\"utf8\")\n            except UnicodeDecodeError:\n                decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n\n    cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n    for key, value in decoded.items():  # path, domain\n        cookie_str += f\"; {key.capitalize()}={value}\"\n    return cookie_str\n\n\ndef _cookies_to_set_cookie_list(cookies):\n    \"\"\"Given a group of cookie defined either as a dictionary or as a list of\n    dictionaries (i.e. in a format supported by the cookies parameter of\n    Request), return the equivalen list of strings that can be associated to a\n    ``Set-Cookie`` header.\"\"\"\n    if not cookies:\n        return []\n    if isinstance(cookies, dict):\n        cookies = ({\"name\": k, \"value\": v} for k, v in cookies.items())\n    return filter(None, (_cookie_to_set_cookie_value(cookie) for cookie in cookies))\n\n\nclass TestCookiesMiddleware:\n    def assertCookieValEqual(self, first, second, msg=None):\n        def split_cookies(cookies):\n            return sorted([s.strip() for s in to_bytes(cookies).split(b\";\")])\n\n        assert split_cookies(first) == split_cookies(second), msg\n\n    def setup_method(self):\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        self.mw = CookiesMiddleware.from_crawler(crawler)\n        self.redirect_middleware = RedirectMiddleware.from_crawler(crawler)\n\n    def teardown_method(self):\n        del self.mw\n        del self.redirect_middleware\n\n    def test_basic(self):\n        req = Request(\"http://scrapytest.org/\")\n        assert self.mw.process_request(req) is None\n        assert \"Cookie\" not in req.headers\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res) is res\n\n        req2 = Request(\"http://scrapytest.org/sub1/\")\n        assert self.mw.process_request(req2) is None\n        assert req2.headers.get(\"Cookie\") == b\"C1=value1\"\n\n    def test_setting_false_cookies_enabled(self):\n        with pytest.raises(NotConfigured):\n            CookiesMiddleware.from_crawler(\n                get_crawler(settings_dict={\"COOKIES_ENABLED\": False})\n            )\n\n    def test_setting_default_cookies_enabled(self):\n        assert isinstance(\n            CookiesMiddleware.from_crawler(get_crawler()), CookiesMiddleware\n        )\n\n    def test_setting_true_cookies_enabled(self):\n        assert isinstance(\n            CookiesMiddleware.from_crawler(\n                get_crawler(settings_dict={\"COOKIES_ENABLED\": True})\n            ),\n            CookiesMiddleware,\n        )\n\n    def test_setting_enabled_cookies_debug(self):\n        crawler = get_crawler(settings_dict={\"COOKIES_DEBUG\": True})\n        mw = CookiesMiddleware.from_crawler(crawler)\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.cookies\",\n            propagate=False,\n            level=logging.DEBUG,\n        ) as log:\n            req = Request(\"http://scrapytest.org/\")\n            res = Response(\n                \"http://scrapytest.org/\", headers={\"Set-Cookie\": \"C1=value1; path=/\"}\n            )\n            mw.process_response(req, res)\n            req2 = Request(\"http://scrapytest.org/sub1/\")\n            mw.process_request(req2)\n\n            log.check(\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"DEBUG\",\n                    \"Received cookies from: <200 http://scrapytest.org/>\\n\"\n                    \"Set-Cookie: C1=value1; path=/\\n\",\n                ),\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"DEBUG\",\n                    \"Sending cookies to: <GET http://scrapytest.org/sub1/>\\n\"\n                    \"Cookie: C1=value1\\n\",\n                ),\n            )\n\n    def test_setting_disabled_cookies_debug(self):\n        crawler = get_crawler(settings_dict={\"COOKIES_DEBUG\": False})\n        mw = CookiesMiddleware.from_crawler(crawler)\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.cookies\",\n            propagate=False,\n            level=logging.DEBUG,\n        ) as log:\n            req = Request(\"http://scrapytest.org/\")\n            res = Response(\n                \"http://scrapytest.org/\", headers={\"Set-Cookie\": \"C1=value1; path=/\"}\n            )\n            mw.process_response(req, res)\n            req2 = Request(\"http://scrapytest.org/sub1/\")\n            mw.process_request(req2)\n\n            log.check()\n", "n_tokens": 1190, "byte_len": 5626, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 1, "end_line": 151}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 2, "symbols": ["test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "seted", "test", "complex", "mergeme", "cookies", "merged", "req", "req2", "dontmerge", "dont", "merge", "xa3me", "value", "value4", "name", "check", "domain", "meta", "path", "ignore", "not", "passed", "headers", "scrapy", "nothing", "some", "_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header", "test_request_cookies_encoding", "test_request_headers_cookie_encoding", "test_invalid_cookies", "test_primitive_type_cookies"], "ast_kind": "function_or_method", "text": "    def test_do_not_break_on_non_utf8_header(self):\n        req = Request(\"http://scrapytest.org/\")\n        assert self.mw.process_request(req) is None\n        assert \"Cookie\" not in req.headers\n\n        headers = {\"Set-Cookie\": b\"C1=in\\xa3valid; path=/\", \"Other\": b\"ignore\\xa3me\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res) is res\n\n        req2 = Request(\"http://scrapytest.org/sub1/\")\n        assert self.mw.process_request(req2) is None\n        assert \"Cookie\" in req2.headers\n\n    def test_dont_merge_cookies(self):\n        # merge some cookies into jar\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        req = Request(\"http://scrapytest.org/\")\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res) is res\n\n        # test Cookie header is not seted to request\n        req = Request(\"http://scrapytest.org/dontmerge\", meta={\"dont_merge_cookies\": 1})\n        assert self.mw.process_request(req) is None\n        assert \"Cookie\" not in req.headers\n\n        # check that returned cookies are not merged back to jar\n        res = Response(\n            \"http://scrapytest.org/dontmerge\",\n            headers={\"Set-Cookie\": \"dont=mergeme; path=/\"},\n        )\n        assert self.mw.process_response(req, res) is res\n\n        # check that cookies are merged back\n        req = Request(\"http://scrapytest.org/mergeme\")\n        assert self.mw.process_request(req) is None\n        assert req.headers.get(\"Cookie\") == b\"C1=value1\"\n\n        # check that cookies are merged when dont_merge_cookies is passed as 0\n        req = Request(\"http://scrapytest.org/mergeme\", meta={\"dont_merge_cookies\": 0})\n        assert self.mw.process_request(req) is None\n        assert req.headers.get(\"Cookie\") == b\"C1=value1\"\n\n    def test_complex_cookies(self):\n        # merge some cookies into jar\n        cookies = [\n            {\n                \"name\": \"C1\",\n                \"value\": \"value1\",\n                \"path\": \"/foo\",\n                \"domain\": \"scrapytest.org\",\n            },\n            {\n                \"name\": \"C2\",\n                \"value\": \"value2\",\n                \"path\": \"/bar\",\n                \"domain\": \"scrapytest.org\",\n            },\n            {\n                \"name\": \"C3\",\n                \"value\": \"value3\",\n                \"path\": \"/foo\",\n                \"domain\": \"scrapytest.org\",\n            },\n            {\"name\": \"C4\", \"value\": \"value4\", \"path\": \"/foo\", \"domain\": \"scrapy.org\"},\n        ]\n\n        req = Request(\"http://scrapytest.org/\", cookies=cookies)\n        self.mw.process_request(req)\n\n        # embed C1 and C3 for scrapytest.org/foo\n        req = Request(\"http://scrapytest.org/foo\")\n        self.mw.process_request(req)\n        assert req.headers.get(\"Cookie\") in (\n            b\"C1=value1; C3=value3\",\n            b\"C3=value3; C1=value1\",\n        )\n\n        # embed C2 for scrapytest.org/bar\n        req = Request(\"http://scrapytest.org/bar\")\n        self.mw.process_request(req)\n        assert req.headers.get(\"Cookie\") == b\"C2=value2\"\n\n        # embed nothing for scrapytest.org/baz\n        req = Request(\"http://scrapytest.org/baz\")\n        self.mw.process_request(req)\n        assert \"Cookie\" not in req.headers\n\n    def test_merge_request_cookies(self):\n        req = Request(\"http://scrapytest.org/\", cookies={\"galleta\": \"salada\"})\n        assert self.mw.process_request(req) is None\n        assert req.headers.get(\"Cookie\") == b\"galleta=salada\"\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res) is res\n\n        req2 = Request(\"http://scrapytest.org/sub1/\")\n        assert self.mw.process_request(req2) is None\n\n        self.assertCookieValEqual(\n            req2.headers.get(\"Cookie\"), b\"C1=value1; galleta=salada\"\n        )\n", "n_tokens": 968, "byte_len": 3921, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 152, "end_line": 254}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 3, "symbols": ["test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header", "processed", "cookies", "port", "dulce", "pytest", "retrieval", "items", "none", "assert", "cookie", "reason", "default", "headers", "http", "req", "req5", "response", "values", "test", "local", "qwerty", "argument", "keep", "meta", "store", "store2", "_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "test_request_cookies_encoding", "test_request_headers_cookie_encoding", "test_invalid_cookies", "test_primitive_type_cookies"], "ast_kind": "function_or_method", "text": "    def test_cookiejar_key(self):\n        req = Request(\n            \"http://scrapytest.org/\",\n            cookies={\"galleta\": \"salada\"},\n            meta={\"cookiejar\": \"store1\"},\n        )\n        assert self.mw.process_request(req) is None\n        assert req.headers.get(\"Cookie\") == b\"galleta=salada\"\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers, request=req)\n        assert self.mw.process_response(req, res) is res\n\n        req2 = Request(\"http://scrapytest.org/\", meta=res.meta)\n        assert self.mw.process_request(req2) is None\n        self.assertCookieValEqual(\n            req2.headers.get(\"Cookie\"), b\"C1=value1; galleta=salada\"\n        )\n\n        req3 = Request(\n            \"http://scrapytest.org/\",\n            cookies={\"galleta\": \"dulce\"},\n            meta={\"cookiejar\": \"store2\"},\n        )\n        assert self.mw.process_request(req3) is None\n        assert req3.headers.get(\"Cookie\") == b\"galleta=dulce\"\n\n        headers = {\"Set-Cookie\": \"C2=value2; path=/\"}\n        res2 = Response(\"http://scrapytest.org/\", headers=headers, request=req3)\n        assert self.mw.process_response(req3, res2) is res2\n\n        req4 = Request(\"http://scrapytest.org/\", meta=res2.meta)\n        assert self.mw.process_request(req4) is None\n        self.assertCookieValEqual(\n            req4.headers.get(\"Cookie\"), b\"C2=value2; galleta=dulce\"\n        )\n\n        # cookies from hosts with port\n        req5_1 = Request(\"http://scrapytest.org:1104/\")\n        assert self.mw.process_request(req5_1) is None\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res5_1 = Response(\n            \"http://scrapytest.org:1104/\", headers=headers, request=req5_1\n        )\n        assert self.mw.process_response(req5_1, res5_1) is res5_1\n\n        req5_2 = Request(\"http://scrapytest.org:1104/some-redirected-path\")\n        assert self.mw.process_request(req5_2) is None\n        assert req5_2.headers.get(\"Cookie\") == b\"C1=value1\"\n\n        req5_3 = Request(\"http://scrapytest.org/some-redirected-path\")\n        assert self.mw.process_request(req5_3) is None\n        assert req5_3.headers.get(\"Cookie\") == b\"C1=value1\"\n\n        # skip cookie retrieval for not http request\n        req6 = Request(\"file:///scrapy/sometempfile\")\n        assert self.mw.process_request(req6) is None\n        assert req6.headers.get(\"Cookie\") is None\n\n    def test_local_domain(self):\n        request = Request(\"http://example-host/\", cookies={\"currencyCookie\": \"USD\"})\n        assert self.mw.process_request(request) is None\n        assert \"Cookie\" in request.headers\n        assert request.headers[\"Cookie\"] == b\"currencyCookie=USD\"\n\n    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n    def test_keep_cookie_from_default_request_headers_middleware(self):\n        DEFAULT_REQUEST_HEADERS = {\"Cookie\": \"default=value; asdf=qwerty\"}\n        mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n        # overwrite with values from 'cookies' request argument\n        req1 = Request(\"http://example.org\", cookies={\"default\": \"something\"})\n        assert mw_default_headers.process_request(req1) is None\n        assert self.mw.process_request(req1) is None\n        self.assertCookieValEqual(\n            req1.headers[\"Cookie\"], b\"default=something; asdf=qwerty\"\n        )\n        # keep both\n        req2 = Request(\"http://example.com\", cookies={\"a\": \"b\"})\n        assert mw_default_headers.process_request(req2) is None\n        assert self.mw.process_request(req2) is None\n        self.assertCookieValEqual(\n            req2.headers[\"Cookie\"], b\"default=value; a=b; asdf=qwerty\"\n        )\n\n    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n    def test_keep_cookie_header(self):\n        # keep only cookies from 'Cookie' request header\n        req1 = Request(\"http://scrapytest.org\", headers={\"Cookie\": \"a=b; c=d\"})\n        assert self.mw.process_request(req1) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], \"a=b; c=d\")\n        # keep cookies from both 'Cookie' request header and 'cookies' keyword\n        req2 = Request(\n            \"http://scrapytest.org\", headers={\"Cookie\": \"a=b; c=d\"}, cookies={\"e\": \"f\"}\n        )\n        assert self.mw.process_request(req2) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], \"a=b; c=d; e=f\")\n        # overwrite values from 'Cookie' request header with 'cookies' keyword\n        req3 = Request(\n            \"http://scrapytest.org\",\n            headers={\"Cookie\": \"a=b; c=d\"},\n            cookies={\"a\": \"new\", \"e\": \"f\"},\n        )\n        assert self.mw.process_request(req3) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], \"a=new; c=d; e=f\")\n", "n_tokens": 1199, "byte_len": 4770, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 255, "end_line": 360}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 4, "symbols": ["test_request_cookies_encoding", "test_request_headers_cookie_encoding", "test_invalid_cookies", "test_primitive_type_cookies", "processed", "warning", "false", "discarded", "boolean", "float", "req", "req4", "cookies", "req2", "latin", "latin1", "value", "value2", "cookies2", "name", "check", "mark", "cookies1", "cookies3", "with", "downloadermiddlewares", "test", "primitive", "scrapy", "headers", "_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header"], "ast_kind": "function_or_method", "text": "    def test_request_cookies_encoding(self):\n        # 1) UTF8-encoded bytes\n        req1 = Request(\"http://example.org\", cookies={\"a\": \"á\".encode()})\n        assert self.mw.process_request(req1) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 2) Non UTF8-encoded bytes\n        req2 = Request(\"http://example.org\", cookies={\"a\": \"á\".encode(\"latin1\")})\n        assert self.mw.process_request(req2) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 3) String\n        req3 = Request(\"http://example.org\", cookies={\"a\": \"á\"})\n        assert self.mw.process_request(req3) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n    def test_request_headers_cookie_encoding(self):\n        # 1) UTF8-encoded bytes\n        req1 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\".encode()})\n        assert self.mw.process_request(req1) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 2) Non UTF8-encoded bytes\n        req2 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\".encode(\"latin1\")})\n        assert self.mw.process_request(req2) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 3) String\n        req3 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\"})\n        assert self.mw.process_request(req3) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n    def test_invalid_cookies(self):\n        \"\"\"\n        Invalid cookies are logged as warnings and discarded\n        \"\"\"\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.cookies\",\n            propagate=False,\n            level=logging.INFO,\n        ) as lc:\n            cookies1 = [{\"value\": \"bar\"}, {\"name\": \"key\", \"value\": \"value1\"}]\n            req1 = Request(\"http://example.org/1\", cookies=cookies1)\n            assert self.mw.process_request(req1) is None\n            cookies2 = [{\"name\": \"foo\"}, {\"name\": \"key\", \"value\": \"value2\"}]\n            req2 = Request(\"http://example.org/2\", cookies=cookies2)\n            assert self.mw.process_request(req2) is None\n            cookies3 = [{\"name\": \"foo\", \"value\": None}, {\"name\": \"key\", \"value\": \"\"}]\n            req3 = Request(\"http://example.org/3\", cookies=cookies3)\n            assert self.mw.process_request(req3) is None\n            lc.check(\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"WARNING\",\n                    \"Invalid cookie found in request <GET http://example.org/1>:\"\n                    \" {'value': 'bar', 'secure': False} ('name' is missing)\",\n                ),\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"WARNING\",\n                    \"Invalid cookie found in request <GET http://example.org/2>:\"\n                    \" {'name': 'foo', 'secure': False} ('value' is missing)\",\n                ),\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"WARNING\",\n                    \"Invalid cookie found in request <GET http://example.org/3>:\"\n                    \" {'name': 'foo', 'value': None, 'secure': False} ('value' is missing)\",\n                ),\n            )\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], \"key=value1\")\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], \"key=value2\")\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], \"key=\")\n\n    def test_primitive_type_cookies(self):\n        # Boolean\n        req1 = Request(\"http://example.org\", cookies={\"a\": True})\n        assert self.mw.process_request(req1) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=True\")\n\n        # Float\n        req2 = Request(\"http://example.org\", cookies={\"a\": 9.5})\n        assert self.mw.process_request(req2) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=9.5\")\n\n        # Integer\n        req3 = Request(\"http://example.org\", cookies={\"a\": 10})\n        assert self.mw.process_request(req3) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=10\")\n\n        # String\n        req4 = Request(\"http://example.org\", cookies={\"a\": \"b\"})\n        assert self.mw.process_request(req4) is None\n        self.assertCookieValEqual(req4.headers[\"Cookie\"], b\"a=b\")\n", "n_tokens": 1070, "byte_len": 4462, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 361, "end_line": 456}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 5, "symbols": ["_test_cookie_redirect", "test_cookie_redirect_same_domain", "test_cookie_redirect_same_domain_forcing_get", "test_cookie_redirect_different_domain", "test_cookie_redirect_different_domain_forcing_get", "_test_cookie_header_redirect", "test_cookie_header_redirect_same_domain", "test_cookie_header_redirect_same_domain_forcing_get", "test_cookie_header_redirect_different_domain", "test_cookie_header_redirect_different_domain_forcing_get", "_test_user_set_cookie_domain_followup", "test_user_set_cookie_domain_suffix_private", "method", "books", "those", "getting", "leaked", "cookies", "test", "user", "cookies2", "name", "domain", "about", "target", "enabled", "rules", "subdomains", "https", "path", "_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header"], "ast_kind": "function_or_method", "text": "    def _test_cookie_redirect(\n        self,\n        source,\n        target,\n        *,\n        cookies1,\n        cookies2,\n    ):\n        input_cookies = {\"a\": \"b\"}\n\n        if not isinstance(source, dict):\n            source = {\"url\": source}\n        if not isinstance(target, dict):\n            target = {\"url\": target}\n        target.setdefault(\"status\", 301)\n\n        request1 = Request(cookies=input_cookies, **source)\n        self.mw.process_request(request1)\n        cookies = request1.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies1 else None)\n\n        response = Response(\n            headers={\n                \"Location\": target[\"url\"],\n            },\n            **target,\n        )\n        assert self.mw.process_response(request1, response) == response\n\n        request2 = self.redirect_middleware.process_response(request1, response)\n        assert isinstance(request2, Request)\n\n        self.mw.process_request(request2)\n        cookies = request2.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies2 else None)\n\n    def test_cookie_redirect_same_domain(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            \"https://toscrape.com\",\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def test_cookie_redirect_same_domain_forcing_get(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://toscrape.com\", \"status\": 302},\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def test_cookie_redirect_different_domain(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            \"https://example.com\",\n            cookies1=True,\n            cookies2=False,\n        )\n\n    def test_cookie_redirect_different_domain_forcing_get(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://example.com\", \"status\": 302},\n            cookies1=True,\n            cookies2=False,\n        )\n\n    def _test_cookie_header_redirect(\n        self,\n        source,\n        target,\n        *,\n        cookies2,\n    ):\n        \"\"\"Test the handling of a user-defined Cookie header when building a\n        redirect follow-up request.\n\n        We follow RFC 6265 for cookie handling. The Cookie header can only\n        contain a list of key-value pairs (i.e. no additional cookie\n        parameters like Domain or Path). Because of that, we follow the same\n        rules that we would follow for the handling of the Set-Cookie response\n        header when the Domain is not set: the cookies must be limited to the\n        target URL domain (not even subdomains can receive those cookies).\n\n        .. note:: This method tests the scenario where the cookie middleware is\n                  disabled. Because of known issue #1992, when the cookies\n                  middleware is enabled we do not need to be concerned about\n                  the Cookie header getting leaked to unintended domains,\n                  because the middleware empties the header from every request.\n        \"\"\"\n        if not isinstance(source, dict):\n            source = {\"url\": source}\n        if not isinstance(target, dict):\n            target = {\"url\": target}\n        target.setdefault(\"status\", 301)\n\n        request1 = Request(headers={\"Cookie\": b\"a=b\"}, **source)\n\n        response = Response(\n            headers={\n                \"Location\": target[\"url\"],\n            },\n            **target,\n        )\n\n        request2 = self.redirect_middleware.process_response(request1, response)\n        assert isinstance(request2, Request)\n\n        cookies = request2.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies2 else None)\n\n    def test_cookie_header_redirect_same_domain(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            \"https://toscrape.com\",\n            cookies2=True,\n        )\n\n    def test_cookie_header_redirect_same_domain_forcing_get(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://toscrape.com\", \"status\": 302},\n            cookies2=True,\n        )\n\n    def test_cookie_header_redirect_different_domain(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            \"https://example.com\",\n            cookies2=False,\n        )\n\n    def test_cookie_header_redirect_different_domain_forcing_get(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://example.com\", \"status\": 302},\n            cookies2=False,\n        )\n\n    def _test_user_set_cookie_domain_followup(\n        self,\n        url1,\n        url2,\n        domain,\n        *,\n        cookies1,\n        cookies2,\n    ):\n        input_cookies = [\n            {\n                \"name\": \"a\",\n                \"value\": \"b\",\n                \"domain\": domain,\n            }\n        ]\n\n        request1 = Request(url1, cookies=input_cookies)\n        self.mw.process_request(request1)\n        cookies = request1.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies1 else None)\n\n        request2 = Request(url2)\n        self.mw.process_request(request2)\n        cookies = request2.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies2 else None)\n\n    def test_user_set_cookie_domain_suffix_private(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://books.toscrape.com\",\n            \"https://quotes.toscrape.com\",\n            \"toscrape.com\",\n            cookies1=True,\n            cookies2=True,\n        )\n", "n_tokens": 1173, "byte_len": 5661, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 457, "end_line": 632}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 6, "symbols": ["test_user_set_cookie_domain_suffix_public_period", "test_user_set_cookie_domain_suffix_public_private", "test_user_set_cookie_domain_public_period", "_test_server_set_cookie_domain_followup", "test_server_set_cookie_domain_suffix_private", "test_server_set_cookie_domain_suffix_public_period", "test_server_set_cookie_domain_suffix_public_private", "test_server_set_cookie_domain_public_period", "_test_cookie_redirect_scheme_change", "test_cookie_redirect_secure_undefined_downgrade", "test_cookie_redirect_secure_undefined_upgrade", "test_cookie_redirect_secure_false_downgrade", "test_cookie_redirect_secure_false_upgrade", "test_cookie_redirect_secure_true_downgrade", "books", "remain", "cookies", "cookies2", "name", "domain", "initial", "https", "port", "test", "cookie", "user", "request", "request2", "isinstance", "none", "_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header"], "ast_kind": "function_or_method", "text": "    def test_user_set_cookie_domain_suffix_public_period(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://foo.co.uk\",\n            \"https://bar.co.uk\",\n            \"co.uk\",\n            cookies1=False,\n            cookies2=False,\n        )\n\n    def test_user_set_cookie_domain_suffix_public_private(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://foo.blogspot.com\",\n            \"https://bar.blogspot.com\",\n            \"blogspot.com\",\n            cookies1=False,\n            cookies2=False,\n        )\n\n    def test_user_set_cookie_domain_public_period(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://co.uk\",\n            \"https://co.uk\",\n            \"co.uk\",\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def _test_server_set_cookie_domain_followup(\n        self,\n        url1,\n        url2,\n        domain,\n        *,\n        cookies,\n    ):\n        request1 = Request(url1)\n        self.mw.process_request(request1)\n\n        input_cookies = [\n            {\n                \"name\": \"a\",\n                \"value\": \"b\",\n                \"domain\": domain,\n            }\n        ]\n\n        headers = {\n            \"Set-Cookie\": _cookies_to_set_cookie_list(input_cookies),\n        }\n        response = Response(url1, status=200, headers=headers)\n        assert self.mw.process_response(request1, response) == response\n\n        request2 = Request(url2)\n        self.mw.process_request(request2)\n        actual_cookies = request2.headers.get(\"Cookie\")\n        assert actual_cookies == (b\"a=b\" if cookies else None)\n\n    def test_server_set_cookie_domain_suffix_private(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://books.toscrape.com\",\n            \"https://quotes.toscrape.com\",\n            \"toscrape.com\",\n            cookies=True,\n        )\n\n    def test_server_set_cookie_domain_suffix_public_period(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://foo.co.uk\",\n            \"https://bar.co.uk\",\n            \"co.uk\",\n            cookies=False,\n        )\n\n    def test_server_set_cookie_domain_suffix_public_private(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://foo.blogspot.com\",\n            \"https://bar.blogspot.com\",\n            \"blogspot.com\",\n            cookies=False,\n        )\n\n    def test_server_set_cookie_domain_public_period(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://co.uk\",\n            \"https://co.uk\",\n            \"co.uk\",\n            cookies=True,\n        )\n\n    def _test_cookie_redirect_scheme_change(\n        self, secure, from_scheme, to_scheme, cookies1, cookies2, cookies3\n    ):\n        \"\"\"When a redirect causes the URL scheme to change from *from_scheme*\n        to *to_scheme*, while domain and port remain the same, and given a\n        cookie on the initial request with its secure attribute set to\n        *secure*, check if the cookie should be set on the Cookie header of the\n        initial request (*cookies1*), if it should be kept by the redirect\n        middleware (*cookies2*), and if it should be present on the Cookie\n        header in the redirected request (*cookie3*).\"\"\"\n        cookie_kwargs = {}\n        if secure is not UNSET:\n            cookie_kwargs[\"secure\"] = secure\n        input_cookies = [{\"name\": \"a\", \"value\": \"b\", **cookie_kwargs}]\n\n        request1 = Request(f\"{from_scheme}://a.example\", cookies=input_cookies)\n        self.mw.process_request(request1)\n        cookies = request1.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies1 else None)\n\n        response = Response(\n            f\"{from_scheme}://a.example\",\n            headers={\"Location\": f\"{to_scheme}://a.example\"},\n            status=301,\n        )\n        assert self.mw.process_response(request1, response) == response\n\n        request2 = self.redirect_middleware.process_response(request1, response)\n        assert isinstance(request2, Request)\n        cookies = request2.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies2 else None)\n\n        self.mw.process_request(request2)\n        cookies = request2.headers.get(\"Cookie\")\n        assert cookies == (b\"a=b\" if cookies3 else None)\n\n    def test_cookie_redirect_secure_undefined_downgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=UNSET,\n            from_scheme=\"https\",\n            to_scheme=\"http\",\n            cookies1=True,\n            cookies2=False,\n            cookies3=False,\n        )\n\n    def test_cookie_redirect_secure_undefined_upgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=UNSET,\n            from_scheme=\"http\",\n            to_scheme=\"https\",\n            cookies1=True,\n            cookies2=True,\n            cookies3=True,\n        )\n\n    def test_cookie_redirect_secure_false_downgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=False,\n            from_scheme=\"https\",\n            to_scheme=\"http\",\n            cookies1=True,\n            cookies2=False,\n            cookies3=True,\n        )\n\n    def test_cookie_redirect_secure_false_upgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=False,\n            from_scheme=\"http\",\n            to_scheme=\"https\",\n            cookies1=True,\n            cookies2=True,\n            cookies3=True,\n        )\n\n    def test_cookie_redirect_secure_true_downgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=True,\n            from_scheme=\"https\",\n            to_scheme=\"http\",\n            cookies1=True,\n            cookies2=False,\n            cookies3=False,\n        )\n", "n_tokens": 1181, "byte_len": 5748, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 633, "end_line": 807}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_cookies.py", "rel_path": "tests/test_downloadermiddleware_cookies.py", "module": "tests.test_downloadermiddleware_cookies", "ext": "py", "chunk_number": 7, "symbols": ["test_cookie_redirect_secure_true_upgrade", "cookies", "cookies2", "cookies3", "true", "scheme", "from", "cookies1", "test", "cookie", "false", "self", "http", "secure", "https", "_cookie_to_set_cookie_value", "_cookies_to_set_cookie_list", "assertCookieValEqual", "split_cookies", "setup_method", "teardown_method", "test_basic", "test_setting_false_cookies_enabled", "test_setting_default_cookies_enabled", "test_setting_true_cookies_enabled", "test_setting_enabled_cookies_debug", "test_setting_disabled_cookies_debug", "test_do_not_break_on_non_utf8_header", "test_dont_merge_cookies", "test_complex_cookies", "test_merge_request_cookies", "test_cookiejar_key", "test_local_domain", "test_keep_cookie_from_default_request_headers_middleware", "test_keep_cookie_header", "test_request_cookies_encoding", "test_request_headers_cookie_encoding", "test_invalid_cookies", "test_primitive_type_cookies", "_test_cookie_redirect", "test_cookie_redirect_same_domain", "test_cookie_redirect_same_domain_forcing_get", "test_cookie_redirect_different_domain", "test_cookie_redirect_different_domain_forcing_get", "_test_cookie_header_redirect", "test_cookie_header_redirect_same_domain", "test_cookie_header_redirect_same_domain_forcing_get", "test_cookie_header_redirect_different_domain", "test_cookie_header_redirect_different_domain_forcing_get", "_test_user_set_cookie_domain_followup"], "ast_kind": "function_or_method", "text": "    def test_cookie_redirect_secure_true_upgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=True,\n            from_scheme=\"http\",\n            to_scheme=\"https\",\n            cookies1=False,\n            cookies2=False,\n            cookies3=True,\n        )\n", "n_tokens": 52, "byte_len": 287, "file_sha1": "33ce90224f763ed96d65c9d35e64318dd5c3af4d", "start_line": 808, "end_line": 817}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 1, "symbols": ["mw", "get_request", "get_response", "test", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "referer", "resp", "headers", "mixin", "referrer", "origin", "filtering", "polic", "sam", "credentials", "when", "target", "future", "typ", "checking", "https", "default", "get", "crawler", "pytest", "settings", "origi", "none", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "referrermw", "redirectmw", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin", "MixinUnsafeUrl", "TestRefererMiddlewareDefault", "TestSettingsNoReferrer", "TestSettingsNoReferrerWhenDowngrade", "TestSettingsSameOrigin", "TestSettingsOrigin", "TestSettingsStrictOrigin"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, cast\nfrom urllib.parse import urlparse\n\nimport pytest\n\nfrom scrapy.downloadermiddlewares.redirect import RedirectMiddleware\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spidermiddlewares.referer import (\n    POLICY_NO_REFERRER,\n    POLICY_NO_REFERRER_WHEN_DOWNGRADE,\n    POLICY_ORIGIN,\n    POLICY_ORIGIN_WHEN_CROSS_ORIGIN,\n    POLICY_SAME_ORIGIN,\n    POLICY_SCRAPY_DEFAULT,\n    POLICY_STRICT_ORIGIN,\n    POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n    POLICY_UNSAFE_URL,\n    DefaultReferrerPolicy,\n    NoReferrerPolicy,\n    NoReferrerWhenDowngradePolicy,\n    OriginPolicy,\n    OriginWhenCrossOriginPolicy,\n    RefererMiddleware,\n    ReferrerPolicy,\n    SameOriginPolicy,\n    StrictOriginPolicy,\n    StrictOriginWhenCrossOriginPolicy,\n    UnsafeUrlPolicy,\n)\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n    from scrapy.crawler import Crawler\n\n\nclass TestRefererMiddleware:\n    req_meta: dict[str, Any] = {}\n    resp_headers: dict[str, str] = {}\n    settings: dict[str, Any] = {}\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n    ]\n\n    @pytest.fixture\n    def mw(self) -> RefererMiddleware:\n        settings = Settings(self.settings)\n        return RefererMiddleware(settings)\n\n    def get_request(self, target: str) -> Request:\n        return Request(target, meta=self.req_meta)\n\n    def get_response(self, origin: str) -> Response:\n        return Response(origin, headers=self.resp_headers)\n\n    def test(self, mw: RefererMiddleware) -> None:\n        for origin, target, referrer in self.scenarii:\n            response = self.get_response(origin)\n            request = self.get_request(target)\n            out = list(mw.process_spider_output(response, [request]))\n            assert out[0].headers.get(\"Referer\") == referrer\n\n\nclass MixinDefault:\n    \"\"\"\n    Based on https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n\n    with some additional filtering of s3://\n    \"\"\"\n\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        (\"https://example.com/\", \"https://scrapy.org/\", b\"https://example.com/\"),\n        (\"http://example.com/\", \"http://scrapy.org/\", b\"http://example.com/\"),\n        (\"http://example.com/\", \"https://scrapy.org/\", b\"http://example.com/\"),\n        (\"https://example.com/\", \"http://scrapy.org/\", None),\n        # no credentials leak\n        (\n            \"http://user:password@example.com/\",\n            \"https://scrapy.org/\",\n            b\"http://example.com/\",\n        ),\n        # no referrer leak for local schemes\n        (\"file:///home/path/to/somefile.html\", \"https://scrapy.org/\", None),\n        (\"file:///home/path/to/somefile.html\", \"http://scrapy.org/\", None),\n        # no referrer leak for s3 origins\n        (\"s3://mybucket/path/to/data.csv\", \"https://scrapy.org/\", None),\n        (\"s3://mybucket/path/to/data.csv\", \"http://scrapy.org/\", None),\n    ]\n\n\nclass MixinNoReferrer:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        (\"https://example.com/page.html\", \"https://example.com/\", None),\n        (\"http://www.example.com/\", \"https://scrapy.org/\", None),\n        (\"http://www.example.com/\", \"http://scrapy.org/\", None),\n        (\"https://www.example.com/\", \"http://scrapy.org/\", None),\n        (\"file:///home/path/to/somefile.html\", \"http://scrapy.org/\", None),\n    ]\n\n", "n_tokens": 881, "byte_len": 3588, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 1, "end_line": 106}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 2, "symbols": ["MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "protocol", "user", "scenarii", "ftps", "different", "mixin", "same", "origin", "password", "referrer", "class", "scrapy", "https", "example", "port", "host", "test", "otherpage", "list", "protocols", "tuple", "page", "none", "stripping", "html", "bytes", "http", "empty", "mw", "get_request", "get_response", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin", "MixinUnsafeUrl", "TestRefererMiddlewareDefault"], "ast_kind": "class_or_type", "text": "class MixinNoReferrerWhenDowngrade:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # TLS to TLS: send non-empty referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://not.example.com/\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"https://example.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:444/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example.com:444/page.html\",\n        ),\n        (\n            \"ftps://example.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftps://example.com/urls.zip\",\n        ),\n        # TLS to non-TLS: do not send referrer\n        (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n        (\"https://example.com/page.html\", \"http://scrapy.org/\", None),\n        (\"ftps://example.com/urls.zip\", \"http://scrapy.org/\", None),\n        # non-TLS to TLS or non-TLS: send referrer\n        (\n            \"http://example.com/page.html\",\n            \"https://not.example.com/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8080/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example.com:8080/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://not.example.com/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:443/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example.com:443/page.html\",\n        ),\n        (\n            \"ftp://example.com/urls.zip\",\n            \"http://scrapy.org/\",\n            b\"ftp://example.com/urls.zip\",\n        ),\n        (\n            \"ftp://example.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftp://example.com/urls.zip\",\n        ),\n        # test for user/password stripping\n        (\n            \"http://user:password@example.com/page.html\",\n            \"https://not.example.com/\",\n            b\"http://example.com/page.html\",\n        ),\n    ]\n\n\nclass MixinSameOrigin:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # Same origin (protocol, host, port): send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com:80/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8888/page.html\",\n            \"http://example.com:8888/not-page.html\",\n            b\"http://example.com:8888/page.html\",\n        ),\n        # Different host: do NOT send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://not.example.com/otherpage.html\",\n            None,\n        ),\n        (\"http://example.com/page.html\", \"http://not.example.com/otherpage.html\", None),\n        (\"http://example.com/page.html\", \"http://www.example.com/otherpage.html\", None),\n        # Different port: do NOT send referrer\n        (\n            \"https://example.com:444/page.html\",\n            \"https://example.com/not-page.html\",\n            None,\n        ),\n        (\"http://example.com:81/page.html\", \"http://example.com/not-page.html\", None),\n        (\"http://example.com/page.html\", \"http://example.com:81/not-page.html\", None),\n        # Different protocols: do NOT send referrer\n        (\"https://example.com/page.html\", \"http://example.com/not-page.html\", None),\n        (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n        (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n        (\"ftp://example.com/urls.zip\", \"http://example.com/not-page.html\", None),\n        (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n        # test for user/password stripping\n        (\n            \"https://user:password@example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            None,\n        ),\n        (\n            \"https://user:password@example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n    ]\n\n", "n_tokens": 1188, "byte_len": 5263, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 107, "end_line": 257}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 3, "symbols": ["MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "protocol", "user", "scenarii", "mixin", "strict", "different", "ftps", "origin", "password", "class", "example", "example4", "scrapy", "nothing", "https", "port", "upgrade", "host", "sent", "test", "otherpage", "list", "protocols", "example5", "tuple", "even", "page", "mw", "get_request", "get_response", "referrer", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinStrictOriginWhenCrossOrigin", "MixinUnsafeUrl", "TestRefererMiddlewareDefault"], "ast_kind": "class_or_type", "text": "class MixinOrigin:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/\",\n        ),\n        (\n            \"https://example.com/page.html\",\n            \"https://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n        (\"https://example.com/page.html\", \"http://scrapy.org\", b\"https://example.com/\"),\n        (\"http://example.com/page.html\", \"http://scrapy.org\", b\"http://example.com/\"),\n        # test for user/password stripping\n        (\n            \"https://user:password@example.com/page.html\",\n            \"http://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n    ]\n\n\nclass MixinStrictOrigin:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/\",\n        ),\n        (\n            \"https://example.com/page.html\",\n            \"https://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n        (\"http://example.com/page.html\", \"http://scrapy.org\", b\"http://example.com/\"),\n        # downgrade: send nothing\n        (\"https://example.com/page.html\", \"http://scrapy.org\", None),\n        # upgrade: send origin\n        (\"http://example.com/page.html\", \"https://scrapy.org\", b\"http://example.com/\"),\n        # test for user/password stripping\n        (\n            \"https://user:password@example.com/page.html\",\n            \"https://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n        (\"https://user:password@example.com/page.html\", \"http://scrapy.org\", None),\n    ]\n\n\nclass MixinOriginWhenCrossOrigin:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # Same origin (protocol, host, port): send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com:80/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8888/page.html\",\n            \"http://example.com:8888/not-page.html\",\n            b\"http://example.com:8888/page.html\",\n        ),\n        # Different host: send origin as referrer\n        (\n            \"https://example2.com/page.html\",\n            \"https://scrapy.org/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"https://example2.com/page.html\",\n            \"https://not.example2.com/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"http://example2.com/page.html\",\n            \"http://not.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # exact match required\n        (\n            \"http://example2.com/page.html\",\n            \"http://www.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # Different port: send origin as referrer\n        (\n            \"https://example3.com:444/page.html\",\n            \"https://example3.com/not-page.html\",\n            b\"https://example3.com:444/\",\n        ),\n        (\n            \"http://example3.com:81/page.html\",\n            \"http://example3.com/not-page.html\",\n            b\"http://example3.com:81/\",\n        ),\n        # Different protocols: send origin as referrer\n        (\n            \"https://example4.com/page.html\",\n            \"http://example4.com/not-page.html\",\n            b\"https://example4.com/\",\n        ),\n        (\n            \"https://example4.com/page.html\",\n            \"http://not.example4.com/\",\n            b\"https://example4.com/\",\n        ),\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        (\n            \"ftp://example4.com/urls.zip\",\n            \"http://example4.com/not-page.html\",\n            b\"ftp://example4.com/\",\n        ),\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        # test for user/password stripping\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"https://example5.com/not-page.html\",\n            b\"https://example5.com/page.html\",\n        ),\n        # TLS to non-TLS downgrade: send origin\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"http://example5.com/not-page.html\",\n            b\"https://example5.com/\",\n        ),\n    ]\n\n", "n_tokens": 1211, "byte_len": 5347, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 258, "end_line": 416}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 4, "symbols": ["MixinStrictOriginWhenCrossOrigin", "protocol", "user", "scenarii", "different", "ftps", "origin", "password", "mixin", "strict", "class", "example", "example4", "scrapy", "nothing", "port", "https", "upgrade", "host", "test", "otherpage", "list", "protocols", "example5", "tuple", "page", "referrer", "same", "none", "stripping", "mw", "get_request", "get_response", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinUnsafeUrl"], "ast_kind": "class_or_type", "text": "class MixinStrictOriginWhenCrossOrigin:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # Same origin (protocol, host, port): send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com:80/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8888/page.html\",\n            \"http://example.com:8888/not-page.html\",\n            b\"http://example.com:8888/page.html\",\n        ),\n        # Different host: send origin as referrer\n        (\n            \"https://example2.com/page.html\",\n            \"https://scrapy.org/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"https://example2.com/page.html\",\n            \"https://not.example2.com/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"http://example2.com/page.html\",\n            \"http://not.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # exact match required\n        (\n            \"http://example2.com/page.html\",\n            \"http://www.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # Different port: send origin as referrer\n        (\n            \"https://example3.com:444/page.html\",\n            \"https://example3.com/not-page.html\",\n            b\"https://example3.com:444/\",\n        ),\n        (\n            \"http://example3.com:81/page.html\",\n            \"http://example3.com/not-page.html\",\n            b\"http://example3.com:81/\",\n        ),\n        # downgrade\n        (\"https://example4.com/page.html\", \"http://example4.com/not-page.html\", None),\n        (\"https://example4.com/page.html\", \"http://not.example4.com/\", None),\n        # non-TLS to non-TLS\n        (\n            \"ftp://example4.com/urls.zip\",\n            \"http://example4.com/not-page.html\",\n            b\"ftp://example4.com/\",\n        ),\n        # upgrade\n        (\n            \"http://example4.com/page.html\",\n            \"https://example4.com/not-page.html\",\n            b\"http://example4.com/\",\n        ),\n        (\n            \"http://example4.com/page.html\",\n            \"https://not.example4.com/\",\n            b\"http://example4.com/\",\n        ),\n        # Different protocols: send origin as referrer\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        # test for user/password stripping\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"https://example5.com/not-page.html\",\n            b\"https://example5.com/page.html\",\n        ),\n        # TLS to non-TLS downgrade: send nothing\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"http://example5.com/not-page.html\",\n            None,\n        ),\n    ]\n\n", "n_tokens": 825, "byte_len": 3675, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 417, "end_line": 528}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 5, "symbols": ["MixinUnsafeUrl", "TestRefererMiddlewareDefault", "TestSettingsNoReferrer", "TestSettingsNoReferrerWhenDowngrade", "TestSettingsSameOrigin", "TestSettingsOrigin", "TestSettingsStrictOrigin", "TestSettingsOriginWhenCrossOrigin", "TestSettingsStrictOriginWhenCrossOrigin", "TestSettingsUnsafeUrl", "CustomPythonOrgPolicy", "policy", "user", "referer", "scenarii", "ftps", "mixin", "strict", "pass", "referrer", "same", "test", "settings", "python", "unsafe", "password", "when", "dummy", "class", "target", "mw", "get_request", "get_response", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin"], "ast_kind": "class_or_type", "text": "class MixinUnsafeUrl:\n    scenarii: list[tuple[str, str, bytes | None]] = [\n        # TLS to TLS: send referrer\n        (\n            \"https://example.com/sekrit.html\",\n            \"http://not.example.com/\",\n            b\"https://example.com/sekrit.html\",\n        ),\n        (\n            \"https://example1.com/page.html\",\n            \"https://not.example1.com/\",\n            b\"https://example1.com/page.html\",\n        ),\n        (\n            \"https://example1.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example1.com/page.html\",\n        ),\n        (\n            \"https://example1.com:443/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example1.com/page.html\",\n        ),\n        (\n            \"https://example1.com:444/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example1.com:444/page.html\",\n        ),\n        (\n            \"ftps://example1.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftps://example1.com/urls.zip\",\n        ),\n        # TLS to non-TLS: send referrer (yes, it's unsafe)\n        (\n            \"https://example2.com/page.html\",\n            \"http://not.example2.com/\",\n            b\"https://example2.com/page.html\",\n        ),\n        (\n            \"https://example2.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"https://example2.com/page.html\",\n        ),\n        (\n            \"ftps://example2.com/urls.zip\",\n            \"http://scrapy.org/\",\n            b\"ftps://example2.com/urls.zip\",\n        ),\n        # non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)\n        (\n            \"http://example3.com/page.html\",\n            \"https://not.example3.com/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com:8080/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example3.com:8080/page.html\",\n        ),\n        (\n            \"http://example3.com:80/page.html\",\n            \"http://not.example3.com/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com:443/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example3.com:443/page.html\",\n        ),\n        (\n            \"ftp://example3.com/urls.zip\",\n            \"http://scrapy.org/\",\n            b\"ftp://example3.com/urls.zip\",\n        ),\n        (\n            \"ftp://example3.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftp://example3.com/urls.zip\",\n        ),\n        # test for user/password stripping\n        (\n            \"http://user:password@example4.com/page.html\",\n            \"https://not.example4.com/\",\n            b\"http://example4.com/page.html\",\n        ),\n        (\n            \"https://user:password@example4.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"https://example4.com/page.html\",\n        ),\n    ]\n\n\nclass TestRefererMiddlewareDefault(MixinDefault, TestRefererMiddleware):\n    pass\n\n\n# --- Tests using settings to set policy using class path\nclass TestSettingsNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerPolicy\"}\n\n\nclass TestSettingsNoReferrerWhenDowngrade(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n    }\n\n\nclass TestSettingsSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n\n\nclass TestSettingsOrigin(MixinOrigin, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginPolicy\"}\n\n\nclass TestSettingsStrictOrigin(MixinStrictOrigin, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.StrictOriginPolicy\"\n    }\n\n\nclass TestSettingsOriginWhenCrossOrigin(\n    MixinOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n\n\nclass TestSettingsStrictOriginWhenCrossOrigin(\n    MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy\"\n    }\n\n\nclass TestSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n\n\nclass CustomPythonOrgPolicy(ReferrerPolicy):\n    \"\"\"\n    A dummy policy that returns referrer as http(s)://python.org\n    depending on the scheme of the target URL.\n    \"\"\"\n", "n_tokens": 1165, "byte_len": 4961, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 529, "end_line": 689}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 6, "symbols": ["referrer", "TestSettingsCustomPolicy", "TestRequestMetaDefault", "TestRequestMetaNoReferrer", "TestRequestMetaNoReferrerWhenDowngrade", "TestRequestMetaSameOrigin", "TestRequestMetaOrigin", "TestRequestMetaSrictOrigin", "TestRequestMetaOriginWhenCrossOrigin", "TestRequestMetaStrictOriginWhenCrossOrigin", "TestRequestMetaUnsafeUrl", "TestRequestMetaPrecedence001", "TestRequestMetaPrecedence002", "TestRequestMetaPrecedence003", "TestRequestMetaSettingFallback", "does", "takes", "referer", "mixin", "same", "origin", "unsafe", "polic", "sam", "when", "policy", "test", "request", "https", "string", "mw", "get_request", "get_response", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin"], "ast_kind": "class_or_type", "text": "    def referrer(self, response, request):\n        scheme = urlparse(request).scheme\n        if scheme == \"https\":\n            return b\"https://python.org/\"\n        if scheme == \"http\":\n            return b\"http://python.org/\"\n        return None\n\n\nclass TestSettingsCustomPolicy(TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": CustomPythonOrgPolicy}\n    scenarii = [\n        (\"https://example.com/\", \"https://scrapy.org/\", b\"https://python.org/\"),\n        (\"http://example.com/\", \"http://scrapy.org/\", b\"http://python.org/\"),\n        (\"http://example.com/\", \"https://scrapy.org/\", b\"https://python.org/\"),\n        (\"https://example.com/\", \"http://scrapy.org/\", b\"http://python.org/\"),\n        (\n            \"file:///home/path/to/somefile.html\",\n            \"https://scrapy.org/\",\n            b\"https://python.org/\",\n        ),\n        (\n            \"file:///home/path/to/somefile.html\",\n            \"http://scrapy.org/\",\n            b\"http://python.org/\",\n        ),\n    ]\n\n\n# --- Tests using Request meta dict to set policy\nclass TestRequestMetaDefault(MixinDefault, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_SCRAPY_DEFAULT}\n\n\nclass TestRequestMetaNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n\n\nclass TestRequestMetaNoReferrerWhenDowngrade(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE}\n\n\nclass TestRequestMetaSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_SAME_ORIGIN}\n\n\nclass TestRequestMetaOrigin(MixinOrigin, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_ORIGIN}\n\n\nclass TestRequestMetaSrictOrigin(MixinStrictOrigin, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_STRICT_ORIGIN}\n\n\nclass TestRequestMetaOriginWhenCrossOrigin(\n    MixinOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    req_meta = {\"referrer_policy\": POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n\n\nclass TestRequestMetaStrictOriginWhenCrossOrigin(\n    MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    req_meta = {\"referrer_policy\": POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n\n\nclass TestRequestMetaUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n\n\nclass TestRequestMetaPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n\n\nclass TestRequestMetaPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n    }\n    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n\n\nclass TestRequestMetaPrecedence003(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n\n\nclass TestRequestMetaSettingFallback:\n    params = [\n        (\n            # When an unknown policy is referenced in Request.meta\n            # (here, a typo error),\n            # the policy defined in settings takes precedence\n            {\n                \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n            },\n            {},\n            {\"referrer_policy\": \"ssscrapy-default\"},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n        (\n            # same as above but with string value for settings policy\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {},\n            {\"referrer_policy\": \"ssscrapy-default\"},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n        (\n            # request meta references a wrong policy but it is set,\n            # so the Referrer-Policy header in response is not used,\n            # and the settings' policy is applied\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {\"Referrer-Policy\": \"unsafe-url\"},\n            {\"referrer_policy\": \"ssscrapy-default\"},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n        (\n            # here, request meta does not set the policy\n            # so response headers take precedence\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {\"Referrer-Policy\": \"unsafe-url\"},\n            {},\n            UnsafeUrlPolicy,\n            False,\n        ),\n        (\n            # here, request meta does not set the policy,\n            # but response headers also use an unknown policy,\n            # so the settings' policy is used\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {\"Referrer-Policy\": \"unknown\"},\n            {},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n    ]\n", "n_tokens": 1105, "byte_len": 4928, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 690, "end_line": 833}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 7, "symbols": ["test", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "TestSettingsPolicyByName", "TestPolicyHeaderPrecedence001", "TestPolicyHeaderPrecedence002", "TestPolicyHeaderPrecedence003", "TestPolicyHeaderPrecedence004", "referer", "resp", "headers", "mixin", "referrer", "origin", "polic", "sam", "after", "when", "target", "runtime", "warning", "valid", "policy", "string", "class", "unknown", "pytest", "mw", "get_request", "get_response", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin", "MixinUnsafeUrl", "TestRefererMiddlewareDefault", "TestSettingsNoReferrer", "TestSettingsNoReferrerWhenDowngrade", "TestSettingsSameOrigin"], "ast_kind": "class_or_type", "text": "    def test(self):\n        origin = \"http://www.scrapy.org\"\n        target = \"http://www.example.com\"\n\n        for (\n            settings,\n            response_headers,\n            request_meta,\n            policy_class,\n            check_warning,\n        ) in self.params[3:]:\n            mw = RefererMiddleware(Settings(settings))\n\n            response = Response(origin, headers=response_headers)\n            request = Request(target, meta=request_meta)\n\n            with warnings.catch_warnings(record=True) as w:\n                policy = mw.policy(response, request)\n                assert isinstance(policy, policy_class)\n\n                if check_warning:\n                    assert len(w) == 1\n                    assert w[0].category is RuntimeWarning, w[0].message\n\n\nclass TestSettingsPolicyByName:\n    def test_valid_name(self):\n        for s, p in [\n            (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n            (POLICY_NO_REFERRER, NoReferrerPolicy),\n            (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),\n            (POLICY_SAME_ORIGIN, SameOriginPolicy),\n            (POLICY_ORIGIN, OriginPolicy),\n            (POLICY_STRICT_ORIGIN, StrictOriginPolicy),\n            (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),\n            (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),\n            (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n        ]:\n            settings = Settings({\"REFERRER_POLICY\": s})\n            mw = RefererMiddleware(settings)\n            assert mw.default_policy == p\n\n    def test_valid_name_casevariants(self):\n        for s, p in [\n            (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n            (POLICY_NO_REFERRER, NoReferrerPolicy),\n            (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),\n            (POLICY_SAME_ORIGIN, SameOriginPolicy),\n            (POLICY_ORIGIN, OriginPolicy),\n            (POLICY_STRICT_ORIGIN, StrictOriginPolicy),\n            (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),\n            (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),\n            (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n        ]:\n            settings = Settings({\"REFERRER_POLICY\": s.upper()})\n            mw = RefererMiddleware(settings)\n            assert mw.default_policy == p\n\n    def test_invalid_name(self):\n        settings = Settings({\"REFERRER_POLICY\": \"some-custom-unknown-policy\"})\n        with pytest.raises(RuntimeError):\n            RefererMiddleware(settings)\n\n    def test_multiple_policy_tokens(self):\n        # test parsing without space(s) after the comma\n        settings1 = Settings(\n            {\n                \"REFERRER_POLICY\": (\n                    f\"some-custom-unknown-policy,\"\n                    f\"{POLICY_SAME_ORIGIN},\"\n                    f\"{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},\"\n                    f\"another-custom-unknown-policy\"\n                )\n            }\n        )\n        mw1 = RefererMiddleware(settings1)\n        assert mw1.default_policy == StrictOriginWhenCrossOriginPolicy\n\n        # test parsing with space(s) after the comma\n        settings2 = Settings(\n            {\n                \"REFERRER_POLICY\": (\n                    f\"{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},\"\n                    f\"    another-custom-unknown-policy,\"\n                    f\"    {POLICY_UNSAFE_URL}\"\n                )\n            }\n        )\n        mw2 = RefererMiddleware(settings2)\n        assert mw2.default_policy == UnsafeUrlPolicy\n\n    def test_multiple_policy_tokens_all_invalid(self):\n        settings = Settings(\n            {\n                \"REFERRER_POLICY\": (\n                    \"some-custom-unknown-policy,\"\n                    \"another-custom-unknown-policy,\"\n                    \"yet-another-custom-unknown-policy\"\n                )\n            }\n        )\n        with pytest.raises(RuntimeError):\n            RefererMiddleware(settings)\n\n\nclass TestPolicyHeaderPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n    resp_headers = {\"Referrer-Policy\": POLICY_UNSAFE_URL.upper()}\n\n\nclass TestPolicyHeaderPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n    }\n    resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER.swapcase()}\n\n\nclass TestPolicyHeaderPrecedence003(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n    resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n\n\nclass TestPolicyHeaderPrecedence004(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    \"\"\"\n    The empty string means \"no-referrer-when-downgrade\"\n    \"\"\"\n\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n    resp_headers = {\"Referrer-Policy\": \"\"}\n\n", "n_tokens": 1110, "byte_len": 5087, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 834, "end_line": 972}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 8, "symbols": ["crawler", "referrermw", "redirectmw", "test", "TestReferrerOnRedirect", "TestReferrerOnRedirectNoReferrer", "referer", "origin", "redirection", "init", "referrer", "redirecting", "initial", "target", "https", "get", "pytest", "settings", "final", "isinstance", "none", "fixture", "never", "type", "code", "http", "sequence", "sets", "response", "middleware", "mw", "get_request", "get_response", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin", "MixinUnsafeUrl", "TestRefererMiddlewareDefault", "TestSettingsNoReferrer"], "ast_kind": "class_or_type", "text": "class TestReferrerOnRedirect(TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n    scenarii: Sequence[\n        tuple[str, str, tuple[tuple[int, str], ...], bytes | None, bytes | None]\n    ] = [  # type: ignore[assignment]\n        (\n            \"http://scrapytest.org/1\",  # parent\n            \"http://scrapytest.org/2\",  # target\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/3\"),\n                (301, \"http://scrapytest.org/4\"),\n            ),\n            b\"http://scrapytest.org/1\",  # expected initial referer\n            b\"http://scrapytest.org/1\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://scrapytest.org/2\",\n            (\n                # redirecting to non-secure URL\n                (301, \"http://scrapytest.org/3\"),\n            ),\n            b\"https://scrapytest.org/1\",\n            b\"https://scrapytest.org/1\",\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://scrapytest.com/2\",\n            (\n                # redirecting to non-secure URL: different origin\n                (301, \"http://scrapytest.com/3\"),\n            ),\n            b\"https://scrapytest.org/1\",\n            b\"https://scrapytest.org/1\",\n        ),\n    ]\n\n    @pytest.fixture\n    def crawler(self) -> Crawler:\n        crawler = get_crawler(DefaultSpider, self.settings)\n        crawler.spider = crawler._create_spider()\n        return crawler\n\n    @pytest.fixture\n    def referrermw(self, crawler: Crawler) -> RefererMiddleware:\n        return RefererMiddleware.from_crawler(crawler)\n\n    @pytest.fixture\n    def redirectmw(self, crawler: Crawler) -> RedirectMiddleware:\n        return RedirectMiddleware.from_crawler(crawler)\n\n    def test(  # type: ignore[override]\n        self,\n        crawler: Crawler,\n        referrermw: RefererMiddleware,\n        redirectmw: RedirectMiddleware,\n    ) -> None:\n        for (\n            parent,\n            target,\n            redirections,\n            init_referrer,\n            final_referrer,\n        ) in self.scenarii:\n            response = self.get_response(parent)\n            request = self.get_request(target)\n\n            out = list(referrermw.process_spider_output(response, [request]))\n            assert out[0].headers.get(\"Referer\") == init_referrer\n\n            for status, url in redirections:\n                response = Response(\n                    request.url, headers={\"Location\": url}, status=status\n                )\n                request = cast(\n                    \"Request\", redirectmw.process_response(request, response)\n                )\n                assert crawler.spider\n                referrermw.request_scheduled(request, crawler.spider)\n\n            assert isinstance(request, Request)\n            assert request.headers.get(\"Referer\") == final_referrer\n\n\nclass TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n    \"\"\"\n    No Referrer policy never sets the \"Referer\" header.\n    HTTP redirections should not change that.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": \"no-referrer\"}\n    scenarii = [\n        (\n            \"http://scrapytest.org/1\",  # parent\n            \"http://scrapytest.org/2\",  # target\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/3\"),\n                (301, \"http://scrapytest.org/4\"),\n            ),\n            None,  # expected initial \"Referer\"\n            None,  # expected \"Referer\" for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://scrapytest.org/2\",\n            ((301, \"http://scrapytest.org/3\"),),\n            None,\n            None,\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://example.com/2\",  # different origin\n            ((301, \"http://scrapytest.com/3\"),),\n            None,\n            None,\n        ),\n    ]\n\n", "n_tokens": 928, "byte_len": 4006, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 973, "end_line": 1093}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#9", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 9, "symbols": ["TestReferrerOnRedirectSameOrigin", "TestReferrerOnRedirectStrictOrigin", "necessary", "full", "policy", "unless", "protocol", "expected", "referer", "scenarii", "will", "again", "origin", "redirection", "still", "urls", "http", "redirecting", "domain", "class", "initial", "target", "https", "along", "polic", "stric", "part", "path", "scrapy", "removed", "mw", "get_request", "get_response", "test", "referrer", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin"], "ast_kind": "class_or_type", "text": "class TestReferrerOnRedirectSameOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Same Origin policy sends the full URL as \"Referer\" if the target origin\n    is the same as the parent response (same protocol, same domain, same port).\n\n    HTTP redirections to a different domain or a lower secure level\n    should have the \"Referer\" removed.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": \"same-origin\"}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",  # origin\n            \"http://scrapytest.org/102\",  # target\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/101\",  # expected initial \"Referer\"\n            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting from secure to non-secure URL == different origin\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/201\",\n            None,\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # different domain == different origin\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/301\",\n            None,\n        ),\n    ]\n\n\nclass TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Strict Origin policy will always send the \"origin\" as referrer\n    (think of it as the parent URL without the path part),\n    unless the security level is lower and no \"Referer\" is sent.\n\n    Redirections from secure to non-secure URLs should have the\n    \"Referrer\" header removed if necessary.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": POLICY_STRICT_ORIGIN}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",\n            \"http://scrapytest.org/102\",\n            (\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/\",  # send origin\n            b\"http://scrapytest.org/\",  # redirects to same origin: send origin\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting to non-secure URL: no referrer\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/\",\n            None,\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # redirecting to non-secure URL (different domain): no referrer\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/\",\n            None,\n        ),\n        (\n            \"http://scrapy.org/401\",\n            \"http://example.com/402\",\n            ((301, \"http://scrapytest.org/403\"),),\n            b\"http://scrapy.org/\",\n            b\"http://scrapy.org/\",\n        ),\n        (\n            \"https://scrapy.org/501\",\n            \"https://example.com/502\",\n            (\n                # HTTPS all along, so origin referrer is kept as-is\n                (301, \"https://google.com/503\"),\n                (301, \"https://facebook.com/504\"),\n            ),\n            b\"https://scrapy.org/\",\n            b\"https://scrapy.org/\",\n        ),\n        (\n            \"https://scrapytest.org/601\",\n            \"http://scrapytest.org/602\",  # TLS to non-TLS: no referrer\n            (\n                (\n                    301,\n                    \"https://scrapytest.org/603\",\n                ),  # TLS URL again: (still) no referrer\n            ),\n            None,\n            None,\n        ),\n    ]\n\n", "n_tokens": 910, "byte_len": 3900, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 1094, "end_line": 1213}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py#10", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_referer.py", "rel_path": "tests/test_spidermiddleware_referer.py", "module": "tests.test_spidermiddleware_referer", "ext": "py", "chunk_number": 10, "symbols": ["TestReferrerOnRedirectOriginWhenCrossOrigin", "TestReferrerOnRedirectStrictOriginWhenCrossOrigin", "full", "policy", "unless", "protocol", "expected", "referer", "scenarii", "test", "referrer", "will", "origin", "cross", "redirection", "still", "case", "strict", "redirecting", "domain", "class", "initial", "target", "when", "scrapy", "nothing", "https", "example", "remove", "also", "mw", "get_request", "get_response", "test_valid_name", "test_valid_name_casevariants", "test_invalid_name", "test_multiple_policy_tokens", "test_multiple_policy_tokens_all_invalid", "crawler", "referrermw", "redirectmw", "TestRefererMiddleware", "MixinDefault", "MixinNoReferrer", "MixinNoReferrerWhenDowngrade", "MixinSameOrigin", "MixinOrigin", "MixinStrictOrigin", "MixinOriginWhenCrossOrigin", "MixinStrictOriginWhenCrossOrigin"], "ast_kind": "class_or_type", "text": "class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Origin When Cross-Origin policy sends the full URL as \"Referer\",\n    unless the target's origin is different (different domain, different protocol)\n    in which case only the origin is sent.\n\n    Redirections to a different origin should strip the \"Referer\"\n    to the parent origin.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",  # origin\n            \"http://scrapytest.org/102\",  # target + redirection\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/101\",  # expected initial referer\n            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting to non-secure URL: send origin\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/201\",\n            b\"https://scrapytest.org/\",\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # redirecting to non-secure URL (different domain): send origin\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/301\",\n            b\"https://scrapytest.org/\",\n        ),\n        (\n            \"http://scrapy.org/401\",\n            \"http://example.com/402\",\n            ((301, \"http://scrapytest.org/403\"),),\n            b\"http://scrapy.org/\",\n            b\"http://scrapy.org/\",\n        ),\n        (\n            \"https://scrapy.org/501\",\n            \"https://example.com/502\",\n            (\n                # all different domains: send origin\n                (301, \"https://google.com/503\"),\n                (301, \"https://facebook.com/504\"),\n            ),\n            b\"https://scrapy.org/\",\n            b\"https://scrapy.org/\",\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"http://scrapytest.org/302\",  # TLS to non-TLS: send origin\n            ((301, \"https://scrapytest.org/303\"),),  # TLS URL again: send origin (also)\n            b\"https://scrapytest.org/\",\n            b\"https://scrapytest.org/\",\n        ),\n    ]\n\n\nclass TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Strict Origin When Cross-Origin policy sends the full URL as \"Referer\",\n    unless the target's origin is different (different domain, different protocol)\n    in which case only the origin is sent...\n    Unless there's also a downgrade in security and then the \"Referer\" header\n    is not sent.\n\n    Redirections to a different origin should strip the \"Referer\" to the parent origin,\n    and from https:// to http:// will remove the \"Referer\" header.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",  # origin\n            \"http://scrapytest.org/102\",  # target + redirection\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/101\",  # expected initial referer\n            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting to non-secure URL: do not send the \"Referer\" header\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/201\",\n            None,\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # redirecting to non-secure URL (different domain): send origin\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/301\",\n            None,\n        ),\n        (\n            \"http://scrapy.org/401\",\n            \"http://example.com/402\",\n            ((301, \"http://scrapytest.org/403\"),),\n            b\"http://scrapy.org/\",\n            b\"http://scrapy.org/\",\n        ),\n        (\n            \"https://scrapy.org/501\",\n            \"https://example.com/502\",\n            (\n                # all different domains: send origin\n                (301, \"https://google.com/503\"),\n                (301, \"https://facebook.com/504\"),\n            ),\n            b\"https://scrapy.org/\",\n            b\"https://scrapy.org/\",\n        ),\n        (\n            \"https://scrapytest.org/601\",\n            \"http://scrapytest.org/602\",  # TLS to non-TLS: do not send \"Referer\"\n            (\n                (\n                    301,\n                    \"https://scrapytest.org/603\",\n                ),  # TLS URL again: (still) send nothing\n            ),\n            None,\n            None,\n        ),\n    ]\n", "n_tokens": 1217, "byte_len": 5200, "file_sha1": "dedf1ed944f6f161d006c7a950f3062e8a39fc0d", "start_line": 1214, "end_line": 1361}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_asyncgen.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_asyncgen.py", "rel_path": "tests/test_utils_asyncgen.py", "module": "tests.test_utils_asyncgen", "ext": "py", "chunk_number": 1, "symbols": ["TestAsyncgenUtils", "async", "await", "test", "collect", "deferred", "from", "class", "scrapy", "asyncgen", "defer", "list", "assert", "generator", "results", "range", "utils", "import", "self"], "ast_kind": "class_or_type", "text": "from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.defer import deferred_f_from_coro_f\n\n\nclass TestAsyncgenUtils:\n    @deferred_f_from_coro_f\n    async def test_as_async_generator(self):\n        ag = as_async_generator(range(42))\n        results = [i async for i in ag]\n        assert results == list(range(42))\n\n    @deferred_f_from_coro_f\n    async def test_collect_asyncgen(self):\n        ag = as_async_generator(range(42))\n        results = await collect_asyncgen(ag)\n        assert results == list(range(42))\n", "n_tokens": 128, "byte_len": 552, "file_sha1": "5081911820a3c852166cb5353fb89329aadfe044", "start_line": 1, "end_line": 17}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py", "rel_path": "tests/spiders.py", "module": "tests.spiders", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "closed", "parse", "errback", "log_debug", "log_info", "log_warning", "log_error", "log_critical", "MockServerSpider", "MetaSpider", "FollowAllSpider", "DelaySpider", "LogSpider", "SlowSpider", "SimpleSpider", "AsyncDefSpider", "AsyncDefAsyncioSpider", "AsyncDefAsyncioReturnSpider", "AsyncDefAsyncioReturnSingleElementSpider", "AsyncDefAsyncioReqsReturnSpider", "deferred", "future", "async", "delay", "spider", "append", "slow", "name", "def", "_get_req", "raise_exception", "on_error", "process_request", "from_crawler", "bytes_received", "headers_received", "AsyncDefAsyncioGenExcSpider", "AsyncDefDeferredDirectSpider", "AsyncDefDeferredWrappedSpider", "AsyncDefDeferredMaybeWrappedSpider", "AsyncDefAsyncioGenSpider", "AsyncDefAsyncioGenLoopSpider", "AsyncDefAsyncioGenComplexSpider", "ItemSpider", "MaxItemsAndRequestsSpider", "DefaultError", "ErrorSpider", "BrokenStartSpider", "StartItemSpider"], "ast_kind": "class_or_type", "text": "\"\"\"\nSome spiders used for testing and benchmarking\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport time\nfrom urllib.parse import urlencode\n\nfrom twisted.internet import defer\n\nfrom scrapy import signals\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.http import Request\nfrom scrapy.item import Item\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider\nfrom scrapy.spiders.crawl import CrawlSpider, Rule\nfrom scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future\nfrom scrapy.utils.test import get_from_asyncio_queue, get_web_client_agent_req\n\n\nclass MockServerSpider(Spider):\n    def __init__(self, mockserver=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mockserver = mockserver\n\n\nclass MetaSpider(MockServerSpider):\n    name = \"meta\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.meta = {}\n\n    def closed(self, reason):\n        self.meta[\"close_reason\"] = reason\n\n\nclass FollowAllSpider(MetaSpider):\n    name = \"follow\"\n    link_extractor = LinkExtractor()\n\n    def __init__(\n        self, total=10, show=20, order=\"rand\", maxlatency=0.0, *args, **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.urls_visited = []\n        self.times = []\n        qargs = {\"total\": total, \"show\": show, \"order\": order, \"maxlatency\": maxlatency}\n        url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=True)}\")\n        self.start_urls = [url]\n\n    def parse(self, response):\n        self.urls_visited.append(response.url)\n        self.times.append(time.time())\n        for link in self.link_extractor.extract_links(response):\n            yield Request(link.url, callback=self.parse)\n\n\nclass DelaySpider(MetaSpider):\n    name = \"delay\"\n\n    def __init__(self, n=1, b=0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n = n\n        self.b = b\n        self.t1 = self.t2 = self.t2_err = 0\n\n    async def start(self):\n        self.t1 = time.time()\n        url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n        yield Request(url, callback=self.parse, errback=self.errback)\n\n    def parse(self, response):\n        self.t2 = time.time()\n\n    def errback(self, failure):\n        self.t2_err = time.time()\n\n\nclass LogSpider(MetaSpider):\n    name = \"log_spider\"\n\n    def log_debug(self, message: str, extra: dict | None = None):\n        self.logger.debug(message, extra=extra)\n\n    def log_info(self, message: str, extra: dict | None = None):\n        self.logger.info(message, extra=extra)\n\n    def log_warning(self, message: str, extra: dict | None = None):\n        self.logger.warning(message, extra=extra)\n\n    def log_error(self, message: str, extra: dict | None = None):\n        self.logger.error(message, extra=extra)\n\n    def log_critical(self, message: str, extra: dict | None = None):\n        self.logger.critical(message, extra=extra)\n\n    def parse(self, response):\n        pass\n\n\nclass SlowSpider(DelaySpider):\n    name = \"slow\"\n\n    async def start(self):\n        # 1st response is fast\n        url = self.mockserver.url(\"/delay?n=0&b=0\")\n        yield Request(url, callback=self.parse, errback=self.errback)\n\n        # 2nd response is slow\n        url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n        yield Request(url, callback=self.parse, errback=self.errback)\n\n    def parse(self, response):\n        yield Item()\n\n\nclass SimpleSpider(MetaSpider):\n    name = \"simple\"\n\n    def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = [url]\n\n    def parse(self, response):\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefSpider(SimpleSpider):\n    name = \"asyncdef\"\n\n    async def parse(self, response):\n        await defer.succeed(42)\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefAsyncioSpider(SimpleSpider):\n    name = \"asyncdef_asyncio\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}\")\n\n\nclass AsyncDefAsyncioReturnSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_return\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}\")\n        return [{\"id\": 1}, {\"id\": 2}]\n\n\nclass AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_return_single_element\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.1)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}\")\n        return {\"foo\": 42}\n\n\nclass AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_reqs_return\"\n", "n_tokens": 1190, "byte_len": 4918, "file_sha1": "6f4dd101f65df72434da082d8a26949ccd7423af", "start_line": 1, "end_line": 171}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py", "rel_path": "tests/spiders.py", "module": "tests.spiders", "ext": "py", "chunk_number": 2, "symbols": ["_get_req", "parse", "__init__", "raise_exception", "AsyncDefAsyncioGenExcSpider", "AsyncDefDeferredDirectSpider", "AsyncDefDeferredWrappedSpider", "AsyncDefDeferredMaybeWrappedSpider", "AsyncDefAsyncioGenSpider", "AsyncDefAsyncioGenLoopSpider", "AsyncDefAsyncioGenComplexSpider", "ItemSpider", "MaxItemsAndRequestsSpider", "DefaultError", "ErrorSpider", "BrokenStartSpider", "StartItemSpider", "max", "requests", "items", "deferred", "future", "async", "fail", "before", "seed", "def", "append", "default", "error", "closed", "errback", "log_debug", "log_info", "log_warning", "log_error", "log_critical", "on_error", "process_request", "from_crawler", "bytes_received", "headers_received", "MockServerSpider", "MetaSpider", "FollowAllSpider", "DelaySpider", "LogSpider", "SlowSpider", "SimpleSpider", "AsyncDefSpider"], "ast_kind": "class_or_type", "text": "    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        req_id = response.meta.get(\"req_id\", 0)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}, req_id {req_id}\")\n        if req_id > 0:\n            return None\n        reqs = []\n        for i in range(1, 3):\n            req = Request(self.start_urls[0], dont_filter=True, meta={\"req_id\": i})\n            reqs.append(req)\n        return reqs\n\n\nclass AsyncDefAsyncioGenExcSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen_exc\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {\"foo\": i}\n            if i > 5:\n                raise ValueError(\"Stopping the processing\")\n\n\nclass AsyncDefDeferredDirectSpider(SimpleSpider):\n    name = \"asyncdef_deferred_direct\"\n\n    async def parse(self, response):\n        resp = await get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n        yield {\"code\": resp.code}\n\n\nclass AsyncDefDeferredWrappedSpider(SimpleSpider):\n    name = \"asyncdef_deferred_wrapped\"\n\n    async def parse(self, response):\n        resp = await deferred_to_future(\n            get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n        )\n        yield {\"code\": resp.code}\n\n\nclass AsyncDefDeferredMaybeWrappedSpider(SimpleSpider):\n    name = \"asyncdef_deferred_wrapped\"\n\n    async def parse(self, response):\n        resp = await maybe_deferred_to_future(\n            get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n        )\n        yield {\"code\": resp.code}\n\n\nclass AsyncDefAsyncioGenSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        yield {\"foo\": 42}\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen_loop\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {\"foo\": i}\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen_complex\"\n    initial_reqs = 4\n    following_reqs = 3\n    depth = 2\n\n    def _get_req(self, index, cb=None):\n        return Request(\n            self.mockserver.url(f\"/status?n=200&request={index}\"),\n            meta={\"index\": index},\n            dont_filter=True,\n            callback=cb,\n        )\n\n    async def start(self):\n        for i in range(1, self.initial_reqs + 1):\n            yield self._get_req(i)\n\n    async def parse(self, response):\n        index = response.meta[\"index\"]\n        yield {\"index\": index}\n        if index < 10**self.depth:\n            for new_index in range(10 * index, 10 * index + self.following_reqs):\n                yield self._get_req(new_index)\n        yield self._get_req(index, cb=self.parse2)\n        await asyncio.sleep(0.1)\n        yield {\"index\": index + 5}\n\n    async def parse2(self, response):\n        await asyncio.sleep(0.1)\n        yield {\"index2\": response.meta[\"index\"]}\n\n\nclass ItemSpider(FollowAllSpider):\n    name = \"item\"\n\n    def parse(self, response):\n        for request in super().parse(response):\n            yield request\n            yield Item()\n            yield {}\n\n\nclass MaxItemsAndRequestsSpider(FollowAllSpider):\n    def __init__(self, max_items=10, max_requests=10, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.max_items = max_items\n        self.max_requests = max_requests\n\n    def parse(self, response):\n        self.items_scraped = 0\n        self.pages_crawled = 1  # account for the start url\n        for request in super().parse(response):\n            if self.pages_crawled < self.max_requests:\n                yield request\n                self.pages_crawled += 1\n            if self.items_scraped < self.max_items:\n                yield Item()\n                self.items_scraped += 1\n\n\nclass DefaultError(Exception):\n    pass\n\n\nclass ErrorSpider(FollowAllSpider):\n    name = \"error\"\n    exception_cls = DefaultError\n\n    def raise_exception(self):\n        raise self.exception_cls(\"Expected exception\")\n\n    def parse(self, response):\n        for request in super().parse(response):\n            yield request\n            self.raise_exception()\n\n\nclass BrokenStartSpider(FollowAllSpider):\n    fail_before_yield = False\n    fail_yielding = False\n\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        self.seedsseen = []\n\n    async def start(self):\n        if self.fail_before_yield:\n            1 / 0\n\n        for s in range(100):\n            qargs = {\"total\": 10, \"seed\": s}\n            url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=True)}\")\n            yield Request(url, meta={\"seed\": s})\n            if self.fail_yielding:\n                2 / 0\n\n        assert self.seedsseen, \"All seeds consumed before any download happened\"\n\n    def parse(self, response):\n        self.seedsseen.append(response.meta.get(\"seed\"))\n        yield from super().parse(response)\n\n\nclass StartItemSpider(FollowAllSpider):", "n_tokens": 1221, "byte_len": 5173, "file_sha1": "6f4dd101f65df72434da082d8a26949ccd7423af", "start_line": 172, "end_line": 348}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py", "rel_path": "tests/spiders.py", "module": "tests.spiders", "ext": "py", "chunk_number": 3, "symbols": ["parse", "on_error", "__init__", "errback", "StartGoodAndBadOutput", "SingleRequestSpider", "DuplicateStartSpider", "CrawlSpiderWithParseMethod", "CrawlSpiderWithAsyncCallback", "CrawlSpiderWithAsyncGeneratorCallback", "CrawlSpiderWithErrback", "CrawlSpiderWithProcessRequestCallbackKeywordArguments", "method", "async", "crawl", "spider", "seed", "append", "generator", "name", "retr", "htt", "responses", "dont", "filter", "rules", "mockserver", "dupe", "factor", "visited", "closed", "log_debug", "log_info", "log_warning", "log_error", "log_critical", "_get_req", "raise_exception", "process_request", "from_crawler", "bytes_received", "headers_received", "MockServerSpider", "MetaSpider", "FollowAllSpider", "DelaySpider", "LogSpider", "SlowSpider", "SimpleSpider", "AsyncDefSpider"], "ast_kind": "class_or_type", "text": "    async def start(self):\n        yield {\"name\": \"test item\"}\n\n\nclass StartGoodAndBadOutput(FollowAllSpider):\n    async def start(self):\n        yield {\"a\": \"a\"}\n        yield Request(\"data:,a\")\n        yield \"data:,b\"\n        yield object()\n\n\nclass SingleRequestSpider(MetaSpider):\n    seed = None\n    callback_func = None\n    errback_func = None\n\n    async def start(self):\n        if isinstance(self.seed, Request):\n            yield self.seed.replace(callback=self.parse, errback=self.on_error)\n        else:\n            yield Request(self.seed, callback=self.parse, errback=self.on_error)\n\n    def parse(self, response):\n        self.meta.setdefault(\"responses\", []).append(response)\n        if callable(self.callback_func):\n            return self.callback_func(response)\n        if \"next\" in response.meta:\n            return response.meta[\"next\"]\n        return None\n\n    def on_error(self, failure):\n        self.meta[\"failure\"] = failure\n        if callable(self.errback_func):\n            return self.errback_func(failure)\n        return None\n\n\nclass DuplicateStartSpider(MockServerSpider):\n    dont_filter = True\n    name = \"duplicatestartrequests\"\n    distinct_urls = 2\n    dupe_factor = 3\n\n    async def start(self):\n        for i in range(self.distinct_urls):\n            for j in range(self.dupe_factor):\n                url = self.mockserver.url(f\"/echo?headers=1&body=test{i}\")\n                yield Request(url, dont_filter=self.dont_filter)\n\n    def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.visited = 0\n\n    def parse(self, response):\n        self.visited += 1\n\n\nclass CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n    \"\"\"\n    A CrawlSpider which overrides the 'parse' method\n    \"\"\"\n\n    name = \"crawl_spider_with_parse_method\"\n    custom_settings: dict = {\n        \"RETRY_HTTP_CODES\": [],  # no need to retry\n    }\n    rules = (Rule(LinkExtractor(), callback=\"parse\", follow=True),)\n\n    async def start(self):\n        test_body = b\"\"\"\n        <html>\n            <head><title>Page title</title></head>\n            <body>\n                <p><a href=\"/status?n=200\">Item 200</a></p>  <!-- callback -->\n                <p><a href=\"/status?n=201\">Item 201</a></p>  <!-- callback -->\n            </body>\n        </html>\n        \"\"\"\n        url = self.mockserver.url(\"/alpayload\")\n        yield Request(url, method=\"POST\", body=test_body)\n\n    def parse(self, response, foo=None):\n        self.logger.info(\"[parse] status %i (foo: %s)\", response.status, foo)\n        yield Request(\n            self.mockserver.url(\"/status?n=202\"), self.parse, cb_kwargs={\"foo\": \"bar\"}\n        )\n\n\nclass CrawlSpiderWithAsyncCallback(CrawlSpiderWithParseMethod):\n    \"\"\"A CrawlSpider with an async def callback\"\"\"\n\n    name = \"crawl_spider_with_async_callback\"\n    rules = (Rule(LinkExtractor(), callback=\"parse_async\", follow=True),)\n\n    async def parse_async(self, response, foo=None):\n        self.logger.info(\"[parse_async] status %i (foo: %s)\", response.status, foo)\n        return Request(\n            self.mockserver.url(\"/status?n=202\"),\n            self.parse_async,\n            cb_kwargs={\"foo\": \"bar\"},\n        )\n\n\nclass CrawlSpiderWithAsyncGeneratorCallback(CrawlSpiderWithParseMethod):\n    \"\"\"A CrawlSpider with an async generator callback\"\"\"\n\n    name = \"crawl_spider_with_async_generator_callback\"\n    rules = (Rule(LinkExtractor(), callback=\"parse_async_gen\", follow=True),)\n\n    async def parse_async_gen(self, response, foo=None):\n        self.logger.info(\"[parse_async_gen] status %i (foo: %s)\", response.status, foo)\n        yield Request(\n            self.mockserver.url(\"/status?n=202\"),\n            self.parse_async_gen,\n            cb_kwargs={\"foo\": \"bar\"},\n        )\n\n\nclass CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n    name = \"crawl_spider_with_errback\"\n    rules = (Rule(LinkExtractor(), callback=\"parse\", errback=\"errback\", follow=True),)\n\n    async def start(self):\n        test_body = b\"\"\"\n        <html>\n            <head><title>Page title</title></head>\n            <body>\n                <p><a href=\"/status?n=200\">Item 200</a></p>  <!-- callback -->\n                <p><a href=\"/status?n=201\">Item 201</a></p>  <!-- callback -->\n                <p><a href=\"/status?n=404\">Item 404</a></p>  <!-- errback -->\n                <p><a href=\"/status?n=500\">Item 500</a></p>  <!-- errback -->\n                <p><a href=\"/status?n=501\">Item 501</a></p>  <!-- errback -->\n            </body>\n        </html>\n        \"\"\"\n        url = self.mockserver.url(\"/alpayload\")\n        yield Request(url, method=\"POST\", body=test_body)\n\n    def errback(self, failure):\n        self.logger.info(\"[errback] status %i\", failure.value.response.status)\n\n\nclass CrawlSpiderWithProcessRequestCallbackKeywordArguments(CrawlSpiderWithParseMethod):\n    name = \"crawl_spider_with_process_request_cb_kwargs\"\n    rules = (\n        Rule(\n            LinkExtractor(),\n            callback=\"parse\",\n            follow=True,\n            process_request=\"process_request\",\n        ),\n    )\n", "n_tokens": 1207, "byte_len": 5087, "file_sha1": "6f4dd101f65df72434da082d8a26949ccd7423af", "start_line": 349, "end_line": 502}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/spiders.py", "rel_path": "tests/spiders.py", "module": "tests.spiders", "ext": "py", "chunk_number": 4, "symbols": ["process_request", "from_crawler", "parse", "errback", "bytes_received", "headers_received", "BytesReceivedCallbackSpider", "BytesReceivedErrbackSpider", "HeadersReceivedCallbackSpider", "HeadersReceivedErrbackSpider", "post", "method", "async", "false", "kwargs", "spider", "meta", "bytes", "received", "return", "class", "headers", "classmethod", "mockserver", "body", "connect", "failure", "from", "crawler", "yield", "__init__", "closed", "log_debug", "log_info", "log_warning", "log_error", "log_critical", "_get_req", "raise_exception", "on_error", "MockServerSpider", "MetaSpider", "FollowAllSpider", "DelaySpider", "LogSpider", "SlowSpider", "SimpleSpider", "AsyncDefSpider", "AsyncDefAsyncioSpider", "AsyncDefAsyncioReturnSpider"], "ast_kind": "class_or_type", "text": "    def process_request(self, request, response):\n        request.cb_kwargs[\"foo\"] = \"process_request\"\n        return request\n\n\nclass BytesReceivedCallbackSpider(MetaSpider):\n    full_response_length = 2**18\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.bytes_received, signals.bytes_received)\n        return spider\n\n    async def start(self):\n        body = b\"a\" * self.full_response_length\n        url = self.mockserver.url(\"/alpayload\")\n        yield Request(url, method=\"POST\", body=body, errback=self.errback)\n\n    def parse(self, response):\n        self.meta[\"response\"] = response\n\n    def errback(self, failure):\n        self.meta[\"failure\"] = failure\n\n    def bytes_received(self, data, request, spider):\n        self.meta[\"bytes_received\"] = data\n        raise StopDownload(fail=False)\n\n\nclass BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):\n    def bytes_received(self, data, request, spider):\n        self.meta[\"bytes_received\"] = data\n        raise StopDownload(fail=True)\n\n\nclass HeadersReceivedCallbackSpider(MetaSpider):\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.headers_received, signals.headers_received)\n        return spider\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status\"), errback=self.errback)\n\n    def parse(self, response):\n        self.meta[\"response\"] = response\n\n    def errback(self, failure):\n        self.meta[\"failure\"] = failure\n\n    def headers_received(self, headers, body_length, request, spider):\n        self.meta[\"headers_received\"] = headers\n        raise StopDownload(fail=False)\n\n\nclass HeadersReceivedErrbackSpider(HeadersReceivedCallbackSpider):\n    def headers_received(self, headers, body_length, request, spider):\n        self.meta[\"headers_received\"] = headers\n        raise StopDownload(fail=True)\n", "n_tokens": 445, "byte_len": 2047, "file_sha1": "6f4dd101f65df72434da082d8a26949ccd7423af", "start_line": 503, "end_line": 564}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_stats.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_stats.py", "rel_path": "tests/test_downloadermiddleware_stats.py", "module": "tests.test_downloadermiddleware_stats", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "assertStatsEqual", "test_process_request", "test_process_response", "test_process_exception", "teardown_method", "MyException", "TestDownloaderStats", "test", "downloader", "process", "exception", "teardown", "method", "pass", "open", "spider", "downloadermiddleware", "class", "downloadermiddlewares", "close", "spiders", "scrapy", "type", "value", "get", "crawler", "from", "response", "scrapytest", "assert", "count", "request", "stats", "utils", "import", "myexception", "http", "self", "tests", "setup", "status"], "ast_kind": "class_or_type", "text": "from scrapy.downloadermiddlewares.stats import DownloaderStats\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass MyException(Exception):\n    pass\n\n\nclass TestDownloaderStats:\n    def setup_method(self):\n        self.crawler = get_crawler(Spider)\n        self.mw = DownloaderStats(self.crawler.stats)\n\n        self.crawler.stats.open_spider()\n\n        self.req = Request(\"http://scrapytest.org\")\n        self.res = Response(\"scrapytest.org\", status=400)\n\n    def assertStatsEqual(self, key, value):\n        assert self.crawler.stats.get_value(key) == value, str(\n            self.crawler.stats.get_stats()\n        )\n\n    def test_process_request(self):\n        self.mw.process_request(self.req)\n        self.assertStatsEqual(\"downloader/request_count\", 1)\n\n    def test_process_response(self):\n        self.mw.process_response(self.req, self.res)\n        self.assertStatsEqual(\"downloader/response_count\", 1)\n\n    def test_process_exception(self):\n        self.mw.process_exception(self.req, MyException())\n        self.assertStatsEqual(\"downloader/exception_count\", 1)\n        self.assertStatsEqual(\n            \"downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException\",\n            1,\n        )\n\n    def teardown_method(self):\n        self.crawler.stats.close_spider()\n", "n_tokens": 293, "byte_len": 1372, "file_sha1": "59929428a7405c1703d96f15a0bfec94c1c30f91", "start_line": 1, "end_line": 44}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_project.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_project.py", "rel_path": "tests/test_utils_project.py", "module": "tests.test_utils_project", "ext": "py", "chunk_number": 1, "symbols": ["proj_path", "test_data_path_outside_project", "test_data_path_inside_project", "test_valid_envvar", "test_invalid_envvar", "test_valid_and_invalid_envvars", "TestGetProjectSettings", "abspath", "expected", "simplefilter", "project", "dir", "chdir", "absolute", "prev", "test", "cmdline", "class", "somepath", "envvars", "scrap", "foo", "with", "path", "scrapy", "data", "invalid", "tmp", "pathlib", "get", "value", "warnings", "yield", "pytest", "finally", "from", "settings", "setting", "set", "environ", "assert", "valid", "touch", "none", "fixture", "catch", "utils", "import", "misc", "self"], "ast_kind": "class_or_type", "text": "import os\nimport warnings\nfrom pathlib import Path\n\nimport pytest\n\nfrom scrapy.utils.misc import set_environ\nfrom scrapy.utils.project import data_path, get_project_settings\n\n\n@pytest.fixture\ndef proj_path(tmp_path):\n    prev_dir = Path.cwd()\n    project_dir = tmp_path\n\n    try:\n        os.chdir(project_dir)\n        Path(\"scrapy.cfg\").touch()\n\n        yield project_dir\n    finally:\n        os.chdir(prev_dir)\n\n\ndef test_data_path_outside_project():\n    assert str(Path(\".scrapy\", \"somepath\")) == data_path(\"somepath\")\n    abspath = str(Path(os.path.sep, \"absolute\", \"path\"))\n    assert abspath == data_path(abspath)\n\n\ndef test_data_path_inside_project(proj_path: Path) -> None:\n    expected = proj_path / \".scrapy\" / \"somepath\"\n    assert expected.resolve() == Path(data_path(\"somepath\")).resolve()\n    abspath = str(Path(os.path.sep, \"absolute\", \"path\").resolve())\n    assert abspath == data_path(abspath)\n\n\nclass TestGetProjectSettings:\n    def test_valid_envvar(self):\n        value = \"tests.test_cmdline.settings\"\n        envvars = {\n            \"SCRAPY_SETTINGS_MODULE\": value,\n        }\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            with set_environ(**envvars):\n                settings = get_project_settings()\n\n        assert settings.get(\"SETTINGS_MODULE\") == value\n\n    def test_invalid_envvar(self):\n        envvars = {\n            \"SCRAPY_FOO\": \"bar\",\n        }\n        with set_environ(**envvars):\n            settings = get_project_settings()\n\n        assert settings.get(\"SCRAPY_FOO\") is None\n\n    def test_valid_and_invalid_envvars(self):\n        value = \"tests.test_cmdline.settings\"\n        envvars = {\n            \"SCRAPY_FOO\": \"bar\",\n            \"SCRAPY_SETTINGS_MODULE\": value,\n        }\n        with set_environ(**envvars):\n            settings = get_project_settings()\n        assert settings.get(\"SETTINGS_MODULE\") == value\n        assert settings.get(\"SCRAPY_FOO\") is None\n", "n_tokens": 441, "byte_len": 1950, "file_sha1": "2218b5d5514e431e7d35b8e89d145983a6ab90ea", "start_line": 1, "end_line": 70}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_robotstxt.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_robotstxt.py", "rel_path": "tests/test_downloadermiddleware_robotstxt.py", "module": "tests.test_downloadermiddleware_robotstxt", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "teardown_method", "test_robotstxt_settings", "_get_successful_crawler", "_get_garbage_crawler", "TestRobotsTxtMiddleware", "async", "site", "call", "later", "agent", "test", "robotstxt", "assert", "robots", "callback", "deferred", "from", "future", "typ", "checking", "https", "main", "mock", "download", "pytest", "settings", "gather", "none", "encode", "_get_emptybody_crawler", "test_robotstxt_user_agent_setting", "assertRobotsTxtRequested", "TestRobotsTxtMiddlewareWithRerp", "failure", "address", "robotstx", "use", "linux", "mozilla", "failed", "return", "value", "base", "url", "html", "reason", "world", "http", "fail"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nfrom typing import TYPE_CHECKING\nfrom unittest import mock\n\nimport pytest\nfrom twisted.internet import error\nfrom twisted.internet.defer import Deferred, DeferredList\nfrom twisted.python import failure\n\nfrom scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response, TextResponse\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.settings import Settings\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n)\nfrom tests.test_robotstxt_interface import rerp_available\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nclass TestRobotsTxtMiddleware:\n    def setup_method(self):\n        self.crawler = mock.MagicMock()\n        self.crawler.settings = Settings()\n        self.crawler.engine.download_async = mock.AsyncMock()\n\n    def teardown_method(self):\n        del self.crawler\n\n    def test_robotstxt_settings(self):\n        self.crawler.settings = Settings()\n        self.crawler.settings.set(\"USER_AGENT\", \"CustomAgent\")\n        with pytest.raises(NotConfigured):\n            RobotsTxtMiddleware(self.crawler)\n\n    def _get_successful_crawler(self) -> Crawler:\n        crawler = self.crawler\n        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        ROBOTS = \"\"\"\nUser-Agent: *\nDisallow: /admin/\nDisallow: /static/\n# taken from https://en.wikipedia.org/robots.txt\nDisallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\nDisallow: /wiki/Käyttäjä:\nUser-Agent: UnicödeBöt\nDisallow: /some/randome/page.html\n\"\"\".encode()\n        response = TextResponse(\"http://site.local/robots.txt\", body=ROBOTS)\n\n        async def return_response(request):\n            deferred = Deferred()\n            call_later(0, deferred.callback, response)\n            return await maybe_deferred_to_future(deferred)\n\n        crawler.engine.download_async.side_effect = return_response\n        return crawler\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt(self):\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n        self.assertRobotsTxtRequested(\"http://site.local\")\n        await self.assertIgnored(Request(\"http://site.local/admin/main\"), middleware)\n        await self.assertIgnored(Request(\"http://site.local/static/\"), middleware)\n        await self.assertIgnored(\n            Request(\"http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\"), middleware\n        )\n        await self.assertIgnored(\n            Request(\"http://site.local/wiki/Käyttäjä:\"), middleware\n        )\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_multiple_reqs(self) -> None:\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        d1 = deferred_from_coro(\n            middleware.process_request(Request(\"http://site.local/allowed1\"))\n        )\n        d2 = deferred_from_coro(\n            middleware.process_request(Request(\"http://site.local/allowed2\"))\n        )\n        await maybe_deferred_to_future(DeferredList([d1, d2], fireOnOneErrback=True))\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_robotstxt_multiple_reqs_asyncio(self) -> None:\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        c1 = middleware.process_request(Request(\"http://site.local/allowed1\"))\n        c2 = middleware.process_request(Request(\"http://site.local/allowed2\"))\n        await asyncio.gather(c1, c2)\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_ready_parser(self):\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_meta(self):\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        meta = {\"dont_obey_robotstxt\": True}\n        await self.assertNotIgnored(\n            Request(\"http://site.local/allowed\", meta=meta), middleware\n        )\n        await self.assertNotIgnored(\n            Request(\"http://site.local/admin/main\", meta=meta), middleware\n        )\n        await self.assertNotIgnored(\n            Request(\"http://site.local/static/\", meta=meta), middleware\n        )\n\n    def _get_garbage_crawler(self) -> Crawler:\n        crawler = self.crawler\n        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        response = Response(\n            \"http://site.local/robots.txt\", body=b\"GIF89a\\xd3\\x00\\xfe\\x00\\xa2\"\n        )\n\n        async def return_response(request):\n            deferred = Deferred()\n            call_later(0, deferred.callback, response)\n            return await maybe_deferred_to_future(deferred)\n\n        crawler.engine.download_async.side_effect = return_response\n        return crawler\n", "n_tokens": 1131, "byte_len": 5039, "file_sha1": "a64ee32f0949d9023999643e25d192502bfd7af8", "start_line": 1, "end_line": 134}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_robotstxt.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_robotstxt.py", "rel_path": "tests/test_downloadermiddleware_robotstxt.py", "module": "tests.test_downloadermiddleware_robotstxt", "ext": "py", "chunk_number": 2, "symbols": ["_get_emptybody_crawler", "test_robotstxt_user_agent_setting", "assertRobotsTxtRequested", "setup_method", "TestRobotsTxtMiddlewareWithRerp", "failure", "address", "async", "site", "call", "later", "robotstx", "use", "assert", "robots", "callback", "deferred", "from", "robotstxt", "linux", "main", "mozilla", "mock", "download", "failed", "pytest", "settings", "none", "return", "value", "teardown_method", "test_robotstxt_settings", "_get_successful_crawler", "_get_garbage_crawler", "TestRobotsTxtMiddleware", "agent", "test", "future", "typ", "checking", "https", "gather", "encode", "base", "url", "html", "reason", "world", "http", "fail"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_robotstxt_garbage(self):\n        # garbage response should be discarded, equal 'allow all'\n        middleware = RobotsTxtMiddleware(self._get_garbage_crawler())\n        await self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n        await self.assertNotIgnored(Request(\"http://site.local/admin/main\"), middleware)\n        await self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware)\n\n    def _get_emptybody_crawler(self) -> Crawler:\n        crawler = self.crawler\n        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        response = Response(\"http://site.local/robots.txt\")\n\n        async def return_response(request):\n            deferred = Deferred()\n            call_later(0, deferred.callback, response)\n            return await maybe_deferred_to_future(deferred)\n\n        crawler.engine.download_async.side_effect = return_response\n        return crawler\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_empty_response(self):\n        # empty response should equal 'allow all'\n        middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n        await self.assertNotIgnored(Request(\"http://site.local/admin/main\"), middleware)\n        await self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware)\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_error(self, caplog: pytest.LogCaptureFixture) -> None:\n        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        err = error.DNSLookupError(\"Robotstxt address not found\")\n\n        async def return_failure(request):\n            deferred = Deferred()\n            call_later(0, deferred.errback, failure.Failure(err))\n            return await maybe_deferred_to_future(deferred)\n\n        self.crawler.engine.download_async.side_effect = return_failure\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        await middleware.process_request(Request(\"http://site.local\"))\n        assert \"DNS lookup failed: Robotstxt address not found\" in caplog.text\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_immediate_error(self):\n        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        err = error.DNSLookupError(\"Robotstxt address not found\")\n\n        async def immediate_failure(request):\n            raise err\n\n        self.crawler.engine.download_async.side_effect = immediate_failure\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        await self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n\n    @deferred_f_from_coro_f\n    async def test_ignore_robotstxt_request(self):\n        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n\n        async def ignore_request(request):\n            deferred = Deferred()\n            call_later(0, deferred.errback, failure.Failure(IgnoreRequest()))\n            return await maybe_deferred_to_future(deferred)\n\n        self.crawler.engine.download_async.side_effect = ignore_request\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        with mock.patch(\n            \"scrapy.downloadermiddlewares.robotstxt.logger\"\n        ) as mw_module_logger:\n            await self.assertNotIgnored(\n                Request(\"http://site.local/allowed\"), middleware\n            )\n            assert not mw_module_logger.error.called\n\n    def test_robotstxt_user_agent_setting(self):\n        crawler = self._get_successful_crawler()\n        crawler.settings.set(\"ROBOTSTXT_USER_AGENT\", \"Examplebot\")\n        crawler.settings.set(\"USER_AGENT\", \"Mozilla/5.0 (X11; Linux x86_64)\")\n        middleware = RobotsTxtMiddleware(crawler)\n        rp = mock.MagicMock(return_value=True)\n        middleware.process_request_2(rp, Request(\"http://site.local/allowed\"))\n        rp.allowed.assert_called_once_with(\"http://site.local/allowed\", \"Examplebot\")\n\n    @deferred_f_from_coro_f\n    async def test_robotstxt_local_file(self):\n        middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n        middleware.process_request_2 = mock.MagicMock()\n\n        await middleware.process_request(Request(\"data:text/plain,Hello World data\"))\n        assert not middleware.process_request_2.called\n\n        await middleware.process_request(\n            Request(\"file:///tests/sample_data/test_site/nothinghere.html\")\n        )\n        assert not middleware.process_request_2.called\n\n        await middleware.process_request(Request(\"http://site.local/allowed\"))\n        assert middleware.process_request_2.called\n\n    async def assertNotIgnored(\n        self, request: Request, middleware: RobotsTxtMiddleware\n    ) -> None:\n        try:\n            await middleware.process_request(request)\n        except IgnoreRequest:\n            pytest.fail(\"IgnoreRequest was raised unexpectedly\")\n\n    async def assertIgnored(\n        self, request: Request, middleware: RobotsTxtMiddleware\n    ) -> None:\n        with pytest.raises(IgnoreRequest):\n            await middleware.process_request(request)\n\n    def assertRobotsTxtRequested(self, base_url: str) -> None:\n        calls = self.crawler.engine.download_async.call_args_list\n        request = calls[0][0][0]\n        assert request.url == f\"{base_url}/robots.txt\"\n        assert request.callback == NO_CALLBACK\n\n\n@pytest.mark.skipif(not rerp_available(), reason=\"Rerp parser is not installed\")\nclass TestRobotsTxtMiddlewareWithRerp(TestRobotsTxtMiddleware):\n    def setup_method(self):\n        super().setup_method()\n        self.crawler.settings.set(\n            \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.RerpRobotParser\"\n        )\n", "n_tokens": 1201, "byte_len": 5696, "file_sha1": "a64ee32f0949d9023999643e25d192502bfd7af8", "start_line": 135, "end_line": 267}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_robotstxt_interface.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_robotstxt_interface.py", "rel_path": "tests/test_robotstxt_interface.py", "module": "tests.test_robotstxt_interface", "ext": "py", "chunk_number": 1, "symbols": ["rerp_available", "_setUp", "test_allowed", "test_allowed_wildcards", "test_length_based_precedence", "test_order_based_precedence", "test_empty_response", "test_garbage_response", "test_unicode_url_and_useragent", "test_native_string_conversion", "BaseRobotParserTest", "TestDecodeRobotsTxt", "site", "test", "allowed", "agent", "robots", "endinglater", "robotstxt", "https", "delay", "chrome", "pytest", "too", "decoded", "content", "none", "encode", "html", "set", "test_decode_utf8", "test_decode_non_utf8", "setup_method", "TestPythonRobotParser", "TestRerpRobotParser", "TestProtegoRobotParser", "does", "length", "directives", "order", "reason", "setup", "allow", "nallow", "empty", "based", "disallowed", "spider", "garbage", "taken"], "ast_kind": "class_or_type", "text": "import pytest\n\nfrom scrapy.robotstxt import (\n    ProtegoRobotParser,\n    PythonRobotParser,\n    RerpRobotParser,\n    decode_robotstxt,\n)\n\n\ndef rerp_available():\n    # check if robotexclusionrulesparser is installed\n    try:\n        from robotexclusionrulesparser import (  # noqa: PLC0415\n            RobotExclusionRulesParser,  # noqa: F401\n        )\n    except ImportError:\n        return False\n    return True\n\n\nclass BaseRobotParserTest:\n    def _setUp(self, parser_cls):\n        self.parser_cls = parser_cls\n\n    def test_allowed(self):\n        robotstxt_robotstxt_body = (\n            b\"User-agent: * \\nDisallow: /disallowed \\nAllow: /allowed \\nCrawl-delay: 10\"\n        )\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        assert rp.allowed(\"https://www.site.local/allowed\", \"*\")\n        assert not rp.allowed(\"https://www.site.local/disallowed\", \"*\")\n\n    def test_allowed_wildcards(self):\n        robotstxt_robotstxt_body = b\"\"\"User-agent: first\n                                Disallow: /disallowed/*/end$\n\n                                User-agent: second\n                                Allow: /*allowed\n                                Disallow: /\n                                \"\"\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n\n        assert rp.allowed(\"https://www.site.local/disallowed\", \"first\")\n        assert not rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\")\n        assert not rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\")\n        assert rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\")\n\n        assert rp.allowed(\"https://www.site.local/allowed\", \"second\")\n        assert rp.allowed(\"https://www.site.local/is_still_allowed\", \"second\")\n        assert rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\")\n\n    def test_length_based_precedence(self):\n        robotstxt_robotstxt_body = b\"User-agent: * \\nDisallow: / \\nAllow: /page\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        assert rp.allowed(\"https://www.site.local/page\", \"*\")\n\n    def test_order_based_precedence(self):\n        robotstxt_robotstxt_body = b\"User-agent: * \\nDisallow: / \\nAllow: /page\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        assert not rp.allowed(\"https://www.site.local/page\", \"*\")\n\n    def test_empty_response(self):\n        \"\"\"empty response should equal 'allow all'\"\"\"\n        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b\"\")\n        assert rp.allowed(\"https://site.local/\", \"*\")\n        assert rp.allowed(\"https://site.local/\", \"chrome\")\n        assert rp.allowed(\"https://site.local/index.html\", \"*\")\n        assert rp.allowed(\"https://site.local/disallowed\", \"*\")\n\n    def test_garbage_response(self):\n        \"\"\"garbage response should be discarded, equal 'allow all'\"\"\"\n        robotstxt_robotstxt_body = b\"GIF89a\\xd3\\x00\\xfe\\x00\\xa2\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        assert rp.allowed(\"https://site.local/\", \"*\")\n        assert rp.allowed(\"https://site.local/\", \"chrome\")\n        assert rp.allowed(\"https://site.local/index.html\", \"*\")\n        assert rp.allowed(\"https://site.local/disallowed\", \"*\")\n\n    def test_unicode_url_and_useragent(self):\n        robotstxt_robotstxt_body = \"\"\"\n        User-Agent: *\n        Disallow: /admin/\n        Disallow: /static/\n        # taken from https://en.wikipedia.org/robots.txt\n        Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n        Disallow: /wiki/Käyttäjä:\n\n        User-Agent: UnicödeBöt\n        Disallow: /some/randome/page.html\"\"\".encode()\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        assert rp.allowed(\"https://site.local/\", \"*\")\n        assert not rp.allowed(\"https://site.local/admin/\", \"*\")\n        assert not rp.allowed(\"https://site.local/static/\", \"*\")\n        assert rp.allowed(\"https://site.local/admin/\", \"UnicödeBöt\")\n        assert not rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\")\n        assert not rp.allowed(\"https://site.local/wiki/Käyttäjä:\", \"*\")\n        assert rp.allowed(\"https://site.local/some/randome/page.html\", \"*\")\n        assert not rp.allowed(\"https://site.local/some/randome/page.html\", \"UnicödeBöt\")\n\n\nclass TestDecodeRobotsTxt:\n    def test_native_string_conversion(self):\n        robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n        decoded_content = decode_robotstxt(\n            robotstxt_body, spider=None, to_native_str_type=True\n        )\n        assert decoded_content == \"User-agent: *\\nDisallow: /\\n\"\n", "n_tokens": 1190, "byte_len": 4914, "file_sha1": "2e391ff331eea563174f3bbd9b0a7236f993bb4a", "start_line": 1, "end_line": 121}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_robotstxt_interface.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_robotstxt_interface.py", "rel_path": "tests/test_robotstxt_interface.py", "module": "tests.test_robotstxt_interface", "ext": "py", "chunk_number": 2, "symbols": ["test_decode_utf8", "test_decode_non_utf8", "setup_method", "test_length_based_precedence", "test_allowed_wildcards", "test_order_based_precedence", "TestPythonRobotParser", "TestRerpRobotParser", "TestProtegoRobotParser", "test", "python", "does", "rerp", "robot", "spider", "protego", "installed", "wildcards", "mark", "class", "support", "skipif", "length", "directives", "decode", "available", "robotstxt", "body", "pytest", "order", "rerp_available", "_setUp", "test_allowed", "test_empty_response", "test_garbage_response", "test_unicode_url_and_useragent", "test_native_string_conversion", "BaseRobotParserTest", "TestDecodeRobotsTxt", "site", "allowed", "agent", "robots", "endinglater", "https", "delay", "chrome", "too", "decoded", "content"], "ast_kind": "class_or_type", "text": "    def test_decode_utf8(self):\n        robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n        decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n        assert decoded_content == \"User-agent: *\\nDisallow: /\\n\"\n\n    def test_decode_non_utf8(self):\n        robotstxt_body = b\"User-agent: *\\n\\xffDisallow: /\\n\"\n        decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n        assert decoded_content == \"User-agent: *\\nDisallow: /\\n\"\n\n\nclass TestPythonRobotParser(BaseRobotParserTest):\n    def setup_method(self):\n        super()._setUp(PythonRobotParser)\n\n    def test_length_based_precedence(self):\n        pytest.skip(\n            \"RobotFileParser does not support length based directives precedence.\"\n        )\n\n    def test_allowed_wildcards(self):\n        pytest.skip(\"RobotFileParser does not support wildcards.\")\n\n\n@pytest.mark.skipif(not rerp_available(), reason=\"Rerp parser is not installed\")\nclass TestRerpRobotParser(BaseRobotParserTest):\n    def setup_method(self):\n        super()._setUp(RerpRobotParser)\n\n    def test_length_based_precedence(self):\n        pytest.skip(\"Rerp does not support length based directives precedence.\")\n\n\nclass TestProtegoRobotParser(BaseRobotParserTest):\n    def setup_method(self):\n        super()._setUp(ProtegoRobotParser)\n\n    def test_order_based_precedence(self):\n        pytest.skip(\"Protego does not support order based directives precedence.\")\n", "n_tokens": 318, "byte_len": 1424, "file_sha1": "2e391ff331eea563174f3bbd9b0a7236f993bb4a", "start_line": 122, "end_line": 161}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_ajaxcrawlable.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_ajaxcrawlable.py", "rel_path": "tests/test_downloadermiddleware_ajaxcrawlable.py", "module": "tests.test_downloadermiddleware_ajaxcrawlable", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "_ajaxcrawlable_body", "_req_resp", "test_non_get", "test_binary_response", "test_ajaxcrawl", "test_ajaxcrawl_loop", "test_noncrawlable_body", "TestAjaxCrawlMiddleware", "fragment", "method", "test", "ajaxcrawl", "resp", "kwargs", "non", "scrapy", "deprecation", "spider", "req", "req2", "foobar", "resp3", "return", "ajaxcraw", "enabled", "name", "mark", "class", "meta", "downloadermiddlewares", "ignore", "spiders", "example", "create", "head", "body", "get", "crawler", "binary", "ajax", "true", "pytest", "from", "ajaxcrawlable", "process", "response", "escaped", "filterwarnings", "assert"], "ast_kind": "class_or_type", "text": "import pytest\n\nfrom scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestAjaxCrawlMiddleware:\n    def setup_method(self):\n        crawler = get_crawler(Spider, {\"AJAXCRAWL_ENABLED\": True})\n        self.spider = crawler._create_spider(\"foo\")\n        self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n\n    def _ajaxcrawlable_body(self):\n        return b'<html><head><meta name=\"fragment\" content=\"!\"/></head><body></body></html>'\n\n    def _req_resp(self, url, req_kwargs=None, resp_kwargs=None):\n        req = Request(url, **(req_kwargs or {}))\n        resp = HtmlResponse(url, request=req, **(resp_kwargs or {}))\n        return req, resp\n\n    def test_non_get(self):\n        req, resp = self._req_resp(\"http://example.com/\", {\"method\": \"HEAD\"})\n        resp2 = self.mw.process_response(req, resp, self.spider)\n        assert resp == resp2\n\n    def test_binary_response(self):\n        req = Request(\"http://example.com/\")\n        resp = Response(\"http://example.com/\", body=b\"foobar\\x00\\x01\\x02\", request=req)\n        resp2 = self.mw.process_response(req, resp, self.spider)\n        assert resp is resp2\n\n    def test_ajaxcrawl(self):\n        req, resp = self._req_resp(\n            \"http://example.com/\",\n            {\"meta\": {\"foo\": \"bar\"}},\n            {\"body\": self._ajaxcrawlable_body()},\n        )\n        req2 = self.mw.process_response(req, resp, self.spider)\n        assert req2.url == \"http://example.com/?_escaped_fragment_=\"\n        assert req2.meta[\"foo\"] == \"bar\"\n\n    def test_ajaxcrawl_loop(self):\n        req, resp = self._req_resp(\n            \"http://example.com/\", {}, {\"body\": self._ajaxcrawlable_body()}\n        )\n        req2 = self.mw.process_response(req, resp, self.spider)\n        resp2 = HtmlResponse(req2.url, body=resp.body, request=req2)\n        resp3 = self.mw.process_response(req2, resp2, self.spider)\n\n        assert isinstance(resp3, HtmlResponse), (resp3.__class__, resp3)\n        assert resp3.request.url == \"http://example.com/?_escaped_fragment_=\"\n        assert resp3 is resp2\n\n    def test_noncrawlable_body(self):\n        req, resp = self._req_resp(\n            \"http://example.com/\", {}, {\"body\": b\"<html></html>\"}\n        )\n        resp2 = self.mw.process_response(req, resp, self.spider)\n        assert resp is resp2\n", "n_tokens": 619, "byte_len": 2502, "file_sha1": "eacd6735695aa3635fec8ebe6467b55c55952bd8", "start_line": 1, "end_line": 63}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_dupefilters.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_dupefilters.py", "rel_path": "tests/test_dupefilters.py", "module": "tests.test_dupefilters", "ext": "py", "chunk_number": 1, "symbols": ["_get_dupefilter", "from_crawler", "test_df_from_crawler_scheduler", "test_df_direct_scheduler", "test_filter", "test_dupefilter_path", "test_request_fingerprint", "fingerprint", "test_seenreq_newlines", "FromCrawlerRFPDupeFilter", "DirectDupeFilter", "TestRFPDupeFilter", "RequestFingerprinter", "method", "seen", "from", "crawler", "scheduler", "spiders", "path", "dupefilter", "get", "reques", "fingerprinte", "test", "seenreq", "request", "debug", "settings", "digest", "test_log", "test_log_debug", "test_log_debug_default_dupefilter", "test_log_deprecation", "TestBaseDupeFilter", "referer", "filtered", "deprecated", "more", "case", "insensitive", "simple", "spider", "lower", "jobdir", "none", "html", "line", "http", "bytes"], "ast_kind": "class_or_type", "text": "import hashlib\nimport shutil\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom warnings import catch_warnings\n\nfrom testfixtures import LogCapture\n\nfrom scrapy.core.scheduler import Scheduler\nfrom scrapy.dupefilters import BaseDupeFilter, RFPDupeFilter\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import SimpleSpider\n\n\ndef _get_dupefilter(*, crawler=None, settings=None, open_=True):\n    if crawler is None:\n        crawler = get_crawler(settings_dict=settings)\n    scheduler = Scheduler.from_crawler(crawler)\n    dupefilter = scheduler.df\n    if open_:\n        dupefilter.open()\n    return dupefilter\n\n\nclass FromCrawlerRFPDupeFilter(RFPDupeFilter):\n    @classmethod\n    def from_crawler(cls, crawler):\n        df = super().from_crawler(crawler)\n        df.method = \"from_crawler\"\n        return df\n\n\nclass DirectDupeFilter:\n    method = \"n/a\"\n\n\nclass TestRFPDupeFilter:\n    def test_df_from_crawler_scheduler(self):\n        settings = {\n            \"DUPEFILTER_DEBUG\": True,\n            \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        assert scheduler.df.debug\n        assert scheduler.df.method == \"from_crawler\"\n\n    def test_df_direct_scheduler(self):\n        settings = {\n            \"DUPEFILTER_CLASS\": DirectDupeFilter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        assert scheduler.df.method == \"n/a\"\n\n    def test_filter(self):\n        dupefilter = _get_dupefilter()\n        r1 = Request(\"http://scrapytest.org/1\")\n        r2 = Request(\"http://scrapytest.org/2\")\n        r3 = Request(\"http://scrapytest.org/2\")\n\n        assert not dupefilter.request_seen(r1)\n        assert dupefilter.request_seen(r1)\n\n        assert not dupefilter.request_seen(r2)\n        assert dupefilter.request_seen(r3)\n\n        dupefilter.close(\"finished\")\n\n    def test_dupefilter_path(self):\n        r1 = Request(\"http://scrapytest.org/1\")\n        r2 = Request(\"http://scrapytest.org/2\")\n\n        path = tempfile.mkdtemp()\n        try:\n            df = _get_dupefilter(settings={\"JOBDIR\": path}, open_=False)\n            try:\n                df.open()\n                assert not df.request_seen(r1)\n                assert df.request_seen(r1)\n            finally:\n                df.close(\"finished\")\n\n            df2 = _get_dupefilter(settings={\"JOBDIR\": path}, open_=False)\n            assert df != df2\n            try:\n                df2.open()\n                assert df2.request_seen(r1)\n                assert not df2.request_seen(r2)\n                assert df2.request_seen(r2)\n            finally:\n                df2.close(\"finished\")\n        finally:\n            shutil.rmtree(path)\n\n    def test_request_fingerprint(self):\n        \"\"\"Test if customization of request_fingerprint method will change\n        output of request_seen.\n\n        \"\"\"\n        dupefilter = _get_dupefilter()\n        r1 = Request(\"http://scrapytest.org/index.html\")\n        r2 = Request(\"http://scrapytest.org/INDEX.html\")\n\n        assert not dupefilter.request_seen(r1)\n        assert not dupefilter.request_seen(r2)\n\n        dupefilter.close(\"finished\")\n\n        class RequestFingerprinter:\n            def fingerprint(self, request):\n                fp = hashlib.sha1()\n                fp.update(to_bytes(request.url.lower()))\n                return fp.digest()\n\n        settings = {\"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter}\n        case_insensitive_dupefilter = _get_dupefilter(settings=settings)\n\n        assert not case_insensitive_dupefilter.request_seen(r1)\n        assert case_insensitive_dupefilter.request_seen(r2)\n\n        case_insensitive_dupefilter.close(\"finished\")\n\n    def test_seenreq_newlines(self):\n        r\"\"\"Checks against adding duplicate \\r to\n        line endings on Windows platforms.\"\"\"\n\n        r1 = Request(\"http://scrapytest.org/1\")\n\n        path = tempfile.mkdtemp()\n        crawler = get_crawler(settings_dict={\"JOBDIR\": path})\n        try:\n            scheduler = Scheduler.from_crawler(crawler)\n            df = scheduler.df\n            df.open()\n            df.request_seen(r1)\n            df.close(\"finished\")\n\n            with Path(path, \"requests.seen\").open(\"rb\") as seen_file:\n                line = next(seen_file).decode()\n                assert not line.endswith(\"\\r\\r\\n\")\n                if sys.platform == \"win32\":\n                    assert line.endswith(\"\\r\\n\")\n                else:\n                    assert line.endswith(\"\\n\")\n\n        finally:\n            shutil.rmtree(path)\n", "n_tokens": 1049, "byte_len": 4761, "file_sha1": "52f1773838748e2a60ed752d825b90b768d04f73", "start_line": 1, "end_line": 153}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_dupefilters.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_dupefilters.py", "rel_path": "tests/test_dupefilters.py", "module": "tests.test_dupefilters", "ext": "py", "chunk_number": 2, "symbols": ["test_log", "test_log_debug", "test_log_debug_default_dupefilter", "test_log_deprecation", "TestBaseDupeFilter", "settings", "dict", "will", "referer", "test", "log", "false", "duplicates", "dupefilte", "debug", "filtered", "spider", "calling", "from", "crawler", "scrapy", "deprecation", "dupefilters", "stats", "class", "show", "with", "duplicate", "deprecated", "more", "_get_dupefilter", "from_crawler", "test_df_from_crawler_scheduler", "test_df_direct_scheduler", "test_filter", "test_dupefilter_path", "test_request_fingerprint", "fingerprint", "test_seenreq_newlines", "FromCrawlerRFPDupeFilter", "DirectDupeFilter", "TestRFPDupeFilter", "RequestFingerprinter", "method", "seen", "scheduler", "spiders", "path", "dupefilter", "get"], "ast_kind": "class_or_type", "text": "    def test_log(self):\n        with LogCapture() as log:\n            settings = {\n                \"DUPEFILTER_DEBUG\": False,\n                \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n            }\n            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n            spider = SimpleSpider.from_crawler(crawler)\n            dupefilter = _get_dupefilter(crawler=crawler)\n\n            r1 = Request(\"http://scrapytest.org/index.html\")\n            r2 = Request(\"http://scrapytest.org/index.html\")\n\n            dupefilter.log(r1, spider)\n            dupefilter.log(r2, spider)\n\n            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more\"\n                    \" duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\",\n                )\n            )\n\n            dupefilter.close(\"finished\")\n\n    def test_log_debug(self):\n        with LogCapture() as log:\n            settings = {\n                \"DUPEFILTER_DEBUG\": True,\n                \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n            }\n            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n            spider = SimpleSpider.from_crawler(crawler)\n            dupefilter = _get_dupefilter(crawler=crawler)\n\n            r1 = Request(\"http://scrapytest.org/index.html\")\n            r2 = Request(\n                \"http://scrapytest.org/index.html\",\n                headers={\"Referer\": \"http://scrapytest.org/INDEX.html\"},\n            )\n\n            dupefilter.log(r1, spider)\n            dupefilter.log(r2, spider)\n\n            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)\",\n                )\n            )\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html>\"\n                    \" (referer: http://scrapytest.org/INDEX.html)\",\n                )\n            )\n\n            dupefilter.close(\"finished\")\n\n    def test_log_debug_default_dupefilter(self):\n        with LogCapture() as log:\n            settings = {\n                \"DUPEFILTER_DEBUG\": True,\n            }\n            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n            spider = SimpleSpider.from_crawler(crawler)\n            dupefilter = _get_dupefilter(crawler=crawler)\n\n            r1 = Request(\"http://scrapytest.org/index.html\")\n            r2 = Request(\n                \"http://scrapytest.org/index.html\",\n                headers={\"Referer\": \"http://scrapytest.org/INDEX.html\"},\n            )\n\n            dupefilter.log(r1, spider)\n            dupefilter.log(r2, spider)\n\n            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)\",\n                )\n            )\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html>\"\n                    \" (referer: http://scrapytest.org/INDEX.html)\",\n                )\n            )\n\n            dupefilter.close(\"finished\")\n\n\nclass TestBaseDupeFilter:\n    def test_log_deprecation(self):\n        dupefilter = _get_dupefilter(\n            settings={\"DUPEFILTER_CLASS\": BaseDupeFilter},\n        )\n        with catch_warnings(record=True) as warning_list:\n            dupefilter.log(None, None)\n        assert len(warning_list) == 1\n        assert (\n            str(warning_list[0].message)\n            == \"Calling BaseDupeFilter.log() is deprecated.\"\n        )\n        assert warning_list[0].category == ScrapyDeprecationWarning\n", "n_tokens": 889, "byte_len": 4272, "file_sha1": "52f1773838748e2a60ed752d825b90b768d04f73", "start_line": 154, "end_line": 271}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_stop_download_bytes.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_stop_download_bytes.py", "rel_path": "tests/test_engine_stop_download_bytes.py", "module": "tests.test_engine_stop_download_bytes", "ext": "py", "chunk_number": 1, "symbols": ["bytes_received", "_assert_bytes_received", "BytesReceivedCrawlerRun", "TestBytesReceivedEngine", "async", "signal", "deferred", "from", "joined", "data", "future", "typ", "checking", "mockserver", "numbers", "amount", "handler", "items", "than", "none", "handlers", "encode", "join", "http", "fail", "assert", "downloaded", "utf", "utf8", "fired", "await", "test", "bytes", "http11", "spider", "typing", "annotations", "class", "getpath", "staticmethod", "less", "strictly", "check", "present", "exceptions", "super", "buffer", "depends", "self", "tests"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom testfixtures import LogCapture\n\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom tests.test_engine import (\n    AttrsItemsSpider,\n    CrawlerRun,\n    DataClassItemsSpider,\n    DictItemsSpider,\n    MySpider,\n    TestEngineBase,\n)\n\nif TYPE_CHECKING:\n    from tests.mockserver.http import MockServer\n\n\nclass BytesReceivedCrawlerRun(CrawlerRun):\n    def bytes_received(self, data, request, spider):\n        super().bytes_received(data, request, spider)\n        raise StopDownload(fail=False)\n\n\nclass TestBytesReceivedEngine(TestEngineBase):\n    @deferred_f_from_coro_f\n    async def test_crawler(self, mockserver: MockServer) -> None:\n        for spider in (\n            MySpider,\n            DictItemsSpider,\n            AttrsItemsSpider,\n            DataClassItemsSpider,\n        ):\n            run = BytesReceivedCrawlerRun(spider)\n            with LogCapture() as log:\n                await run.run(mockserver)\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET {mockserver.url('/redirected')}> \"\n                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET {mockserver.url('/static/')}> \"\n                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET {mockserver.url('/numbers')}> \"\n                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n                    )\n                )\n            self._assert_visited_urls(run)\n            self._assert_scheduled_requests(run, count=9)\n            self._assert_downloaded_responses(run, count=9)\n            self._assert_signals_caught(run)\n            self._assert_headers_received(run)\n            self._assert_bytes_received(run)\n\n    @staticmethod\n    def _assert_bytes_received(run: CrawlerRun) -> None:\n        assert len(run.bytes) == 9\n        for request, data in run.bytes.items():\n            joined_data = b\"\".join(data)\n            assert len(data) == 1  # signal was fired only once\n            if run.getpath(request.url) == \"/numbers\":\n                # Received bytes are not the complete response. The exact amount depends\n                # on the buffer size, which can vary, so we only check that the amount\n                # of received bytes is strictly less than the full response.\n                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n                assert len(joined_data) < len(b\"\".join(numbers))\n", "n_tokens": 610, "byte_len": 3147, "file_sha1": "5e07bd63cf290e82dfcd7e5a4e2190311bba8b14", "start_line": 1, "end_line": 83}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_trackref.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_trackref.py", "rel_path": "tests/test_utils_trackref.py", "module": "tests.test_utils_trackref", "ext": "py", "chunk_number": 1, "symbols": ["clear_refs", "test_format_live_refs", "test_print_live_refs_empty", "test_print_live_refs_with_objects", "test_get_oldest", "test_iter_all", "Foo", "Bar", "test", "print", "time", "pass", "patch", "references", "object", "ref", "autouse", "stdout", "getvalue", "iter", "all", "live", "format", "class", "get", "sleep", "new", "callable", "oldest", "ignore", "scrapy", "mock", "true", "pytest", "string", "stringio", "from", "noqa", "clear", "assert", "unittest", "precise", "none", "fixture", "refs", "utils", "import", "skip", "trackref", "f841"], "ast_kind": "class_or_type", "text": "from io import StringIO\nfrom time import sleep, time\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.utils import trackref\n\n\nclass Foo(trackref.object_ref):\n    pass\n\n\nclass Bar(trackref.object_ref):\n    pass\n\n\n@pytest.fixture(autouse=True)\ndef clear_refs() -> None:\n    trackref.live_refs.clear()\n\n\ndef test_format_live_refs():\n    o1 = Foo()  # noqa: F841\n    o2 = Bar()  # noqa: F841\n    o3 = Foo()  # noqa: F841\n    assert (\n        trackref.format_live_refs()\n        == \"\"\"\\\nLive References\n\nBar                                 1   oldest: 0s ago\nFoo                                 2   oldest: 0s ago\n\"\"\"\n    )\n\n    assert (\n        trackref.format_live_refs(ignore=Foo)\n        == \"\"\"\\\nLive References\n\nBar                                 1   oldest: 0s ago\n\"\"\"\n    )\n\n\n@mock.patch(\"sys.stdout\", new_callable=StringIO)\ndef test_print_live_refs_empty(stdout):\n    trackref.print_live_refs()\n    assert stdout.getvalue() == \"Live References\\n\\n\\n\"\n\n\n@mock.patch(\"sys.stdout\", new_callable=StringIO)\ndef test_print_live_refs_with_objects(stdout):\n    o1 = Foo()  # noqa: F841\n    trackref.print_live_refs()\n    assert (\n        stdout.getvalue()\n        == \"\"\"\\\nLive References\n\nFoo                                 1   oldest: 0s ago\\n\\n\"\"\"\n    )\n\n\ndef test_get_oldest():\n    o1 = Foo()\n\n    o1_time = time()\n\n    o2 = Bar()\n\n    o3_time = time()\n    if o3_time <= o1_time:\n        sleep(0.01)\n        o3_time = time()\n    if o3_time <= o1_time:\n        pytest.skip(\"time.time is not precise enough\")\n\n    o3 = Foo()  # noqa: F841\n    assert trackref.get_oldest(\"Foo\") is o1\n    assert trackref.get_oldest(\"Bar\") is o2\n    assert trackref.get_oldest(\"XXX\") is None\n\n\ndef test_iter_all():\n    o1 = Foo()\n    o2 = Bar()  # noqa: F841\n    o3 = Foo()\n    assert set(trackref.iter_all(\"Foo\")) == {o1, o3}\n", "n_tokens": 488, "byte_len": 1811, "file_sha1": "22d8c1504a45ffd84c8da03a01d8e83065ba3b62", "start_line": 1, "end_line": 91}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipelines.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipelines.py", "rel_path": "tests/test_pipelines.py", "module": "tests.test_pipelines", "ext": "py", "chunk_number": 1, "symbols": ["process_item", "open_spider", "close_spider", "cb", "parse", "setup_class", "teardown_class", "_on_item_scraped", "_create_crawler", "test_simple_pipeline", "test_deferred_pipeline", "test_asyncdef_pipeline", "test_asyncdef_asyncio_pipeline", "test_asyncdef_not_asyncio_pipeline", "test_deprecated_process_item_spider_arg", "SimplePipeline", "DeprecatedSpiderArgPipeline", "DeferredPipeline", "AsyncDefPipeline", "AsyncDefAsyncioPipeline", "AsyncDefNotAsyncioPipeline", "ItemSpider", "TestPipeline", "TestCustomPipelineManager", "CustomPipelineManager", "deferred", "future", "async", "call", "later", "__init__", "from_crawler", "crawler", "test_deprecated_spider_arg_no_crawler_spider", "test_deprecated_spider_arg_with_crawler", "test_deprecated_spider_arg_without_crawler", "test_no_spider_arg_without_crawler", "TestMiddlewareManagerSpider", "method", "different", "test", "deprecated", "append", "itemproc", "instance", "were", "middleware", "spider", "name", "from"], "ast_kind": "class_or_type", "text": "import asyncio\n\nimport pytest\nfrom twisted.internet.defer import Deferred, inlineCallbacks, succeed\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.pipelines import ItemPipelineManager\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_to_future,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\nfrom tests.mockserver.http import MockServer\n\n\nclass SimplePipeline:\n    def process_item(self, item):\n        item[\"pipeline_passed\"] = True\n        return item\n\n\nclass DeprecatedSpiderArgPipeline:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n\nclass DeferredPipeline:\n    def cb(self, item):\n        item[\"pipeline_passed\"] = True\n        return item\n\n    def process_item(self, item):\n        d = Deferred()\n        d.addCallback(self.cb)\n        d.callback(item)\n        return d\n\n\nclass AsyncDefPipeline:\n    async def process_item(self, item):\n        d = Deferred()\n        call_later(0, d.callback, None)\n        await maybe_deferred_to_future(d)\n        item[\"pipeline_passed\"] = True\n        return item\n\n\nclass AsyncDefAsyncioPipeline:\n    async def process_item(self, item):\n        d = Deferred()\n        loop = asyncio.get_event_loop()\n        loop.call_later(0, d.callback, None)\n        await deferred_to_future(d)\n        await asyncio.sleep(0.2)\n        item[\"pipeline_passed\"] = await get_from_asyncio_queue(True)\n        return item\n\n\nclass AsyncDefNotAsyncioPipeline:\n    async def process_item(self, item):\n        d1 = Deferred()\n        from twisted.internet import reactor\n\n        reactor.callLater(0, d1.callback, None)\n        await d1\n        d2 = Deferred()\n        reactor.callLater(0, d2.callback, None)\n        await maybe_deferred_to_future(d2)\n        item[\"pipeline_passed\"] = True\n        return item\n\n\nclass ItemSpider(Spider):\n    name = \"itemspider\"\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        return {\"field\": 42}\n\n\nclass TestPipeline:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def _on_item_scraped(self, item):\n        assert isinstance(item, dict)\n        assert item.get(\"pipeline_passed\")\n        self.items.append(item)\n\n    def _create_crawler(self, pipeline_class):\n        settings = {\n            \"ITEM_PIPELINES\": {pipeline_class: 1},\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n        self.items = []\n        return crawler\n\n    @inlineCallbacks\n    def test_simple_pipeline(self):\n        crawler = self._create_crawler(SimplePipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(self.items) == 1\n\n    @inlineCallbacks\n    def test_deferred_pipeline(self):\n        crawler = self._create_crawler(DeferredPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(self.items) == 1\n\n    @inlineCallbacks\n    def test_asyncdef_pipeline(self):\n        crawler = self._create_crawler(AsyncDefPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(self.items) == 1\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_asyncdef_asyncio_pipeline(self):\n        crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(self.items) == 1\n\n    @pytest.mark.only_not_asyncio\n    @inlineCallbacks\n    def test_asyncdef_not_asyncio_pipeline(self):\n        crawler = self._create_crawler(AsyncDefNotAsyncioPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(self.items) == 1\n\n\nclass TestCustomPipelineManager:\n    def test_deprecated_process_item_spider_arg(self) -> None:\n        class CustomPipelineManager(ItemPipelineManager):\n            def process_item(self, item, spider):  # pylint: disable=signature-differs\n                return super().process_item(item, spider)\n\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        itemproc = CustomPipelineManager.from_crawler(crawler)\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"CustomPipelineManager.process_item\\(\\) is deprecated, use process_item_async\\(\\)\",\n        ):\n            itemproc.process_item({}, crawler.spider)\n\n    @deferred_f_from_coro_f\n    async def test_integration_recommended(self, mockserver: MockServer) -> None:\n        class CustomPipelineManager(ItemPipelineManager):\n            async def process_item_async(self, item):\n                return await super().process_item_async(item)\n\n        items = []\n", "n_tokens": 1132, "byte_len": 5133, "file_sha1": "cbb6b268e56fae10d0f90c511398a987a2aa59cd", "start_line": 1, "end_line": 171}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipelines.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipelines.py", "rel_path": "tests/test_pipelines.py", "module": "tests.test_pipelines", "ext": "py", "chunk_number": 2, "symbols": ["_on_item_scraped", "open_spider", "close_spider", "process_item", "__init__", "from_crawler", "crawler", "CustomPipelineManager", "TestMiddlewareManagerSpider", "method", "async", "append", "test", "middleware", "spider", "deferred", "from", "doesn", "deprecated", "mockserver", "succeed", "get", "pytest", "settings", "passing", "isinstance", "items", "none", "fixture", "methods", "cb", "parse", "setup_class", "teardown_class", "_create_crawler", "test_simple_pipeline", "test_deferred_pipeline", "test_asyncdef_pipeline", "test_asyncdef_asyncio_pipeline", "test_asyncdef_not_asyncio_pipeline", "test_deprecated_process_item_spider_arg", "test_deprecated_spider_arg_no_crawler_spider", "test_deprecated_spider_arg_with_crawler", "test_deprecated_spider_arg_without_crawler", "test_no_spider_arg_without_crawler", "SimplePipeline", "DeprecatedSpiderArgPipeline", "DeferredPipeline", "AsyncDefPipeline", "AsyncDefAsyncioPipeline"], "ast_kind": "class_or_type", "text": "        def _on_item_scraped(item):\n            assert isinstance(item, dict)\n            assert item.get(\"pipeline_passed\")\n            items.append(item)\n\n        crawler = get_crawler(\n            ItemSpider,\n            {\n                \"ITEM_PROCESSOR\": CustomPipelineManager,\n                \"ITEM_PIPELINES\": {SimplePipeline: 1},\n            },\n        )\n        crawler.spider = crawler._create_spider()\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        await maybe_deferred_to_future(crawler.crawl(mockserver=mockserver))\n\n        assert len(items) == 1\n\n    @deferred_f_from_coro_f\n    async def test_integration_no_async_subclass(self, mockserver: MockServer) -> None:\n        class CustomPipelineManager(ItemPipelineManager):\n            def open_spider(self, spider):  # pylint: disable=signature-differs\n                return super().open_spider(spider)\n\n            def close_spider(self, spider):  # pylint: disable=signature-differs\n                return super().close_spider(spider)\n\n            def process_item(self, item, spider):  # pylint: disable=signature-differs\n                with pytest.warns(\n                    ScrapyDeprecationWarning,\n                    match=r\"CustomPipelineManager.process_item\\(\\) is deprecated, use process_item_async\\(\\)\",\n                ):\n                    return super().process_item(item, spider)\n\n        items = []\n\n        def _on_item_scraped(item):\n            assert isinstance(item, dict)\n            assert item.get(\"pipeline_passed\")\n            items.append(item)\n\n        crawler = get_crawler(\n            ItemSpider,\n            {\n                \"ITEM_PROCESSOR\": CustomPipelineManager,\n                \"ITEM_PIPELINES\": {SimplePipeline: 1},\n            },\n        )\n        crawler.spider = crawler._create_spider()\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"The open_spider\\(\\) method of .+\\.CustomPipelineManager requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"The close_spider\\(\\) method of .+\\.CustomPipelineManager requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"The process_item\\(\\) method of .+\\.CustomPipelineManager requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"Passing a spider argument to CustomPipelineManager.open_spider\\(\\) is deprecated\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"Passing a spider argument to CustomPipelineManager.close_spider\\(\\) is deprecated\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"CustomPipelineManager overrides process_item\\(\\) but doesn't override process_item_async\\(\\)\",\n            ),\n        ):\n            await maybe_deferred_to_future(crawler.crawl(mockserver=mockserver))\n\n        assert len(items) == 1\n\n    @deferred_f_from_coro_f\n    async def test_integration_no_async_not_subclass(\n        self, mockserver: MockServer\n    ) -> None:\n        class CustomPipelineManager:\n            def __init__(self, crawler):\n                self.pipelines = [\n                    p()\n                    for p in build_component_list(\n                        crawler.settings.getwithbase(\"ITEM_PIPELINES\")\n                    )\n                ]\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler)\n\n            def open_spider(self, spider):\n                return succeed(None)\n\n            def close_spider(self, spider):\n                return succeed(None)\n\n            def process_item(self, item, spider):\n                for pipeline in self.pipelines:\n                    item = pipeline.process_item(item)\n                return succeed(item)\n\n        items = []\n\n        def _on_item_scraped(item):\n            assert isinstance(item, dict)\n            assert item.get(\"pipeline_passed\")\n            items.append(item)\n\n        crawler = get_crawler(\n            ItemSpider,\n            {\n                \"ITEM_PROCESSOR\": CustomPipelineManager,\n                \"ITEM_PIPELINES\": {SimplePipeline: 1},\n            },\n        )\n        crawler.spider = crawler._create_spider()\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"CustomPipelineManager doesn't define a process_item_async\\(\\) method\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"The open_spider\\(\\) method of .+\\.CustomPipelineManager requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"The close_spider\\(\\) method of .+\\.CustomPipelineManager requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"The process_item\\(\\) method of .+\\.CustomPipelineManager requires a spider argument\",\n            ),\n        ):\n            await maybe_deferred_to_future(crawler.crawl(mockserver=mockserver))\n\n        assert len(items) == 1\n\n\nclass TestMiddlewareManagerSpider:\n    \"\"\"Tests for the deprecated spider arg handling in MiddlewareManager.\n\n    Here because MiddlewareManager doesn't have methods that could take a spider arg.\"\"\"\n\n    @pytest.fixture\n    def crawler(self) -> Crawler:\n        return get_crawler(Spider)\n", "n_tokens": 1155, "byte_len": 5787, "file_sha1": "cbb6b268e56fae10d0f90c511398a987a2aa59cd", "start_line": 172, "end_line": 327}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipelines.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipelines.py", "rel_path": "tests/test_pipelines.py", "module": "tests.test_pipelines", "ext": "py", "chunk_number": 3, "symbols": ["test_deprecated_spider_arg_no_crawler_spider", "test_deprecated_spider_arg_with_crawler", "test_deprecated_spider_arg_without_crawler", "test_no_spider_arg_without_crawler", "used", "default", "spider", "method", "mwman", "works", "test", "different", "deprecated", "requires", "open", "argument", "instance", "warns", "scrapy", "deprecation", "item", "pipeline", "were", "value", "error", "doesn", "ignored", "warning", "with", "close", "process_item", "open_spider", "close_spider", "cb", "parse", "setup_class", "teardown_class", "_on_item_scraped", "_create_crawler", "test_simple_pipeline", "test_deferred_pipeline", "test_asyncdef_pipeline", "test_asyncdef_asyncio_pipeline", "test_asyncdef_not_asyncio_pipeline", "test_deprecated_process_item_spider_arg", "__init__", "from_crawler", "crawler", "SimplePipeline", "DeprecatedSpiderArgPipeline"], "ast_kind": "function_or_method", "text": "    def test_deprecated_spider_arg_no_crawler_spider(self, crawler: Crawler) -> None:\n        \"\"\"Crawler is provided, but doesn't have a spider. The instance passed to the method is\n        ignored and raises a warning.\"\"\"\n        mwman = ItemPipelineManager(crawler=crawler)\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"DeprecatedSpiderArgPipeline.open_spider\\(\\) requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"DeprecatedSpiderArgPipeline.close_spider\\(\\) requires a spider argument\",\n            ),\n        ):\n            mwman._add_middleware(DeprecatedSpiderArgPipeline())\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"Passing a spider argument to ItemPipelineManager.open_spider\\(\\) is deprecated\",\n            ),\n            pytest.raises(\n                ValueError,\n                match=\"ItemPipelineManager needs to access self.crawler.spider but it is None\",\n            ),\n        ):\n            mwman.open_spider(DefaultSpider())\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"Passing a spider argument to ItemPipelineManager.close_spider\\(\\) is deprecated\",\n            ),\n            pytest.raises(\n                ValueError,\n                match=\"ItemPipelineManager needs to access self.crawler.spider but it is None\",\n            ),\n        ):\n            mwman.close_spider(DefaultSpider())\n\n    def test_deprecated_spider_arg_with_crawler(self, crawler: Crawler) -> None:\n        \"\"\"Crawler is provided and has a spider, works. The instance passed to the method is ignored,\n        even if mismatched, but raises a warning.\"\"\"\n        mwman = ItemPipelineManager(crawler=crawler)\n        crawler.spider = crawler._create_spider(\"foo\")\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"Passing a spider argument to ItemPipelineManager.open_spider\\(\\) is deprecated\",\n        ):\n            mwman.open_spider(DefaultSpider())\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"Passing a spider argument to ItemPipelineManager.close_spider\\(\\) is deprecated\",\n        ):\n            mwman.close_spider(DefaultSpider())\n\n    def test_deprecated_spider_arg_without_crawler(self) -> None:\n        \"\"\"The first instance passed to the method is used, with a warning. Mismatched ones raise an error.\"\"\"\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=\"was called without the crawler argument\",\n        ):\n            mwman = ItemPipelineManager()\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"DeprecatedSpiderArgPipeline.open_spider\\(\\) requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"DeprecatedSpiderArgPipeline.close_spider\\(\\) requires a spider argument\",\n            ),\n        ):\n            mwman._add_middleware(DeprecatedSpiderArgPipeline())\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"Passing a spider argument to ItemPipelineManager.open_spider\\(\\) is deprecated\",\n        ):\n            mwman.open_spider(DefaultSpider())\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"Passing a spider argument to ItemPipelineManager.close_spider\\(\\) is deprecated\",\n            ),\n            pytest.raises(\n                RuntimeError, match=\"Different instances of Spider were passed\"\n            ),\n        ):\n            mwman.close_spider(DefaultSpider())\n        mwman.close_spider()\n\n    def test_no_spider_arg_without_crawler(self) -> None:\n        \"\"\"If no crawler and no spider arg, raise an error.\"\"\"\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=\"was called without the crawler argument\",\n        ):\n            mwman = ItemPipelineManager()\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"DeprecatedSpiderArgPipeline.open_spider\\(\\) requires a spider argument\",\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"DeprecatedSpiderArgPipeline.close_spider\\(\\) requires a spider argument\",\n            ),\n        ):\n            mwman._add_middleware(DeprecatedSpiderArgPipeline())\n        with (\n            pytest.raises(\n                ValueError,\n                match=\"has no known Spider instance\",\n            ),\n        ):\n            mwman.open_spider()\n", "n_tokens": 946, "byte_len": 4761, "file_sha1": "cbb6b268e56fae10d0f90c511398a987a2aa59cd", "start_line": 328, "end_line": 442}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_core_downloader.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_core_downloader.py", "rel_path": "tests/test_core_downloader.py", "module": "tests.test_core_downloader", "ext": "py", "chunk_number": 1, "symbols": ["test_repr", "_listen", "test_override_getContext", "getContext", "test_setting_none", "test_setting_bad", "TestSlot", "TestContextFactoryBase", "TestContextFactory", "MyFactory", "TestContextFactoryTLSMethod", "contextfactory", "does", "method", "async", "site", "body", "producer", "read", "agent", "deprecation", "warning", "payload", "deferred", "from", "browser", "like", "test", "context", "override", "fetch", "CustomDownloader", "downloader", "deprecated", "future", "typ", "checking", "port", "https", "mockserver", "interface", "get", "crawler", "stop", "listening", "delay", "pytest", "settings", "ssl", "randomize"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, cast\n\nimport OpenSSL.SSL\nimport pytest\nfrom pytest_twisted import async_yield_fixture\nfrom twisted.web import server, static\nfrom twisted.web.client import Agent, BrowserLikePolicyForHTTPS, readBody\nfrom twisted.web.client import Response as TxResponse\n\nfrom scrapy.core.downloader import Downloader, Slot\nfrom scrapy.core.downloader.contextfactory import (\n    ScrapyClientContextFactory,\n    load_context_factory_from_settings,\n)\nfrom scrapy.core.downloader.handlers.http11 import _RequestBodyProducer\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.settings import Settings\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http_resources import PayloadResource\nfrom tests.mockserver.utils import ssl_context_factory\n\nif TYPE_CHECKING:\n    from twisted.internet.defer import Deferred\n    from twisted.web.iweb import IBodyProducer\n\n\nclass TestSlot:\n    def test_repr(self):\n        slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n        assert repr(slot) == \"Slot(concurrency=8, delay=0.10, randomize_delay=True)\"\n\n\nclass TestContextFactoryBase:\n    context_factory = None\n\n    @async_yield_fixture\n    async def server_url(self, tmp_path):\n        (tmp_path / \"file\").write_bytes(b\"0123456789\")\n        r = static.File(str(tmp_path))\n        r.putChild(b\"payload\", PayloadResource())\n        site = server.Site(r, timeout=None)\n        port = self._listen(site)\n        portno = port.getHost().port\n\n        yield f\"https://127.0.0.1:{portno}/\"\n\n        await port.stopListening()\n\n    def _listen(self, site):\n        from twisted.internet import reactor\n\n        return reactor.listenSSL(\n            0,\n            site,\n            contextFactory=self.context_factory or ssl_context_factory(),\n            interface=\"127.0.0.1\",\n        )\n\n    @staticmethod\n    async def get_page(\n        url: str,\n        client_context_factory: BrowserLikePolicyForHTTPS,\n        body: str | None = None,\n    ) -> bytes:\n        from twisted.internet import reactor\n\n        agent = Agent(reactor, contextFactory=client_context_factory)\n        body_producer = _RequestBodyProducer(body.encode()) if body else None\n        response: TxResponse = cast(\n            \"TxResponse\",\n            await maybe_deferred_to_future(\n                agent.request(\n                    b\"GET\",\n                    url.encode(),\n                    bodyProducer=cast(\"IBodyProducer\", body_producer),\n                )\n            ),\n        )\n        with warnings.catch_warnings():\n            # https://github.com/twisted/twisted/issues/8227\n            warnings.filterwarnings(\n                \"ignore\",\n                category=DeprecationWarning,\n                message=r\".*does not have an abortConnection method\",\n            )\n            d: Deferred[bytes] = readBody(response)  # type: ignore[arg-type]\n        return await maybe_deferred_to_future(d)\n\n\nclass TestContextFactory(TestContextFactoryBase):\n    @deferred_f_from_coro_f\n    async def testPayload(self, server_url: str) -> None:\n        s = \"0123456789\" * 10\n        crawler = get_crawler()\n        settings = Settings()\n        client_context_factory = load_context_factory_from_settings(settings, crawler)\n        body = await self.get_page(\n            server_url + \"payload\", client_context_factory, body=s\n        )\n        assert body == to_bytes(s)\n\n    def test_override_getContext(self):\n        class MyFactory(ScrapyClientContextFactory):\n            def getContext(\n                self, hostname: Any = None, port: Any = None\n            ) -> OpenSSL.SSL.Context:\n                ctx: OpenSSL.SSL.Context = super().getContext(hostname, port)\n                return ctx\n\n        with warnings.catch_warnings(record=True) as w:\n            MyFactory()\n            assert len(w) == 1\n            assert (\n                \"Overriding ScrapyClientContextFactory.getContext() is deprecated\"\n                in str(w[0].message)\n            )\n\n\nclass TestContextFactoryTLSMethod(TestContextFactoryBase):\n    async def _assert_factory_works(\n        self, server_url: str, client_context_factory: ScrapyClientContextFactory\n    ) -> None:\n        s = \"0123456789\" * 10\n        body = await self.get_page(\n            server_url + \"payload\", client_context_factory, body=s\n        )\n        assert body == to_bytes(s)\n\n    @deferred_f_from_coro_f\n    async def test_setting_default(self, server_url: str) -> None:\n        crawler = get_crawler()\n        settings = Settings()\n        client_context_factory = load_context_factory_from_settings(settings, crawler)\n        assert client_context_factory._ssl_method == OpenSSL.SSL.SSLv23_METHOD\n        await self._assert_factory_works(server_url, client_context_factory)\n\n    def test_setting_none(self):\n        crawler = get_crawler()\n        settings = Settings({\"DOWNLOADER_CLIENT_TLS_METHOD\": None})\n        with pytest.raises(KeyError):\n            load_context_factory_from_settings(settings, crawler)\n\n    def test_setting_bad(self):\n        crawler = get_crawler()\n        settings = Settings({\"DOWNLOADER_CLIENT_TLS_METHOD\": \"bad\"})\n        with pytest.raises(KeyError):\n            load_context_factory_from_settings(settings, crawler)\n", "n_tokens": 1163, "byte_len": 5505, "file_sha1": "559d4e670570c2ed1d6f36cce6342766bdf6129b", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_core_downloader.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_core_downloader.py", "rel_path": "tests/test_core_downloader.py", "module": "tests.test_core_downloader", "ext": "py", "chunk_number": 2, "symbols": ["fetch", "CustomDownloader", "scrapy", "client", "default", "spider", "downloader", "settings", "dict", "test", "direct", "async", "downloade", "clien", "method", "await", "ssl", "requires", "server", "url", "argument", "sslv23", "tlsv1", "warns", "deprecation", "return", "deferred", "from", "maybe", "class", "test_repr", "_listen", "test_override_getContext", "getContext", "test_setting_none", "test_setting_bad", "TestSlot", "TestContextFactoryBase", "TestContextFactory", "MyFactory", "TestContextFactoryTLSMethod", "contextfactory", "does", "site", "body", "producer", "read", "agent", "warning", "payload"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_setting_explicit(self, server_url: str) -> None:\n        crawler = get_crawler()\n        settings = Settings({\"DOWNLOADER_CLIENT_TLS_METHOD\": \"TLSv1.2\"})\n        client_context_factory = load_context_factory_from_settings(settings, crawler)\n        assert client_context_factory._ssl_method == OpenSSL.SSL.TLSv1_2_METHOD\n        await self._assert_factory_works(server_url, client_context_factory)\n\n    @deferred_f_from_coro_f\n    async def test_direct_from_crawler(self, server_url: str) -> None:\n        # the setting is ignored\n        crawler = get_crawler(settings_dict={\"DOWNLOADER_CLIENT_TLS_METHOD\": \"bad\"})\n        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n        assert client_context_factory._ssl_method == OpenSSL.SSL.SSLv23_METHOD\n        await self._assert_factory_works(server_url, client_context_factory)\n\n    @deferred_f_from_coro_f\n    async def test_direct_init(self, server_url: str) -> None:\n        client_context_factory = ScrapyClientContextFactory(OpenSSL.SSL.TLSv1_2_METHOD)\n        assert client_context_factory._ssl_method == OpenSSL.SSL.TLSv1_2_METHOD\n        await self._assert_factory_works(server_url, client_context_factory)\n\n\n@deferred_f_from_coro_f\nasync def test_fetch_deprecated_spider_arg():\n    class CustomDownloader(Downloader):\n        def fetch(self, request, spider):  # pylint: disable=signature-differs\n            return super().fetch(request, spider)\n\n    crawler = get_crawler(DefaultSpider, {\"DOWNLOADER\": CustomDownloader})\n    with pytest.warns(\n        ScrapyDeprecationWarning,\n        match=r\"The fetch\\(\\) method of .+\\.CustomDownloader requires a spider argument\",\n    ):\n        await maybe_deferred_to_future(crawler.crawl())\n", "n_tokens": 412, "byte_len": 1772, "file_sha1": "559d4e670570c2ed1d6f36cce6342766bdf6129b", "start_line": 156, "end_line": 191}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_start.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_start.py", "rel_path": "tests/test_spidermiddleware_start.py", "module": "tests.test_spidermiddleware_start", "ext": "py", "chunk_number": 1, "symbols": ["start", "TestMiddleware", "async", "false", "result", "spider", "deferred", "from", "class", "meta", "process", "request", "spiders", "scrapy", "defer", "test", "middleware", "get", "crawler", "build", "yield", "true", "assert", "sync", "spidermiddlewares", "data", "utils", "import", "misc", "http", "self"], "ast_kind": "class_or_type", "text": "from scrapy.http import Request\nfrom scrapy.spidermiddlewares.start import StartSpiderMiddleware\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestMiddleware:\n    @deferred_f_from_coro_f\n    async def test_async(self):\n        crawler = get_crawler(Spider)\n        mw = build_from_crawler(StartSpiderMiddleware, crawler)\n\n        async def start():\n            yield Request(\"data:,1\")\n            yield Request(\"data:,2\", meta={\"is_start_request\": True})\n            yield Request(\"data:,2\", meta={\"is_start_request\": False})\n            yield Request(\"data:,2\", meta={\"is_start_request\": \"foo\"})\n\n        result = [\n            request.meta[\"is_start_request\"]\n            async for request in mw.process_start(start())\n        ]\n        assert result == [True, True, False, \"foo\"]\n\n    @deferred_f_from_coro_f\n    async def test_sync(self):\n        crawler = get_crawler(Spider)\n        mw = build_from_crawler(StartSpiderMiddleware, crawler)\n\n        def start():\n            yield Request(\"data:,1\")\n            yield Request(\"data:,2\", meta={\"is_start_request\": True})\n            yield Request(\"data:,2\", meta={\"is_start_request\": False})\n            yield Request(\"data:,2\", meta={\"is_start_request\": \"foo\"})\n\n        result = [\n            request.meta[\"is_start_request\"]\n            for request in mw.process_start_requests(start(), Spider(\"test\"))\n        ]\n        assert result == [True, True, False, \"foo\"]\n", "n_tokens": 346, "byte_len": 1563, "file_sha1": "5a05c39fd634b71352a7e11f5029a553aee7a0b3", "start_line": 1, "end_line": 43}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_cookies.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_cookies.py", "rel_path": "tests/test_http_cookies.py", "module": "tests.test_http_cookies", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_get_full_url", "test_get_host", "test_get_type", "test_is_unverifiable", "test_is_unverifiable2", "test_get_origin_req_host", "test_has_header", "test_get_header", "test_header_items", "test_add_unredirected_header", "test_info", "test_get_all", "TestWrappedRequest", "TestWrappedResponse", "ype", "type", "test", "wrapped", "get", "header", "text", "add", "unredirected", "result", "origin", "req", "host", "all", "cookies", "netloc", "response", "info", "class", "full", "url", "meta", "has", "xxxxx", "world", "must", "scrapy", "headers", "string", "example", "unverifiable", "urlparse", "cached", "true", "hello"], "ast_kind": "class_or_type", "text": "from scrapy.http import Request, Response\nfrom scrapy.http.cookies import WrappedRequest, WrappedResponse\nfrom scrapy.utils.httpobj import urlparse_cached\n\n\nclass TestWrappedRequest:\n    def setup_method(self):\n        self.request = Request(\n            \"http://www.example.com/page.html\", headers={\"Content-Type\": \"text/html\"}\n        )\n        self.wrapped = WrappedRequest(self.request)\n\n    def test_get_full_url(self):\n        assert self.wrapped.get_full_url() == self.request.url\n        assert self.wrapped.full_url == self.request.url\n\n    def test_get_host(self):\n        assert self.wrapped.get_host() == urlparse_cached(self.request).netloc\n        assert self.wrapped.host == urlparse_cached(self.request).netloc\n\n    def test_get_type(self):\n        assert self.wrapped.get_type() == urlparse_cached(self.request).scheme\n        assert self.wrapped.type == urlparse_cached(self.request).scheme\n\n    def test_is_unverifiable(self):\n        assert not self.wrapped.is_unverifiable()\n        assert not self.wrapped.unverifiable\n\n    def test_is_unverifiable2(self):\n        self.request.meta[\"is_unverifiable\"] = True\n        assert self.wrapped.is_unverifiable()\n        assert self.wrapped.unverifiable\n\n    def test_get_origin_req_host(self):\n        assert self.wrapped.origin_req_host == \"www.example.com\"\n\n    def test_has_header(self):\n        assert self.wrapped.has_header(\"content-type\")\n        assert not self.wrapped.has_header(\"xxxxx\")\n\n    def test_get_header(self):\n        assert self.wrapped.get_header(\"content-type\") == \"text/html\"\n        assert self.wrapped.get_header(\"xxxxx\", \"def\") == \"def\"\n        assert self.wrapped.get_header(\"xxxxx\") is None\n        wrapped = WrappedRequest(\n            Request(\n                \"http://www.example.com/page.html\", headers={\"empty-binary-header\": b\"\"}\n            )\n        )\n        assert wrapped.get_header(\"empty-binary-header\") == \"\"\n\n    def test_header_items(self):\n        assert self.wrapped.header_items() == [(\"Content-Type\", [\"text/html\"])]\n\n    def test_add_unredirected_header(self):\n        self.wrapped.add_unredirected_header(\"hello\", \"world\")\n        assert self.request.headers[\"hello\"] == b\"world\"\n\n\nclass TestWrappedResponse:\n    def setup_method(self):\n        self.response = Response(\n            \"http://www.example.com/page.html\", headers={\"Content-TYpe\": \"text/html\"}\n        )\n        self.wrapped = WrappedResponse(self.response)\n\n    def test_info(self):\n        assert self.wrapped.info() is self.wrapped\n\n    def test_get_all(self):\n        # get_all result must be native string\n        assert self.wrapped.get_all(\"content-type\") == [\"text/html\"]\n", "n_tokens": 574, "byte_len": 2658, "file_sha1": "49a6f88b58a32f3d6b01926c43592841210957d5", "start_line": 1, "end_line": 73}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler_base.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler_base.py", "rel_path": "tests/test_scheduler_base.py", "module": "tests.test_scheduler_base", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "has_pending_requests", "enqueue_request", "next_request", "open", "close", "__len__", "parse", "test_scheduler_class", "setup_method", "test_methods", "test_open_close", "test_len", "test_enqueue_dequeue", "test_crawl", "MinimalScheduler", "SimpleScheduler", "PathsSpider", "InterfaceCheckMixin", "TestBaseScheduler", "TestMinimalScheduler", "TestSimpleScheduler", "TestMinimalSchedulerCrawl", "TestSimpleSchedulerCrawl", "while", "bool", "append", "test", "crawl", "spider", "name", "scheduler", "simple", "spiders", "future", "https", "mockserver", "succeed", "urlparse", "cached", "get", "crawler", "result", "pytest", "settings", "fingerprint", "dequeued", "isinstance", "none", "methods"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom urllib.parse import urljoin\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.core.scheduler import BaseScheduler\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.request import fingerprint\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\n\nPATHS = [\"/a\", \"/b\", \"/c\"]\nURLS = [urljoin(\"https://example.org\", p) for p in PATHS]\n\n\nclass MinimalScheduler:\n    def __init__(self) -> None:\n        self.requests: dict[bytes, Request] = {}\n\n    def has_pending_requests(self) -> bool:\n        return bool(self.requests)\n\n    def enqueue_request(self, request: Request) -> bool:\n        fp = fingerprint(request)\n        if fp not in self.requests:\n            self.requests[fp] = request\n            return True\n        return False\n\n    def next_request(self) -> Request | None:\n        if self.has_pending_requests():\n            fp, request = self.requests.popitem()\n            return request\n        return None\n\n\nclass SimpleScheduler(MinimalScheduler):\n    def open(self, spider: Spider) -> defer.Deferred:\n        return defer.succeed(\"open\")\n\n    def close(self, reason: str) -> defer.Deferred:\n        return defer.succeed(\"close\")\n\n    def __len__(self) -> int:\n        return len(self.requests)\n\n\nclass PathsSpider(Spider):\n    name = \"paths\"\n\n    def __init__(self, mockserver, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = map(mockserver.url, PATHS)\n\n    def parse(self, response):\n        return {\"path\": urlparse_cached(response).path}\n\n\nclass InterfaceCheckMixin:\n    def test_scheduler_class(self):\n        assert isinstance(self.scheduler, BaseScheduler)\n        assert issubclass(self.scheduler.__class__, BaseScheduler)\n\n\nclass TestBaseScheduler(InterfaceCheckMixin):\n    def setup_method(self):\n        self.scheduler = BaseScheduler()\n\n    def test_methods(self):\n        assert self.scheduler.open(Spider(\"foo\")) is None\n        assert self.scheduler.close(\"finished\") is None\n        with pytest.raises(NotImplementedError):\n            self.scheduler.has_pending_requests()\n        with pytest.raises(NotImplementedError):\n            self.scheduler.enqueue_request(Request(\"https://example.org\"))\n        with pytest.raises(NotImplementedError):\n            self.scheduler.next_request()\n\n\nclass TestMinimalScheduler(InterfaceCheckMixin):\n    def setup_method(self):\n        self.scheduler = MinimalScheduler()\n\n    def test_open_close(self):\n        with pytest.raises(AttributeError):\n            self.scheduler.open(Spider(\"foo\"))\n        with pytest.raises(AttributeError):\n            self.scheduler.close(\"finished\")\n\n    def test_len(self):\n        with pytest.raises(AttributeError):\n            self.scheduler.__len__()\n        with pytest.raises(TypeError):\n            len(self.scheduler)\n\n    def test_enqueue_dequeue(self):\n        assert not self.scheduler.has_pending_requests()\n        for url in URLS:\n            assert self.scheduler.enqueue_request(Request(url))\n            assert not self.scheduler.enqueue_request(Request(url))\n        assert self.scheduler.has_pending_requests\n\n        dequeued = []\n        while self.scheduler.has_pending_requests():\n            request = self.scheduler.next_request()\n            dequeued.append(request.url)\n        assert set(dequeued) == set(URLS)\n        assert not self.scheduler.has_pending_requests()\n\n\nclass TestSimpleScheduler(InterfaceCheckMixin):\n    def setup_method(self):\n        self.scheduler = SimpleScheduler()\n\n    @inlineCallbacks\n    def test_enqueue_dequeue(self):\n        open_result = yield self.scheduler.open(Spider(\"foo\"))\n        assert open_result == \"open\"\n        assert not self.scheduler.has_pending_requests()\n\n        for url in URLS:\n            assert self.scheduler.enqueue_request(Request(url))\n            assert not self.scheduler.enqueue_request(Request(url))\n\n        assert self.scheduler.has_pending_requests()\n        assert len(self.scheduler) == len(URLS)\n\n        dequeued = []\n        while self.scheduler.has_pending_requests():\n            request = self.scheduler.next_request()\n            dequeued.append(request.url)\n        assert set(dequeued) == set(URLS)\n\n        assert not self.scheduler.has_pending_requests()\n        assert len(self.scheduler) == 0\n\n        close_result = yield self.scheduler.close(\"\")\n        assert close_result == \"close\"\n\n\nclass TestMinimalSchedulerCrawl:\n    scheduler_cls = MinimalScheduler\n\n    @inlineCallbacks\n    def test_crawl(self):\n        with MockServer() as mockserver:\n            settings = {\n                \"SCHEDULER\": self.scheduler_cls,\n            }\n            with LogCapture() as log:\n                crawler = get_crawler(PathsSpider, settings)\n                yield crawler.crawl(mockserver)\n            for path in PATHS:\n                assert f\"{{'path': '{path}'}}\" in str(log)\n            assert f\"'item_scraped_count': {len(PATHS)}\" in str(log)\n\n\nclass TestSimpleSchedulerCrawl(TestMinimalSchedulerCrawl):\n    scheduler_cls = SimpleScheduler\n", "n_tokens": 1040, "byte_len": 5243, "file_sha1": "732af879fc66ea30007a4083337265225d4cc2b4", "start_line": 1, "end_line": 166}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/__init__.py", "rel_path": "tests/__init__.py", "module": "tests.__init__", "ext": "py", "chunk_number": 1, "symbols": ["get_testdata", "unittests", "get", "testdata", "does", "twiste", "keep", "file", "spec", "doesn", "going", "http", "proxy", "https", "unsuspecting", "path", "existin", "resolvable", "system", "proxies", "server", "docs", "html", "socket", "paths", "requests", "contributing", "return", "ignore", "some", "understand", "rely", "gaierror", "existing", "such", "package", "tests", "version", "raise", "environments", "cases", "python", "twisted", "environ", "scrapy", "versions", "parent", "data", "would", "bytes"], "ast_kind": "function_or_method", "text": "\"\"\"\ntests: this package contains all Scrapy unittests\n\nsee https://docs.scrapy.org/en/latest/contributing.html#running-tests\n\"\"\"\n\nimport os\nimport socket\nfrom pathlib import Path\n\nfrom twisted import version as TWISTED_VERSION\nfrom twisted.python.versions import Version\n\n# ignore system-wide proxies for tests\n# which would send requests to a totally unsuspecting server\n# (e.g. because urllib does not fully understand the proxy spec)\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\nos.environ[\"ftp_proxy\"] = \"\"\n\ntests_datadir = str(Path(__file__).parent.resolve() / \"sample_data\")\n\n\n# In some environments accessing a non-existing host doesn't raise an\n# error. In such cases we're going to skip tests which rely on it.\ntry:\n    socket.getaddrinfo(\"non-existing-host\", 80)\n    NON_EXISTING_RESOLVABLE = True\nexcept socket.gaierror:\n    NON_EXISTING_RESOLVABLE = False\n\n\ndef get_testdata(*paths: str) -> bytes:\n    \"\"\"Return test data\"\"\"\n    return Path(tests_datadir, *paths).read_bytes()\n\n\nTWISTED_KEEPS_TRACEBACKS = TWISTED_VERSION >= Version(\"twisted\", 24, 10, 0)\n", "n_tokens": 270, "byte_len": 1083, "file_sha1": "d7a2beb467c043c5c07dd01ac3afbf1a242d069a", "start_line": 1, "end_line": 39}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware.py", "rel_path": "tests/test_downloadermiddleware.py", "module": "tests.test_downloadermiddleware", "ext": "py", "chunk_number": 1, "symbols": ["download_func", "process_request", "process_response", "TestManagerBase", "TestDefaults", "TestResponseFromProcessRequest", "ResponseMiddleware", "TestResponseFromProcessException", "method", "async", "test", "response", "append", "decompress", "case", "spider", "redirecting", "deferred", "from", "spiders", "invalid", "output", "bad", "gzip", "future", "typ", "checking", "contextlib", "mock", "succeed", "process_exception", "cb", "TestInvalidOutput", "InvalidProcessRequestMiddleware", "InvalidProcessResponseMiddleware", "InvalidProcessExceptionMiddleware", "TestMiddlewareUsingDeferreds", "DeferredMiddleware", "TestMiddlewareUsingCoro", "CoroMiddleware", "TestDownloadDeprecated", "TestDeprecatedSpiderArg", "DeprecatedSpiderArgMiddleware", "coroutines", "sleep", "deprecated", "get", "crawler", "mwman", "asyncdef"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom gzip import BadGzipFile\nfrom typing import TYPE_CHECKING\nfrom unittest import mock\n\nimport pytest\nfrom twisted.internet.defer import Deferred, succeed\n\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import ScrapyDeprecationWarning, _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator\n\n\nclass TestManagerBase:\n    settings_dict = None\n\n    # should be a fixture but async fixtures that use Futures are problematic with pytest-twisted\n    @asynccontextmanager\n    async def get_mwman(self) -> AsyncGenerator[DownloaderMiddlewareManager]:\n        crawler = get_crawler(Spider, self.settings_dict)\n        crawler.spider = crawler._create_spider(\"foo\")\n        mwman = DownloaderMiddlewareManager.from_crawler(crawler)\n        crawler.engine = crawler._create_engine()\n        await crawler.engine.open_spider_async()\n        yield mwman\n        await crawler.engine.close_spider_async()\n\n    @staticmethod\n    async def _download(\n        mwman: DownloaderMiddlewareManager,\n        request: Request,\n        response: Response | None = None,\n    ) -> Response | Request:\n        \"\"\"Executes downloader mw manager's download method and returns\n        the result (Request or Response) or raises exception in case of\n        failure.\n        \"\"\"\n        if not response:\n            response = Response(request.url)\n\n        def download_func(request: Request) -> Deferred[Response]:\n            return succeed(response)\n\n        return await maybe_deferred_to_future(mwman.download(download_func, request))\n\n\nclass TestDefaults(TestManagerBase):\n    \"\"\"Tests default behavior with default settings\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_request_response(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(req.url, status=200)\n        async with self.get_mwman() as mwman:\n            ret = await self._download(mwman, req, resp)\n        assert isinstance(ret, Response), \"Non-response returned\"\n\n    @deferred_f_from_coro_f\n    async def test_3xx_and_invalid_gzipped_body_must_redirect(self):\n        \"\"\"Regression test for a failure when redirecting a compressed\n        request.\n\n        This happens when httpcompression middleware is executed before redirect\n        middleware and attempts to decompress a non-compressed body.\n        In particular when some website returns a 30x response with header\n        'Content-Encoding: gzip' giving as result the error below:\n\n            BadGzipFile: Not a gzipped file (...)\n\n        \"\"\"\n        req = Request(\"http://example.com\")\n        body = b\"<p>You are being redirected</p>\"\n        resp = Response(\n            req.url,\n            status=302,\n            body=body,\n            headers={\n                \"Content-Length\": str(len(body)),\n                \"Content-Type\": \"text/html\",\n                \"Content-Encoding\": \"gzip\",\n                \"Location\": \"http://example.com/login\",\n            },\n        )\n        async with self.get_mwman() as mwman:\n            ret = await self._download(mwman, req, resp)\n        assert isinstance(ret, Request), f\"Not redirected: {ret!r}\"\n        assert to_bytes(ret.url) == resp.headers[\"Location\"], (\n            \"Not redirected to location header\"\n        )\n\n    @deferred_f_from_coro_f\n    async def test_200_and_invalid_gzipped_body_must_fail(self):\n        req = Request(\"http://example.com\")\n        body = b\"<p>You are being redirected</p>\"\n        resp = Response(\n            req.url,\n            status=200,\n            body=body,\n            headers={\n                \"Content-Length\": str(len(body)),\n                \"Content-Type\": \"text/html\",\n                \"Content-Encoding\": \"gzip\",\n                \"Location\": \"http://example.com/login\",\n            },\n        )\n        with pytest.raises(BadGzipFile):\n            async with self.get_mwman() as mwman:\n                await self._download(mwman, req, resp)\n\n\nclass TestResponseFromProcessRequest(TestManagerBase):\n    \"\"\"Tests middleware returning a response from process_request.\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_download_func_not_called(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n\n        class ResponseMiddleware:\n            def process_request(self, request):\n                return resp\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(ResponseMiddleware())\n            result = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert result is resp\n        assert not download_func.called\n\n\nclass TestResponseFromProcessException(TestManagerBase):\n    \"\"\"Tests middleware returning a response from process_exception.\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_process_response_called(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(\"http://example.com/index.html\")\n        calls = []\n\n        def download_func(request):\n            raise ValueError(\"test\")\n\n        class ResponseMiddleware:\n            def process_response(self, request, response):\n                calls.append(\"process_response\")\n                return resp\n", "n_tokens": 1159, "byte_len": 5632, "file_sha1": "6051e8691388876dabd987ab6f65f457fa3ca481", "start_line": 1, "end_line": 157}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware.py", "rel_path": "tests/test_downloadermiddleware.py", "module": "tests.test_downloadermiddleware", "ext": "py", "chunk_number": 2, "symbols": ["process_exception", "process_request", "process_response", "cb", "download_func", "TestInvalidOutput", "InvalidProcessRequestMiddleware", "InvalidProcessResponseMiddleware", "InvalidProcessExceptionMiddleware", "TestMiddlewareUsingDeferreds", "DeferredMiddleware", "TestMiddlewareUsingCoro", "CoroMiddleware", "TestDownloadDeprecated", "TestDeprecatedSpiderArg", "method", "async", "append", "spider", "coroutines", "deferred", "from", "sleep", "deprecated", "invalid", "output", "mock", "succeed", "get", "mwman", "TestManagerBase", "TestDefaults", "TestResponseFromProcessRequest", "ResponseMiddleware", "TestResponseFromProcessException", "DeprecatedSpiderArgMiddleware", "test", "response", "decompress", "case", "redirecting", "spiders", "bad", "gzip", "future", "typ", "checking", "contextlib", "crawler", "asyncdef"], "ast_kind": "class_or_type", "text": "            def process_exception(self, request, exception):\n                calls.append(\"process_exception\")\n                return resp\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(ResponseMiddleware())\n            result = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert result is resp\n        assert calls == [\n            \"process_exception\",\n            \"process_response\",\n        ]\n\n\nclass TestInvalidOutput(TestManagerBase):\n    @deferred_f_from_coro_f\n    async def test_invalid_process_request(self):\n        \"\"\"Invalid return value for process_request method should raise an exception\"\"\"\n        req = Request(\"http://example.com/index.html\")\n\n        class InvalidProcessRequestMiddleware:\n            def process_request(self, request):\n                return 1\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(InvalidProcessRequestMiddleware())\n            with pytest.raises(_InvalidOutput):\n                await self._download(mwman, req)\n\n    @deferred_f_from_coro_f\n    async def test_invalid_process_response(self):\n        \"\"\"Invalid return value for process_response method should raise an exception\"\"\"\n        req = Request(\"http://example.com/index.html\")\n\n        class InvalidProcessResponseMiddleware:\n            def process_response(self, request, response):\n                return 1\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(InvalidProcessResponseMiddleware())\n            with pytest.raises(_InvalidOutput):\n                await self._download(mwman, req)\n\n    @deferred_f_from_coro_f\n    async def test_invalid_process_exception(self):\n        \"\"\"Invalid return value for process_exception method should raise an exception\"\"\"\n        req = Request(\"http://example.com/index.html\")\n\n        class InvalidProcessExceptionMiddleware:\n            def process_request(self, request):\n                raise RuntimeError\n\n            def process_exception(self, request, exception):\n                return 1\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(InvalidProcessExceptionMiddleware())\n            with pytest.raises(_InvalidOutput):\n                await self._download(mwman, req)\n\n\nclass TestMiddlewareUsingDeferreds(TestManagerBase):\n    \"\"\"Middlewares using Deferreds should work\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_deferred(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n\n        class DeferredMiddleware:\n            def cb(self, result):\n                return result\n\n            def process_request(self, request):\n                d = Deferred()\n                d.addCallback(self.cb)\n                d.callback(resp)\n                return d\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(DeferredMiddleware())\n            result = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert result is resp\n        assert not download_func.called\n\n\nclass TestMiddlewareUsingCoro(TestManagerBase):\n    \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_asyncdef(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n\n        class CoroMiddleware:\n            async def process_request(self, request):\n                await succeed(42)\n                return resp\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(CoroMiddleware())\n            result = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert result is resp\n        assert not download_func.called\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_asyncdef_asyncio(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n\n        class CoroMiddleware:\n            async def process_request(self, request):\n                await asyncio.sleep(0.1)\n                return await get_from_asyncio_queue(resp)\n\n        async with self.get_mwman() as mwman:\n            mwman._add_middleware(CoroMiddleware())\n            result = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert result is resp\n        assert not download_func.called\n\n\nclass TestDownloadDeprecated(TestManagerBase):\n    @deferred_f_from_coro_f\n    async def test_download_func_spider_arg(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(req.url, status=200)\n\n        def download_func(request: Request, spider: Spider) -> Deferred[Response]:\n            return succeed(resp)\n\n        async with self.get_mwman() as mwman:\n            with pytest.warns(\n                ScrapyDeprecationWarning,\n                match=\"The spider argument of download_func is deprecated\",\n            ):\n                ret = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert isinstance(ret, Response)\n\n    @deferred_f_from_coro_f\n    async def test_mwman_download_spider_arg(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(req.url, status=200)\n\n        def download_func(request: Request) -> Deferred[Response]:\n            return succeed(resp)\n\n        async with self.get_mwman() as mwman:\n            with pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"Passing a spider argument to DownloaderMiddlewareManager.download\\(\\) is deprecated\",\n            ):\n                ret = await maybe_deferred_to_future(\n                    mwman.download(download_func, req, mwman.crawler.spider)\n                )\n        assert isinstance(ret, Response)\n\n\nclass TestDeprecatedSpiderArg(TestManagerBase):", "n_tokens": 1214, "byte_len": 6029, "file_sha1": "6051e8691388876dabd987ab6f65f457fa3ca481", "start_line": 158, "end_line": 320}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware.py", "rel_path": "tests/test_downloadermiddleware.py", "module": "tests.test_downloadermiddleware", "ext": "py", "chunk_number": 3, "symbols": ["process_request", "process_response", "process_exception", "DeprecatedSpiderArgMiddleware", "mwman", "async", "process", "exception", "await", "download", "func", "result", "requires", "magic", "mock", "argument", "spider", "scrapy", "deprecation", "warns", "return", "deprecated", "deferred", "from", "maybe", "class", "with", "resp", "example", "get", "download_func", "cb", "TestManagerBase", "TestDefaults", "TestResponseFromProcessRequest", "ResponseMiddleware", "TestResponseFromProcessException", "TestInvalidOutput", "InvalidProcessRequestMiddleware", "InvalidProcessResponseMiddleware", "InvalidProcessExceptionMiddleware", "TestMiddlewareUsingDeferreds", "DeferredMiddleware", "TestMiddlewareUsingCoro", "CoroMiddleware", "TestDownloadDeprecated", "TestDeprecatedSpiderArg", "method", "test", "response"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_deprecated_spider_arg(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n\n        class DeprecatedSpiderArgMiddleware:\n            def process_request(self, request, spider):\n                1 / 0\n\n            def process_response(self, request, response, spider):\n                return response\n\n            def process_exception(self, request, exception, spider):\n                return resp\n\n        async with self.get_mwman() as mwman:\n            with (\n                pytest.warns(\n                    ScrapyDeprecationWarning,\n                    match=r\"process_request\\(\\) requires a spider argument\",\n                ),\n                pytest.warns(\n                    ScrapyDeprecationWarning,\n                    match=r\"process_response\\(\\) requires a spider argument\",\n                ),\n                pytest.warns(\n                    ScrapyDeprecationWarning,\n                    match=r\"process_exception\\(\\) requires a spider argument\",\n                ),\n            ):\n                mwman._add_middleware(DeprecatedSpiderArgMiddleware())\n            result = await maybe_deferred_to_future(mwman.download(download_func, req))\n        assert result is resp\n        assert not download_func.called\n", "n_tokens": 260, "byte_len": 1380, "file_sha1": "6051e8691388876dabd987ab6f65f457fa3ca481", "start_line": 321, "end_line": 356}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_useragent.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_useragent.py", "rel_path": "tests/test_downloadermiddleware_useragent.py", "module": "tests.test_downloadermiddleware_useragent", "ext": "py", "chunk_number": 1, "symbols": ["get_spider_and_mw", "test_default_agent", "test_remove_agent", "test_spider_agent", "test_header_agent", "test_no_agent", "TestUserAgentMiddleware", "user", "agent", "test", "spider", "default", "useragent", "return", "header", "remove", "class", "downloadermiddlewares", "spiders", "scrapy", "headers", "create", "get", "crawler", "from", "settings", "scrapytest", "assert", "request", "process", "none", "opened", "utils", "import", "should", "http", "self", "use"], "ast_kind": "class_or_type", "text": "from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestUserAgentMiddleware:\n    def get_spider_and_mw(self, default_useragent):\n        crawler = get_crawler(Spider, {\"USER_AGENT\": default_useragent})\n        spider = crawler._create_spider(\"foo\")\n        return spider, UserAgentMiddleware.from_crawler(crawler)\n\n    def test_default_agent(self):\n        _, mw = self.get_spider_and_mw(\"default_useragent\")\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req) is None\n        assert req.headers[\"User-Agent\"] == b\"default_useragent\"\n\n    def test_remove_agent(self):\n        # settings USER_AGENT to None should remove the user agent\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        spider.user_agent = None\n        mw.spider_opened(spider)\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req) is None\n        assert req.headers.get(\"User-Agent\") is None\n\n    def test_spider_agent(self):\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        spider.user_agent = \"spider_useragent\"\n        mw.spider_opened(spider)\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req) is None\n        assert req.headers[\"User-Agent\"] == b\"spider_useragent\"\n\n    def test_header_agent(self):\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        spider.user_agent = \"spider_useragent\"\n        mw.spider_opened(spider)\n        req = Request(\n            \"http://scrapytest.org/\", headers={\"User-Agent\": \"header_useragent\"}\n        )\n        assert mw.process_request(req) is None\n        assert req.headers[\"User-Agent\"] == b\"header_useragent\"\n\n    def test_no_agent(self):\n        spider, mw = self.get_spider_and_mw(None)\n        spider.user_agent = None\n        mw.spider_opened(spider)\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req) is None\n        assert \"User-Agent\" not in req.headers\n", "n_tokens": 490, "byte_len": 2114, "file_sha1": "ec11ba0a0988bcb60f0cd388be952f719d01b2cd", "start_line": 1, "end_line": 53}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 1, "symbols": ["test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "TestSpider", "TestInitSpider", "get", "testdata", "sitemap", "spider", "method", "async", "signal", "lib", "w3lib", "test", "logger", "name", "deferred", "from", "responses", "spiders", "test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "dummy_process_links", "test_process_links_filter", "filter_process_links", "test_process_links_generator", "test_process_request", "process_request_change_domain", "test_process_request_with_response", "process_request_meta_response_class", "test_process_request_instance_method", "process_request_upper", "test_process_request_instance_method_with_response", "test_follow_links_attribute_population", "test_start_url", "test_parse_response_use"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport gzip\nimport re\nimport warnings\nfrom datetime import datetime\nfrom io import BytesIO\nfrom logging import ERROR, WARNING\nfrom pathlib import Path\nfrom typing import Any\nfrom unittest import mock\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import HtmlResponse, Request, Response, TextResponse, XmlResponse\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import (\n    CrawlSpider,\n    CSVFeedSpider,\n    Rule,\n    SitemapSpider,\n    Spider,\n    XMLFeedSpider,\n)\nfrom scrapy.spiders.init import InitSpider\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler, get_reactor_settings\nfrom tests import get_testdata, tests_datadir\n\n\nclass TestSpider:\n    spider_class = Spider\n\n    def test_base_spider(self):\n        spider = self.spider_class(\"example.com\")\n        assert spider.name == \"example.com\"\n        assert spider.start_urls == []  # pylint: disable=use-implicit-booleaness-not-comparison\n\n    def test_spider_args(self):\n        \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n        spider = self.spider_class(\"example.com\", foo=\"bar\")\n        assert spider.foo == \"bar\"\n\n    def test_spider_without_name(self):\n        \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n        msg = \"must have a name\"\n        with pytest.raises(ValueError, match=msg):\n            self.spider_class()\n        with pytest.raises(ValueError, match=msg):\n            self.spider_class(somearg=\"foo\")\n\n    def test_from_crawler_crawler_and_settings_population(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        assert hasattr(spider, \"crawler\")\n        assert spider.crawler is crawler\n        assert hasattr(spider, \"settings\")\n        assert spider.settings is crawler.settings\n\n    def test_from_crawler_init_call(self):\n        with mock.patch.object(\n            self.spider_class, \"__init__\", return_value=None\n        ) as mock_init:\n            self.spider_class.from_crawler(get_crawler(), \"example.com\", foo=\"bar\")\n            mock_init.assert_called_once_with(\"example.com\", foo=\"bar\")\n\n    def test_closed_signal_call(self):\n        class TestSpider(self.spider_class):\n            closed_called = False\n\n            def closed(self, reason):\n                self.closed_called = True\n\n        crawler = get_crawler()\n        spider = TestSpider.from_crawler(crawler, \"example.com\")\n        crawler.signals.send_catch_log(signal=signals.spider_opened, spider=spider)\n        crawler.signals.send_catch_log(\n            signal=signals.spider_closed, spider=spider, reason=None\n        )\n        assert spider.closed_called\n\n    def test_update_settings(self):\n        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n        self.spider_class.custom_settings = spider_settings\n        settings = Settings(project_settings, priority=\"project\")\n\n        self.spider_class.update_settings(settings)\n        assert settings.get(\"TEST1\") == \"spider\"\n        assert settings.get(\"TEST2\") == \"spider\"\n        assert settings.get(\"TEST3\") == \"project\"\n\n    @inlineCallbacks\n    def test_settings_in_from_crawler(self):\n        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n        project_settings = {\n            \"TEST1\": \"project\",\n            \"TEST3\": \"project\",\n            **get_reactor_settings(),\n        }\n\n        class TestSpider(self.spider_class):\n            name = \"test\"\n            custom_settings = spider_settings\n\n            @classmethod\n            def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n                spider = super().from_crawler(crawler, *args, **kwargs)\n                spider.settings.set(\"TEST1\", \"spider_instance\", priority=\"spider\")\n                return spider\n\n        crawler = Crawler(TestSpider, project_settings)\n        assert crawler.settings.get(\"TEST1\") == \"spider\"\n        assert crawler.settings.get(\"TEST2\") == \"spider\"\n        assert crawler.settings.get(\"TEST3\") == \"project\"\n        yield crawler.crawl()\n        assert crawler.settings.get(\"TEST1\") == \"spider_instance\"\n\n    def test_logger(self):\n        spider = self.spider_class(\"example.com\")\n        with LogCapture() as lc:\n            spider.logger.info(\"test log msg\")\n        lc.check((\"example.com\", \"INFO\", \"test log msg\"))\n\n        record = lc.records[0]\n        assert \"spider\" in record.__dict__\n        assert record.spider is spider\n\n    def test_log(self):\n        spider = self.spider_class(\"example.com\")\n        with mock.patch(\"scrapy.spiders.Spider.logger\") as mock_logger:\n            spider.log(\"test log msg\", \"INFO\")\n        mock_logger.log.assert_called_once_with(\"INFO\", \"test log msg\")\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestInitSpider(TestSpider):\n    spider_class = InitSpider\n\n    @deferred_f_from_coro_f\n    async def test_start_urls(self):\n        responses = []\n\n        class TestSpider(self.spider_class):\n            name = \"test\"\n            start_urls = [\"data:,\"]\n", "n_tokens": 1189, "byte_len": 5421, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 1, "end_line": 153}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 2, "symbols": ["test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "dummy_process_links", "test_process_links_filter", "TestXMLFeedSpider", "_XMLSpider", "TestCSVFeedSpider", "_CrawlSpider", "TestCrawlSpider", "encoding", "get", "testdata", "async", "parse", "row", "xpath", "append", "test", "rule", "name", "about", "responses", "rules", "iternodes", "requests", "follow", "test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "filter_process_links", "test_process_links_generator", "test_process_request", "process_request_change_domain", "test_process_request_with_response", "process_request_meta_response_class", "test_process_request_instance_method", "process_request_upper"], "ast_kind": "class_or_type", "text": "            async def parse(self, response):\n                responses.append(response)\n\n        crawler = get_crawler(TestSpider)\n        await maybe_deferred_to_future(crawler.crawl())\n        assert len(responses) == 1\n        assert responses[0].url == \"data:,\"\n\n\nclass TestXMLFeedSpider(TestSpider):\n    spider_class = XMLFeedSpider\n\n    def test_register_namespace(self):\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n        <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"\n                xmlns:y=\"http://www.example.com/schemas/extras/1.0\">\n        <url><x:loc>http://www.example.com/Special-Offers.html</x:loc><y:updated>2009-08-16</y:updated>\n            <other value=\"bar\" y:custom=\"fuu\"/>\n        </url>\n        <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</y:updated><other value=\"foo\"/></url>\n        </urlset>\"\"\"\n        response = XmlResponse(url=\"http://example.com/sitemap.xml\", body=body)\n\n        class _XMLSpider(self.spider_class):\n            itertag = \"url\"\n            namespaces = (\n                (\"a\", \"http://www.google.com/schemas/sitemap/0.84\"),\n                (\"b\", \"http://www.example.com/schemas/extras/1.0\"),\n            )\n\n            def parse_node(self, response, selector):\n                yield {\n                    \"loc\": selector.xpath(\"a:loc/text()\").getall(),\n                    \"updated\": selector.xpath(\"b:updated/text()\").getall(),\n                    \"other\": selector.xpath(\"other/@value\").getall(),\n                    \"custom\": selector.xpath(\"other/@b:custom\").getall(),\n                }\n\n        for iterator in (\"iternodes\", \"xml\"):\n            spider = _XMLSpider(\"example\", iterator=iterator)\n            output = list(spider._parse(response))\n            assert len(output) == 2, iterator\n            assert output == [\n                {\n                    \"loc\": [\"http://www.example.com/Special-Offers.html\"],\n                    \"updated\": [\"2009-08-16\"],\n                    \"custom\": [\"fuu\"],\n                    \"other\": [\"bar\"],\n                },\n                {\n                    \"loc\": [],\n                    \"updated\": [\"2009-08-16\"],\n                    \"other\": [\"foo\"],\n                    \"custom\": [],\n                },\n            ], iterator\n\n\nclass TestCSVFeedSpider(TestSpider):\n    spider_class = CSVFeedSpider\n\n    def test_parse_rows(self):\n        body = get_testdata(\"feeds\", \"feed-sample6.csv\")\n        response = Response(\"http://example.org/dummy.csv\", body=body)\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            delimiter = \",\"\n            quotechar = \"'\"\n\n            def parse_row(self, response, row):\n                return row\n\n        spider = _CrawlSpider()\n        rows = list(spider.parse_rows(response))\n        assert rows[0] == {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"}\n        assert len(rows) == 4\n\n\nclass TestCrawlSpider(TestSpider):\n    test_body = b\"\"\"<html><head><title>Page title</title></head>\n    <body>\n    <p><a href=\"item/12.html\">Item 12</a></p>\n    <div class='links'>\n    <p><a href=\"/about.html\">About us</a></p>\n    </div>\n    <div>\n    <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n    </div>\n    </body></html>\"\"\"\n    spider_class = CrawlSpider\n\n    def test_rule_without_link_extractor(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(),)\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.org/somepage/item/12.html\",\n            \"http://example.org/about.html\",\n            \"http://example.org/nofollow.html\",\n        ]\n\n    def test_process_links(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n\n            def dummy_process_links(self, links):\n                return links\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.org/somepage/item/12.html\",\n            \"http://example.org/about.html\",\n            \"http://example.org/nofollow.html\",\n        ]\n\n    def test_process_links_filter(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n", "n_tokens": 1167, "byte_len": 4958, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 154, "end_line": 293}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 3, "symbols": ["filter_process_links", "test_process_links_generator", "dummy_process_links", "test_process_request", "process_request_change_domain", "test_process_request_with_response", "process_request_meta_response_class", "test_process_request_instance_method", "process_request_upper", "test_process_request_instance_method_with_response", "_CrawlSpider", "test", "regex", "filter", "process", "allowed", "domains", "compile", "upper", "response", "class", "spider", "item", "return", "request", "name", "about", "replace", "meta", "output", "test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "test_process_links_filter", "test_follow_links_attribute_population"], "ast_kind": "class_or_type", "text": "        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_links=\"filter_process_links\"),)\n            _test_regex = re.compile(\"nofollow\")\n\n            def filter_process_links(self, links):\n                return [link for link in links if not self._test_regex.search(link.url)]\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 2\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.org/somepage/item/12.html\",\n            \"http://example.org/about.html\",\n        ]\n\n    def test_process_links_generator(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n\n            def dummy_process_links(self, links):\n                yield from links\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.org/somepage/item/12.html\",\n            \"http://example.org/about.html\",\n            \"http://example.org/nofollow.html\",\n        ]\n\n    def test_process_request(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        def process_request_change_domain(request, response):\n            return request.replace(url=request.url.replace(\".org\", \".com\"))\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (\n                Rule(LinkExtractor(), process_request=process_request_change_domain),\n            )\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.com/somepage/item/12.html\",\n            \"http://example.com/about.html\",\n            \"http://example.com/nofollow.html\",\n        ]\n\n    def test_process_request_with_response(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        def process_request_meta_response_class(request, response):\n            request.meta[\"response_class\"] = response.__class__.__name__\n            return request\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (\n                Rule(\n                    LinkExtractor(), process_request=process_request_meta_response_class\n                ),\n            )\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.org/somepage/item/12.html\",\n            \"http://example.org/about.html\",\n            \"http://example.org/nofollow.html\",\n        ]\n        assert [r.meta[\"response_class\"] for r in output] == [\n            \"HtmlResponse\",\n            \"HtmlResponse\",\n            \"HtmlResponse\",\n        ]\n\n    def test_process_request_instance_method(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_request=\"process_request_upper\"),)\n\n            def process_request_upper(self, request, response):\n                return request.replace(url=request.url.upper())\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            safe_url_string(\"http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML\"),\n            safe_url_string(\"http://EXAMPLE.ORG/ABOUT.HTML\"),\n            safe_url_string(\"http://EXAMPLE.ORG/NOFOLLOW.HTML\"),\n        ]\n\n    def test_process_request_instance_method_with_response(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (\n                Rule(\n                    LinkExtractor(),\n                    process_request=\"process_request_meta_response_class\",\n                ),\n            )\n\n            def process_request_meta_response_class(self, request, response):\n                request.meta[\"response_class\"] = response.__class__.__name__\n                return request\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        assert len(output) == 3\n        assert all(isinstance(r, Request) for r in output)\n        assert [r.url for r in output] == [\n            \"http://example.org/somepage/item/12.html\",\n            \"http://example.org/about.html\",\n            \"http://example.org/nofollow.html\",\n        ]\n        assert [r.meta[\"response_class\"] for r in output] == [\n            \"HtmlResponse\",\n            \"HtmlResponse\",\n            \"HtmlResponse\",\n        ]\n", "n_tokens": 1201, "byte_len": 5822, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 294, "end_line": 449}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 4, "symbols": ["test_follow_links_attribute_population", "test_start_url", "test_parse_response_use", "test_parse_response_override", "_parse_response", "test_parse_with_rules", "assertSitemapBody", "test_get_sitemap_body", "test_get_sitemap_body_gzip_headers", "test_get_sitemap_body_xml_url", "test_get_sitemap_body_xml_url_compressed", "test_get_sitemap_urls_from_robotstxt", "TestSpider", "_CrawlSpider", "TestSitemapSpider", "sitemap", "spider", "parse", "with", "decoded", "miss", "robots", "name", "body", "error", "https", "get", "crawler", "gzip", "file", "test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "dummy_process_links", "test_process_links_filter"], "ast_kind": "class_or_type", "text": "    def test_follow_links_attribute_population(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        assert hasattr(spider, \"_follow_links\")\n        assert spider._follow_links\n\n        settings_dict = {\"CRAWLSPIDER_FOLLOW_LINKS\": False}\n        crawler = get_crawler(settings_dict=settings_dict)\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        assert hasattr(spider, \"_follow_links\")\n        assert not spider._follow_links\n\n    @inlineCallbacks\n    def test_start_url(self):\n        class TestSpider(self.spider_class):\n            name = \"test\"\n            start_url = \"https://www.example.com\"\n\n        crawler = get_crawler(TestSpider)\n        with LogCapture(\"scrapy.core.engine\", propagate=False, level=ERROR) as log:\n            yield crawler.crawl()\n        assert \"Error while reading start items and requests\" in str(log)\n        assert \"did you miss an 's'?\" in str(log)\n\n    def test_parse_response_use(self):\n        class _CrawlSpider(CrawlSpider):\n            name = \"test\"\n            start_urls = \"https://www.example.com\"\n            _follow_links = False\n\n        with warnings.catch_warnings(record=True) as w:\n            spider = _CrawlSpider()\n            assert len(w) == 0\n            spider._parse_response(\n                TextResponse(spider.start_urls, body=b\"\"), None, None\n            )\n            assert len(w) == 1\n\n    def test_parse_response_override(self):\n        class _CrawlSpider(CrawlSpider):\n            def _parse_response(self, response, callback, cb_kwargs, follow=True):\n                pass\n\n            name = \"test\"\n            start_urls = \"https://www.example.com\"\n            _follow_links = False\n\n        with warnings.catch_warnings(record=True) as w:\n            assert len(w) == 0\n            spider = _CrawlSpider()\n            assert len(w) == 1\n            spider._parse_response(\n                TextResponse(spider.start_urls, body=b\"\"), None, None\n            )\n            assert len(w) == 1\n\n    def test_parse_with_rules(self):\n        class _CrawlSpider(CrawlSpider):\n            name = \"test\"\n            start_urls = \"https://www.example.com\"\n\n        with warnings.catch_warnings(record=True) as w:\n            spider = _CrawlSpider()\n            spider.parse_with_rules(\n                TextResponse(spider.start_urls, body=b\"\"), None, None\n            )\n            assert len(w) == 0\n\n\nclass TestSitemapSpider(TestSpider):\n    spider_class = SitemapSpider\n\n    BODY = b\"SITEMAP\"\n    f = BytesIO()\n    g = gzip.GzipFile(fileobj=f, mode=\"w+b\")\n    g.write(BODY)\n    g.close()\n    GZBODY = f.getvalue()\n\n    def assertSitemapBody(self, response: Response, body: bytes | None) -> None:\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        assert spider._get_sitemap_body(response) == body\n\n    def test_get_sitemap_body(self):\n        r = XmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n        self.assertSitemapBody(r, self.BODY)\n\n        r = HtmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n        self.assertSitemapBody(r, None)\n\n        r = Response(url=\"http://www.example.com/favicon.ico\", body=self.BODY)\n        self.assertSitemapBody(r, None)\n\n    def test_get_sitemap_body_gzip_headers(self):\n        r = Response(\n            url=\"http://www.example.com/sitemap\",\n            body=self.GZBODY,\n            headers={\"content-type\": \"application/gzip\"},\n            request=Request(\"http://www.example.com/sitemap\"),\n        )\n        self.assertSitemapBody(r, self.BODY)\n\n    def test_get_sitemap_body_xml_url(self):\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=self.BODY)\n        self.assertSitemapBody(r, self.BODY)\n\n    def test_get_sitemap_body_xml_url_compressed(self):\n        r = Response(\n            url=\"http://www.example.com/sitemap.xml.gz\",\n            body=self.GZBODY,\n            request=Request(\"http://www.example.com/sitemap\"),\n        )\n        self.assertSitemapBody(r, self.BODY)\n\n        # .xml.gz but body decoded by HttpCompression middleware already\n        r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.BODY)\n        self.assertSitemapBody(r, self.BODY)\n\n    def test_get_sitemap_urls_from_robotstxt(self):\n        robots = b\"\"\"# Sitemap files\nSitemap: http://example.com/sitemap.xml\nSitemap: http://example.com/sitemap-product-index.xml\nSitemap: HTTP://example.com/sitemap-uppercase.xml\nSitemap: /sitemap-relative-url.xml\n\"\"\"\n\n        r = TextResponse(url=\"http://www.example.com/robots.txt\", body=robots)\n        spider = self.spider_class(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://example.com/sitemap.xml\",\n            \"http://example.com/sitemap-product-index.xml\",\n            \"http://example.com/sitemap-uppercase.xml\",\n            \"http://www.example.com/sitemap-relative-url.xml\",\n        ]\n", "n_tokens": 1149, "byte_len": 4995, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 450, "end_line": 585}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 5, "symbols": ["test_alternate_url_locs", "test_sitemap_filter", "sitemap_filter", "test_sitemap_filter_with_alternate_links", "test_sitemapindex_filter", "FilteredSitemapSpider", "encoding", "text", "response", "year", "sitemap", "sitemap1", "alternate", "strptime", "xhtml", "date", "time", "spider", "filtered", "entries", "class", "parse", "links", "example", "body", "article", "test", "sitemapindex", "xmlns", "sitemap2", "test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "dummy_process_links", "test_process_links_filter"], "ast_kind": "class_or_type", "text": "    def test_alternate_url_locs(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/</loc>\n            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n                href=\"http://www.example.com/deutsch/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"de-ch\"\n                href=\"http://www.example.com/schweiz-deutsch/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"it\"\n                href=\"http://www.example.com/italiano/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"it\"/><!-- wrong tag without href -->\n        </url>\n    </urlset>\"\"\"\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/english/\"\n        ]\n\n        spider.sitemap_alternate_links = True\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/english/\",\n            \"http://www.example.com/deutsch/\",\n            \"http://www.example.com/schweiz-deutsch/\",\n            \"http://www.example.com/italiano/\",\n        ]\n\n    def test_sitemap_filter(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/</loc>\n            <lastmod>2010-01-01</lastmod>\n        </url>\n        <url>\n            <loc>http://www.example.com/portuguese/</loc>\n            <lastmod>2005-01-01</lastmod>\n        </url>\n    </urlset>\"\"\"\n\n        class FilteredSitemapSpider(self.spider_class):\n            def sitemap_filter(self, entries):\n                for entry in entries:\n                    date_time = datetime.strptime(entry[\"lastmod\"], \"%Y-%m-%d\")\n                    if date_time.year > 2008:\n                        yield entry\n\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/english/\",\n            \"http://www.example.com/portuguese/\",\n        ]\n\n        spider = FilteredSitemapSpider(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/english/\"\n        ]\n\n    def test_sitemap_filter_with_alternate_links(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/article_1/</loc>\n            <lastmod>2010-01-01</lastmod>\n            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n                href=\"http://www.example.com/deutsch/article_1/\"/>\n        </url>\n        <url>\n            <loc>http://www.example.com/english/article_2/</loc>\n            <lastmod>2015-01-01</lastmod>\n        </url>\n    </urlset>\"\"\"\n\n        class FilteredSitemapSpider(self.spider_class):\n            def sitemap_filter(self, entries):\n                for entry in entries:\n                    alternate_links = entry.get(\"alternate\", ())\n                    for link in alternate_links:\n                        if \"/deutsch/\" in link:\n                            entry[\"loc\"] = link\n                            yield entry\n\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/english/article_1/\",\n            \"http://www.example.com/english/article_2/\",\n        ]\n\n        spider = FilteredSitemapSpider(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/deutsch/article_1/\"\n        ]\n\n    def test_sitemapindex_filter(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n        <sitemap>\n            <loc>http://www.example.com/sitemap1.xml</loc>\n            <lastmod>2004-01-01T20:00:00+00:00</lastmod>\n        </sitemap>\n        <sitemap>\n            <loc>http://www.example.com/sitemap2.xml</loc>\n            <lastmod>2005-01-01</lastmod>\n        </sitemap>\n    </sitemapindex>\"\"\"\n\n        class FilteredSitemapSpider(self.spider_class):", "n_tokens": 1178, "byte_len": 4681, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 586, "end_line": 698}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 6, "symbols": ["sitemap_filter", "test_compression_bomb_setting", "test_compression_bomb_spider_attr", "test_compression_bomb_request_meta", "test_download_warnsize_setting", "test_download_warnsize_spider_attr", "DownloadMaxSizeSpider", "DownloadWarnSizeSpider", "sitemap", "sitemap1", "date", "time", "filtered", "after", "spiders", "test", "compression", "https", "path", "get", "crawler", "pytest", "settings", "download", "warn", "than", "none", "http", "spider", "class", "test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "dummy_process_links", "test_process_links_filter"], "ast_kind": "class_or_type", "text": "            def sitemap_filter(self, entries):\n                for entry in entries:\n                    date_time = datetime.strptime(\n                        entry[\"lastmod\"].split(\"T\")[0], \"%Y-%m-%d\"\n                    )\n                    if date_time.year > 2004:\n                        yield entry\n\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/sitemap1.xml\",\n            \"http://www.example.com/sitemap2.xml\",\n        ]\n\n        spider = FilteredSitemapSpider(\"example.com\")\n        assert [req.url for req in spider._parse_sitemap(r)] == [\n            \"http://www.example.com/sitemap2.xml\"\n        ]\n\n    def test_compression_bomb_setting(self):\n        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n        crawler = get_crawler(settings_dict=settings)\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(url=\"https://example.com\")\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        assert spider._get_sitemap_body(response) is None\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_compression_bomb_spider_attr(self):\n        class DownloadMaxSizeSpider(self.spider_class):\n            download_maxsize = 10_000_000\n\n        crawler = get_crawler()\n        spider = DownloadMaxSizeSpider.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(url=\"https://example.com\")\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        assert spider._get_sitemap_body(response) is None\n\n    def test_compression_bomb_request_meta(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(\n            url=\"https://example.com\", meta={\"download_maxsize\": 10_000_000}\n        )\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        assert spider._get_sitemap_body(response) is None\n\n    def test_download_warnsize_setting(self):\n        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n        crawler = get_crawler(settings_dict=settings)\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(url=\"https://example.com\")\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        with LogCapture(\n            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n        ) as log:\n            spider._get_sitemap_body(response)\n        log.check(\n            (\n                \"scrapy.spiders.sitemap\",\n                \"WARNING\",\n                (\n                    \"<200 https://example.com> body size after decompression \"\n                    \"(11511612 B) is larger than the download warning size \"\n                    \"(10000000 B).\"\n                ),\n            ),\n        )\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_download_warnsize_spider_attr(self):\n        class DownloadWarnSizeSpider(self.spider_class):\n            download_warnsize = 10_000_000\n\n        crawler = get_crawler()\n        spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(\n            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n        )\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        with LogCapture(\n            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n        ) as log:\n            spider._get_sitemap_body(response)\n        log.check(\n            (\n                \"scrapy.spiders.sitemap\",\n                \"WARNING\",\n                (\n                    \"<200 https://example.com> body size after decompression \"\n                    \"(11511612 B) is larger than the download warning size \"\n                    \"(10000000 B).\"\n                ),\n            ),\n        )\n", "n_tokens": 1020, "byte_len": 4645, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 699, "end_line": 805}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider.py", "rel_path": "tests/test_spider.py", "module": "tests.test_spider", "ext": "py", "chunk_number": 7, "symbols": ["test_download_warnsize_request_meta", "test_crawl_spider", "test_undefined_parse_method", "TestSpider", "TestDeprecation", "TestNoParseMethodSpider", "async", "after", "spider", "name", "deferred", "from", "dont", "filter", "spiders", "https", "path", "test", "parse", "download", "get", "crawler", "pytest", "isinstance", "than", "undefined", "random", "http", "match", "class", "test_base_spider", "test_spider_args", "test_spider_without_name", "test_from_crawler_crawler_and_settings_population", "test_from_crawler_init_call", "test_closed_signal_call", "closed", "test_update_settings", "test_settings_in_from_crawler", "from_crawler", "test_logger", "test_log", "test_register_namespace", "parse_node", "test_parse_rows", "parse_row", "test_rule_without_link_extractor", "test_process_links", "dummy_process_links", "test_process_links_filter"], "ast_kind": "class_or_type", "text": "    def test_download_warnsize_request_meta(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(\n            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n        )\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        with LogCapture(\n            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n        ) as log:\n            spider._get_sitemap_body(response)\n        log.check(\n            (\n                \"scrapy.spiders.sitemap\",\n                \"WARNING\",\n                (\n                    \"<200 https://example.com> body size after decompression \"\n                    \"(11511612 B) is larger than the download warning size \"\n                    \"(10000000 B).\"\n                ),\n            ),\n        )\n\n    @deferred_f_from_coro_f\n    async def test_sitemap_urls(self):\n        class TestSpider(self.spider_class):\n            name = \"test\"\n            sitemap_urls = [\"https://toscrape.com/sitemap.xml\"]\n\n        crawler = get_crawler(TestSpider)\n        spider = TestSpider.from_crawler(crawler)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            requests = [request async for request in spider.start()]\n\n        assert len(requests) == 1\n        request = requests[0]\n        assert request.url == \"https://toscrape.com/sitemap.xml\"\n        assert request.dont_filter is False\n        assert request.callback == spider._parse_sitemap\n\n\nclass TestDeprecation:\n    def test_crawl_spider(self):\n        assert issubclass(CrawlSpider, Spider)\n        assert isinstance(CrawlSpider(name=\"foo\"), Spider)\n\n\nclass TestNoParseMethodSpider:\n    spider_class = Spider\n\n    def test_undefined_parse_method(self):\n        spider = self.spider_class(\"example.com\")\n        text = b\"Random text\"\n        resp = TextResponse(url=\"http://www.example.com/random_url\", body=text)\n\n        exc_msg = \"Spider.parse callback is not defined\"\n        with pytest.raises(NotImplementedError, match=exc_msg):\n            spider.parse(resp)\n", "n_tokens": 481, "byte_len": 2236, "file_sha1": "feb1cb7ff64fd82169ba31a5455cf421ba9fd327", "start_line": 806, "end_line": 867}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_squeues_request.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_squeues_request.py", "rel_path": "tests/test_squeues_request.py", "module": "tests.test_squeues_request", "ext": "py", "chunk_number": 1, "symbols": ["crawler", "is_fifo", "test_one_element", "test_order", "q", "TestRequestQueueBase", "TestPickleFifoDiskQueueRequest", "TestPickleLifoDiskQueueRequest", "TestMarshalFifoDiskQueueRequest", "TestMarshalLifoDiskQueueRequest", "TestFifoMemoryQueueRequest", "TestLifoMemoryQueueRequest", "does", "test", "one", "bool", "queues", "marshal", "spider", "spiders", "future", "typ", "checking", "pickle", "lifo", "get", "squeues", "pytest", "queuelib", "fifo", "none", "fixture", "request", "http", "match", "requests", "implement", "typing", "define", "return", "annotations", "push", "class", "not", "implemented", "example", "memory", "from", "req", "req3"], "ast_kind": "class_or_type", "text": "\"\"\"\nQueues that handle requests\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING\n\nimport pytest\nimport queuelib\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.squeues import (\n    FifoMemoryQueue,\n    LifoMemoryQueue,\n    MarshalFifoDiskQueue,\n    MarshalLifoDiskQueue,\n    PickleFifoDiskQueue,\n    PickleLifoDiskQueue,\n)\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nHAVE_PEEK = hasattr(queuelib.queue.FifoMemoryQueue, \"peek\")\n\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider)\n\n\nclass TestRequestQueueBase(ABC):\n    @property\n    @abstractmethod\n    def is_fifo(self) -> bool:\n        raise NotImplementedError\n\n    @pytest.mark.parametrize(\"test_peek\", [True, False])\n    def test_one_element(self, q: queuelib.queue.BaseQueue, test_peek: bool):\n        if test_peek and not HAVE_PEEK:\n            pytest.skip(\"The queuelib queues do not define peek\")\n        if not test_peek and HAVE_PEEK:\n            pytest.skip(\"The queuelib queues define peek\")\n        assert len(q) == 0\n        if test_peek:\n            assert q.peek() is None\n        assert q.pop() is None\n        req = Request(\"http://www.example.com\")\n        q.push(req)\n        assert len(q) == 1\n        if test_peek:\n            result = q.peek()\n            assert result is not None\n            assert result.url == req.url\n        else:\n            with pytest.raises(\n                NotImplementedError,\n                match=\"The underlying queue class does not implement 'peek'\",\n            ):\n                q.peek()\n        result = q.pop()\n        assert result is not None\n        assert result.url == req.url\n        assert len(q) == 0\n        if test_peek:\n            assert q.peek() is None\n        assert q.pop() is None\n        q.close()\n\n    @pytest.mark.parametrize(\"test_peek\", [True, False])\n    def test_order(self, q: queuelib.queue.BaseQueue, test_peek: bool):\n        if test_peek and not HAVE_PEEK:\n            pytest.skip(\"The queuelib queues do not define peek\")\n        if not test_peek and HAVE_PEEK:\n            pytest.skip(\"The queuelib queues define peek\")\n        assert len(q) == 0\n        if test_peek:\n            assert q.peek() is None\n        assert q.pop() is None\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        q.push(req1)\n        q.push(req2)\n        q.push(req3)\n        if not test_peek:\n            with pytest.raises(\n                NotImplementedError,\n                match=\"The underlying queue class does not implement 'peek'\",\n            ):\n                q.peek()\n        reqs = [req1, req2, req3] if self.is_fifo else [req3, req2, req1]\n        for i, req in enumerate(reqs):\n            assert len(q) == 3 - i\n            if test_peek:\n                result = q.peek()\n                assert result is not None\n                assert result.url == req.url\n            result = q.pop()\n            assert result is not None\n            assert result.url == req.url\n        assert len(q) == 0\n        if test_peek:\n            assert q.peek() is None\n        assert q.pop() is None\n        q.close()\n\n\nclass TestPickleFifoDiskQueueRequest(TestRequestQueueBase):\n    is_fifo = True\n\n    @pytest.fixture\n    def q(self, crawler, tmp_path):\n        return PickleFifoDiskQueue.from_crawler(\n            crawler=crawler, key=str(tmp_path / \"pickle\" / \"fifo\")\n        )\n\n\nclass TestPickleLifoDiskQueueRequest(TestRequestQueueBase):\n    is_fifo = False\n\n    @pytest.fixture\n    def q(self, crawler, tmp_path):\n        return PickleLifoDiskQueue.from_crawler(\n            crawler=crawler, key=str(tmp_path / \"pickle\" / \"lifo\")\n        )\n\n\nclass TestMarshalFifoDiskQueueRequest(TestRequestQueueBase):\n    is_fifo = True\n\n    @pytest.fixture\n    def q(self, crawler, tmp_path):\n        return MarshalFifoDiskQueue.from_crawler(\n            crawler=crawler, key=str(tmp_path / \"marshal\" / \"fifo\")\n        )\n\n\nclass TestMarshalLifoDiskQueueRequest(TestRequestQueueBase):\n    is_fifo = False\n\n    @pytest.fixture\n    def q(self, crawler, tmp_path):\n        return MarshalLifoDiskQueue.from_crawler(\n            crawler=crawler, key=str(tmp_path / \"marshal\" / \"lifo\")\n        )\n\n\nclass TestFifoMemoryQueueRequest(TestRequestQueueBase):\n    is_fifo = True\n\n    @pytest.fixture\n    def q(self, crawler):\n        return FifoMemoryQueue.from_crawler(crawler=crawler)\n\n\nclass TestLifoMemoryQueueRequest(TestRequestQueueBase):\n    is_fifo = False\n\n    @pytest.fixture\n    def q(self, crawler):\n        return LifoMemoryQueue.from_crawler(crawler=crawler)\n", "n_tokens": 1135, "byte_len": 4752, "file_sha1": "6f75fb1072c298d7bcd6e4082dae890f0d829c9a", "start_line": 1, "end_line": 168}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pqueues.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pqueues.py", "rel_path": "tests/test_pqueues.py", "module": "tests.test_pqueues", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_queue_push_pop_one", "test_no_peek_raises", "test_peek", "test_queue_push_pop_priorities", "teardown_method", "test_push_pop", "TestPriorityQueue", "TestDownloaderAwarePriorityQueue", "underlying", "does", "downloader", "hasattr", "teardown", "method", "implement", "aware", "spider", "mkdtemp", "priority", "req", "req2", "queue", "test", "push", "class", "with", "tempfile", "defined", "spiders", "test_pop_order", "make_url", "make_request", "schedule", "memor", "append", "start", "request", "https", "get", "crawler", "squeues", "queuelib", "pytest", "input", "requests", "settings", "dequeued", "none", "misc"], "ast_kind": "class_or_type", "text": "import tempfile\n\nimport pytest\nimport queuelib\n\nfrom scrapy.http.request import Request\nfrom scrapy.pqueues import DownloaderAwarePriorityQueue, ScrapyPriorityQueue\nfrom scrapy.spiders import Spider\nfrom scrapy.squeues import FifoMemoryQueue\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.test import get_crawler\nfrom tests.test_scheduler import MockDownloader, MockEngine\n\n\nclass TestPriorityQueue:\n    def setup_method(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"foo\")\n\n    def test_queue_push_pop_one(self):\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir\n        )\n        assert queue.pop() is None\n        assert len(queue) == 0\n        req1 = Request(\"https://example.org/1\", priority=1)\n        queue.push(req1)\n        assert len(queue) == 1\n        dequeued = queue.pop()\n        assert len(queue) == 0\n        assert dequeued.url == req1.url\n        assert dequeued.priority == req1.priority\n        assert not queue.close()\n\n    def test_no_peek_raises(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir\n        )\n        queue.push(Request(\"https://example.org\"))\n        with pytest.raises(\n            NotImplementedError,\n            match=\"The underlying queue class does not implement 'peek'\",\n        ):\n            queue.peek()\n        queue.close()\n\n    def test_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir\n        )\n        assert len(queue) == 0\n        assert queue.peek() is None\n        req1 = Request(\"https://example.org/1\")\n        req2 = Request(\"https://example.org/2\")\n        req3 = Request(\"https://example.org/3\")\n        queue.push(req1)\n        queue.push(req2)\n        queue.push(req3)\n        assert len(queue) == 3\n        assert queue.peek().url == req1.url\n        assert queue.pop().url == req1.url\n        assert len(queue) == 2\n        assert queue.peek().url == req2.url\n        assert queue.pop().url == req2.url\n        assert len(queue) == 1\n        assert queue.peek().url == req3.url\n        assert queue.pop().url == req3.url\n        assert not queue.close()\n\n    def test_queue_push_pop_priorities(self):\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3]\n        )\n        assert queue.pop() is None\n        assert len(queue) == 0\n        req1 = Request(\"https://example.org/1\", priority=1)\n        req2 = Request(\"https://example.org/2\", priority=2)\n        req3 = Request(\"https://example.org/3\", priority=3)\n        queue.push(req1)\n        queue.push(req2)\n        queue.push(req3)\n        assert len(queue) == 3\n        dequeued = queue.pop()\n        assert len(queue) == 2\n        assert dequeued.url == req3.url\n        assert dequeued.priority == req3.priority\n        assert set(queue.close()) == {-1, -2}\n\n\nclass TestDownloaderAwarePriorityQueue:\n    def setup_method(self):\n        crawler = get_crawler(Spider)\n        crawler.engine = MockEngine(downloader=MockDownloader())\n        self.queue = DownloaderAwarePriorityQueue.from_crawler(\n            crawler=crawler,\n            downstream_queue_cls=FifoMemoryQueue,\n            key=\"foo/bar\",\n        )\n\n    def teardown_method(self):\n        self.queue.close()\n\n    def test_push_pop(self):\n        assert len(self.queue) == 0\n        assert self.queue.pop() is None\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        self.queue.push(req1)\n        self.queue.push(req2)\n        self.queue.push(req3)\n        assert len(self.queue) == 3\n        assert self.queue.pop().url == req1.url\n        assert len(self.queue) == 2\n        assert self.queue.pop().url == req2.url\n        assert len(self.queue) == 1\n        assert self.queue.pop().url == req3.url\n        assert len(self.queue) == 0\n        assert self.queue.pop() is None\n\n    def test_no_peek_raises(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n        self.queue.push(Request(\"https://example.org\"))\n        with pytest.raises(\n            NotImplementedError,\n            match=\"The underlying queue class does not implement 'peek'\",\n        ):\n            self.queue.peek()\n", "n_tokens": 1155, "byte_len": 4923, "file_sha1": "6be9d4f5706285a89b5b187680e64014ac5fddd6", "start_line": 1, "end_line": 138}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pqueues.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pqueues.py", "rel_path": "tests/test_pqueues.py", "module": "tests.test_pqueues", "ext": "py", "chunk_number": 2, "symbols": ["test_peek", "test_pop_order", "make_url", "make_request", "requests", "schedule", "memor", "make", "url", "hasattr", "false", "request", "append", "priority", "req", "req2", "return", "spider", "mark", "push", "last", "parametrize", "meta", "output", "start", "downstream", "queue", "https", "example", "toscrape", "setup_method", "test_queue_push_pop_one", "test_no_peek_raises", "test_queue_push_pop_priorities", "teardown_method", "test_push_pop", "TestPriorityQueue", "TestDownloaderAwarePriorityQueue", "does", "spiders", "get", "crawler", "squeues", "queuelib", "pytest", "test", "input", "settings", "dequeued", "none"], "ast_kind": "function_or_method", "text": "    def test_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n        assert len(self.queue) == 0\n        req1 = Request(\"https://example.org/1\")\n        req2 = Request(\"https://example.org/2\")\n        req3 = Request(\"https://example.org/3\")\n        self.queue.push(req1)\n        self.queue.push(req2)\n        self.queue.push(req3)\n        assert len(self.queue) == 3\n        assert self.queue.peek().url == req1.url\n        assert self.queue.pop().url == req1.url\n        assert len(self.queue) == 2\n        assert self.queue.peek().url == req2.url\n        assert self.queue.pop().url == req2.url\n        assert len(self.queue) == 1\n        assert self.queue.peek().url == req3.url\n        assert self.queue.pop().url == req3.url\n        assert self.queue.peek() is None\n\n\n@pytest.mark.parametrize(\n    (\"input_\", \"output\"),\n    [\n        # By default, start requests are FIFO, other requests are LIFO.\n        ([{}, {}], [2, 1]),\n        ([{\"start\": True}, {\"start\": True}], [1, 2]),\n        # Priority matters.\n        ([{\"priority\": 1}, {\"start\": True}], [1, 2]),\n        ([{}, {\"start\": True, \"priority\": 1}], [2, 1]),\n        # For the same priority, start requests pop last.\n        ([{}, {\"start\": True}], [1, 2]),\n        ([{\"start\": True}, {}], [2, 1]),\n    ],\n)\ndef test_pop_order(input_, output):\n    def make_url(index):\n        return f\"https://toscrape.com/{index}\"\n\n    def make_request(index, data):\n        meta = {}\n        if data.get(\"start\", False):\n            meta[\"is_start_request\"] = True\n        return Request(\n            url=make_url(index),\n            priority=data.get(\"priority\", 0),\n            meta=meta,\n        )\n\n    input_requests = [\n        make_request(index, data) for index, data in enumerate(input_, start=1)\n    ]\n    expected_output_urls = [make_url(index) for index in output]\n\n    crawler = get_crawler(Spider)\n    settings = crawler.settings\n    queue = build_from_crawler(\n        ScrapyPriorityQueue,\n        crawler,\n        downstream_queue_cls=load_object(settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n        key=\"\",\n        start_queue_cls=load_object(settings[\"SCHEDULER_START_MEMORY_QUEUE\"]),\n    )\n\n    for request in input_requests:\n        queue.push(request)\n\n    actual_output_urls = []\n    while request := queue.pop():\n        actual_output_urls.append(request.url)\n\n    assert actual_output_urls == expected_output_urls\n", "n_tokens": 606, "byte_len": 2483, "file_sha1": "6be9d4f5706285a89b5b187680e64014ac5fddd6", "start_line": 139, "end_line": 212}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_base.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_base.py", "rel_path": "tests/test_spidermiddleware_base.py", "module": "tests.test_spidermiddleware_base", "ext": "py", "chunk_number": 1, "symbols": ["crawler", "test_trivial", "test_processed_request", "get_processed_request", "test_processed_item", "get_processed_item", "test_processed_both", "TrivialSpiderMiddleware", "ProcessReqSpiderMiddleware", "ProcessItemSpiderMiddleware", "ProcessBothSpiderMiddleware", "processed", "test", "req", "hasattr", "pass", "process", "both", "typing", "return", "spider", "item", "annotations", "class", "start", "trivial", "ignore", "output", "scrapy", "future", "typ", "checking", "get", "base", "from", "pytest", "list", "assert", "request", "isinstance", "spidermiddlewares", "none", "fixture", "data", "utils", "type", "import", "http", "self", "response"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import Response\nfrom scrapy.spidermiddlewares.base import BaseSpiderMiddleware\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider)\n\n\ndef test_trivial(crawler: Crawler) -> None:\n    class TrivialSpiderMiddleware(BaseSpiderMiddleware):\n        pass\n\n    mw = TrivialSpiderMiddleware.from_crawler(crawler)\n    assert hasattr(mw, \"crawler\")\n    assert mw.crawler is crawler\n    test_req = Request(\"data:,\")\n    spider_output = [test_req, {\"foo\": \"bar\"}]\n    for processed in [\n        list(mw.process_spider_output(Response(\"data:,\"), spider_output)),\n        list(mw.process_start_requests(spider_output, None)),  # type: ignore[arg-type]\n    ]:\n        assert processed == [test_req, {\"foo\": \"bar\"}]\n\n\ndef test_processed_request(crawler: Crawler) -> None:\n    class ProcessReqSpiderMiddleware(BaseSpiderMiddleware):\n        def get_processed_request(\n            self, request: Request, response: Response | None\n        ) -> Request | None:\n            if request.url == \"data:2,\":\n                return None\n            if request.url == \"data:3,\":\n                return Request(\"data:30,\")\n            return request\n\n    mw = ProcessReqSpiderMiddleware.from_crawler(crawler)\n    test_req1 = Request(\"data:1,\")\n    test_req2 = Request(\"data:2,\")\n    test_req3 = Request(\"data:3,\")\n    spider_output = [test_req1, {\"foo\": \"bar\"}, test_req2, test_req3]\n    for processed in [\n        list(mw.process_spider_output(Response(\"data:,\"), spider_output)),\n        list(mw.process_start_requests(spider_output, None)),  # type: ignore[arg-type]\n    ]:\n        assert len(processed) == 3\n        assert isinstance(processed[0], Request)\n        assert processed[0].url == \"data:1,\"\n        assert processed[1] == {\"foo\": \"bar\"}\n        assert isinstance(processed[2], Request)\n        assert processed[2].url == \"data:30,\"\n\n\ndef test_processed_item(crawler: Crawler) -> None:\n    class ProcessItemSpiderMiddleware(BaseSpiderMiddleware):\n        def get_processed_item(self, item: Any, response: Response | None) -> Any:\n            if item[\"foo\"] == 2:\n                return None\n            if item[\"foo\"] == 3:\n                item[\"foo\"] = 30\n            return item\n\n    mw = ProcessItemSpiderMiddleware.from_crawler(crawler)\n    test_req = Request(\"data:,\")\n    spider_output = [{\"foo\": 1}, {\"foo\": 2}, test_req, {\"foo\": 3}]\n    for processed in [\n        list(mw.process_spider_output(Response(\"data:,\"), spider_output)),\n        list(mw.process_start_requests(spider_output, None)),  # type: ignore[arg-type]\n    ]:\n        assert processed == [{\"foo\": 1}, test_req, {\"foo\": 30}]\n\n\ndef test_processed_both(crawler: Crawler) -> None:\n    class ProcessBothSpiderMiddleware(BaseSpiderMiddleware):\n        def get_processed_request(\n            self, request: Request, response: Response | None\n        ) -> Request | None:\n            if request.url == \"data:2,\":\n                return None\n            if request.url == \"data:3,\":\n                return Request(\"data:30,\")\n            return request\n\n        def get_processed_item(self, item: Any, response: Response | None) -> Any:\n            if item[\"foo\"] == 2:\n                return None\n            if item[\"foo\"] == 3:\n                item[\"foo\"] = 30\n            return item\n\n    mw = ProcessBothSpiderMiddleware.from_crawler(crawler)\n    test_req1 = Request(\"data:1,\")\n    test_req2 = Request(\"data:2,\")\n    test_req3 = Request(\"data:3,\")\n    spider_output = [\n        test_req1,\n        {\"foo\": 1},\n        {\"foo\": 2},\n        test_req2,\n        {\"foo\": 3},\n        test_req3,\n    ]\n    for processed in [\n        list(mw.process_spider_output(Response(\"data:,\"), spider_output)),\n        list(mw.process_start_requests(spider_output, None)),  # type: ignore[arg-type]\n    ]:\n        assert len(processed) == 4\n        assert isinstance(processed[0], Request)\n        assert processed[0].url == \"data:1,\"\n        assert processed[1] == {\"foo\": 1}\n        assert processed[2] == {\"foo\": 30}\n        assert isinstance(processed[3], Request)\n        assert processed[3].url == \"data:30,\"\n", "n_tokens": 1060, "byte_len": 4314, "file_sha1": "8ce6f7735323b90758423ddc7ab3bf6906a546bc", "start_line": 1, "end_line": 125}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler.py", "rel_path": "tests/test_scheduler.py", "module": "tests.test_scheduler", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "enqueue_request", "has_pending_requests", "next_request", "pause", "unpause", "get_slot_key", "increment", "decrement", "close", "priority_queue_cls", "create_scheduler", "close_scheduler", "setup_method", "teardown_method", "test_length", "test_dequeue", "test_dequeue_priorities", "MemoryScheduler", "MockEngine", "MockSlot", "MockDownloader", "MockCrawler", "SchedulerHandler", "TestSchedulerInMemoryBase", "TestSchedulerOnDiskBase", "while", "schedule", "memor", "bool", "test_migration", "_is_scheduling_fair", "test_logic", "parse", "test_integration_downloader_aware_priority_queue", "_incompatible", "test_incompatibility", "TestSchedulerInMemory", "TestSchedulerOnDisk", "TestMigration", "PrevSchedulerHandler", "NextSchedulerHandler", "DownloaderAwareSchedulerTestMixin", "TestSchedulerWithDownloaderAwareInMemory", "TestSchedulerWithDownloaderAwareOnDisk", "StartUrlsSpider", "TestIntegrationWithDownloaderAwareInMemory", "TestIncompatibility", "does", "spidercls"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport shutil\nimport tempfile\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections import deque\nfrom typing import Any, NamedTuple\n\nimport pytest\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.core.downloader import Downloader\nfrom scrapy.core.scheduler import BaseScheduler, Scheduler\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import _schedule_coro\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\n\n\nclass MemoryScheduler(BaseScheduler):\n    paused = False\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.queue = deque(\n            Request(value) if isinstance(value, str) else value\n            for value in getattr(self, \"queue\", [])\n        )\n\n    def enqueue_request(self, request: Request) -> bool:\n        self.queue.append(request)\n        return True\n\n    def has_pending_requests(self) -> bool:\n        return self.paused or bool(self.queue)\n\n    def next_request(self) -> Request | None:\n        if self.paused:\n            return None\n        try:\n            return self.queue.pop()\n        except IndexError:\n            return None\n\n    def pause(self) -> None:\n        self.paused = True\n\n    def unpause(self) -> None:\n        self.paused = False\n\n\nclass MockEngine(NamedTuple):\n    downloader: MockDownloader\n\n\nclass MockSlot(NamedTuple):\n    active: list[Any]\n\n\nclass MockDownloader:\n    def __init__(self):\n        self.slots = {}\n\n    def get_slot_key(self, request):\n        if Downloader.DOWNLOAD_SLOT in request.meta:\n            return request.meta[Downloader.DOWNLOAD_SLOT]\n\n        return urlparse_cached(request).hostname or \"\"\n\n    def increment(self, slot_key):\n        slot = self.slots.setdefault(slot_key, MockSlot(active=[]))\n        slot.active.append(1)\n\n    def decrement(self, slot_key):\n        slot = self.slots.get(slot_key)\n        slot.active.pop()\n\n    def close(self):\n        pass\n\n\nclass MockCrawler(Crawler):\n    def __init__(self, priority_queue_cls, jobdir):\n        settings = {\n            \"SCHEDULER_DEBUG\": False,\n            \"SCHEDULER_DISK_QUEUE\": \"scrapy.squeues.PickleLifoDiskQueue\",\n            \"SCHEDULER_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n            \"SCHEDULER_PRIORITY_QUEUE\": priority_queue_cls,\n            \"JOBDIR\": jobdir,\n            \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n        }\n        super().__init__(Spider, settings)\n        self.engine = MockEngine(downloader=MockDownloader())\n        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n\n\nclass SchedulerHandler(ABC):\n    jobdir = None\n\n    @property\n    @abstractmethod\n    def priority_queue_cls(self) -> str:\n        raise NotImplementedError\n\n    def create_scheduler(self):\n        self.mock_crawler = MockCrawler(self.priority_queue_cls, self.jobdir)\n        self.scheduler = Scheduler.from_crawler(self.mock_crawler)\n        self.spider = Spider(name=\"spider\")\n        self.scheduler.open(self.spider)\n\n    def close_scheduler(self):\n        self.scheduler.close(\"finished\")\n        _schedule_coro(self.mock_crawler.stop_async())\n        self.mock_crawler.engine.downloader.close()\n\n    def setup_method(self):\n        self.create_scheduler()\n\n    def teardown_method(self):\n        self.close_scheduler()\n\n\n_PRIORITIES = [\n    (\"http://foo.com/a\", -2),\n    (\"http://foo.com/d\", 1),\n    (\"http://foo.com/b\", -1),\n    (\"http://foo.com/c\", 0),\n    (\"http://foo.com/e\", 2),\n]\n\n\n_URLS = {\"http://foo.com/a\", \"http://foo.com/b\", \"http://foo.com/c\"}\n\n\nclass TestSchedulerInMemoryBase(SchedulerHandler):\n    def test_length(self):\n        assert not self.scheduler.has_pending_requests()\n        assert len(self.scheduler) == 0\n\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        assert self.scheduler.has_pending_requests()\n        assert len(self.scheduler) == len(_URLS)\n\n    def test_dequeue(self):\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        urls = set()\n        while self.scheduler.has_pending_requests():\n            urls.add(self.scheduler.next_request().url)\n\n        assert urls == _URLS\n\n    def test_dequeue_priorities(self):\n        for url, priority in _PRIORITIES:\n            self.scheduler.enqueue_request(Request(url, priority=priority))\n\n        priorities = []\n        while self.scheduler.has_pending_requests():\n            priorities.append(self.scheduler.next_request().priority)\n\n        assert priorities == sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n\n\nclass TestSchedulerOnDiskBase(SchedulerHandler):\n    def setup_method(self):\n        self.jobdir = tempfile.mkdtemp()\n        self.create_scheduler()\n\n    def teardown_method(self):\n        self.close_scheduler()\n\n        shutil.rmtree(self.jobdir)\n        self.jobdir = None\n\n    def test_length(self):\n        assert not self.scheduler.has_pending_requests()\n        assert len(self.scheduler) == 0\n\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        self.close_scheduler()\n        self.create_scheduler()\n\n        assert self.scheduler.has_pending_requests()\n        assert len(self.scheduler) == len(_URLS)\n\n    def test_dequeue(self):\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        self.close_scheduler()\n        self.create_scheduler()\n\n        urls = set()\n        while self.scheduler.has_pending_requests():\n            urls.add(self.scheduler.next_request().url)\n\n        assert urls == _URLS\n", "n_tokens": 1228, "byte_len": 5769, "file_sha1": "3f58513cb1f611ba433d1688f8e83747d50428d0", "start_line": 1, "end_line": 208}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler.py", "rel_path": "tests/test_scheduler.py", "module": "tests.test_scheduler", "ext": "py", "chunk_number": 2, "symbols": ["test_dequeue_priorities", "priority_queue_cls", "test_migration", "_is_scheduling_fair", "test_logic", "__init__", "parse", "setup_method", "test_integration_downloader_aware_priority_queue", "_incompatible", "TestSchedulerInMemory", "TestSchedulerOnDisk", "TestMigration", "PrevSchedulerHandler", "NextSchedulerHandler", "DownloaderAwareSchedulerTestMixin", "TestSchedulerWithDownloaderAwareInMemory", "TestSchedulerWithDownloaderAwareOnDisk", "StartUrlsSpider", "TestIntegrationWithDownloaderAwareInMemory", "TestIncompatibility", "spidercls", "test", "scheduler", "append", "priorities", "spider", "name", "handler", "get", "enqueue_request", "has_pending_requests", "next_request", "pause", "unpause", "get_slot_key", "increment", "decrement", "close", "create_scheduler", "close_scheduler", "teardown_method", "test_length", "test_dequeue", "test_incompatibility", "MemoryScheduler", "MockEngine", "MockSlot", "MockDownloader", "MockCrawler"], "ast_kind": "class_or_type", "text": "    def test_dequeue_priorities(self):\n        for url, priority in _PRIORITIES:\n            self.scheduler.enqueue_request(Request(url, priority=priority))\n\n        self.close_scheduler()\n        self.create_scheduler()\n\n        priorities = []\n        while self.scheduler.has_pending_requests():\n            priorities.append(self.scheduler.next_request().priority)\n\n        assert priorities == sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n\n\nclass TestSchedulerInMemory(TestSchedulerInMemoryBase):\n    @property\n    def priority_queue_cls(self) -> str:\n        return \"scrapy.pqueues.ScrapyPriorityQueue\"\n\n\nclass TestSchedulerOnDisk(TestSchedulerOnDiskBase):\n    @property\n    def priority_queue_cls(self) -> str:\n        return \"scrapy.pqueues.ScrapyPriorityQueue\"\n\n\n_URLS_WITH_SLOTS = [\n    (\"http://foo.com/a\", \"a\"),\n    (\"http://foo.com/b\", \"a\"),\n    (\"http://foo.com/c\", \"b\"),\n    (\"http://foo.com/d\", \"b\"),\n    (\"http://foo.com/e\", \"c\"),\n    (\"http://foo.com/f\", \"c\"),\n]\n\n\nclass TestMigration:\n    def test_migration(self, tmpdir):\n        class PrevSchedulerHandler(SchedulerHandler):\n            jobdir = tmpdir\n\n            @property\n            def priority_queue_cls(self) -> str:\n                return \"scrapy.pqueues.ScrapyPriorityQueue\"\n\n        class NextSchedulerHandler(SchedulerHandler):\n            jobdir = tmpdir\n\n            @property\n            def priority_queue_cls(self) -> str:\n                return \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n\n        prev_scheduler_handler = PrevSchedulerHandler()\n        prev_scheduler_handler.create_scheduler()\n        for url in _URLS:\n            prev_scheduler_handler.scheduler.enqueue_request(Request(url))\n        prev_scheduler_handler.close_scheduler()\n\n        next_scheduler_handler = NextSchedulerHandler()\n        with pytest.raises(\n            ValueError,\n            match=\"DownloaderAwarePriorityQueue accepts ``slot_startprios`` as a dict\",\n        ):\n            next_scheduler_handler.create_scheduler()\n\n\ndef _is_scheduling_fair(enqueued_slots, dequeued_slots):\n    \"\"\"\n    We enqueued same number of requests for every slot.\n    Assert correct order, e.g.\n\n    >>> enqueued = ['a', 'b', 'c'] * 2\n    >>> correct = ['a', 'c', 'b', 'b', 'a', 'c']\n    >>> incorrect = ['a', 'a', 'b', 'c', 'c', 'b']\n    >>> _is_scheduling_fair(enqueued, correct)\n    True\n    >>> _is_scheduling_fair(enqueued, incorrect)\n    False\n    \"\"\"\n    if len(dequeued_slots) != len(enqueued_slots):\n        return False\n\n    slots_number = len(set(enqueued_slots))\n    for i in range(0, len(dequeued_slots), slots_number):\n        part = dequeued_slots[i : i + slots_number]\n        if len(part) != len(set(part)):\n            return False\n\n    return True\n\n\nclass DownloaderAwareSchedulerTestMixin:\n    reopen = False\n\n    @property\n    def priority_queue_cls(self) -> str:\n        return \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n\n    def test_logic(self):\n        for url, slot in _URLS_WITH_SLOTS:\n            request = Request(url)\n            request.meta[Downloader.DOWNLOAD_SLOT] = slot\n            self.scheduler.enqueue_request(request)\n\n        if self.reopen:\n            self.close_scheduler()\n            self.create_scheduler()\n\n        dequeued_slots = []\n        requests = []\n        downloader = self.mock_crawler.engine.downloader\n        while self.scheduler.has_pending_requests():\n            request = self.scheduler.next_request()\n            slot = downloader.get_slot_key(request)\n            dequeued_slots.append(slot)\n            downloader.increment(slot)\n            requests.append(request)\n\n        for request in requests:\n            slot = downloader.get_slot_key(request)\n            downloader.decrement(slot)\n\n        assert _is_scheduling_fair([s for u, s in _URLS_WITH_SLOTS], dequeued_slots)\n        assert sum(len(s.active) for s in downloader.slots.values()) == 0\n\n\nclass TestSchedulerWithDownloaderAwareInMemory(\n    DownloaderAwareSchedulerTestMixin, TestSchedulerInMemoryBase\n):\n    pass\n\n\nclass TestSchedulerWithDownloaderAwareOnDisk(\n    DownloaderAwareSchedulerTestMixin, TestSchedulerOnDiskBase\n):\n    reopen = True\n\n\nclass StartUrlsSpider(Spider):\n    def __init__(self, start_urls):\n        self.start_urls = start_urls\n        super().__init__(name=\"StartUrlsSpider\")\n\n    def parse(self, response):\n        pass\n\n\nclass TestIntegrationWithDownloaderAwareInMemory:\n    def setup_method(self):\n        self.crawler = get_crawler(\n            spidercls=StartUrlsSpider,\n            settings_dict={\n                \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n                \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n            },\n        )\n\n    @inlineCallbacks\n    def test_integration_downloader_aware_priority_queue(self):\n        with MockServer() as mockserver:\n            url = mockserver.url(\"/status?n=200\", is_secure=False)\n            start_urls = [url] * 6\n            yield self.crawler.crawl(start_urls)\n            assert self.crawler.stats.get_value(\"downloader/response_count\") == len(\n                start_urls\n            )\n\n\nclass TestIncompatibility:\n    def _incompatible(self):\n        settings = {\n            \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n            \"CONCURRENT_REQUESTS_PER_IP\": 1,\n        }\n        crawler = get_crawler(Spider, settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        spider = Spider(name=\"spider\")\n        scheduler.open(spider)\n", "n_tokens": 1210, "byte_len": 5516, "file_sha1": "3f58513cb1f611ba433d1688f8e83747d50428d0", "start_line": 209, "end_line": 387}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_scheduler.py", "rel_path": "tests/test_scheduler.py", "module": "tests.test_scheduler", "ext": "py", "chunk_number": 3, "symbols": ["test_incompatibility", "warnings", "concurren", "request", "does", "pytest", "support", "with", "self", "filterwarnings", "ignore", "raises", "match", "incompatible", "test", "incompatibility", "value", "error", "catch", "__init__", "enqueue_request", "has_pending_requests", "next_request", "pause", "unpause", "get_slot_key", "increment", "decrement", "close", "priority_queue_cls", "create_scheduler", "close_scheduler", "setup_method", "teardown_method", "test_length", "test_dequeue", "test_dequeue_priorities", "test_migration", "_is_scheduling_fair", "test_logic", "parse", "test_integration_downloader_aware_priority_queue", "_incompatible", "MemoryScheduler", "MockEngine", "MockSlot", "MockDownloader", "MockCrawler", "SchedulerHandler", "TestSchedulerInMemoryBase"], "ast_kind": "function_or_method", "text": "    def test_incompatibility(self):\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n            with pytest.raises(\n                ValueError, match=\"does not support CONCURRENT_REQUESTS_PER_IP\"\n            ):\n                self._incompatible()\n", "n_tokens": 49, "byte_len": 286, "file_sha1": "3f58513cb1f611ba433d1688f8e83747d50428d0", "start_line": 388, "end_line": 395}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 1, "symbols": ["test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "TestResponseBase", "CustomResponse", "encoding", "get", "testdata", "presence", "method", "body", "bytes", "append", "lib", "w3lib", "selector", "custom", "response", "parse", "version", "mock", "preserve", "make", "pytest", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags", "test_follow_all_absolute", "test_follow_all_relative", "test_follow_all_links", "test_follow_all_empty", "test_follow_all_invalid", "test_follow_all_whitespace", "test_follow_all_whitespace_links", "test_follow_all_flags", "_assert_followed_url"], "ast_kind": "class_or_type", "text": "import codecs\nfrom unittest import mock\n\nimport pytest\nfrom packaging.version import Version as parse_version\nfrom w3lib import __version__ as w3lib_version\nfrom w3lib.encoding import resolve_encoding\n\nfrom scrapy.exceptions import NotSupported\nfrom scrapy.http import (\n    Headers,\n    HtmlResponse,\n    Request,\n    Response,\n    TextResponse,\n    XmlResponse,\n)\nfrom scrapy.link import Link\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import to_unicode\nfrom tests import get_testdata\n\n\nclass TestResponseBase:\n    response_class = Response\n\n    def test_init(self):\n        # Response requires url in the constructor\n        with pytest.raises(TypeError):\n            self.response_class()\n        assert isinstance(\n            self.response_class(\"http://example.com/\"), self.response_class\n        )\n        with pytest.raises(TypeError):\n            self.response_class(b\"http://example.com\")\n        with pytest.raises(TypeError):\n            self.response_class(url=\"http://example.com\", body={})\n        # body can be str or None\n        assert isinstance(\n            self.response_class(\"http://example.com/\", body=b\"\"),\n            self.response_class,\n        )\n        assert isinstance(\n            self.response_class(\"http://example.com/\", body=b\"body\"),\n            self.response_class,\n        )\n        # test presence of all optional parameters\n        assert isinstance(\n            self.response_class(\n                \"http://example.com/\", body=b\"\", headers={}, status=200\n            ),\n            self.response_class,\n        )\n\n        r = self.response_class(\"http://www.example.com\")\n        assert isinstance(r.url, str)\n        assert r.url == \"http://www.example.com\"\n        assert r.status == 200\n\n        assert isinstance(r.headers, Headers)\n        assert not r.headers\n\n        headers = {\"foo\": \"bar\"}\n        body = b\"a body\"\n        r = self.response_class(\"http://www.example.com\", headers=headers, body=body)\n\n        assert r.headers is not headers\n        assert r.headers[b\"foo\"] == b\"bar\"\n\n        r = self.response_class(\"http://www.example.com\", status=301)\n        assert r.status == 301\n        r = self.response_class(\"http://www.example.com\", status=\"301\")\n        assert r.status == 301\n        with pytest.raises(ValueError, match=r\"invalid literal for int\\(\\)\"):\n            self.response_class(\"http://example.com\", status=\"lala200\")\n\n    def test_copy(self):\n        \"\"\"Test Response copy\"\"\"\n\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        r1.flags.append(\"cached\")\n        r2 = r1.copy()\n\n        assert r1.status == r2.status\n        assert r1.body == r2.body\n\n        # make sure flags list is shallow copied\n        assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n        assert r1.flags == r2.flags\n\n        # make sure headers attribute is shallow copied\n        assert r1.headers is not r2.headers, (\n            \"headers must be a shallow copy, not identical\"\n        )\n        assert r1.headers == r2.headers\n\n    def test_copy_meta(self):\n        req = Request(\"http://www.example.com\")\n        req.meta[\"foo\"] = \"bar\"\n        r1 = self.response_class(\n            \"http://www.example.com\", body=b\"Some body\", request=req\n        )\n        assert r1.meta is req.meta\n\n    def test_copy_cb_kwargs(self):\n        req = Request(\"http://www.example.com\")\n        req.cb_kwargs[\"foo\"] = \"bar\"\n        r1 = self.response_class(\n            \"http://www.example.com\", body=b\"Some body\", request=req\n        )\n        assert r1.cb_kwargs is req.cb_kwargs\n\n    def test_unavailable_meta(self):\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        with pytest.raises(AttributeError, match=r\"Response\\.meta not available\"):\n            r1.meta\n\n    def test_unavailable_cb_kwargs(self):\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        with pytest.raises(AttributeError, match=r\"Response\\.cb_kwargs not available\"):\n            r1.cb_kwargs\n\n    def test_copy_inherited_classes(self):\n        \"\"\"Test Response children copies preserve their class\"\"\"\n\n        class CustomResponse(self.response_class):\n            pass\n\n        r1 = CustomResponse(\"http://www.example.com\")\n        r2 = r1.copy()\n\n        assert isinstance(r2, CustomResponse)\n\n    def test_replace(self):\n        \"\"\"Test Response.replace() method\"\"\"\n        hdrs = Headers({\"key\": \"value\"})\n        r1 = self.response_class(\"http://www.example.com\")\n        r2 = r1.replace(status=301, body=b\"New body\", headers=hdrs)\n        assert r1.body == b\"\"\n        assert r1.url == r2.url\n        assert (r1.status, r2.status) == (200, 301)\n        assert (r1.body, r2.body) == (b\"\", b\"New body\")\n        assert (r1.headers, r2.headers) == ({}, hdrs)\n\n        # Empty attributes (which may fail if not compared properly)\n        r3 = self.response_class(\"http://www.example.com\", flags=[\"cached\"])\n        r4 = r3.replace(body=b\"\", flags=[])\n        assert r4.body == b\"\"\n        assert not r4.flags\n\n    def _assert_response_values(self, response, encoding, body):\n        if isinstance(body, str):\n            body_unicode = body\n            body_bytes = body.encode(encoding)\n        else:\n            body_unicode = body.decode(encoding)\n            body_bytes = body\n\n        assert isinstance(response.body, bytes)\n        assert isinstance(response.text, str)\n        self._assert_response_encoding(response, encoding)\n        assert response.body == body_bytes\n        assert response.text == body_unicode\n", "n_tokens": 1227, "byte_len": 5591, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 1, "end_line": 164}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 2, "symbols": ["_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags", "test_follow_all_absolute", "test_follow_all_relative", "test_follow_all_links", "test_follow_all_empty", "test_follow_all_invalid", "test_follow_all_whitespace", "test_follow_all_whitespace_links", "encoding", "test", "shortcut", "xpath", "follow", "assert", "followed", "lib", "w3lib", "strict", "https", "parse", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "test_follow_all_flags", "_assert_followed_url", "_assert_followed_all_urls", "_links_response", "_links_response_no_href", "test_unicode_url", "test_unicode_body", "test_encoding", "test_declared_encoding_invalid", "test_utf16", "test_invalid_utf8_encoded_body_with_valid_utf8_BOM"], "ast_kind": "class_or_type", "text": "    def _assert_response_encoding(self, response, encoding):\n        assert response.encoding == resolve_encoding(encoding)\n\n    def test_immutable_attributes(self):\n        r = self.response_class(\"http://example.com\")\n        with pytest.raises(AttributeError):\n            r.url = \"http://example2.com\"\n        with pytest.raises(AttributeError):\n            r.body = \"xxx\"\n\n    def test_urljoin(self):\n        \"\"\"Test urljoin shortcut (only for existence, since behavior equals urljoin)\"\"\"\n        joined = self.response_class(\"http://www.example.com\").urljoin(\"/test\")\n        absolute = \"http://www.example.com/test\"\n        assert joined == absolute\n\n    def test_shortcut_attributes(self):\n        r = self.response_class(\"http://example.com\", body=b\"hello\")\n        if self.response_class == Response:\n            msg = \"Response content isn't text\"\n            with pytest.raises(AttributeError, match=msg):\n                r.text\n            with pytest.raises(NotSupported, match=msg):\n                r.css(\"body\")\n            with pytest.raises(NotSupported, match=msg):\n                r.xpath(\"//body\")\n            with pytest.raises(NotSupported, match=msg):\n                r.jmespath(\"body\")\n        else:\n            r.text\n            r.css(\"body\")\n            r.xpath(\"//body\")\n\n    # Response.follow\n\n    def test_follow_url_absolute(self):\n        self._assert_followed_url(\"http://foo.example.com\", \"http://foo.example.com\")\n\n    def test_follow_url_relative(self):\n        self._assert_followed_url(\"foo\", \"http://example.com/foo\")\n\n    def test_follow_link(self):\n        self._assert_followed_url(\n            Link(\"http://example.com/foo\"), \"http://example.com/foo\"\n        )\n\n    def test_follow_None_url(self):\n        r = self.response_class(\"http://example.com\")\n        with pytest.raises(ValueError, match=\"url can't be None\"):\n            r.follow(None)\n\n    @pytest.mark.xfail(\n        parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n        reason=\"https://github.com/scrapy/w3lib/pull/207\",\n        strict=True,\n    )\n    def test_follow_whitespace_url(self):\n        self._assert_followed_url(\"foo \", \"http://example.com/foo\")\n\n    @pytest.mark.xfail(\n        parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n        reason=\"https://github.com/scrapy/w3lib/pull/207\",\n        strict=True,\n    )\n    def test_follow_whitespace_link(self):\n        self._assert_followed_url(\n            Link(\"http://example.com/foo \"), \"http://example.com/foo\"\n        )\n\n    def test_follow_flags(self):\n        res = self.response_class(\"http://example.com/\")\n        fol = res.follow(\"http://example.com/\", flags=[\"cached\", \"allowed\"])\n        assert fol.flags == [\"cached\", \"allowed\"]\n\n    # Response.follow_all\n\n    def test_follow_all_absolute(self):\n        url_list = [\n            \"http://example.org\",\n            \"http://www.example.org\",\n            \"http://example.com\",\n            \"http://www.example.com\",\n        ]\n        self._assert_followed_all_urls(url_list, url_list)\n\n    def test_follow_all_relative(self):\n        relative = [\"foo\", \"bar\", \"foo/bar\", \"bar/foo\"]\n        absolute = [\n            \"http://example.com/foo\",\n            \"http://example.com/bar\",\n            \"http://example.com/foo/bar\",\n            \"http://example.com/bar/foo\",\n        ]\n        self._assert_followed_all_urls(relative, absolute)\n\n    def test_follow_all_links(self):\n        absolute = [\n            \"http://example.com/foo\",\n            \"http://example.com/bar\",\n            \"http://example.com/foo/bar\",\n            \"http://example.com/bar/foo\",\n        ]\n        links = map(Link, absolute)\n        self._assert_followed_all_urls(links, absolute)\n\n    def test_follow_all_empty(self):\n        r = self.response_class(\"http://example.com\")\n        assert not list(r.follow_all([]))\n\n    def test_follow_all_invalid(self):\n        r = self.response_class(\"http://example.com\")\n        if self.response_class == Response:\n            with pytest.raises(TypeError):\n                list(r.follow_all(urls=None))\n            with pytest.raises(TypeError):\n                list(r.follow_all(urls=12345))\n            with pytest.raises(ValueError, match=\"url can't be None\"):\n                list(r.follow_all(urls=[None]))\n        else:\n            with pytest.raises(\n                ValueError, match=\"Please supply exactly one of the following arguments\"\n            ):\n                list(r.follow_all(urls=None))\n            with pytest.raises(TypeError):\n                list(r.follow_all(urls=12345))\n            with pytest.raises(ValueError, match=\"url can't be None\"):\n                list(r.follow_all(urls=[None]))\n\n    def test_follow_all_whitespace(self):\n        relative = [\"foo \", \"bar \", \"foo/bar \", \"bar/foo \"]\n        absolute = [\n            \"http://example.com/foo%20\",\n            \"http://example.com/bar%20\",\n            \"http://example.com/foo/bar%20\",\n            \"http://example.com/bar/foo%20\",\n        ]\n        self._assert_followed_all_urls(relative, absolute)\n\n    def test_follow_all_whitespace_links(self):\n        absolute = [\n            \"http://example.com/foo \",\n            \"http://example.com/bar \",\n            \"http://example.com/foo/bar \",\n            \"http://example.com/bar/foo \",\n        ]\n        links = map(Link, absolute)\n        expected = [u.replace(\" \", \"%20\") for u in absolute]\n        self._assert_followed_all_urls(links, expected)\n", "n_tokens": 1158, "byte_len": 5431, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 165, "end_line": 313}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 3, "symbols": ["test_follow_all_flags", "_assert_followed_url", "_assert_followed_all_urls", "_links_response", "_links_response_no_href", "test_replace", "test_unicode_url", "test_unicode_body", "TestTextResponse", "encoding", "get", "testdata", "price", "test", "follow", "assert", "followed", "latin", "latin1", "cp1251", "target", "make", "u0447", "pytest", "hello", "unicode", "isinstance", "all", "none", "encode", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags", "test_follow_all_absolute"], "ast_kind": "class_or_type", "text": "    def test_follow_all_flags(self):\n        re = self.response_class(\"http://www.example.com/\")\n        urls = [\n            \"http://www.example.com/\",\n            \"http://www.example.com/2\",\n            \"http://www.example.com/foo\",\n        ]\n        fol = re.follow_all(urls, flags=[\"cached\", \"allowed\"])\n        for req in fol:\n            assert req.flags == [\"cached\", \"allowed\"]\n\n    def _assert_followed_url(self, follow_obj, target_url, response=None):\n        if response is None:\n            response = self._links_response()\n        req = response.follow(follow_obj)\n        assert req.url == target_url\n        return req\n\n    def _assert_followed_all_urls(self, follow_obj, target_urls, response=None):\n        if response is None:\n            response = self._links_response()\n        followed = response.follow_all(follow_obj)\n        for req, target in zip(followed, target_urls):\n            assert req.url == target\n            yield req\n\n    def _links_response(self):\n        body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n        return self.response_class(\"http://example.com/index\", body=body)\n\n    def _links_response_no_href(self):\n        body = get_testdata(\"link_extractor\", \"linkextractor_no_href.html\")\n        return self.response_class(\"http://example.com/index\", body=body)\n\n\nclass TestTextResponse(TestResponseBase):\n    response_class = TextResponse\n\n    def test_replace(self):\n        super().test_replace()\n        r1 = self.response_class(\n            \"http://www.example.com\", body=\"hello\", encoding=\"cp852\"\n        )\n        r2 = r1.replace(url=\"http://www.example.com/other\")\n        r3 = r1.replace(url=\"http://www.example.com/other\", encoding=\"latin1\")\n\n        assert isinstance(r2, self.response_class)\n        assert r2.url == \"http://www.example.com/other\"\n        self._assert_response_encoding(r2, \"cp852\")\n        assert r3.url == \"http://www.example.com/other\"\n        assert r3._declared_encoding() == \"latin1\"\n\n    def test_unicode_url(self):\n        # instantiate with unicode url without encoding (should set default encoding)\n        resp = self.response_class(\"http://www.example.com/\")\n        self._assert_response_encoding(resp, self.response_class._DEFAULT_ENCODING)\n\n        # make sure urls are converted to str\n        resp = self.response_class(url=\"http://www.example.com/\", encoding=\"utf-8\")\n        assert isinstance(resp.url, str)\n\n        resp = self.response_class(\n            url=\"http://www.example.com/price/\\xa3\", encoding=\"utf-8\"\n        )\n        assert resp.url == to_unicode(b\"http://www.example.com/price/\\xc2\\xa3\")\n        resp = self.response_class(\n            url=\"http://www.example.com/price/\\xa3\", encoding=\"latin-1\"\n        )\n        assert resp.url == \"http://www.example.com/price/\\xa3\"\n        resp = self.response_class(\n            \"http://www.example.com/price/\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n        )\n        assert resp.url == to_unicode(b\"http://www.example.com/price/\\xc2\\xa3\")\n        resp = self.response_class(\n            \"http://www.example.com/price/\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n        )\n        assert resp.url == \"http://www.example.com/price/\\xa3\"\n\n    def test_unicode_body(self):\n        unicode_string = (\n            \"\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \"\n            \"\\u0442\\u0435\\u043a\\u0441\\u0442\"\n        )\n        with pytest.raises(TypeError):\n            self.response_class(\"http://www.example.com\", body=\"unicode body\")\n\n        original_string = unicode_string.encode(\"cp1251\")\n        r1 = self.response_class(\n            \"http://www.example.com\", body=original_string, encoding=\"cp1251\"\n        )\n\n        # check response.text\n        assert isinstance(r1.text, str)\n        assert r1.text == unicode_string\n", "n_tokens": 911, "byte_len": 3888, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 314, "end_line": 410}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 4, "symbols": ["test_encoding", "test_declared_encoding_invalid", "test_utf16", "test_invalid_utf8_encoded_body_with_valid_utf8_BOM", "encoding", "test", "text", "response", "problems", "gb2312", "u2015", "subclasses", "gb18030", "charset", "application", "declared", "xfeh", "class", "xbf", "word", "headers", "xa8d", "cp1251", "lib", "w3lib", "assert", "ignored", "download", "with", "cp1252", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags"], "ast_kind": "function_or_method", "text": "    def test_encoding(self):\n        r1 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xc2\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n        )\n        r2 = self.response_class(\n            \"http://www.example.com\", encoding=\"utf-8\", body=\"\\xa3\"\n        )\n        r3 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n        )\n        r4 = self.response_class(\"http://www.example.com\", body=b\"\\xa2\\xa3\")\n        r5 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xc2\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=None\"]},\n        )\n        r6 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xa8D\",\n            headers={\"Content-type\": [\"text/html; charset=gb2312\"]},\n        )\n        r7 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xa8D\",\n            headers={\"Content-type\": [\"text/html; charset=gbk\"]},\n        )\n        r8 = self.response_class(\n            \"http://www.example.com\",\n            body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=cp1251\"]},\n        )\n        r9 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\x80\",\n            headers={\n                \"Content-type\": [b\"application/x-download; filename=\\x80dummy.txt\"]\n            },\n        )\n\n        assert r1._headers_encoding() == \"utf-8\"\n        assert r2._headers_encoding() is None\n        assert r2._declared_encoding() == \"utf-8\"\n        self._assert_response_encoding(r2, \"utf-8\")\n        assert r3._headers_encoding() == \"cp1252\"\n        assert r3._declared_encoding() == \"cp1252\"\n        assert r4._headers_encoding() is None\n        assert r5._headers_encoding() is None\n        assert r8._headers_encoding() == \"cp1251\"\n        assert r9._headers_encoding() is None\n        assert r8._declared_encoding() == \"utf-8\"\n        assert r9._declared_encoding() is None\n        self._assert_response_encoding(r5, \"utf-8\")\n        self._assert_response_encoding(r8, \"utf-8\")\n        self._assert_response_encoding(r9, \"cp1252\")\n        assert r4._body_inferred_encoding() is not None\n        assert r4._body_inferred_encoding() != \"ascii\"\n        self._assert_response_values(r1, \"utf-8\", \"\\xa3\")\n        self._assert_response_values(r2, \"utf-8\", \"\\xa3\")\n        self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n        self._assert_response_values(r6, \"gb18030\", \"\\u2015\")\n        self._assert_response_values(r7, \"gb18030\", \"\\u2015\")\n        self._assert_response_values(r9, \"cp1252\", \"€\")\n\n        # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n        with pytest.raises(TypeError):\n            self.response_class(\"http://www.example.com\", body=\"\\xa3\")\n\n    def test_declared_encoding_invalid(self):\n        \"\"\"Check that unknown declared encodings are ignored\"\"\"\n        r = self.response_class(\n            \"http://www.example.com\",\n            headers={\"Content-type\": [\"text/html; charset=UNKNOWN\"]},\n            body=b\"\\xc2\\xa3\",\n        )\n        assert r._declared_encoding() is None\n        self._assert_response_values(r, \"utf-8\", \"\\xa3\")\n\n    def test_utf16(self):\n        \"\"\"Test utf-16 because UnicodeDammit is known to have problems with\"\"\"\n        r = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xff\\xfeh\\x00i\\x00\",\n            encoding=\"utf-16\",\n        )\n        self._assert_response_values(r, \"utf-16\", \"hi\")\n\n    def test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):\n        r6 = self.response_class(\n            \"http://www.example.com\",\n            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n            body=b\"\\xef\\xbb\\xbfWORD\\xe3\\xab\",\n        )\n        assert r6.encoding == \"utf-8\"\n        assert r6.text in {\n            \"WORD\\ufffd\\ufffd\",  # w3lib < 1.19.0\n            \"WORD\\ufffd\",  # w3lib >= 1.19.0\n        }\n", "n_tokens": 1045, "byte_len": 4111, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 411, "end_line": 512}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 5, "symbols": ["test_bom_is_removed_from_body", "test_replace_wrong_encoding", "test_selector", "test_selector_shortcuts", "test_selector_shortcuts_kwargs", "test_urljoin_with_base_url", "encoding", "decoded", "xpath", "selector", "inferring", "doesn", "test", "policy", "body", "removed", "https", "destroy", "order", "href", "replaced", "isinstance", "caching", "following", "replacement", "bom", "stop", "type", "html", "without", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags"], "ast_kind": "function_or_method", "text": "    def test_bom_is_removed_from_body(self):\n        # Inferring encoding from body also cache decoded body as sideeffect,\n        # this test tries to ensure that calling response.encoding and\n        # response.text in indistinct order doesn't affect final\n        # response.text in indistinct order doesn't affect final\n        # values for encoding and decoded body.\n        url = \"http://example.com\"\n        body = b\"\\xef\\xbb\\xbfWORD\"\n        headers = {\"Content-type\": [\"text/html; charset=utf-8\"]}\n\n        # Test response without content-type and BOM encoding\n        response = self.response_class(url, body=body)\n        assert response.encoding == \"utf-8\"\n        assert response.text == \"WORD\"\n        response = self.response_class(url, body=body)\n        assert response.text == \"WORD\"\n        assert response.encoding == \"utf-8\"\n\n        # Body caching sideeffect isn't triggered when encoding is declared in\n        # content-type header but BOM still need to be removed from decoded\n        # body\n        response = self.response_class(url, headers=headers, body=body)\n        assert response.encoding == \"utf-8\"\n        assert response.text == \"WORD\"\n        response = self.response_class(url, headers=headers, body=body)\n        assert response.text == \"WORD\"\n        assert response.encoding == \"utf-8\"\n\n    def test_replace_wrong_encoding(self):\n        \"\"\"Test invalid chars are replaced properly\"\"\"\n        r = self.response_class(\n            \"http://www.example.com\",\n            encoding=\"utf-8\",\n            body=b\"PREFIX\\xe3\\xabSUFFIX\",\n        )\n        # XXX: Policy for replacing invalid chars may suffer minor variations\n        # but it should always contain the unicode replacement char ('\\ufffd')\n        assert \"\\ufffd\" in r.text, repr(r.text)\n        assert \"PREFIX\" in r.text, repr(r.text)\n        assert \"SUFFIX\" in r.text, repr(r.text)\n\n        # Do not destroy html tags due to encoding bugs\n        r = self.response_class(\n            \"http://example.com\",\n            encoding=\"utf-8\",\n            body=b\"\\xf0<span>value</span>\",\n        )\n        assert \"<span>value</span>\" in r.text, repr(r.text)\n\n        # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n        # r = self.response_class(\"http://www.example.com\", body=b'PREFIX\\xe3\\xabSUFFIX')\n        # assert '\\ufffd' in r.text, repr(r.text)\n\n    def test_selector(self):\n        body = b\"<html><head><title>Some page</title><body></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        assert isinstance(response.selector, Selector)\n        assert response.selector.type == \"html\"\n        assert response.selector is response.selector  # property is cached\n        assert response.selector.response is response\n\n        assert response.selector.xpath(\"//title/text()\").getall() == [\"Some page\"]\n        assert response.selector.css(\"title::text\").getall() == [\"Some page\"]\n        assert response.selector.re(\"Some (.*)</title>\") == [\"page\"]\n\n    def test_selector_shortcuts(self):\n        body = b\"<html><head><title>Some page</title><body></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        assert (\n            response.xpath(\"//title/text()\").getall()\n            == response.selector.xpath(\"//title/text()\").getall()\n        )\n        assert (\n            response.css(\"title::text\").getall()\n            == response.selector.css(\"title::text\").getall()\n        )\n\n    def test_selector_shortcuts_kwargs(self):\n        body = b'<html><head><title>Some page</title><body><p class=\"content\">A nice paragraph.</p></body></html>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        assert (\n            response.xpath(\n                \"normalize-space(//p[@class=$pclass])\", pclass=\"content\"\n            ).getall()\n            == response.xpath('normalize-space(//p[@class=\"content\"])').getall()\n        )\n        assert (\n            response.xpath(\n                \"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n                pclass=\"content\",\n                pcount=1,\n            ).getall()\n            == response.xpath(\n                '//title[count(following::p[@class=\"content\"])=1]/text()'\n            ).getall()\n        )\n\n    def test_urljoin_with_base_url(self):\n        \"\"\"Test urljoin shortcut which also evaluates base-url through get_base_url().\"\"\"\n        body = b'<html><body><base href=\"https://example.net\"></body></html>'\n        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n            \"/test\"\n        )\n        absolute = \"https://example.net/test\"\n        assert joined == absolute\n\n        body = b'<html><body><base href=\"/elsewhere\"></body></html>'\n        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n            \"test\"\n        )\n        absolute = \"http://www.example.com/test\"\n        assert joined == absolute\n\n        body = b'<html><body><base href=\"/elsewhere/\"></body></html>'\n        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n            \"test\"\n        )\n        absolute = \"http://www.example.com/elsewhere/test\"\n        assert joined == absolute\n", "n_tokens": 1182, "byte_len": 5276, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 513, "end_line": 635}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 6, "symbols": ["test_follow_selector", "test_follow_selector_list", "test_follow_selector_invalid", "test_follow_selector_attribute", "test_follow_selector_no_href", "test_follow_whitespace_selector", "test_follow_encoding", "test_follow_flags", "test_follow_all_flags", "test_follow_all_css", "test_follow_all_css_skip_invalid", "encoding", "utf", "utf8", "sellist", "allowed", "expected", "text", "test", "follow", "sample", "sample3", "extracted", "extracted1", "select", "xpath", "assert", "followed", "unsupported", "response", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_all_absolute"], "ast_kind": "function_or_method", "text": "    def test_follow_selector(self):\n        resp = self._links_response()\n        urls = [\n            \"http://example.com/sample2.html\",\n            \"http://example.com/sample3.html\",\n            \"http://example.com/sample3.html\",\n            \"http://example.com/sample3.html\",\n            \"http://example.com/sample3.html#foo\",\n            \"http://www.google.com/something\",\n            \"http://example.com/innertag.html\",\n        ]\n\n        # select <a> elements\n        for sellist in [resp.css(\"a\"), resp.xpath(\"//a\")]:\n            for sel, url in zip(sellist, urls):\n                self._assert_followed_url(sel, url, response=resp)\n\n        # select <link> elements\n        self._assert_followed_url(\n            Selector(text='<link href=\"foo\"></link>').css(\"link\")[0],\n            \"http://example.com/foo\",\n            response=resp,\n        )\n\n        # href attributes should work\n        for sellist in [resp.css(\"a::attr(href)\"), resp.xpath(\"//a/@href\")]:\n            for sel, url in zip(sellist, urls):\n                self._assert_followed_url(sel, url, response=resp)\n\n        # non-a elements are not supported\n        with pytest.raises(\n            ValueError, match=\"Only <a> and <link> elements are supported\"\n        ):\n            resp.follow(resp.css(\"div\")[0])\n\n    def test_follow_selector_list(self):\n        resp = self._links_response()\n        with pytest.raises(ValueError, match=\"SelectorList\"):\n            resp.follow(resp.css(\"a\"))\n\n    def test_follow_selector_invalid(self):\n        resp = self._links_response()\n        with pytest.raises(ValueError, match=\"Unsupported\"):\n            resp.follow(resp.xpath(\"count(//div)\")[0])\n\n    def test_follow_selector_attribute(self):\n        resp = self._links_response()\n        for src in resp.css(\"img::attr(src)\"):\n            self._assert_followed_url(src, \"http://example.com/sample2.jpg\")\n\n    def test_follow_selector_no_href(self):\n        resp = self.response_class(\n            url=\"http://example.com\",\n            body=b\"<html><body><a name=123>click me</a></body></html>\",\n        )\n        with pytest.raises(ValueError, match=\"no href\"):\n            resp.follow(resp.css(\"a\")[0])\n\n    def test_follow_whitespace_selector(self):\n        resp = self.response_class(\n            \"http://example.com\",\n            body=b\"\"\"<html><body><a href=\" foo\\n\">click me</a></body></html>\"\"\",\n        )\n        self._assert_followed_url(\n            resp.css(\"a\")[0], \"http://example.com/foo\", response=resp\n        )\n        self._assert_followed_url(\n            resp.css(\"a::attr(href)\")[0],\n            \"http://example.com/foo\",\n            response=resp,\n        )\n\n    def test_follow_encoding(self):\n        resp1 = self.response_class(\n            \"http://example.com\",\n            encoding=\"utf8\",\n            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode(),\n        )\n        req = self._assert_followed_url(\n            resp1.css(\"a\")[0],\n            \"http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82\",\n            response=resp1,\n        )\n        assert req.encoding == \"utf8\"\n\n        resp2 = self.response_class(\n            \"http://example.com\",\n            encoding=\"cp1251\",\n            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode(\n                \"cp1251\"\n            ),\n        )\n        req = self._assert_followed_url(\n            resp2.css(\"a\")[0],\n            \"http://example.com/foo?%EF%F0%E8%E2%E5%F2\",\n            response=resp2,\n        )\n        assert req.encoding == \"cp1251\"\n\n    def test_follow_flags(self):\n        res = self.response_class(\"http://example.com/\")\n        fol = res.follow(\"http://example.com/\", flags=[\"cached\", \"allowed\"])\n        assert fol.flags == [\"cached\", \"allowed\"]\n\n    def test_follow_all_flags(self):\n        re = self.response_class(\"http://www.example.com/\")\n        urls = [\n            \"http://www.example.com/\",\n            \"http://www.example.com/2\",\n            \"http://www.example.com/foo\",\n        ]\n        fol = re.follow_all(urls, flags=[\"cached\", \"allowed\"])\n        for req in fol:\n            assert req.flags == [\"cached\", \"allowed\"]\n\n    def test_follow_all_css(self):\n        expected = [\n            \"http://example.com/sample3.html\",\n            \"http://example.com/innertag.html\",\n        ]\n        response = self._links_response()\n        extracted = [r.url for r in response.follow_all(css='a[href*=\"example.com\"]')]\n        assert expected == extracted\n\n    def test_follow_all_css_skip_invalid(self):\n        expected = [\n            \"http://example.com/page/1/\",\n            \"http://example.com/page/3/\",\n            \"http://example.com/page/4/\",\n        ]\n        response = self._links_response_no_href()\n        extracted1 = [r.url for r in response.follow_all(css=\".pagination a\")]\n        assert expected == extracted1\n        extracted2 = [r.url for r in response.follow_all(response.css(\".pagination a\"))]\n        assert expected == extracted2\n", "n_tokens": 1163, "byte_len": 5003, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 636, "end_line": 771}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 7, "symbols": ["test_follow_all_xpath", "test_follow_all_xpath_skip_invalid", "test_follow_all_too_many_arguments", "test_json_response", "test_cache_json_response", "test_html_encoding", "test_html5_meta_charset", "TestHtmlResponse", "TestXmlResponse", "test", "follow", "encoding", "loads", "sample", "sample3", "extracted", "extracted1", "xpath", "text", "response", "declarations", "html", "unexpected", "mock", "public", "make", "pytest", "href", "equiv", "following", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags"], "ast_kind": "class_or_type", "text": "    def test_follow_all_xpath(self):\n        expected = [\n            \"http://example.com/sample3.html\",\n            \"http://example.com/innertag.html\",\n        ]\n        response = self._links_response()\n        extracted = response.follow_all(xpath='//a[contains(@href, \"example.com\")]')\n        assert expected == [r.url for r in extracted]\n\n    def test_follow_all_xpath_skip_invalid(self):\n        expected = [\n            \"http://example.com/page/1/\",\n            \"http://example.com/page/3/\",\n            \"http://example.com/page/4/\",\n        ]\n        response = self._links_response_no_href()\n        extracted1 = [\n            r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')\n        ]\n        assert expected == extracted1\n        extracted2 = [\n            r.url\n            for r in response.follow_all(response.xpath('//div[@id=\"pagination\"]/a'))\n        ]\n        assert expected == extracted2\n\n    def test_follow_all_too_many_arguments(self):\n        response = self._links_response()\n        with pytest.raises(\n            ValueError, match=\"Please supply exactly one of the following arguments\"\n        ):\n            response.follow_all(\n                css='a[href*=\"example.com\"]',\n                xpath='//a[contains(@href, \"example.com\")]',\n            )\n\n    def test_json_response(self):\n        json_body = b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\"\n        json_response = self.response_class(\"http://www.example.com\", body=json_body)\n        assert json_response.json() == {\"ip\": \"109.187.217.200\"}\n\n        text_body = b\"\"\"<html><body>text</body></html>\"\"\"\n        text_response = self.response_class(\"http://www.example.com\", body=text_body)\n        with pytest.raises(\n            ValueError, match=\"(Expecting value|Unexpected '<'): line 1\"\n        ):\n            text_response.json()\n\n    def test_cache_json_response(self):\n        json_valid_bodies = [b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\", b\"\"\"null\"\"\"]\n        for json_body in json_valid_bodies:\n            json_response = self.response_class(\n                \"http://www.example.com\", body=json_body\n            )\n\n            with mock.patch(\"json.loads\") as mock_json:\n                for _ in range(2):\n                    json_response.json()\n                mock_json.assert_called_once_with(json_body)\n\n\nclass TestHtmlResponse(TestTextResponse):\n    response_class = HtmlResponse\n\n    def test_html_encoding(self):\n        body = b\"\"\"<html><head><title>Some page</title>\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n        </head><body>Price: \\xa3100</body></html>'\n        \"\"\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, \"iso-8859-1\", body)\n\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n        <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n        Price: \\xa3100\n        \"\"\"\n        r2 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r2, \"iso-8859-1\", body)\n\n        # for conflicting declarations headers must take precedence\n        body = b\"\"\"<html><head><title>Some page</title>\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n        </head><body>Price: \\xa3100</body></html>'\n        \"\"\"\n        r3 = self.response_class(\n            \"http://www.example.com\",\n            body=body,\n            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n        )\n        self._assert_response_values(r3, \"iso-8859-1\", body)\n\n        # make sure replace() preserves the encoding of the original response\n        body = b\"New body \\xa3\"\n        r4 = r3.replace(body=body)\n        self._assert_response_values(r4, \"iso-8859-1\", body)\n\n    def test_html5_meta_charset(self):\n        body = b\"\"\"<html><head><meta charset=\"gb2312\" /><title>Some page</title><body>bla bla</body>\"\"\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, \"gb2312\", body)\n\n\nclass TestXmlResponse(TestTextResponse):\n    response_class = XmlResponse\n", "n_tokens": 997, "byte_len": 4142, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 772, "end_line": 876}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 8, "symbols": ["test_xml_encoding", "test_replace_encoding", "test_selector", "test_selector_shortcuts", "test_selector_shortcuts_kwargs", "__init__", "test_copy", "CustomResponse", "TestCustomResponse", "encoding", "status", "method", "unless", "getall", "text", "response", "somens", "overridden", "preserves", "class", "xpath", "test", "xml", "replace", "copy", "property", "assert", "selector", "args", "passed", "test_init", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "test_replace", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags", "test_follow_all_absolute"], "ast_kind": "class_or_type", "text": "    def test_xml_encoding(self):\n        body = b\"<xml></xml>\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, self.response_class._DEFAULT_ENCODING, body)\n\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        r2 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r2, \"iso-8859-1\", body)\n\n        # make sure replace() preserves the explicit encoding passed in the __init__ method\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        r3 = self.response_class(\"http://www.example.com\", body=body, encoding=\"utf-8\")\n        body2 = b\"New body\"\n        r4 = r3.replace(body=body2)\n        self._assert_response_values(r4, \"utf-8\", body2)\n\n    def test_replace_encoding(self):\n        # make sure replace() keeps the previous encoding unless overridden explicitly\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        body2 = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><xml></xml>\"\"\"\n        r5 = self.response_class(\"http://www.example.com\", body=body)\n        r6 = r5.replace(body=body2)\n        r7 = r5.replace(body=body2, encoding=\"utf-8\")\n        self._assert_response_values(r5, \"iso-8859-1\", body)\n        self._assert_response_values(r6, \"iso-8859-1\", body2)\n        self._assert_response_values(r7, \"utf-8\", body2)\n\n    def test_selector(self):\n        body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        assert isinstance(response.selector, Selector)\n        assert response.selector.type == \"xml\"\n        assert response.selector is response.selector  # property is cached\n        assert response.selector.response is response\n\n        assert response.selector.xpath(\"//elem/text()\").getall() == [\"value\"]\n\n    def test_selector_shortcuts(self):\n        body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        assert (\n            response.xpath(\"//elem/text()\").getall()\n            == response.selector.xpath(\"//elem/text()\").getall()\n        )\n\n    def test_selector_shortcuts_kwargs(self):\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n        <xml xmlns:somens=\"http://scrapy.org\">\n        <somens:elem>value</somens:elem>\n        </xml>\"\"\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        assert (\n            response.xpath(\n                \"//s:elem/text()\", namespaces={\"s\": \"http://scrapy.org\"}\n            ).getall()\n            == response.selector.xpath(\n                \"//s:elem/text()\", namespaces={\"s\": \"http://scrapy.org\"}\n            ).getall()\n        )\n\n        response.selector.register_namespace(\"s2\", \"http://scrapy.org\")\n        assert (\n            response.xpath(\n                \"//s1:elem/text()\", namespaces={\"s1\": \"http://scrapy.org\"}\n            ).getall()\n            == response.selector.xpath(\"//s2:elem/text()\").getall()\n        )\n\n\nclass CustomResponse(TextResponse):\n    attributes = (*TextResponse.attributes, \"foo\", \"bar\")\n\n    def __init__(self, *args, **kwargs) -> None:\n        self.foo = kwargs.pop(\"foo\", None)\n        self.bar = kwargs.pop(\"bar\", None)\n        self.lost = kwargs.pop(\"lost\", None)\n        super().__init__(*args, **kwargs)\n\n\nclass TestCustomResponse(TestTextResponse):\n    response_class = CustomResponse\n\n    def test_copy(self):\n        super().test_copy()\n        r1 = self.response_class(\n            url=\"https://example.org\",\n            status=200,\n            foo=\"foo\",\n            bar=\"bar\",\n            lost=\"lost\",\n        )\n        r2 = r1.copy()\n        assert isinstance(r2, self.response_class)\n        assert r1.foo == r2.foo\n        assert r1.bar == r2.bar\n        assert r1.lost == \"lost\"\n        assert r2.lost is None\n", "n_tokens": 1002, "byte_len": 3978, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 877, "end_line": 977}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py#9", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_response.py", "rel_path": "tests/test_http_response.py", "module": "tests.test_http_response", "ext": "py", "chunk_number": 9, "symbols": ["test_replace", "argument", "response", "class", "replace", "unexpected", "with", "https", "example", "type", "error", "init", "unknown", "pytest", "lost", "assert", "test", "isinstance", "raises", "super", "none", "self", "match", "keyword", "status", "test_init", "test_copy", "test_copy_meta", "test_copy_cb_kwargs", "test_unavailable_meta", "test_unavailable_cb_kwargs", "test_copy_inherited_classes", "_assert_response_values", "_assert_response_encoding", "test_immutable_attributes", "test_urljoin", "test_shortcut_attributes", "test_follow_url_absolute", "test_follow_url_relative", "test_follow_link", "test_follow_None_url", "test_follow_whitespace_url", "test_follow_whitespace_link", "test_follow_flags", "test_follow_all_absolute", "test_follow_all_relative", "test_follow_all_links", "test_follow_all_empty", "test_follow_all_invalid", "test_follow_all_whitespace"], "ast_kind": "function_or_method", "text": "    def test_replace(self):\n        super().test_replace()\n        r1 = self.response_class(\n            url=\"https://example.org\",\n            status=200,\n            foo=\"foo\",\n            bar=\"bar\",\n            lost=\"lost\",\n        )\n\n        r2 = r1.replace(foo=\"new-foo\", bar=\"new-bar\", lost=\"new-lost\")\n        assert isinstance(r2, self.response_class)\n        assert r1.foo == \"foo\"\n        assert r1.bar == \"bar\"\n        assert r1.lost == \"lost\"\n        assert r2.foo == \"new-foo\"\n        assert r2.bar == \"new-bar\"\n        assert r2.lost == \"new-lost\"\n\n        r3 = r1.replace(foo=\"new-foo\", bar=\"new-bar\")\n        assert isinstance(r3, self.response_class)\n        assert r1.foo == \"foo\"\n        assert r1.bar == \"bar\"\n        assert r1.lost == \"lost\"\n        assert r3.foo == \"new-foo\"\n        assert r3.bar == \"new-bar\"\n        assert r3.lost is None\n\n        r4 = r1.replace(foo=\"new-foo\")\n        assert isinstance(r4, self.response_class)\n        assert r1.foo == \"foo\"\n        assert r1.bar == \"bar\"\n        assert r1.lost == \"lost\"\n        assert r4.foo == \"new-foo\"\n        assert r4.bar == \"bar\"\n        assert r4.lost is None\n\n        with pytest.raises(\n            TypeError,\n            match=r\"__init__\\(\\) got an unexpected keyword argument 'unknown'\",\n        ):\n            r1.replace(unknown=\"unknown\")\n", "n_tokens": 348, "byte_len": 1332, "file_sha1": "128bf3e10f62a8e7a8b0afcfbd8dcaee4e45870c", "start_line": 978, "end_line": 1020}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py", "rel_path": "tests/test_downloadermiddleware_retry.py", "module": "tests.test_downloadermiddleware_retry", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_priority_adjust", "test_404", "test_dont_retry", "test_dont_retry_exc", "test_503", "test_twistederrors", "test_exception_to_retry_added", "_test_retry_exception", "get_middleware", "TestRetry", "TestMaxRetryTimes", "get", "retry", "spider", "test", "exception", "spiders", "crawler", "pytest", "default", "settings", "reason", "count", "isinstance", "none", "timeout", "error", "max", "http", "test_with_settings_zero", "test_with_metakey_zero", "test_without_metakey", "test_with_metakey_greater", "test_with_metakey_lesser", "test_with_dont_retry", "_test_retry", "get_spider", "test_basic_usage", "test_max_retries_reached", "test_one_retry", "test_two_retries", "test_no_spider", "test_max_retry_times_setting", "test_max_retry_times_meta", "test_max_retry_times_argument", "test_priority_adjust_setting", "test_priority_adjust_argument", "test_log_extra_retry_success", "test_log_extra_retries_exceeded"], "ast_kind": "class_or_type", "text": "import logging\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.error import (\n    ConnectError,\n    ConnectionDone,\n    ConnectionLost,\n    DNSLookupError,\n    TCPTimedOutError,\n)\nfrom twisted.internet.error import ConnectionRefusedError as TxConnectionRefusedError\nfrom twisted.internet.error import TimeoutError as TxTimeoutError\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.settings.default_settings import RETRY_EXCEPTIONS\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestRetry:\n    def setup_method(self):\n        self.crawler = get_crawler(DefaultSpider)\n        self.crawler.spider = self.crawler._create_spider()\n        self.mw = RetryMiddleware.from_crawler(self.crawler)\n        self.mw.max_retry_times = 2\n\n    def test_priority_adjust(self):\n        req = Request(\"http://www.scrapytest.org/503\")\n        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n        req2 = self.mw.process_response(req, rsp)\n        assert req2.priority < req.priority\n\n    def test_404(self):\n        req = Request(\"http://www.scrapytest.org/404\")\n        rsp = Response(\"http://www.scrapytest.org/404\", body=b\"\", status=404)\n\n        # dont retry 404s\n        assert self.mw.process_response(req, rsp) is rsp\n\n    def test_dont_retry(self):\n        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": True})\n        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n\n        # first retry\n        r = self.mw.process_response(req, rsp)\n        assert r is rsp\n\n        # Test retry when dont_retry set to False\n        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": False})\n        rsp = Response(\"http://www.scrapytest.org/503\")\n\n        # first retry\n        r = self.mw.process_response(req, rsp)\n        assert r is rsp\n\n    def test_dont_retry_exc(self):\n        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": True})\n\n        r = self.mw.process_exception(req, DNSLookupError())\n        assert r is None\n\n    def test_503(self):\n        req = Request(\"http://www.scrapytest.org/503\")\n        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n\n        # first retry\n        req = self.mw.process_response(req, rsp)\n        assert isinstance(req, Request)\n        assert req.meta[\"retry_times\"] == 1\n\n        # second retry\n        req = self.mw.process_response(req, rsp)\n        assert isinstance(req, Request)\n        assert req.meta[\"retry_times\"] == 2\n\n        # discard it\n        assert self.mw.process_response(req, rsp) is rsp\n\n        assert self.crawler.stats.get_value(\"retry/max_reached\") == 1\n        assert (\n            self.crawler.stats.get_value(\"retry/reason_count/503 Service Unavailable\")\n            == 2\n        )\n        assert self.crawler.stats.get_value(\"retry/count\") == 2\n\n    def test_twistederrors(self):\n        exceptions = [\n            ConnectError,\n            ConnectionDone,\n            ConnectionLost,\n            TxConnectionRefusedError,\n            defer.TimeoutError,\n            DNSLookupError,\n            ResponseFailed,\n            TCPTimedOutError,\n            TxTimeoutError,\n        ]\n\n        for exc in exceptions:\n            req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n            self._test_retry_exception(req, exc(\"foo\"))\n\n        stats = self.crawler.stats\n        assert stats.get_value(\"retry/max_reached\") == len(exceptions)\n        assert stats.get_value(\"retry/count\") == len(exceptions) * 2\n        assert (\n            stats.get_value(\"retry/reason_count/twisted.internet.defer.TimeoutError\")\n            == 2\n        )\n\n    def test_exception_to_retry_added(self):\n        exc = ValueError\n        settings_dict = {\n            \"RETRY_EXCEPTIONS\": [*RETRY_EXCEPTIONS, exc],\n        }\n        crawler = get_crawler(DefaultSpider, settings_dict=settings_dict)\n        crawler.spider = crawler._create_spider()\n        mw = RetryMiddleware.from_crawler(crawler)\n        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n        self._test_retry_exception(req, exc(\"foo\"), mw)\n\n    def _test_retry_exception(self, req, exception, mw=None):\n        if mw is None:\n            mw = self.mw\n\n        # first retry\n        req = mw.process_exception(req, exception)\n        assert isinstance(req, Request)\n        assert req.meta[\"retry_times\"] == 1\n\n        # second retry\n        req = mw.process_exception(req, exception)\n        assert isinstance(req, Request)\n        assert req.meta[\"retry_times\"] == 2\n\n        # discard it\n        req = mw.process_exception(req, exception)\n        assert req is None\n\n\nclass TestMaxRetryTimes:\n    invalid_url = \"http://www.scrapytest.org/invalid_url\"\n\n    def get_middleware(self, settings=None):\n        crawler = get_crawler(DefaultSpider, settings or {})\n        crawler.spider = crawler._create_spider()\n        return RetryMiddleware.from_crawler(crawler)\n", "n_tokens": 1188, "byte_len": 5227, "file_sha1": "d5e8e25cc2cdd9b602ab8ff1c9301ec75fede3ec", "start_line": 1, "end_line": 154}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py", "rel_path": "tests/test_downloadermiddleware_retry.py", "module": "tests.test_downloadermiddleware_retry", "ext": "py", "chunk_number": 2, "symbols": ["test_with_settings_zero", "test_with_metakey_zero", "test_without_metakey", "test_with_metakey_greater", "test_with_metakey_lesser", "test_with_dont_retry", "_test_retry", "get_spider", "test_basic_usage", "test_max_retries_reached", "TestGetRetryRequest", "unspecified", "meta", "max", "new", "request", "process", "exception", "test", "basic", "stat", "spider", "priority", "req", "req2", "discard", "get", "retry", "expected", "dont", "setup_method", "test_priority_adjust", "test_404", "test_dont_retry", "test_dont_retry_exc", "test_503", "test_twistederrors", "test_exception_to_retry_added", "_test_retry_exception", "get_middleware", "test_one_retry", "test_two_retries", "test_no_spider", "test_max_retry_times_setting", "test_max_retry_times_meta", "test_max_retry_times_argument", "test_priority_adjust_setting", "test_priority_adjust_argument", "test_log_extra_retry_success", "test_log_extra_retries_exceeded"], "ast_kind": "class_or_type", "text": "    def test_with_settings_zero(self):\n        max_retry_times = 0\n        settings = {\"RETRY_TIMES\": max_retry_times}\n        middleware = self.get_middleware(settings)\n        req = Request(self.invalid_url)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            max_retry_times,\n            middleware=middleware,\n        )\n\n    def test_with_metakey_zero(self):\n        max_retry_times = 0\n        middleware = self.get_middleware()\n        meta = {\"max_retry_times\": max_retry_times}\n        req = Request(self.invalid_url, meta=meta)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            max_retry_times,\n            middleware=middleware,\n        )\n\n    def test_without_metakey(self):\n        max_retry_times = 5\n        settings = {\"RETRY_TIMES\": max_retry_times}\n        middleware = self.get_middleware(settings)\n        req = Request(self.invalid_url)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            max_retry_times,\n            middleware=middleware,\n        )\n\n    def test_with_metakey_greater(self):\n        meta_max_retry_times = 3\n        middleware_max_retry_times = 2\n\n        req1 = Request(self.invalid_url, meta={\"max_retry_times\": meta_max_retry_times})\n        req2 = Request(self.invalid_url)\n\n        settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n        middleware = self.get_middleware(settings)\n\n        self._test_retry(\n            req1,\n            DNSLookupError(\"foo\"),\n            meta_max_retry_times,\n            middleware=middleware,\n        )\n        self._test_retry(\n            req2,\n            DNSLookupError(\"foo\"),\n            middleware_max_retry_times,\n            middleware=middleware,\n        )\n\n    def test_with_metakey_lesser(self):\n        meta_max_retry_times = 4\n        middleware_max_retry_times = 5\n\n        req1 = Request(self.invalid_url, meta={\"max_retry_times\": meta_max_retry_times})\n        req2 = Request(self.invalid_url)\n\n        settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n        middleware = self.get_middleware(settings)\n\n        self._test_retry(\n            req1,\n            DNSLookupError(\"foo\"),\n            meta_max_retry_times,\n            middleware=middleware,\n        )\n        self._test_retry(\n            req2,\n            DNSLookupError(\"foo\"),\n            middleware_max_retry_times,\n            middleware=middleware,\n        )\n\n    def test_with_dont_retry(self):\n        max_retry_times = 4\n        middleware = self.get_middleware()\n        meta = {\n            \"max_retry_times\": max_retry_times,\n            \"dont_retry\": True,\n        }\n        req = Request(self.invalid_url, meta=meta)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            0,\n            middleware=middleware,\n        )\n\n    def _test_retry(\n        self,\n        req,\n        exception,\n        max_retry_times,\n        middleware=None,\n    ):\n        middleware = middleware or self.mw\n\n        for i in range(max_retry_times):\n            req = middleware.process_exception(req, exception)\n            assert isinstance(req, Request)\n\n        # discard it\n        req = middleware.process_exception(req, exception)\n        assert req is None\n\n\nclass TestGetRetryRequest:\n    def get_spider(self, settings=None):\n        crawler = get_crawler(Spider, settings or {})\n        return crawler._create_spider(\"foo\")\n\n    def test_basic_usage(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                request,\n                spider=spider,\n            )\n        assert isinstance(new_request, Request)\n        assert new_request != request\n        assert new_request.dont_filter\n        expected_retry_times = 1\n        assert new_request.meta[\"retry_times\"] == expected_retry_times\n        assert new_request.priority == -1\n        expected_reason = \"unspecified\"\n        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n            assert spider.crawler.stats.get_value(stat) == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_max_retries_reached(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        max_retry_times = 0\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                request,\n                spider=spider,\n                max_retry_times=max_retry_times,\n            )\n        assert new_request is None\n        assert spider.crawler.stats.get_value(\"retry/max_reached\") == 1\n        failure_count = max_retry_times + 1\n        expected_reason = \"unspecified\"\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"ERROR\",\n                f\"Gave up retrying {request} (failed {failure_count} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n", "n_tokens": 1072, "byte_len": 5229, "file_sha1": "d5e8e25cc2cdd9b602ab8ff1c9301ec75fede3ec", "start_line": 155, "end_line": 323}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py", "rel_path": "tests/test_downloadermiddleware_retry.py", "module": "tests.test_downloadermiddleware_retry", "ext": "py", "chunk_number": 3, "symbols": ["test_one_retry", "test_two_retries", "test_no_spider", "test_max_retry_times_setting", "test_max_retry_times_meta", "test_max_retry_times_argument", "test_priority_adjust_setting", "test_priority_adjust_argument", "test_log_extra_retry_success", "test_log_extra_retries_exceeded", "unspecified", "new", "request", "stat", "test", "priority", "spider", "expected", "retry", "get", "stats", "meta", "failure", "count", "dont", "filter", "with", "downloadermiddlewares", "scrapy", "https", "setup_method", "test_priority_adjust", "test_404", "test_dont_retry", "test_dont_retry_exc", "test_503", "test_twistederrors", "test_exception_to_retry_added", "_test_retry_exception", "get_middleware", "test_with_settings_zero", "test_with_metakey_zero", "test_without_metakey", "test_with_metakey_greater", "test_with_metakey_lesser", "test_with_dont_retry", "_test_retry", "get_spider", "test_basic_usage", "test_max_retries_reached"], "ast_kind": "function_or_method", "text": "    def test_one_retry(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                request,\n                spider=spider,\n                max_retry_times=1,\n            )\n        assert isinstance(new_request, Request)\n        assert new_request != request\n        assert new_request.dont_filter\n        expected_retry_times = 1\n        assert new_request.meta[\"retry_times\"] == expected_retry_times\n        assert new_request.priority == -1\n        expected_reason = \"unspecified\"\n        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n            assert spider.crawler.stats.get_value(stat) == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_two_retries(self):\n        spider = self.get_spider()\n        request = Request(\"https://example.com\")\n        new_request = request\n        max_retry_times = 2\n        for index in range(max_retry_times):\n            with LogCapture() as log:\n                new_request = get_retry_request(\n                    new_request,\n                    spider=spider,\n                    max_retry_times=max_retry_times,\n                )\n            assert isinstance(new_request, Request)\n            assert new_request != request\n            assert new_request.dont_filter\n            expected_retry_times = index + 1\n            assert new_request.meta[\"retry_times\"] == expected_retry_times\n            assert new_request.priority == -expected_retry_times\n            expected_reason = \"unspecified\"\n            for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n                value = spider.crawler.stats.get_value(stat)\n                assert value == expected_retry_times\n            log.check_present(\n                (\n                    \"scrapy.downloadermiddlewares.retry\",\n                    \"DEBUG\",\n                    f\"Retrying {request} (failed {expected_retry_times} times): \"\n                    f\"{expected_reason}\",\n                )\n            )\n\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                new_request,\n                spider=spider,\n                max_retry_times=max_retry_times,\n            )\n        assert new_request is None\n        assert spider.crawler.stats.get_value(\"retry/max_reached\") == 1\n        failure_count = max_retry_times + 1\n        expected_reason = \"unspecified\"\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"ERROR\",\n                f\"Gave up retrying {request} (failed {failure_count} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_no_spider(self):\n        request = Request(\"https://example.com\")\n        with pytest.raises(TypeError):\n            get_retry_request(request)  # pylint: disable=missing-kwoa\n\n    def test_max_retry_times_setting(self):\n        max_retry_times = 0\n        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times})\n        request = Request(\"https://example.com\")\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n        )\n        assert new_request is None\n\n    def test_max_retry_times_meta(self):\n        max_retry_times = 0\n        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n        meta = {\"max_retry_times\": max_retry_times}\n        request = Request(\"https://example.com\", meta=meta)\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n        )\n        assert new_request is None\n\n    def test_max_retry_times_argument(self):\n        max_retry_times = 0\n        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n        meta = {\"max_retry_times\": max_retry_times + 1}\n        request = Request(\"https://example.com\", meta=meta)\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n            max_retry_times=max_retry_times,\n        )\n        assert new_request is None\n\n    def test_priority_adjust_setting(self):\n        priority_adjust = 1\n        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust})\n        request = Request(\"https://example.com\")\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n        )\n        assert new_request.priority == priority_adjust\n\n    def test_priority_adjust_argument(self):\n        priority_adjust = 1\n        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust + 1})\n        request = Request(\"https://example.com\")\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n            priority_adjust=priority_adjust,\n        )\n        assert new_request.priority == priority_adjust\n\n    def test_log_extra_retry_success(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture(attributes=(\"spider\",)) as log:\n            get_retry_request(\n                request,\n                spider=spider,\n            )\n        log.check_present(spider)\n\n    def test_log_extra_retries_exceeded(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture(attributes=(\"spider\",)) as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                max_retry_times=0,\n            )\n        log.check_present(spider)\n", "n_tokens": 1150, "byte_len": 5737, "file_sha1": "d5e8e25cc2cdd9b602ab8ff1c9301ec75fede3ec", "start_line": 324, "end_line": 480}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_retry.py", "rel_path": "tests/test_downloadermiddleware_retry.py", "module": "tests.test_downloadermiddleware_retry", "ext": "py", "chunk_number": 4, "symbols": ["test_reason_string", "test_reason_builtin_exception", "test_reason_builtin_exception_class", "test_reason_custom_exception", "test_reason_custom_exception_class", "test_custom_logger", "test_custom_stats_key", "test", "reason", "stats", "base", "stat", "ignore", "request", "spider", "expected", "retry", "custom", "get", "with", "downloadermiddlewares", "scrapy", "logger", "not", "implemented", "https", "example", "times", "logging", "failed", "setup_method", "test_priority_adjust", "test_404", "test_dont_retry", "test_dont_retry_exc", "test_503", "test_twistederrors", "test_exception_to_retry_added", "_test_retry_exception", "get_middleware", "test_with_settings_zero", "test_with_metakey_zero", "test_without_metakey", "test_with_metakey_greater", "test_with_metakey_lesser", "test_with_dont_retry", "_test_retry", "get_spider", "test_basic_usage", "test_max_retries_reached"], "ast_kind": "function_or_method", "text": "    def test_reason_string(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = \"because\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n            assert spider.crawler.stats.get_value(stat) == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_builtin_exception(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = NotImplementedError()\n        expected_reason_string = \"builtins.NotImplementedError\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        assert stat == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_builtin_exception_class(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = NotImplementedError\n        expected_reason_string = \"builtins.NotImplementedError\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        assert stat == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_custom_exception(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = IgnoreRequest()\n        expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        assert stat == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_custom_exception_class(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = IgnoreRequest\n        expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        assert stat == 1\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_custom_logger(self):\n        logger = logging.getLogger(\"custom-logger\")\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = \"because\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n                logger=logger,\n            )\n        log.check_present(\n            (\n                \"custom-logger\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed 1 times): {expected_reason}\",\n            )\n        )\n\n    def test_custom_stats_key(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = \"because\"\n        stats_key = \"custom_retry\"\n        get_retry_request(\n            request,\n            spider=spider,\n            reason=expected_reason,\n            stats_base_key=stats_key,\n        )\n        for stat in (\n            f\"{stats_key}/count\",\n            f\"{stats_key}/reason_count/{expected_reason}\",\n        ):\n            assert spider.crawler.stats.get_value(stat) == 1\n", "n_tokens": 1027, "byte_len": 5315, "file_sha1": "d5e8e25cc2cdd9b602ab8ff1c9301ec75fede3ec", "start_line": 481, "end_line": 639}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py", "rel_path": "tests/test_exporters.py", "module": "tests.test_exporters", "ext": "py", "chunk_number": 1, "symbols": ["custom_serializer", "setup_method", "_get_exporter", "_check_output", "_assert_expected_item", "_get_nonstring_types_item", "assertItemExportWorks", "test_export_item", "test_export_dict_item", "test_serialize_field", "test_fields_to_export", "test_field_custom_serializer", "test_invalid_option", "test_nested_item", "test_export_list", "MyItem", "CustomFieldItem", "MyDataClass", "CustomFieldDataclass", "TestBaseItemExporter", "TestPythonItemExporter", "encoding", "joseph", "custom", "field", "test", "base", "serializer", "assert", "item", "test_export_item_dict_list", "test_nonstring_types_item", "test_export_multiple_items", "assertCsvEqual", "split_csv", "assertExportResult", "test_header_export_all", "test_header_export_all_dict", "test_header_export_single_field", "test_header_export_two_items", "test_header_no_header_line", "test_join_multivalue", "test_join_multivalue_not_strings", "test_errors_default", "test_errors_xmlcharrefreplace", "assertXmlEquivalent", "xmltuple", "xmlsplit", "test_multivalued_fields", "test_nested_list_item"], "ast_kind": "class_or_type", "text": "import dataclasses\nimport json\nimport marshal\nimport pickle\nimport re\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom io import BytesIO\nfrom typing import Any\n\nimport lxml.etree\nimport pytest\nfrom itemadapter import ItemAdapter\n\nfrom scrapy.exporters import (\n    BaseItemExporter,\n    CsvItemExporter,\n    JsonItemExporter,\n    JsonLinesItemExporter,\n    MarshalItemExporter,\n    PickleItemExporter,\n    PprintItemExporter,\n    PythonItemExporter,\n    XmlItemExporter,\n)\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.python import to_unicode\n\n\ndef custom_serializer(value):\n    return str(int(value) + 2)\n\n\nclass MyItem(Item):\n    name = Field()\n    age = Field()\n\n\nclass CustomFieldItem(Item):\n    name = Field()\n    age = Field(serializer=custom_serializer)\n\n\n@dataclasses.dataclass\nclass MyDataClass:\n    name: str\n    age: int\n\n\n@dataclasses.dataclass\nclass CustomFieldDataclass:\n    name: str\n    age: int = dataclasses.field(metadata={\"serializer\": custom_serializer})\n\n\nclass TestBaseItemExporter(ABC):\n    item_class: type = MyItem\n    custom_field_item_class: type = CustomFieldItem\n\n    def setup_method(self):\n        self.i = self.item_class(name=\"John\\xa3\", age=\"22\")\n        self.output = BytesIO()\n        self.ie = self._get_exporter()\n\n    @abstractmethod\n    def _get_exporter(self, **kwargs) -> BaseItemExporter:\n        raise NotImplementedError\n\n    def _check_output(self):  # noqa: B027\n        pass\n\n    def _assert_expected_item(self, exported_dict):\n        for k, v in exported_dict.items():\n            exported_dict[k] = to_unicode(v)\n        assert self.i == self.item_class(**exported_dict)\n\n    def _get_nonstring_types_item(self):\n        return {\n            \"boolean\": False,\n            \"number\": 22,\n            \"time\": datetime(2015, 1, 1, 1, 1, 1),\n            \"float\": 3.14,\n        }\n\n    def assertItemExportWorks(self, item):\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        # Delete the item exporter object, so that if it causes the output\n        # file handle to be closed, which should not be the case, follow-up\n        # interactions with the output file handle will surface the issue.\n        del self.ie\n        self._check_output()\n\n    def test_export_item(self):\n        self.assertItemExportWorks(self.i)\n\n    def test_export_dict_item(self):\n        self.assertItemExportWorks(ItemAdapter(self.i).asdict())\n\n    def test_serialize_field(self):\n        a = ItemAdapter(self.i)\n        res = self.ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"])\n        assert res == \"John\\xa3\"\n\n        res = self.ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"])\n        assert res == \"22\"\n\n    def test_fields_to_export(self):\n        ie = self._get_exporter(fields_to_export=[\"name\"])\n        assert list(ie._get_serialized_fields(self.i)) == [(\"name\", \"John\\xa3\")]\n\n        ie = self._get_exporter(fields_to_export=[\"name\"], encoding=\"latin-1\")\n        _, name = next(iter(ie._get_serialized_fields(self.i)))\n        assert isinstance(name, str)\n        assert name == \"John\\xa3\"\n\n        ie = self._get_exporter(fields_to_export={\"name\": \"名稱\"})\n        assert list(ie._get_serialized_fields(self.i)) == [(\"名稱\", \"John\\xa3\")]\n\n    def test_field_custom_serializer(self):\n        i = self.custom_field_item_class(name=\"John\\xa3\", age=\"22\")\n        a = ItemAdapter(i)\n        ie = self._get_exporter()\n        assert (\n            ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"])\n            == \"John\\xa3\"\n        )\n        assert ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"]) == \"24\"\n\n\nclass TestPythonItemExporter(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        return PythonItemExporter(**kwargs)\n\n    def test_invalid_option(self):\n        with pytest.raises(TypeError, match=\"Unexpected options: invalid_option\"):\n            PythonItemExporter(invalid_option=\"something\")\n\n    def test_nested_item(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = {\"name\": \"Maria\", \"age\": i1}\n        i3 = self.item_class(name=\"Jesus\", age=i2)\n        ie = self._get_exporter()\n        exported = ie.export_item(i3)\n        assert isinstance(exported, dict)\n        assert exported == {\n            \"age\": {\"age\": {\"age\": \"22\", \"name\": \"Joseph\"}, \"name\": \"Maria\"},\n            \"name\": \"Jesus\",\n        }\n        assert isinstance(exported[\"age\"], dict)\n        assert isinstance(exported[\"age\"][\"age\"], dict)\n\n    def test_export_list(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = self.item_class(name=\"Maria\", age=[i1])\n        i3 = self.item_class(name=\"Jesus\", age=[i2])\n        ie = self._get_exporter()\n        exported = ie.export_item(i3)\n        assert exported == {\n            \"age\": [{\"age\": [{\"age\": \"22\", \"name\": \"Joseph\"}], \"name\": \"Maria\"}],\n            \"name\": \"Jesus\",\n        }\n        assert isinstance(exported[\"age\"][0], dict)\n        assert isinstance(exported[\"age\"][0][\"age\"][0], dict)\n", "n_tokens": 1227, "byte_len": 5089, "file_sha1": "f26c70d29a196bf3c037c7e8e829d8c8aec4ae80", "start_line": 1, "end_line": 167}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py", "rel_path": "tests/test_exporters.py", "module": "tests.test_exporters", "ext": "py", "chunk_number": 2, "symbols": ["test_export_item_dict_list", "test_nonstring_types_item", "_get_exporter", "_check_output", "test_export_multiple_items", "assertCsvEqual", "split_csv", "assertExportResult", "test_header_export_all", "test_header_export_all_dict", "test_header_export_single_field", "TestPythonItemExporterDataclass", "TestPprintItemExporter", "TestPprintItemExporterDataclass", "TestPickleItemExporter", "TestPickleItemExporterDataclass", "TestMarshalItemExporter", "TestMarshalItemExporterDataclass", "TestCsvItemExporter", "joseph", "custom", "field", "test", "csv", "base", "loads", "exported", "pprint", "item", "name", "custom_serializer", "setup_method", "_assert_expected_item", "_get_nonstring_types_item", "assertItemExportWorks", "test_export_item", "test_export_dict_item", "test_serialize_field", "test_fields_to_export", "test_field_custom_serializer", "test_invalid_option", "test_nested_item", "test_export_list", "test_header_export_two_items", "test_header_no_header_line", "test_join_multivalue", "test_join_multivalue_not_strings", "test_errors_default", "test_errors_xmlcharrefreplace", "assertXmlEquivalent"], "ast_kind": "class_or_type", "text": "    def test_export_item_dict_list(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = {\"name\": \"Maria\", \"age\": [i1]}\n        i3 = self.item_class(name=\"Jesus\", age=[i2])\n        ie = self._get_exporter()\n        exported = ie.export_item(i3)\n        assert exported == {\n            \"age\": [{\"age\": [{\"age\": \"22\", \"name\": \"Joseph\"}], \"name\": \"Maria\"}],\n            \"name\": \"Jesus\",\n        }\n        assert isinstance(exported[\"age\"][0], dict)\n        assert isinstance(exported[\"age\"][0][\"age\"][0], dict)\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        ie = self._get_exporter()\n        exported = ie.export_item(item)\n        assert exported == item\n\n\nclass TestPythonItemExporterDataclass(TestPythonItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestPprintItemExporter(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        return PprintItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        self._assert_expected_item(\n            eval(self.output.getvalue())  # pylint: disable=eval-used\n        )\n\n\nclass TestPprintItemExporterDataclass(TestPprintItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestPickleItemExporter(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        return PickleItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        self._assert_expected_item(pickle.loads(self.output.getvalue()))\n\n    def test_export_multiple_items(self):\n        i1 = self.item_class(name=\"hello\", age=\"world\")\n        i2 = self.item_class(name=\"bye\", age=\"world\")\n        f = BytesIO()\n        ie = PickleItemExporter(f)\n        ie.start_exporting()\n        ie.export_item(i1)\n        ie.export_item(i2)\n        ie.finish_exporting()\n        del ie  # See the first “del self.ie” in this file for context.\n        f.seek(0)\n        assert self.item_class(**pickle.load(f)) == i1\n        assert self.item_class(**pickle.load(f)) == i2\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        fp = BytesIO()\n        ie = PickleItemExporter(fp)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first “del self.ie” in this file for context.\n        assert pickle.loads(fp.getvalue()) == item\n\n\nclass TestPickleItemExporterDataclass(TestPickleItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestMarshalItemExporter(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        self.output = tempfile.TemporaryFile()\n        return MarshalItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        self.output.seek(0)\n        self._assert_expected_item(marshal.load(self.output))\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        item.pop(\"time\")  # datetime is not marshallable\n        fp = tempfile.TemporaryFile()\n        ie = MarshalItemExporter(fp)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first “del self.ie” in this file for context.\n        fp.seek(0)\n        assert marshal.load(fp) == item\n\n\nclass TestMarshalItemExporterDataclass(TestMarshalItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestCsvItemExporter(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        self.output = tempfile.TemporaryFile()\n        return CsvItemExporter(self.output, **kwargs)\n\n    def assertCsvEqual(self, first, second, msg=None):\n        def split_csv(csv):\n            return [\n                sorted(re.split(r\"(,|\\s+)\", line))\n                for line in to_unicode(csv).splitlines(True)\n            ]\n\n        assert split_csv(first) == split_csv(second), msg\n\n    def _check_output(self):\n        self.output.seek(0)\n        self.assertCsvEqual(\n            to_unicode(self.output.read()), \"age,name\\r\\n22,John\\xa3\\r\\n\"\n        )\n\n    def assertExportResult(self, item, expected, **kwargs):\n        fp = BytesIO()\n        ie = CsvItemExporter(fp, **kwargs)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first “del self.ie” in this file for context.\n        self.assertCsvEqual(fp.getvalue(), expected)\n\n    def test_header_export_all(self):\n        self.assertExportResult(\n            item=self.i,\n            fields_to_export=ItemAdapter(self.i).field_names(),\n            expected=b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n\",\n        )\n\n    def test_header_export_all_dict(self):\n        self.assertExportResult(\n            item=ItemAdapter(self.i).asdict(),\n            expected=b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n\",\n        )\n\n    def test_header_export_single_field(self):\n        for item in [self.i, ItemAdapter(self.i).asdict()]:\n            self.assertExportResult(\n                item=item,\n                fields_to_export=[\"age\"],\n                expected=b\"age\\r\\n22\\r\\n\",\n            )\n", "n_tokens": 1198, "byte_len": 5197, "file_sha1": "f26c70d29a196bf3c037c7e8e829d8c8aec4ae80", "start_line": 168, "end_line": 321}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py", "rel_path": "tests/test_exporters.py", "module": "tests.test_exporters", "ext": "py", "chunk_number": 3, "symbols": ["test_header_export_two_items", "test_header_no_header_line", "test_join_multivalue", "test_join_multivalue_not_strings", "test_nonstring_types_item", "test_errors_default", "test_errors_xmlcharrefreplace", "_get_exporter", "assertXmlEquivalent", "xmltuple", "xmlsplit", "assertExportResult", "_check_output", "test_multivalued_fields", "test_nested_item", "TestItem2", "TestCsvItemExporterDataclass", "TestXmlItemExporter", "encoding", "test", "header", "custom", "field", "csv", "base", "join", "xml", "item", "name", "pytest", "custom_serializer", "setup_method", "_assert_expected_item", "_get_nonstring_types_item", "assertItemExportWorks", "test_export_item", "test_export_dict_item", "test_serialize_field", "test_fields_to_export", "test_field_custom_serializer", "test_invalid_option", "test_export_list", "test_export_item_dict_list", "test_export_multiple_items", "assertCsvEqual", "split_csv", "test_header_export_all", "test_header_export_all_dict", "test_header_export_single_field", "test_nested_list_item"], "ast_kind": "class_or_type", "text": "    def test_header_export_two_items(self):\n        for item in [self.i, ItemAdapter(self.i).asdict()]:\n            output = BytesIO()\n            ie = CsvItemExporter(output)\n            ie.start_exporting()\n            ie.export_item(item)\n            ie.export_item(item)\n            ie.finish_exporting()\n            del ie  # See the first “del self.ie” in this file for context.\n            self.assertCsvEqual(\n                output.getvalue(), b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n\"\n            )\n\n    def test_header_no_header_line(self):\n        for item in [self.i, ItemAdapter(self.i).asdict()]:\n            self.assertExportResult(\n                item=item,\n                include_headers_line=False,\n                expected=b\"22,John\\xc2\\xa3\\r\\n\",\n            )\n\n    def test_join_multivalue(self):\n        class TestItem2(Item):\n            name = Field()\n            friends = Field()\n\n        for cls in TestItem2, dict:\n            self.assertExportResult(\n                item=cls(name=\"John\", friends=[\"Mary\", \"Paul\"]),\n                include_headers_line=False,\n                expected='\"Mary,Paul\",John\\r\\n',\n            )\n\n    def test_join_multivalue_not_strings(self):\n        self.assertExportResult(\n            item={\"name\": \"John\", \"friends\": [4, 8]},\n            include_headers_line=False,\n            expected='\"[4, 8]\",John\\r\\n',\n        )\n\n    def test_nonstring_types_item(self):\n        self.assertExportResult(\n            item=self._get_nonstring_types_item(),\n            include_headers_line=False,\n            expected=\"22,False,3.14,2015-01-01 01:01:01\\r\\n\",\n        )\n\n    def test_errors_default(self):\n        with pytest.raises(UnicodeEncodeError):\n            self.assertExportResult(\n                item={\"text\": \"W\\u0275\\u200brd\"},\n                expected=None,\n                encoding=\"windows-1251\",\n            )\n\n    def test_errors_xmlcharrefreplace(self):\n        self.assertExportResult(\n            item={\"text\": \"W\\u0275\\u200brd\"},\n            include_headers_line=False,\n            expected=\"W&#629;&#8203;rd\\r\\n\",\n            encoding=\"windows-1251\",\n            errors=\"xmlcharrefreplace\",\n        )\n\n\nclass TestCsvItemExporterDataclass(TestCsvItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestXmlItemExporter(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        return XmlItemExporter(self.output, **kwargs)\n\n    def assertXmlEquivalent(self, first, second, msg=None):\n        def xmltuple(elem):\n            children = list(elem.iterchildren())\n            if children:\n                return [(child.tag, sorted(xmltuple(child))) for child in children]\n            return [(elem.tag, [(elem.text, ())])]\n\n        def xmlsplit(xmlcontent):\n            doc = lxml.etree.fromstring(xmlcontent)\n            return xmltuple(doc)\n\n        assert xmlsplit(first) == xmlsplit(second), msg\n\n    def assertExportResult(self, item, expected_value):\n        fp = BytesIO()\n        ie = XmlItemExporter(fp)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first “del self.ie” in this file for context.\n        self.assertXmlEquivalent(fp.getvalue(), expected_value)\n\n    def _check_output(self):\n        expected_value = (\n            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n            b\"<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>\"\n        )\n        self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n\n    def test_multivalued_fields(self):\n        self.assertExportResult(\n            self.item_class(name=[\"John\\xa3\", \"Doe\"], age=[1, 2, 3]),\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n            <items>\n                <item>\n                    <name><value>John\\xc2\\xa3</value><value>Doe</value></name>\n                    <age><value>1</value><value>2</value><value>3</value></age>\n                </item>\n            </items>\n            \"\"\",\n        )\n\n    def test_nested_item(self):\n        i1 = {\"name\": \"foo\\xa3hoo\", \"age\": \"22\"}\n        i2 = {\"name\": \"bar\", \"age\": i1}\n        i3 = self.item_class(name=\"buz\", age=i2)\n\n        self.assertExportResult(\n            i3,\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n                <items>\n                    <item>\n                        <age>\n                            <age>\n                                <age>22</age>\n                                <name>foo\\xc2\\xa3hoo</name>\n                            </age>\n                            <name>bar</name>\n                        </age>\n                        <name>buz</name>\n                    </item>\n                </items>\n            \"\"\",\n        )\n", "n_tokens": 1088, "byte_len": 4772, "file_sha1": "f26c70d29a196bf3c037c7e8e829d8c8aec4ae80", "start_line": 322, "end_line": 460}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py", "rel_path": "tests/test_exporters.py", "module": "tests.test_exporters", "ext": "py", "chunk_number": 4, "symbols": ["test_nested_list_item", "test_nonstring_types_item", "_get_exporter", "_check_output", "test_nested_item", "test_extra_keywords", "assertTwoItemsExported", "test_two_items", "test_two_dict_items", "test_two_items_with_failure_between", "TestXmlItemExporterDataclass", "TestJsonLinesItemExporter", "TestJsonLinesItemExporterDataclass", "TestJsonItemExporter", "encoding", "joseph", "custom", "field", "test", "base", "loads", "exported", "python", "name", "nested", "didn", "between", "pytest", "two", "unicode", "custom_serializer", "setup_method", "_assert_expected_item", "_get_nonstring_types_item", "assertItemExportWorks", "test_export_item", "test_export_dict_item", "test_serialize_field", "test_fields_to_export", "test_field_custom_serializer", "test_invalid_option", "test_export_list", "test_export_item_dict_list", "test_export_multiple_items", "assertCsvEqual", "split_csv", "assertExportResult", "test_header_export_all", "test_header_export_all_dict", "test_header_export_single_field"], "ast_kind": "class_or_type", "text": "    def test_nested_list_item(self):\n        i1 = {\"name\": \"foo\"}\n        i2 = {\"name\": \"bar\", \"v2\": {\"egg\": [\"spam\"]}}\n        i3 = self.item_class(name=\"buz\", age=[i1, i2])\n\n        self.assertExportResult(\n            i3,\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n                <items>\n                    <item>\n                        <age>\n                            <value><name>foo</name></value>\n                            <value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>\n                        </age>\n                        <name>buz</name>\n                    </item>\n                </items>\n            \"\"\",\n        )\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        self.assertExportResult(\n            item,\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n                <items>\n                   <item>\n                       <float>3.14</float>\n                       <boolean>False</boolean>\n                       <number>22</number>\n                       <time>2015-01-01 01:01:01</time>\n                   </item>\n                </items>\n            \"\"\",\n        )\n\n\nclass TestXmlItemExporterDataclass(TestXmlItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestJsonLinesItemExporter(TestBaseItemExporter):\n    _expected_nested: Any = {\n        \"name\": \"Jesus\",\n        \"age\": {\"name\": \"Maria\", \"age\": {\"name\": \"Joseph\", \"age\": \"22\"}},\n    }\n\n    def _get_exporter(self, **kwargs):\n        return JsonLinesItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        exported = json.loads(to_unicode(self.output.getvalue().strip()))\n        assert exported == ItemAdapter(self.i).asdict()\n\n    def test_nested_item(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = {\"name\": \"Maria\", \"age\": i1}\n        i3 = self.item_class(name=\"Jesus\", age=i2)\n        self.ie.start_exporting()\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        del self.ie  # See the first “del self.ie” in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        assert exported == self._expected_nested\n\n    def test_extra_keywords(self):\n        self.ie = self._get_exporter(sort_keys=True)\n        self.test_export_item()\n        self._check_output()\n        with pytest.raises(TypeError):\n            self._get_exporter(foo_unknown_keyword_bar=True)\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        del self.ie  # See the first “del self.ie” in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        item[\"time\"] = str(item[\"time\"])\n        assert exported == item\n\n\nclass TestJsonLinesItemExporterDataclass(TestJsonLinesItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestJsonItemExporter(TestJsonLinesItemExporter):\n    _expected_nested = [TestJsonLinesItemExporter._expected_nested]\n\n    def _get_exporter(self, **kwargs):\n        return JsonItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        exported = json.loads(to_unicode(self.output.getvalue().strip()))\n        assert exported == [ItemAdapter(self.i).asdict()]\n\n    def assertTwoItemsExported(self, item):\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        del self.ie  # See the first “del self.ie” in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        assert exported == [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()]\n\n    def test_two_items(self):\n        self.assertTwoItemsExported(self.i)\n\n    def test_two_dict_items(self):\n        self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n\n    def test_two_items_with_failure_between(self):\n        i1 = MyItem(name=\"Joseph\\xa3\", age=\"22\")\n        i2 = MyItem(\n            name=\"Maria\", age=1j\n        )  # Invalid datetimes didn't consistently fail between Python versions\n        i3 = MyItem(name=\"Jesus\", age=\"44\")\n        self.ie.start_exporting()\n        self.ie.export_item(i1)\n        with pytest.raises(TypeError):\n            self.ie.export_item(i2)\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        assert exported == [dict(i1), dict(i3)]\n\n    def test_nested_item(self):\n        i1 = self.item_class(name=\"Joseph\\xa3\", age=\"22\")\n        i2 = self.item_class(name=\"Maria\", age=i1)\n        i3 = self.item_class(name=\"Jesus\", age=i2)\n        self.ie.start_exporting()\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        del self.ie  # See the first “del self.ie” in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        expected = {\n            \"name\": \"Jesus\",\n            \"age\": {\"name\": \"Maria\", \"age\": ItemAdapter(i1).asdict()},\n        }\n        assert exported == [expected]\n", "n_tokens": 1207, "byte_len": 5256, "file_sha1": "f26c70d29a196bf3c037c7e8e829d8c8aec4ae80", "start_line": 461, "end_line": 604}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_exporters.py", "rel_path": "tests/test_exporters.py", "module": "tests.test_exporters", "ext": "py", "chunk_number": 5, "symbols": ["test_nested_dict_item", "test_nonstring_types_item", "_get_exporter", "test_two_items_with_failure_between", "setup_method", "test_exporter_custom_serializer", "serialize_field", "export_item", "TestJsonItemExporterToBytes", "TestJsonItemExporterDataclass", "TestCustomExporterItem", "CustomItemExporter", "TestCustomExporterDataclass", "encoding", "finish", "exporting", "joseph", "expected", "test", "json", "item", "myitem", "u263a", "loads", "base", "custom", "field", "pass", "exported", "export", "custom_serializer", "_check_output", "_assert_expected_item", "_get_nonstring_types_item", "assertItemExportWorks", "test_export_item", "test_export_dict_item", "test_serialize_field", "test_fields_to_export", "test_field_custom_serializer", "test_invalid_option", "test_nested_item", "test_export_list", "test_export_item_dict_list", "test_export_multiple_items", "assertCsvEqual", "split_csv", "assertExportResult", "test_header_export_all", "test_header_export_all_dict"], "ast_kind": "class_or_type", "text": "    def test_nested_dict_item(self):\n        i1 = {\"name\": \"Joseph\\xa3\", \"age\": \"22\"}\n        i2 = self.item_class(name=\"Maria\", age=i1)\n        i3 = {\"name\": \"Jesus\", \"age\": i2}\n        self.ie.start_exporting()\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        del self.ie  # See the first “del self.ie” in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        expected = {\"name\": \"Jesus\", \"age\": {\"name\": \"Maria\", \"age\": i1}}\n        assert exported == [expected]\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        del self.ie  # See the first “del self.ie” in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        item[\"time\"] = str(item[\"time\"])\n        assert exported == [item]\n\n\nclass TestJsonItemExporterToBytes(TestBaseItemExporter):\n    def _get_exporter(self, **kwargs):\n        kwargs[\"encoding\"] = \"latin\"\n        return JsonItemExporter(self.output, **kwargs)\n\n    def test_two_items_with_failure_between(self):\n        i1 = MyItem(name=\"Joseph\", age=\"22\")\n        i2 = MyItem(name=\"\\u263a\", age=\"11\")\n        i3 = MyItem(name=\"Jesus\", age=\"44\")\n        self.ie.start_exporting()\n        self.ie.export_item(i1)\n        with pytest.raises(UnicodeEncodeError):\n            self.ie.export_item(i2)\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        exported = json.loads(to_unicode(self.output.getvalue(), encoding=\"latin\"))\n        assert exported == [dict(i1), dict(i3)]\n\n\nclass TestJsonItemExporterDataclass(TestJsonItemExporter):\n    item_class = MyDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass TestCustomExporterItem:\n    item_class: type = MyItem\n\n    def setup_method(self):\n        if self.item_class is None:\n            pytest.skip(\"item class is None\")\n\n    def test_exporter_custom_serializer(self):\n        class CustomItemExporter(BaseItemExporter):\n            def serialize_field(self, field, name, value):\n                if name == \"age\":\n                    return str(int(value) + 1)\n                return super().serialize_field(field, name, value)\n\n            def export_item(self, item: Any) -> None:\n                pass\n\n        i = self.item_class(name=\"John\", age=\"22\")\n        a = ItemAdapter(i)\n        ie = CustomItemExporter()\n\n        assert ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"]) == \"John\"\n        assert ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"]) == \"23\"\n\n        i2 = {\"name\": \"John\", \"age\": \"22\"}\n        assert ie.serialize_field({}, \"name\", i2[\"name\"]) == \"John\"\n        assert ie.serialize_field({}, \"age\", i2[\"age\"]) == \"23\"\n\n\nclass TestCustomExporterDataclass(TestCustomExporterItem):\n    item_class = MyDataClass\n", "n_tokens": 693, "byte_len": 2929, "file_sha1": "f26c70d29a196bf3c037c7e8e829d8c8aec4ae80", "start_line": 605, "end_line": 683}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_throttle.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_throttle.py", "rel_path": "tests/test_extension_throttle.py", "module": "tests.test_extension_throttle", "ext": "py", "chunk_number": 1, "symbols": ["get_crawler", "test_enabled", "test_target_concurrency_invalid", "test_mindelay_definition", "test_maxdelay_definition", "test_startdelay_definition", "_TestSpider", "default", "spider", "settings", "dict", "spidercls", "autothrottl", "star", "expected", "test", "maxdelay", "false", "mock", "return", "mindelay", "target", "name", "mark", "class", "parametrize", "start", "setting", "with", "scrapy", "test_skipped", "test_adjustment", "test_adjustment_limits", "test_adjustment_bad_response", "test_debug", "test_debug_disabled", "adjustment", "latency", "slot", "delay", "level", "concurrency", "https", "get", "crawler", "instead", "pytest", "record", "tuples", "targe"], "ast_kind": "class_or_type", "text": "from logging import INFO\nfrom unittest.mock import Mock\n\nimport pytest\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions.throttle import AutoThrottle\nfrom scrapy.http.response import Response\nfrom scrapy.settings.default_settings import (\n    AUTOTHROTTLE_MAX_DELAY,\n    AUTOTHROTTLE_START_DELAY,\n    DOWNLOAD_DELAY,\n)\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler as _get_crawler\n\nUNSET = object()\n\n\ndef get_crawler(settings=None, spidercls=None):\n    settings = settings or {}\n    settings[\"AUTOTHROTTLE_ENABLED\"] = True\n    return _get_crawler(settings_dict=settings, spidercls=spidercls)\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    [\n        (UNSET, False),\n        (False, False),\n        (True, True),\n    ],\n)\ndef test_enabled(value, expected):\n    settings = {}\n    if value is not UNSET:\n        settings[\"AUTOTHROTTLE_ENABLED\"] = value\n    crawler = _get_crawler(settings_dict=settings)\n    if expected:\n        build_from_crawler(AutoThrottle, crawler)\n    else:\n        with pytest.raises(NotConfigured):\n            build_from_crawler(AutoThrottle, crawler)\n\n\n@pytest.mark.parametrize(\n    \"value\",\n    [\n        0.0,\n        -1.0,\n    ],\n)\ndef test_target_concurrency_invalid(value):\n    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": value}\n    crawler = get_crawler(settings)\n    with pytest.raises(NotConfigured):\n        build_from_crawler(AutoThrottle, crawler)\n\n\n@pytest.mark.parametrize(\n    (\"spider\", \"setting\", \"expected\"),\n    [\n        (UNSET, UNSET, DOWNLOAD_DELAY),\n        (1.0, UNSET, 1.0),\n        (UNSET, 1.0, 1.0),\n        (1.0, 2.0, 1.0),\n        (3.0, 2.0, 3.0),\n    ],\n)\ndef test_mindelay_definition(spider, setting, expected):\n    settings = {}\n    if setting is not UNSET:\n        settings[\"DOWNLOAD_DELAY\"] = setting\n\n    class _TestSpider(Spider):\n        name = \"test\"\n\n    if spider is not UNSET:\n        _TestSpider.download_delay = spider\n\n    crawler = get_crawler(settings, _TestSpider)\n    at = build_from_crawler(AutoThrottle, crawler)\n    at._spider_opened(_TestSpider())\n    assert at.mindelay == expected\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    [\n        (UNSET, AUTOTHROTTLE_MAX_DELAY),\n        (1.0, 1.0),\n    ],\n)\ndef test_maxdelay_definition(value, expected):\n    settings = {}\n    if value is not UNSET:\n        settings[\"AUTOTHROTTLE_MAX_DELAY\"] = value\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    at._spider_opened(DefaultSpider())\n    assert at.maxdelay == expected\n\n\n@pytest.mark.parametrize(\n    (\"min_spider\", \"min_setting\", \"start_setting\", \"expected\"),\n    [\n        (UNSET, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n        (AUTOTHROTTLE_START_DELAY - 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n        (AUTOTHROTTLE_START_DELAY + 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n        (UNSET, AUTOTHROTTLE_START_DELAY - 1.0, UNSET, AUTOTHROTTLE_START_DELAY),\n        (UNSET, AUTOTHROTTLE_START_DELAY + 1.0, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n        (UNSET, UNSET, AUTOTHROTTLE_START_DELAY - 1.0, AUTOTHROTTLE_START_DELAY - 1.0),\n        (UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0, AUTOTHROTTLE_START_DELAY + 1.0),\n        (\n            AUTOTHROTTLE_START_DELAY + 1.0,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n            UNSET,\n            AUTOTHROTTLE_START_DELAY + 1.0,\n        ),\n        (\n            AUTOTHROTTLE_START_DELAY + 2.0,\n            UNSET,\n            AUTOTHROTTLE_START_DELAY + 1.0,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n        ),\n        (\n            AUTOTHROTTLE_START_DELAY + 1.0,\n            UNSET,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n        ),\n    ],\n)\ndef test_startdelay_definition(min_spider, min_setting, start_setting, expected):\n    settings = {}\n    if min_setting is not UNSET:\n        settings[\"DOWNLOAD_DELAY\"] = min_setting\n    if start_setting is not UNSET:\n        settings[\"AUTOTHROTTLE_START_DELAY\"] = start_setting\n", "n_tokens": 1129, "byte_len": 4124, "file_sha1": "444d3a03363a083cea9ee102fd5f0a9a99facda6", "start_line": 1, "end_line": 142}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_throttle.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_throttle.py", "rel_path": "tests/test_extension_throttle.py", "module": "tests.test_extension_throttle", "ext": "py", "chunk_number": 2, "symbols": ["test_skipped", "test_adjustment", "test_adjustment_limits", "_TestSpider", "default", "spider", "download", "slot", "downloader", "limits", "expected", "mock", "these", "exception", "test", "adjustment", "name", "mark", "class", "parametrize", "meta", "delay", "adjust", "with", "autothrottle", "dont", "target", "concurrency", "latency", "https", "get_crawler", "test_enabled", "test_target_concurrency_invalid", "test_mindelay_definition", "test_maxdelay_definition", "test_startdelay_definition", "test_adjustment_bad_response", "test_debug", "test_debug_disabled", "spidercls", "autothrottl", "star", "level", "get", "crawler", "instead", "pytest", "settings", "maxdelay", "record"], "ast_kind": "class_or_type", "text": "    class _TestSpider(Spider):\n        name = \"test\"\n\n    if min_spider is not UNSET:\n        _TestSpider.download_delay = min_spider\n\n    crawler = get_crawler(settings, _TestSpider)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = _TestSpider()\n    at._spider_opened(spider)\n    assert spider.download_delay == expected\n\n\n@pytest.mark.parametrize(\n    (\"meta\", \"slot\"),\n    [\n        ({}, None),\n        ({\"download_latency\": 1.0}, None),\n        ({\"download_slot\": \"foo\"}, None),\n        ({\"download_slot\": \"foo\"}, \"foo\"),\n        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None),\n        (\n            {\n                \"download_latency\": 1.0,\n                \"download_slot\": \"foo\",\n                \"autothrottle_dont_adjust_delay\": True,\n            },\n            \"foo\",\n        ),\n    ],\n)\ndef test_skipped(meta, slot):\n    crawler = get_crawler()\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = DefaultSpider()\n    at._spider_opened(spider)\n    request = Request(\"https://example.com\", meta=meta)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    if slot is not None:\n        crawler.engine.downloader.slots[slot] = object()\n    at._adjust_delay = None  # Raise exception if called.\n\n    at._response_downloaded(None, request, spider)\n\n\n@pytest.mark.parametrize(\n    (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n    [\n        (2.0, 2.0, 1.0, 1.0),\n        (1.0, 2.0, 1.0, 0.75),\n        (4.0, 2.0, 1.0, 2.0),\n        (2.0, 1.0, 1.0, 2.0),\n        (2.0, 4.0, 1.0, 0.75),\n        (2.0, 2.0, 0.5, 1.0),\n        (2.0, 2.0, 2.0, 1.5),\n    ],\n)\ndef test_adjustment(download_latency, target_concurrency, slot_delay, expected):\n    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = DefaultSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = slot_delay\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    at._response_downloaded(response, request, spider)\n\n    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n\n\n@pytest.mark.parametrize(\n    (\"mindelay\", \"maxdelay\", \"expected\"),\n    [\n        (0.5, 2.0, 1.0),\n        (0.25, 0.5, 0.5),\n        (2.0, 4.0, 2.0),\n    ],\n)\ndef test_adjustment_limits(mindelay, maxdelay, expected):\n    download_latency, target_concurrency, slot_delay = (2.0, 2.0, 1.0)\n    # expected adjustment without limits with these values: 1.0\n    settings = {\n        \"AUTOTHROTTLE_MAX_DELAY\": maxdelay,\n        \"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency,\n        \"DOWNLOAD_DELAY\": mindelay,\n    }\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = DefaultSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = slot_delay\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    at._response_downloaded(response, request, spider)\n\n    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n\n\n@pytest.mark.parametrize(\n    (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n    [\n        (2.0, 2.0, 1.0, 1.0),\n        (1.0, 2.0, 1.0, 1.0),  # Instead of 0.75\n        (4.0, 2.0, 1.0, 2.0),\n    ],\n)", "n_tokens": 1111, "byte_len": 3854, "file_sha1": "444d3a03363a083cea9ee102fd5f0a9a99facda6", "start_line": 143, "end_line": 268}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_throttle.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_throttle.py", "rel_path": "tests/test_extension_throttle.py", "module": "tests.test_extension_throttle", "ext": "py", "chunk_number": 3, "symbols": ["test_adjustment_bad_response", "test_debug", "test_debug_disabled", "default", "spider", "download", "slot", "downloader", "test", "adjustment", "expected", "mock", "debug", "latency", "meta", "delay", "with", "level", "size", "scrapy", "target", "concurrency", "https", "example", "throttle", "conc", "body", "get", "crawler", "build", "get_crawler", "test_enabled", "test_target_concurrency_invalid", "test_mindelay_definition", "test_maxdelay_definition", "test_startdelay_definition", "test_skipped", "test_adjustment", "test_adjustment_limits", "_TestSpider", "spidercls", "autothrottl", "star", "name", "instead", "pytest", "settings", "maxdelay", "record", "tuples"], "ast_kind": "function_or_method", "text": "def test_adjustment_bad_response(\n    download_latency, target_concurrency, slot_delay, expected\n):\n    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = DefaultSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url, status=400)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = slot_delay\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    at._response_downloaded(response, request, spider)\n\n    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n\n\ndef test_debug(caplog):\n    settings = {\"AUTOTHROTTLE_DEBUG\": True}\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = DefaultSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url, body=b\"foo\")\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = 2.0\n    slot.transferring = (None, None)\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    caplog.clear()\n    with caplog.at_level(INFO):\n        at._response_downloaded(response, request, spider)\n\n    assert caplog.record_tuples == [\n        (\n            \"scrapy.extensions.throttle\",\n            INFO,\n            \"slot: foo | conc: 2 | delay: 1500 ms (-500) | latency: 1000 ms | size:     3 bytes\",\n        ),\n    ]\n\n\ndef test_debug_disabled(caplog):\n    crawler = get_crawler()\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = DefaultSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url, body=b\"foo\")\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = 2.0\n    slot.transferring = (None, None)\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    caplog.clear()\n    with caplog.at_level(INFO):\n        at._response_downloaded(response, request, spider)\n\n    assert caplog.record_tuples == []\n", "n_tokens": 627, "byte_len": 2500, "file_sha1": "444d3a03363a083cea9ee102fd5f0a9a99facda6", "start_line": 269, "end_line": 346}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_periodic_log.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_periodic_log.py", "rel_path": "tests/test_extension_periodic_log.py", "module": "tests.test_extension_periodic_log", "ext": "py", "chunk_number": 1, "symbols": ["set_a", "set_b", "extension", "test_extension_enabled", "test_log_delta", "emulate", "CustomPeriodicLog", "TestPeriodicLog", "stats", "test", "periodic", "scheduler", "enabled", "filtered", "spiders", "dump", "future", "dupefilter", "get", "crawler", "request", "depth", "periodi", "settings", "dequeued", "none", "reason", "response", "count", "memory", "check", "test_log_stats", "condition", "items", "isinstance", "log", "callable", "expected", "bytes", "warning", "successfully", "spider", "typing", "getdict", "return", "delta", "annotations", "class", "not", "configured"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport datetime\nfrom typing import Any, Callable\n\nfrom scrapy.extensions.periodic_log import PeriodicLog\nfrom scrapy.utils.test import get_crawler\n\nfrom .spiders import MetaSpider\n\nstats_dump_1 = {\n    \"log_count/INFO\": 10,\n    \"log_count/WARNING\": 1,\n    \"start_time\": datetime.datetime(2023, 6, 16, 8, 59, 18, 993170),\n    \"scheduler/enqueued/memory\": 190,\n    \"scheduler/enqueued\": 190,\n    \"scheduler/dequeued/memory\": 166,\n    \"scheduler/dequeued\": 166,\n    \"downloader/request_count\": 166,\n    \"downloader/request_method_count/GET\": 166,\n    \"downloader/request_bytes\": 56803,\n    \"downloader/response_count\": 150,\n    \"downloader/response_status_count/200\": 150,\n    \"downloader/response_bytes\": 595698,\n    \"httpcompression/response_bytes\": 3186068,\n    \"httpcompression/response_count\": 150,\n    \"response_received_count\": 150,\n    \"request_depth_max\": 9,\n    \"dupefilter/filtered\": 180,\n    \"item_scraped_count\": 140,\n}\nstats_dump_2 = {\n    \"log_count/INFO\": 12,\n    \"log_count/WARNING\": 1,\n    \"start_time\": datetime.datetime(2023, 6, 16, 8, 59, 18, 993170),\n    \"scheduler/enqueued/memory\": 337,\n    \"scheduler/enqueued\": 337,\n    \"scheduler/dequeued/memory\": 280,\n    \"scheduler/dequeued\": 280,\n    \"downloader/request_count\": 280,\n    \"downloader/request_method_count/GET\": 280,\n    \"downloader/request_bytes\": 95754,\n    \"downloader/response_count\": 264,\n    \"downloader/response_status_count/200\": 264,\n    \"downloader/response_bytes\": 1046274,\n    \"httpcompression/response_bytes\": 5614484,\n    \"httpcompression/response_count\": 264,\n    \"response_received_count\": 264,\n    \"request_depth_max\": 16,\n    \"dupefilter/filtered\": 320,\n    \"item_scraped_count\": 248,\n}\n\n\nclass CustomPeriodicLog(PeriodicLog):\n    def set_a(self):\n        self.stats._stats = stats_dump_1\n\n    def set_b(self):\n        self.stats._stats = stats_dump_2\n\n\ndef extension(settings: dict[str, Any] | None = None) -> CustomPeriodicLog:\n    crawler = get_crawler(MetaSpider, settings)\n    return CustomPeriodicLog.from_crawler(crawler)\n\n\nclass TestPeriodicLog:\n    def test_extension_enabled(self):\n        # Expected that settings for this extension loaded successfully\n        # And on certain conditions - extension raising NotConfigured\n\n        # \"PERIODIC_LOG_STATS\": True -> set to {\"enabled\": True}\n        # due to TypeError exception from settings.getdict\n        assert extension({\"PERIODIC_LOG_STATS\": True, \"LOGSTATS_INTERVAL\": 60})\n\n        # \"PERIODIC_LOG_STATS\": \"True\" -> set to {\"enabled\": True}\n        # due to JSONDecodeError(ValueError) exception from settings.getdict\n        assert extension({\"PERIODIC_LOG_STATS\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n\n        # The ame for PERIODIC_LOG_DELTA:\n        assert extension({\"PERIODIC_LOG_DELTA\": True, \"LOGSTATS_INTERVAL\": 60})\n        assert extension({\"PERIODIC_LOG_DELTA\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n\n    def test_log_delta(self):\n        def emulate(settings=None):\n            spider = MetaSpider()\n            ext = extension(settings)\n            ext.spider_opened(spider)\n            ext.set_a()\n            a = ext.log_delta()\n            ext.set_a()\n            b = ext.log_delta()\n            ext.spider_closed(spider, reason=\"finished\")\n            return ext, a, b\n", "n_tokens": 859, "byte_len": 3276, "file_sha1": "3590c0650b35a452bf730fc3181f57a1c9c1bbac", "start_line": 1, "end_line": 96}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_periodic_log.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_periodic_log.py", "rel_path": "tests/test_extension_periodic_log.py", "module": "tests.test_extension_periodic_log", "ext": "py", "chunk_number": 2, "symbols": ["check", "test_log_stats", "emulate", "stats", "downloader", "periodi", "delta", "spider", "meta", "return", "dict", "lambda", "scheduler", "condition", "combined", "test", "log", "closed", "extension", "multiple", "true", "settings", "exclude", "list", "assert", "set", "including", "items", "isinstance", "keys", "set_a", "set_b", "test_extension_enabled", "test_log_delta", "CustomPeriodicLog", "TestPeriodicLog", "periodic", "enabled", "filtered", "spiders", "dump", "future", "dupefilter", "get", "crawler", "request", "depth", "dequeued", "none", "reason"], "ast_kind": "function_or_method", "text": "        def check(settings: dict[str, Any], condition: Callable) -> None:\n            ext, a, b = emulate(settings)\n            assert list(a[\"delta\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n            assert list(b[\"delta\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n\n        # Including all\n        check({\"PERIODIC_LOG_DELTA\": True}, lambda k, v: isinstance(v, (int, float)))\n\n        # include:\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]}},\n            lambda k, v: isinstance(v, (int, float)) and \"downloader/\" in k,\n        )\n\n        # include multiple\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: isinstance(v, (int, float))\n            and (\"downloader/\" in k or \"scheduler/\" in k),\n        )\n\n        # exclude\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\"]}},\n            lambda k, v: isinstance(v, (int, float)) and \"downloader/\" not in k,\n        )\n\n        # exclude multiple\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: isinstance(v, (int, float))\n            and (\"downloader/\" not in k and \"scheduler/\" not in k),\n        )\n\n        # include exclude combined\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n            lambda k, v: isinstance(v, (int, float))\n            and (\"downloader/\" in k and \"bytes\" not in k),\n        )\n\n    def test_log_stats(self):\n        def emulate(settings=None):\n            spider = MetaSpider()\n            ext = extension(settings)\n            ext.spider_opened(spider)\n            ext.set_a()\n            a = ext.log_crawler_stats()\n            ext.set_a()\n            b = ext.log_crawler_stats()\n            ext.spider_closed(spider, reason=\"finished\")\n            return ext, a, b\n\n        def check(settings: dict[str, Any], condition: Callable) -> None:\n            ext, a, b = emulate(settings)\n            assert list(a[\"stats\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n            assert list(b[\"stats\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n\n        # Including all\n        check({\"PERIODIC_LOG_STATS\": True}, lambda k, v: True)\n\n        # include:\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"]}},\n            lambda k, v: \"downloader/\" in k,\n        )\n\n        # include multiple\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: \"downloader/\" in k or \"scheduler/\" in k,\n        )\n\n        # exclude\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\"]}},\n            lambda k, v: \"downloader/\" not in k,\n        )\n\n        # exclude multiple\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: \"downloader/\" not in k and \"scheduler/\" not in k,\n        )\n\n        # include exclude combined\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n            lambda k, v: \"downloader/\" in k and \"bytes\" not in k,\n        )\n", "n_tokens": 830, "byte_len": 3443, "file_sha1": "3590c0650b35a452bf730fc3181f57a1c9c1bbac", "start_line": 97, "end_line": 195}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_defer.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_defer.py", "rel_path": "tests/test_utils_defer.py", "module": "tests.test_utils_defer", "ext": "py", "chunk_number": 1, "symbols": ["test_success_function", "_append", "_assert", "test_unfired_deferred", "cb1", "cb2", "cb3", "cb_fail", "eb1", "test_iter_errback_good", "itergood", "test_iter_errback_bad", "iterbad", "TestMustbeDeferred", "TestIterErrback", "TestAiterErrback", "TestAsyncDefTestsuite", "test", "success", "deferred", "future", "async", "arg", "arg2", "iter", "append", "aiter", "strict", "from", "typ", "callable", "callable_wrapped", "decrement", "get_async_iterable", "test_simple", "test_delays", "test_deferred", "test_object", "test_coroutine", "test_coroutine_asyncio", "test_future", "_assert_result", "c_f", "TestParallelAsync", "TestDeferredFromCoro", "TestDeferredFFromCoroF", "TestDeferredToFuture", "TestMaybeDeferredToFutureAsyncio", "TestMaybeDeferredToFutureNotAsyncio", "wrapped"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport random\nfrom asyncio import Future\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\nfrom twisted.internet.defer import Deferred, inlineCallbacks, succeed\n\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.defer import (\n    aiter_errback,\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    deferred_to_future,\n    iter_errback,\n    maybe_deferred_to_future,\n    mustbe_deferred,\n    parallel_async,\n)\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Awaitable, Callable, Generator\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestMustbeDeferred:\n    @inlineCallbacks\n    def test_success_function(self) -> Generator[Deferred[Any], Any, None]:\n        steps: list[int] = []\n\n        def _append(v: int) -> list[int]:\n            steps.append(v)\n            return steps\n\n        def _assert(v: list[int]) -> None:\n            assert v == [1, 2]  # it is [1] with maybeDeferred\n\n        dfd = mustbe_deferred(_append, 1)\n        dfd.addCallback(_assert)\n        steps.append(2)  # add another value, that should be caught by assertEqual\n        yield dfd\n\n    @inlineCallbacks\n    def test_unfired_deferred(self) -> Generator[Deferred[Any], Any, None]:\n        steps: list[int] = []\n\n        def _append(v: int) -> Deferred[list[int]]:\n            from twisted.internet import reactor\n\n            steps.append(v)\n            dfd: Deferred[list[int]] = Deferred()\n            reactor.callLater(0, dfd.callback, steps)\n            return dfd\n\n        def _assert(v: list[int]) -> None:\n            assert v == [1, 2]\n\n        dfd = mustbe_deferred(_append, 1)\n        dfd.addCallback(_assert)\n        steps.append(2)  # add another value, that should be caught by assertEqual\n        yield dfd\n\n\ndef cb1(value, arg1, arg2):\n    return f\"(cb1 {value} {arg1} {arg2})\"\n\n\ndef cb2(value, arg1, arg2):\n    return succeed(f\"(cb2 {value} {arg1} {arg2})\")\n\n\ndef cb3(value, arg1, arg2):\n    return f\"(cb3 {value} {arg1} {arg2})\"\n\n\ndef cb_fail(value, arg1, arg2):\n    raise TypeError\n\n\ndef eb1(failure, arg1, arg2):\n    return f\"(eb1 {failure.value.__class__.__name__} {arg1} {arg2})\"\n\n\nclass TestIterErrback:\n    def test_iter_errback_good(self):\n        def itergood() -> Generator[int, None, None]:\n            yield from range(10)\n\n        errors = []\n        out = list(iter_errback(itergood(), errors.append))\n        assert out == list(range(10))\n        assert not errors\n\n    def test_iter_errback_bad(self):\n        def iterbad() -> Generator[int, None, None]:\n            for x in range(10):\n                if x == 5:\n                    1 / 0\n                yield x\n\n        errors = []\n        out = list(iter_errback(iterbad(), errors.append))\n        assert out == [0, 1, 2, 3, 4]\n        assert len(errors) == 1\n        assert isinstance(errors[0].value, ZeroDivisionError)\n\n\nclass TestAiterErrback:\n    @deferred_f_from_coro_f\n    async def test_aiter_errback_good(self):\n        async def itergood() -> AsyncGenerator[int, None]:\n            for x in range(10):\n                yield x\n\n        errors = []\n        out = await collect_asyncgen(aiter_errback(itergood(), errors.append))\n        assert out == list(range(10))\n        assert not errors\n\n    @deferred_f_from_coro_f\n    async def test_iter_errback_bad(self):\n        async def iterbad() -> AsyncGenerator[int, None]:\n            for x in range(10):\n                if x == 5:\n                    1 / 0\n                yield x\n\n        errors = []\n        out = await collect_asyncgen(aiter_errback(iterbad(), errors.append))\n        assert out == [0, 1, 2, 3, 4]\n        assert len(errors) == 1\n        assert isinstance(errors[0].value, ZeroDivisionError)\n\n\nclass TestAsyncDefTestsuite:\n    @deferred_f_from_coro_f\n    async def test_deferred_f_from_coro_f(self):\n        pass\n\n    @deferred_f_from_coro_f\n    async def test_deferred_f_from_coro_f_generator(self):\n        yield\n\n    @pytest.mark.xfail(reason=\"Checks that the test is actually executed\", strict=True)\n    @deferred_f_from_coro_f\n    async def test_deferred_f_from_coro_f_xfail(self):\n        raise RuntimeError(\"This is expected to be raised\")\n\n", "n_tokens": 1089, "byte_len": 4250, "file_sha1": "3763c32c2a42dad43876ddcbdc6c03b54bf0979a", "start_line": 1, "end_line": 151}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_defer.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_defer.py", "rel_path": "tests/test_utils_defer.py", "module": "tests.test_utils_defer", "ext": "py", "chunk_number": 2, "symbols": ["callable", "callable_wrapped", "decrement", "get_async_iterable", "test_simple", "test_delays", "test_deferred", "test_object", "test_coroutine", "test_coroutine_asyncio", "test_future", "_assert_result", "TestParallelAsync", "TestDeferredFromCoro", "TestDeferredFFromCoroF", "async", "wrapped", "getting", "test", "deferred", "append", "future", "after", "from", "sleep", "between", "length", "assert", "result", "get", "test_success_function", "_append", "_assert", "test_unfired_deferred", "cb1", "cb2", "cb3", "cb_fail", "eb1", "test_iter_errback_good", "itergood", "test_iter_errback_bad", "iterbad", "c_f", "TestMustbeDeferred", "TestIterErrback", "TestAiterErrback", "TestAsyncDefTestsuite", "TestDeferredToFuture", "TestMaybeDeferredToFutureAsyncio"], "ast_kind": "class_or_type", "text": "class TestParallelAsync:\n    \"\"\"This tests _AsyncCooperatorAdapter by testing parallel_async which is its only usage.\n\n    parallel_async is called with the results of a callback (so an iterable of items, requests and None,\n    with arbitrary delays between values), and it uses Scraper._process_spidermw_output as the callable\n    (so a callable that returns a Deferred for an item, which will fire after pipelines process it, and\n    None for everything else). The concurrent task count is the CONCURRENT_ITEMS setting.\n\n    We want to test different concurrency values compared to the iterable length.\n    We also want to simulate the real usage, with arbitrary delays between getting the values\n    from the iterable. We also want to simulate sync and async results from the callable.\n    \"\"\"\n\n    CONCURRENT_ITEMS = 50\n\n    @staticmethod\n    def callable(o: int, results: list[int]) -> Deferred[None] | None:\n        from twisted.internet import reactor\n\n        if random.random() < 0.4:\n            # simulate async processing\n            dfd: Deferred[None] = Deferred()\n            dfd.addCallback(lambda _: results.append(o))\n            delay = random.random() / 8\n            reactor.callLater(delay, dfd.callback, None)\n            return dfd\n        # simulate trivial sync processing\n        results.append(o)\n        return None\n\n    def callable_wrapped(\n        self,\n        o: int,\n        results: list[int],\n        parallel_count: list[int],\n        max_parallel_count: list[int],\n    ) -> Deferred[None] | None:\n        parallel_count[0] += 1\n        max_parallel_count[0] = max(max_parallel_count[0], parallel_count[0])\n        dfd = self.callable(o, results)\n\n        def decrement(_: Any = None) -> None:\n            assert parallel_count[0] > 0, parallel_count[0]\n            parallel_count[0] -= 1\n\n        if dfd is not None:\n            dfd.addBoth(decrement)\n        else:\n            decrement()\n        return dfd\n\n    @staticmethod\n    def get_async_iterable(length: int) -> AsyncGenerator[int, None]:\n        # simulate a simple callback without delays between results\n        return as_async_generator(range(length))\n\n    @staticmethod\n    async def get_async_iterable_with_delays(length: int) -> AsyncGenerator[int, None]:\n        # simulate a callback with delays between some of the results\n        from twisted.internet import reactor\n\n        for i in range(length):\n            if random.random() < 0.1:\n                dfd: Deferred[None] = Deferred()\n                delay = random.random() / 20\n                reactor.callLater(delay, dfd.callback, None)\n                await maybe_deferred_to_future(dfd)\n            yield i\n\n    @inlineCallbacks\n    def test_simple(self):\n        for length in [20, 50, 100]:\n            parallel_count = [0]\n            max_parallel_count = [0]\n            results = []\n            ait = self.get_async_iterable(length)\n            dl = parallel_async(\n                ait,\n                self.CONCURRENT_ITEMS,\n                self.callable_wrapped,\n                results,\n                parallel_count,\n                max_parallel_count,\n            )\n            yield dl\n            assert list(range(length)) == sorted(results)\n            assert parallel_count[0] == 0\n            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS, max_parallel_count[0]\n\n    @inlineCallbacks\n    def test_delays(self):\n        for length in [20, 50, 100]:\n            parallel_count = [0]\n            max_parallel_count = [0]\n            results = []\n            ait = self.get_async_iterable_with_delays(length)\n            dl = parallel_async(\n                ait,\n                self.CONCURRENT_ITEMS,\n                self.callable_wrapped,\n                results,\n                parallel_count,\n                max_parallel_count,\n            )\n            yield dl\n            assert list(range(length)) == sorted(results)\n            assert parallel_count[0] == 0\n            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS, max_parallel_count[0]\n\n\nclass TestDeferredFromCoro:\n    def test_deferred(self):\n        d = Deferred()\n        result = deferred_from_coro(d)\n        assert isinstance(result, Deferred)\n        assert result is d\n\n    def test_object(self):\n        result = deferred_from_coro(42)\n        assert result == 42\n\n    @inlineCallbacks\n    def test_coroutine(self):\n        async def coroutine() -> int:\n            return 42\n\n        result = deferred_from_coro(coroutine())\n        assert isinstance(result, Deferred)\n        coro_result = yield result\n        assert coro_result == 42\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_coroutine_asyncio(self):\n        async def coroutine() -> int:\n            await asyncio.sleep(0.01)\n            return 42\n\n        result = deferred_from_coro(coroutine())\n        assert isinstance(result, Deferred)\n        coro_result = yield result\n        assert coro_result == 42\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_future(self):\n        future = Future()\n        result = deferred_from_coro(future)\n        assert isinstance(result, Deferred)\n        future.set_result(42)\n        future_result = yield result\n        assert future_result == 42\n\n\nclass TestDeferredFFromCoroF:\n    @inlineCallbacks\n    def _assert_result(\n        self, c_f: Callable[[], Awaitable[int]]\n    ) -> Generator[Deferred[Any], Any, None]:\n        d_f = deferred_f_from_coro_f(c_f)\n        d = d_f()\n        assert isinstance(d, Deferred)\n        result = yield d\n        assert result == 42\n\n    @inlineCallbacks\n    def test_coroutine(self):\n        async def c_f() -> int:\n            return 42\n\n        yield self._assert_result(c_f)\n", "n_tokens": 1242, "byte_len": 5722, "file_sha1": "3763c32c2a42dad43876ddcbdc6c03b54bf0979a", "start_line": 152, "end_line": 323}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_defer.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_defer.py", "rel_path": "tests/test_utils_defer.py", "module": "tests.test_utils_defer", "ext": "py", "chunk_number": 3, "symbols": ["test_coroutine_asyncio", "test_future", "c_f", "test_deferred", "TestDeferredToFuture", "TestMaybeDeferredToFutureAsyncio", "TestMaybeDeferredToFutureNotAsyncio", "deferred", "future", "async", "await", "result", "test", "wrapped", "return", "only", "not", "mark", "from", "asyncio", "class", "sleep", "maybe", "assert", "yield", "coroutine", "pytest", "set", "isinstance", "inline", "test_success_function", "_append", "_assert", "test_unfired_deferred", "cb1", "cb2", "cb3", "cb_fail", "eb1", "test_iter_errback_good", "itergood", "test_iter_errback_bad", "iterbad", "callable", "callable_wrapped", "decrement", "get_async_iterable", "test_simple", "test_delays", "test_object"], "ast_kind": "class_or_type", "text": "    @inlineCallbacks\n    def test_coroutine_asyncio(self):\n        async def c_f() -> int:\n            return 42\n\n        yield self._assert_result(c_f)\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_future(self):\n        def c_f() -> Future[int]:\n            f: Future[int] = Future()\n            f.set_result(42)\n            return f\n\n        yield self._assert_result(c_f)\n\n\n@pytest.mark.only_asyncio\nclass TestDeferredToFuture:\n    @deferred_f_from_coro_f\n    async def test_deferred(self):\n        d = Deferred()\n        result = deferred_to_future(d)\n        assert isinstance(result, Future)\n        d.callback(42)\n        future_result = await result\n        assert future_result == 42\n\n    @deferred_f_from_coro_f\n    async def test_wrapped_coroutine(self):\n        async def c_f() -> int:\n            return 42\n\n        d = deferred_from_coro(c_f())\n        result = deferred_to_future(d)\n        assert isinstance(result, Future)\n        future_result = await result\n        assert future_result == 42\n\n    @deferred_f_from_coro_f\n    async def test_wrapped_coroutine_asyncio(self):\n        async def c_f() -> int:\n            await asyncio.sleep(0.01)\n            return 42\n\n        d = deferred_from_coro(c_f())\n        result = deferred_to_future(d)\n        assert isinstance(result, Future)\n        future_result = await result\n        assert future_result == 42\n\n\n@pytest.mark.only_asyncio\nclass TestMaybeDeferredToFutureAsyncio:\n    @deferred_f_from_coro_f\n    async def test_deferred(self):\n        d = Deferred()\n        result = maybe_deferred_to_future(d)\n        assert isinstance(result, Future)\n        d.callback(42)\n        future_result = await result\n        assert future_result == 42\n\n    @deferred_f_from_coro_f\n    async def test_wrapped_coroutine(self):\n        async def c_f() -> int:\n            return 42\n\n        d = deferred_from_coro(c_f())\n        result = maybe_deferred_to_future(d)\n        assert isinstance(result, Future)\n        future_result = await result\n        assert future_result == 42\n\n    @deferred_f_from_coro_f\n    async def test_wrapped_coroutine_asyncio(self):\n        async def c_f() -> int:\n            await asyncio.sleep(0.01)\n            return 42\n\n        d = deferred_from_coro(c_f())\n        result = maybe_deferred_to_future(d)\n        assert isinstance(result, Future)\n        future_result = await result\n        assert future_result == 42\n\n\n@pytest.mark.only_not_asyncio\nclass TestMaybeDeferredToFutureNotAsyncio:\n    def test_deferred(self):\n        d = Deferred()\n        result = maybe_deferred_to_future(d)\n        assert isinstance(result, Deferred)\n        assert result is d\n", "n_tokens": 622, "byte_len": 2671, "file_sha1": "3763c32c2a42dad43876ddcbdc6c03b54bf0979a", "start_line": 324, "end_line": 419}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_toplevel.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_toplevel.py", "rel_path": "tests/test_toplevel.py", "module": "tests.test_toplevel", "ext": "py", "chunk_number": 1, "symbols": ["test_version", "test_version_info", "test_request_shortcut", "test_spider_shortcut", "test_selector_shortcut", "test_item_shortcut", "test", "version", "info", "spider", "selector", "item", "form", "request", "spiders", "scrapy", "field", "plc0415", "noqa", "from", "assert", "tuple", "isinstance", "import", "http"], "ast_kind": "function_or_method", "text": "import scrapy\n\n\ndef test_version():\n    assert isinstance(scrapy.__version__, str)\n\n\ndef test_version_info():\n    assert isinstance(scrapy.version_info, tuple)\n\n\ndef test_request_shortcut():\n    from scrapy.http import FormRequest, Request  # noqa: PLC0415\n\n    assert scrapy.Request is Request\n    assert scrapy.FormRequest is FormRequest\n\n\ndef test_spider_shortcut():\n    from scrapy.spiders import Spider  # noqa: PLC0415\n\n    assert scrapy.Spider is Spider\n\n\ndef test_selector_shortcut():\n    from scrapy.selector import Selector  # noqa: PLC0415\n\n    assert scrapy.Selector is Selector\n\n\ndef test_item_shortcut():\n    from scrapy.item import Field, Item  # noqa: PLC0415\n\n    assert scrapy.Item is Item\n    assert scrapy.Field is Field\n", "n_tokens": 165, "byte_len": 741, "file_sha1": "d41e8eda5b70b2e439d0e28a6f586ce87834ab2c", "start_line": 1, "end_line": 36}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_reactor.py", "rel_path": "tests/test_utils_reactor.py", "module": "tests.test_utils_reactor", "ext": "py", "chunk_number": 1, "symbols": ["test_is_asyncio_reactor_installed", "test_install_asyncio_reactor", "TestAsyncio", "async", "test", "install", "result", "argument", "internet", "set", "asyncio", "twisted", "reactor", "pytest", "deferred", "from", "reimported", "class", "mark", "only", "with", "warning", "scrapy", "record", "defer", "original", "pylint", "warnings", "true", "assert", "none", "catch", "utils", "depend", "import", "should", "get", "running", "self", "disable"], "ast_kind": "class_or_type", "text": "import asyncio\nimport warnings\n\nimport pytest\n\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.reactor import (\n    _asyncio_reactor_path,\n    install_reactor,\n    is_asyncio_reactor_installed,\n    set_asyncio_event_loop,\n)\n\n\nclass TestAsyncio:\n    def test_is_asyncio_reactor_installed(self, reactor_pytest: str) -> None:\n        # the result should depend only on the pytest --reactor argument\n        assert is_asyncio_reactor_installed() == (reactor_pytest == \"asyncio\")\n\n    def test_install_asyncio_reactor(self):\n        from twisted.internet import reactor as original_reactor\n\n        with warnings.catch_warnings(record=True) as w:\n            install_reactor(_asyncio_reactor_path)\n            assert len(w) == 0, [str(warning) for warning in w]\n        from twisted.internet import reactor  # pylint: disable=reimported\n\n        assert original_reactor == reactor\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_set_asyncio_event_loop(self):\n        install_reactor(_asyncio_reactor_path)\n        assert set_asyncio_event_loop(None) is asyncio.get_running_loop()\n", "n_tokens": 257, "byte_len": 1129, "file_sha1": "45bfc282e2137ccef03458ed418cdc9a444fd17e", "start_line": 1, "end_line": 35}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_serialize.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_serialize.py", "rel_path": "tests/test_utils_serialize.py", "module": "tests.test_utils_serialize", "ext": "py", "chunk_number": 1, "symbols": ["encoder", "test_encode_decode", "test_encode_deferred", "test_encode_request", "test_encode_response", "test_encode_dataclass_item", "test_encode_attrs_item", "TestJsonEncoder", "TestDataClass", "AttrsItem", "test", "encode", "dumps", "method", "price", "internet", "scrapy", "json", "twisted", "return", "serialize", "name", "item", "class", "set", "sets", "output", "product", "sort", "keys", "attr", "time", "lala", "decimal", "defer", "example", "decs", "date", "dataclass", "true", "pytest", "data", "from", "input", "assert", "request", "encoded", "none", "fixture", "attrs"], "ast_kind": "class_or_type", "text": "import dataclasses\nimport datetime\nimport json\nfrom decimal import Decimal\n\nimport attr\nimport pytest\nfrom twisted.internet import defer\n\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\n\nclass TestJsonEncoder:\n    @pytest.fixture\n    def encoder(self) -> ScrapyJSONEncoder:\n        return ScrapyJSONEncoder(sort_keys=True)\n\n    def test_encode_decode(self, encoder: ScrapyJSONEncoder) -> None:\n        dt = datetime.datetime(2010, 1, 2, 10, 11, 12)\n        dts = \"2010-01-02 10:11:12\"\n        d = datetime.date(2010, 1, 2)\n        ds = \"2010-01-02\"\n        t = datetime.time(10, 11, 12)\n        ts = \"10:11:12\"\n        dec = Decimal(\"1000.12\")\n        decs = \"1000.12\"\n        s = {\"foo\"}\n        ss = [\"foo\"]\n        dt_set = {dt}\n        dt_sets = [dts]\n\n        for input_, output in [\n            (\"foo\", \"foo\"),\n            (d, ds),\n            (t, ts),\n            (dt, dts),\n            (dec, decs),\n            ([\"foo\", d], [\"foo\", ds]),\n            (s, ss),\n            (dt_set, dt_sets),\n        ]:\n            assert encoder.encode(input_) == json.dumps(output, sort_keys=True)\n\n    def test_encode_deferred(self, encoder: ScrapyJSONEncoder) -> None:\n        assert \"Deferred\" in encoder.encode(defer.Deferred())\n\n    def test_encode_request(self, encoder: ScrapyJSONEncoder) -> None:\n        r = Request(\"http://www.example.com/lala\")\n        rs = encoder.encode(r)\n        assert r.method in rs\n        assert r.url in rs\n\n    def test_encode_response(self, encoder: ScrapyJSONEncoder) -> None:\n        r = Response(\"http://www.example.com/lala\")\n        rs = encoder.encode(r)\n        assert r.url in rs\n        assert str(r.status) in rs\n\n    def test_encode_dataclass_item(self, encoder: ScrapyJSONEncoder) -> None:\n        @dataclasses.dataclass\n        class TestDataClass:\n            name: str\n            url: str\n            price: int\n\n        item = TestDataClass(name=\"Product\", url=\"http://product.org\", price=1)\n        encoded = encoder.encode(item)\n        assert encoded == '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n\n    def test_encode_attrs_item(self, encoder: ScrapyJSONEncoder) -> None:\n        @attr.s\n        class AttrsItem:\n            name = attr.ib(type=str)\n            url = attr.ib(type=str)\n            price = attr.ib(type=int)\n\n        item = AttrsItem(name=\"Product\", url=\"http://product.org\", price=1)\n        encoded = encoder.encode(item)\n        assert encoded == '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n", "n_tokens": 658, "byte_len": 2547, "file_sha1": "64fe76c5096775ddd4710b52bde4be69ec71cab2", "start_line": 1, "end_line": 81}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_gz.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_gz.py", "rel_path": "tests/test_utils_gz.py", "module": "tests.test_utils_gz", "ext": "py", "chunk_number": 1, "symbols": ["test_gunzip_basic", "test_gunzip_truncated", "test_gunzip_no_gzip_file_raises", "test_gunzip_truncated_short", "test_is_gzipped_empty", "test_gunzip_illegal_eof", "encoding", "test", "gunzip", "text", "gzip", "charset", "lib", "w3lib", "output", "unexpected", "with", "cp1252", "scrapy", "bad", "example", "pathlib", "path", "body", "sample", "sample1", "sampledir", "pytest", "from", "gzipped", "tests", "datadir", "assert", "raises", "truncated", "html", "unicode", "endswith", "compressed", "magic", "utils", "expected", "import", "short", "http", "feed", "read", "bytes", "error", "response"], "ast_kind": "function_or_method", "text": "from gzip import BadGzipFile\nfrom pathlib import Path\n\nimport pytest\nfrom w3lib.encoding import html_to_unicode\n\nfrom scrapy.http import Response\nfrom scrapy.utils.gz import gunzip, gzip_magic_number\nfrom tests import tests_datadir\n\nSAMPLEDIR = Path(tests_datadir, \"compressed\")\n\n\ndef test_gunzip_basic():\n    r1 = Response(\n        \"http://www.example.com\",\n        body=(SAMPLEDIR / \"feed-sample1.xml.gz\").read_bytes(),\n    )\n    assert gzip_magic_number(r1)\n\n    r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n    assert not gzip_magic_number(r2)\n    assert len(r2.body) == 9950\n\n\ndef test_gunzip_truncated():\n    text = gunzip((SAMPLEDIR / \"truncated-crc-error.gz\").read_bytes())\n    assert text.endswith(b\"</html\")\n\n\ndef test_gunzip_no_gzip_file_raises():\n    with pytest.raises(BadGzipFile):\n        gunzip((SAMPLEDIR / \"feed-sample1.xml\").read_bytes())\n\n\ndef test_gunzip_truncated_short():\n    r1 = Response(\n        \"http://www.example.com\",\n        body=(SAMPLEDIR / \"truncated-crc-error-short.gz\").read_bytes(),\n    )\n    assert gzip_magic_number(r1)\n\n    r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n    assert r2.body.endswith(b\"</html>\")\n    assert not gzip_magic_number(r2)\n\n\ndef test_is_gzipped_empty():\n    r1 = Response(\"http://www.example.com\")\n    assert not gzip_magic_number(r1)\n\n\ndef test_gunzip_illegal_eof():\n    text = html_to_unicode(\n        \"charset=cp1252\", gunzip((SAMPLEDIR / \"unexpected-eof.gz\").read_bytes())\n    )[1]\n    expected_text = (SAMPLEDIR / \"unexpected-eof-output.txt\").read_text(\n        encoding=\"utf-8\"\n    )\n    assert len(text) == len(expected_text)\n    assert text == expected_text\n", "n_tokens": 430, "byte_len": 1663, "file_sha1": "81d3762f6d23af1c37d633a07a211c8269301dd6", "start_line": 1, "end_line": 62}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_responsetypes.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_responsetypes.py", "rel_path": "tests/test_responsetypes.py", "module": "tests.test_responsetypes", "ext": "py", "chunk_number": 1, "symbols": ["test_from_filename", "test_from_content_disposition", "test_from_content_type", "test_from_body", "test_from_headers", "test_from_args", "test_custom_mime_types_loaded", "TestResponseTypes", "encoding", "takes", "guess", "type", "mimetypes", "from", "filename", "stream", "between", "more", "https", "codersblock", "headers", "test", "encode", "shipped", "html", "http", "response", "gzip", "attachment", "html5", "over", "item", "class", "download", "files", "example", "mime", "xml", "scrapytest", "tabs", "page", "responsetypes", "amazonui", "iso", "iso2022", "ndata", "blog", "json", "self", "tests"], "ast_kind": "class_or_type", "text": "from scrapy.http import (\n    Headers,\n    HtmlResponse,\n    JsonResponse,\n    Response,\n    TextResponse,\n    XmlResponse,\n)\nfrom scrapy.responsetypes import responsetypes\n\n\nclass TestResponseTypes:\n    def test_from_filename(self):\n        mappings = [\n            (\"data.bin\", Response),\n            (\"file.txt\", TextResponse),\n            (\"file.xml.gz\", Response),\n            (\"file.xml\", XmlResponse),\n            (\"file.html\", HtmlResponse),\n            (\"file.unknownext\", Response),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_filename(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_content_disposition(self):\n        mappings = [\n            (b'attachment; filename=\"data.xml\"', XmlResponse),\n            (b\"attachment; filename=data.xml\", XmlResponse),\n            (\"attachment;filename=data£.tar.gz\".encode(), Response),\n            (\"attachment;filename=dataµ.tar.gz\".encode(\"latin-1\"), Response),\n            (\"attachment;filename=data高.doc\".encode(\"gbk\"), Response),\n            (\"attachment;filename=دورهdata.html\".encode(\"cp720\"), HtmlResponse),\n            (\n                \"attachment;filename=日本語版Wikipedia.xml\".encode(\"iso2022_jp\"),\n                XmlResponse,\n            ),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_content_disposition(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_content_type(self):\n        mappings = [\n            (\"text/html; charset=UTF-8\", HtmlResponse),\n            (\"text/xml; charset=UTF-8\", XmlResponse),\n            (\"application/xhtml+xml; charset=UTF-8\", HtmlResponse),\n            (\"application/vnd.wap.xhtml+xml; charset=utf-8\", HtmlResponse),\n            (\"application/xml; charset=UTF-8\", XmlResponse),\n            (\"application/octet-stream\", Response),\n            (\"application/json; encoding=UTF8;charset=UTF-8\", JsonResponse),\n            (\"application/x-json; encoding=UTF8;charset=UTF-8\", JsonResponse),\n            (\"application/json-amazonui-streaming;charset=UTF-8\", JsonResponse),\n            (b\"application/x-download; filename=\\x80dummy.txt\", Response),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_content_type(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_body(self):\n        mappings = [\n            (b\"\\x03\\x02\\xdf\\xdd\\x23\", Response),\n            (b\"Some plain text\\ndata with tabs\\t and null bytes\\0\", TextResponse),\n            (b\"<html><head><title>Hello</title></head>\", HtmlResponse),\n            # https://codersblock.com/blog/the-smallest-valid-html5-page/\n            (b\"<!DOCTYPE html>\\n<title>.</title>\", HtmlResponse),\n            (b'<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_body(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_headers(self):\n        mappings = [\n            ({\"Content-Type\": [\"text/html; charset=utf-8\"]}, HtmlResponse),\n            (\n                {\n                    \"Content-Type\": [\"text/html; charset=utf-8\"],\n                    \"Content-Encoding\": [\"gzip\"],\n                },\n                Response,\n            ),\n            (\n                {\n                    \"Content-Type\": [\"application/octet-stream\"],\n                    \"Content-Disposition\": [\"attachment; filename=data.txt\"],\n                },\n                TextResponse,\n            ),\n        ]\n        for source, cls in mappings:\n            source = Headers(source)\n            retcls = responsetypes.from_headers(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_args(self):\n        # TODO: add more tests that check precedence between the different arguments\n        mappings = [\n            ({\"url\": \"http://www.example.com/data.csv\"}, TextResponse),\n            # headers takes precedence over url\n            (\n                {\n                    \"headers\": Headers({\"Content-Type\": [\"text/html; charset=utf-8\"]}),\n                    \"url\": \"http://www.example.com/item/\",\n                },\n                HtmlResponse,\n            ),\n            (\n                {\n                    \"headers\": Headers(\n                        {\"Content-Disposition\": ['attachment; filename=\"data.xml.gz\"']}\n                    ),\n                    \"url\": \"http://www.example.com/page/\",\n                },\n                Response,\n            ),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_args(**source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_custom_mime_types_loaded(self):\n        # check that mime.types files shipped with scrapy are loaded\n        assert responsetypes.mimetypes.guess_type(\"x.scrapytest\")[0] == \"x-scrapy/test\"\n", "n_tokens": 1064, "byte_len": 4999, "file_sha1": "34c54d655b9cc287cee98f0aa657557b86dc7e51", "start_line": 1, "end_line": 125}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py", "rel_path": "tests/test_downloadermiddleware_httpcompression.py", "module": "tests.test_downloadermiddleware_httpcompression", "ext": "py", "chunk_number": 1, "symbols": ["_skip_if_no_br", "_skip_if_no_zstd", "setup_method", "_getresponse", "assertStatsEqual", "test_setting_false_compression_enabled", "test_setting_default_compression_enabled", "test_setting_true_compression_enabled", "test_process_request", "test_process_response_gzip", "TestHttpCompression", "encoding", "samplefile", "compressio", "enabled", "zstandard", "lib", "w3lib", "spider", "yaws", "spiders", "path", "get", "crawler", "gzip", "file", "newresponse", "pytest", "skip", "zstd", "test_process_response_br", "test_process_response_br_unsupported", "test_process_response_zstd", "test_process_response_zstd_unsupported", "test_process_response_rawdeflate", "test_process_response_zlibdelate", "test_process_response_plain", "test_multipleencodings", "test_multi_compression_single_header", "test_multi_compression_single_header_invalid_compression", "test_multi_compression_multiple_header", "test_multi_compression_multiple_header_invalid_compression", "test_multi_compression_single_and_multiple_header", "test_multi_compression_single_and_multiple_header_invalid_compression", "test_process_response_encoding_inside_body", "test_process_response_force_recalculate_encoding", "test_process_response_no_content_type_header", "test_process_response_gzipped_contenttype", "test_process_response_gzip_app_octetstream_contenttype", "test_process_response_gzip_binary_octetstream_contenttype"], "ast_kind": "class_or_type", "text": "from gzip import GzipFile\nfrom io import BytesIO\nfrom logging import WARNING\nfrom pathlib import Path\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom w3lib.encoding import resolve_encoding\n\nfrom scrapy.downloadermiddlewares.httpcompression import (\n    ACCEPTED_ENCODINGS,\n    HttpCompressionMiddleware,\n)\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.gz import gunzip\nfrom scrapy.utils.test import get_crawler\nfrom tests import tests_datadir\n\nSAMPLEDIR = Path(tests_datadir, \"compressed\")\n\nFORMAT = {\n    \"gzip\": (\"html-gzip.bin\", \"gzip\"),\n    \"x-gzip\": (\"html-gzip.bin\", \"x-gzip\"),\n    \"rawdeflate\": (\"html-rawdeflate.bin\", \"deflate\"),\n    \"zlibdeflate\": (\"html-zlibdeflate.bin\", \"deflate\"),\n    \"gzip-deflate\": (\"html-gzip-deflate.bin\", \"gzip, deflate\"),\n    \"gzip-deflate-gzip\": (\"html-gzip-deflate-gzip.bin\", \"gzip, deflate, gzip\"),\n    \"br\": (\"html-br.bin\", \"br\"),\n    # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin\n    \"zstd-static-content-size\": (\"html-zstd-static-content-size.bin\", \"zstd\"),\n    # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin\n    \"zstd-static-no-content-size\": (\"html-zstd-static-no-content-size.bin\", \"zstd\"),\n    # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin\n    \"zstd-streaming-no-content-size\": (\n        \"html-zstd-streaming-no-content-size.bin\",\n        \"zstd\",\n    ),\n    **{\n        f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)\n        for format_id in (\n            \"br\",  # 34 → 11 511 612\n            \"deflate\",  # 27 968 → 11 511 612\n            \"gzip\",  # 27 988 → 11 511 612\n            \"zstd\",  # 1 096 → 11 511 612\n        )\n    },\n}\n\n\ndef _skip_if_no_br() -> None:\n    try:\n        try:\n            import brotli  # noqa: F401,PLC0415\n        except ImportError:\n            import brotlicffi  # noqa: F401,PLC0415\n    except ImportError:\n        pytest.skip(\"no brotli support\")\n\n\ndef _skip_if_no_zstd() -> None:\n    try:\n        import zstandard  # noqa: F401,PLC0415\n    except ImportError:\n        pytest.skip(\"no zstd support (zstandard)\")\n\n\nclass TestHttpCompression:\n    def setup_method(self):\n        self.crawler = get_crawler(Spider)\n        self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n        self.crawler.stats.open_spider()\n\n    def _getresponse(self, coding):\n        if coding not in FORMAT:\n            raise ValueError\n\n        samplefile, contentencoding = FORMAT[coding]\n\n        body = (SAMPLEDIR / samplefile).read_bytes()\n\n        headers = {\n            \"Server\": \"Yaws/1.49 Yet Another Web Server\",\n            \"Date\": \"Sun, 08 Mar 2009 00:41:03 GMT\",\n            \"Content-Length\": len(body),\n            \"Content-Type\": \"text/html\",\n            \"Content-Encoding\": contentencoding,\n        }\n\n        response = Response(\"http://scrapytest.org/\", body=body, headers=headers)\n        response.request = Request(\n            \"http://scrapytest.org\", headers={\"Accept-Encoding\": \"gzip, deflate\"}\n        )\n        return response\n\n    def assertStatsEqual(self, key, value):\n        assert self.crawler.stats.get_value(key) == value, str(\n            self.crawler.stats.get_stats()\n        )\n\n    def test_setting_false_compression_enabled(self):\n        with pytest.raises(NotConfigured):\n            HttpCompressionMiddleware.from_crawler(\n                get_crawler(settings_dict={\"COMPRESSION_ENABLED\": False})\n            )\n\n    def test_setting_default_compression_enabled(self):\n        assert isinstance(\n            HttpCompressionMiddleware.from_crawler(get_crawler()),\n            HttpCompressionMiddleware,\n        )\n\n    def test_setting_true_compression_enabled(self):\n        assert isinstance(\n            HttpCompressionMiddleware.from_crawler(\n                get_crawler(settings_dict={\"COMPRESSION_ENABLED\": True})\n            ),\n            HttpCompressionMiddleware,\n        )\n\n    def test_process_request(self):\n        request = Request(\"http://scrapytest.org\")\n        assert \"Accept-Encoding\" not in request.headers\n        self.mw.process_request(request)\n        assert request.headers.get(\"Accept-Encoding\") == b\", \".join(ACCEPTED_ENCODINGS)\n\n    def test_process_response_gzip(self):\n        response = self._getresponse(\"gzip\")\n        request = response.request\n\n        assert response.headers[\"Content-Encoding\"] == b\"gzip\"\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n", "n_tokens": 1159, "byte_len": 4880, "file_sha1": "775aabfb1d82b75b9dad21a1537f4e8f44b2548f", "start_line": 1, "end_line": 140}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py", "rel_path": "tests/test_downloadermiddleware_httpcompression.py", "module": "tests.test_downloadermiddleware_httpcompression", "ext": "py", "chunk_number": 2, "symbols": ["test_process_response_br", "test_process_response_br_unsupported", "test_process_response_zstd", "test_process_response_zstd_unsupported", "test_process_response_rawdeflate", "test_process_response_zlibdelate", "test_process_response_plain", "test_multipleencodings", "encoding", "raw", "content", "zstandard", "newresponse", "pytest", "skip", "zstd", "test", "process", "none", "response", "count", "http", "compression", "uuencode", "bytes", "warning", "format", "gzip", "cannot", "startswith", "_skip_if_no_br", "_skip_if_no_zstd", "setup_method", "_getresponse", "assertStatsEqual", "test_setting_false_compression_enabled", "test_setting_default_compression_enabled", "test_setting_true_compression_enabled", "test_process_request", "test_process_response_gzip", "test_multi_compression_single_header", "test_multi_compression_single_header_invalid_compression", "test_multi_compression_multiple_header", "test_multi_compression_multiple_header_invalid_compression", "test_multi_compression_single_and_multiple_header", "test_multi_compression_single_and_multiple_header_invalid_compression", "test_process_response_encoding_inside_body", "test_process_response_force_recalculate_encoding", "test_process_response_no_content_type_header", "test_process_response_gzipped_contenttype"], "ast_kind": "function_or_method", "text": "    def test_process_response_br(self):\n        _skip_if_no_br()\n\n        response = self._getresponse(\"br\")\n        request = response.request\n        assert response.headers[\"Content-Encoding\"] == b\"br\"\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_br_unsupported(self):\n        try:\n            try:\n                import brotli  # noqa: F401,PLC0415\n\n                pytest.skip(\"Requires not having brotli support\")\n            except ImportError:\n                import brotlicffi  # noqa: F401,PLC0415\n\n                pytest.skip(\"Requires not having brotli support\")\n        except ImportError:\n            pass\n        response = self._getresponse(\"br\")\n        request = response.request\n        assert response.headers[\"Content-Encoding\"] == b\"br\"\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            newresponse = self.mw.process_response(request, response)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"HttpCompressionMiddleware cannot decode the response for\"\n                    \" http://scrapytest.org/ from unsupported encoding(s) 'br'.\"\n                    \" You need to install brotli or brotlicffi to decode 'br'.\"\n                ),\n            ),\n        )\n        assert newresponse is not response\n        assert newresponse.headers.getlist(\"Content-Encoding\") == [b\"br\"]\n\n    def test_process_response_zstd(self):\n        _skip_if_no_zstd()\n\n        raw_content = None\n        for check_key in FORMAT:\n            if not check_key.startswith(\"zstd-\"):\n                continue\n            response = self._getresponse(check_key)\n            request = response.request\n            assert response.headers[\"Content-Encoding\"] == b\"zstd\"\n            newresponse = self.mw.process_response(request, response)\n            if raw_content is None:\n                raw_content = newresponse.body\n            else:\n                assert raw_content == newresponse.body\n            assert newresponse is not response\n            assert newresponse.body.startswith(b\"<!DOCTYPE\")\n            assert \"Content-Encoding\" not in newresponse.headers\n\n    def test_process_response_zstd_unsupported(self):\n        try:\n            import zstandard  # noqa: F401,PLC0415\n\n            pytest.skip(\"Requires not having zstandard support\")\n        except ImportError:\n            pass\n        response = self._getresponse(\"zstd-static-content-size\")\n        request = response.request\n        assert response.headers[\"Content-Encoding\"] == b\"zstd\"\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            newresponse = self.mw.process_response(request, response)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"HttpCompressionMiddleware cannot decode the response for\"\n                    \" http://scrapytest.org/ from unsupported encoding(s) 'zstd'.\"\n                    \" You need to install zstandard to decode 'zstd'.\"\n                ),\n            ),\n        )\n        assert newresponse is not response\n        assert newresponse.headers.getlist(\"Content-Encoding\") == [b\"zstd\"]\n\n    def test_process_response_rawdeflate(self):\n        response = self._getresponse(\"rawdeflate\")\n        request = response.request\n\n        assert response.headers[\"Content-Encoding\"] == b\"deflate\"\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74840)\n\n    def test_process_response_zlibdelate(self):\n        response = self._getresponse(\"zlibdeflate\")\n        request = response.request\n\n        assert response.headers[\"Content-Encoding\"] == b\"deflate\"\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74840)\n\n    def test_process_response_plain(self):\n        response = Response(\"http://scrapytest.org\", body=b\"<!DOCTYPE...\")\n        request = Request(\"http://scrapytest.org\")\n\n        assert not response.headers.get(\"Content-Encoding\")\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        self.assertStatsEqual(\"httpcompression/response_count\", None)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n\n    def test_multipleencodings(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Encoding\"] = [\"uuencode\", \"gzip\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.headers.getlist(\"Content-Encoding\") == [b\"uuencode\"]\n", "n_tokens": 1159, "byte_len": 5850, "file_sha1": "775aabfb1d82b75b9dad21a1537f4e8f44b2548f", "start_line": 141, "end_line": 280}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py", "rel_path": "tests/test_downloadermiddleware_httpcompression.py", "module": "tests.test_downloadermiddleware_httpcompression", "ext": "py", "chunk_number": 3, "symbols": ["test_multi_compression_single_header", "test_multi_compression_single_header_invalid_compression", "test_multi_compression_multiple_header", "test_multi_compression_multiple_header_invalid_compression", "test_multi_compression_single_and_multiple_header", "test_multi_compression_single_and_multiple_header_invalid_compression", "test_process_response_encoding_inside_body", "test_process_response_force_recalculate_encoding", "encoding", "test", "process", "some", "gb2312", "warning", "text", "response", "bytes", "gzip", "false", "charset", "doctype", "getvalue", "type", "multi", "cannot", "check", "meta", "startswith", "getlist", "with", "_skip_if_no_br", "_skip_if_no_zstd", "setup_method", "_getresponse", "assertStatsEqual", "test_setting_false_compression_enabled", "test_setting_default_compression_enabled", "test_setting_true_compression_enabled", "test_process_request", "test_process_response_gzip", "test_process_response_br", "test_process_response_br_unsupported", "test_process_response_zstd", "test_process_response_zstd_unsupported", "test_process_response_rawdeflate", "test_process_response_zlibdelate", "test_process_response_plain", "test_multipleencodings", "test_process_response_no_content_type_header", "test_process_response_gzipped_contenttype"], "ast_kind": "function_or_method", "text": "    def test_multi_compression_single_header(self):\n        response = self._getresponse(\"gzip-deflate\")\n        request = response.request\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert \"Content-Encoding\" not in newresponse.headers\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n    def test_multi_compression_single_header_invalid_compression(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [b\"gzip, foo, deflate\"]\n        request = response.request\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            newresponse = self.mw.process_response(request, response)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"HttpCompressionMiddleware cannot decode the response for\"\n                    \" http://scrapytest.org/ from unsupported encoding(s) 'gzip,foo'.\"\n                ),\n            ),\n        )\n        assert newresponse is not response\n        assert newresponse.headers.getlist(\"Content-Encoding\") == [b\"gzip\", b\"foo\"]\n\n    def test_multi_compression_multiple_header(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert \"Content-Encoding\" not in newresponse.headers\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n    def test_multi_compression_multiple_header_invalid_compression(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"foo\", \"deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.headers.getlist(\"Content-Encoding\") == [b\"gzip\", b\"foo\"]\n\n    def test_multi_compression_single_and_multiple_header(self):\n        response = self._getresponse(\"gzip-deflate-gzip\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"deflate, gzip\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert \"Content-Encoding\" not in newresponse.headers\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n    def test_multi_compression_single_and_multiple_header_invalid_compression(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"foo,deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.headers.getlist(\"Content-Encoding\") == [b\"gzip\", b\"foo\"]\n\n    def test_process_response_encoding_inside_body(self):\n        headers = {\n            \"Content-Type\": \"text/html\",\n            \"Content-Encoding\": \"gzip\",\n        }\n        f = BytesIO()\n        plainbody = (\n            b\"<html><head><title>Some page</title>\"\n            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n        )\n        zf = GzipFile(fileobj=f, mode=\"wb\")\n        zf.write(plainbody)\n        zf.close()\n        response = Response(\n            \"http;//www.example.com/\", headers=headers, body=f.getvalue()\n        )\n        request = Request(\"http://www.example.com/\")\n\n        newresponse = self.mw.process_response(request, response)\n        assert isinstance(newresponse, HtmlResponse)\n        assert newresponse.body == plainbody\n        assert newresponse.encoding == resolve_encoding(\"gb2312\")\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n\n    def test_process_response_force_recalculate_encoding(self):\n        headers = {\n            \"Content-Type\": \"text/html\",\n            \"Content-Encoding\": \"gzip\",\n        }\n        f = BytesIO()\n        plainbody = (\n            b\"<html><head><title>Some page</title>\"\n            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n        )\n        zf = GzipFile(fileobj=f, mode=\"wb\")\n        zf.write(plainbody)\n        zf.close()\n        response = HtmlResponse(\n            \"http;//www.example.com/page.html\", headers=headers, body=f.getvalue()\n        )\n        request = Request(\"http://www.example.com/\")\n\n        newresponse = self.mw.process_response(request, response)\n        assert isinstance(newresponse, HtmlResponse)\n        assert newresponse.body == plainbody\n        assert newresponse.encoding == resolve_encoding(\"gb2312\")\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n", "n_tokens": 1045, "byte_len": 5080, "file_sha1": "775aabfb1d82b75b9dad21a1537f4e8f44b2548f", "start_line": 281, "end_line": 395}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py", "rel_path": "tests/test_downloadermiddleware_httpcompression.py", "module": "tests.test_downloadermiddleware_httpcompression", "ext": "py", "chunk_number": 4, "symbols": ["test_process_response_no_content_type_header", "test_process_response_gzipped_contenttype", "test_process_response_gzip_app_octetstream_contenttype", "test_process_response_gzip_binary_octetstream_contenttype", "test_process_response_gzipped_gzip_file", "test_process_response_head_request_no_decode_required", "_test_compression_bomb_setting", "encoding", "method", "changefreq", "test", "process", "containing", "stream", "spider", "gunzipping", "get", "crawler", "daily", "gzip", "file", "newresponse", "pytest", "settings", "google", "equiv", "isinstance", "here", "none", "html", "_skip_if_no_br", "_skip_if_no_zstd", "setup_method", "_getresponse", "assertStatsEqual", "test_setting_false_compression_enabled", "test_setting_default_compression_enabled", "test_setting_true_compression_enabled", "test_process_request", "test_process_response_gzip", "test_process_response_br", "test_process_response_br_unsupported", "test_process_response_zstd", "test_process_response_zstd_unsupported", "test_process_response_rawdeflate", "test_process_response_zlibdelate", "test_process_response_plain", "test_multipleencodings", "test_multi_compression_single_header", "test_multi_compression_single_header_invalid_compression"], "ast_kind": "function_or_method", "text": "    def test_process_response_no_content_type_header(self):\n        headers = {\n            \"Content-Encoding\": \"identity\",\n        }\n        plainbody = (\n            b\"<html><head><title>Some page</title>\"\n            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n        )\n        respcls = responsetypes.from_args(\n            url=\"http://www.example.com/index\", headers=headers, body=plainbody\n        )\n        response = respcls(\n            \"http://www.example.com/index\", headers=headers, body=plainbody\n        )\n        request = Request(\"http://www.example.com/index\")\n\n        newresponse = self.mw.process_response(request, response)\n        assert isinstance(newresponse, respcls)\n        assert newresponse.body == plainbody\n        assert newresponse.encoding == resolve_encoding(\"gb2312\")\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n\n    def test_process_response_gzipped_contenttype(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Type\"] = \"application/gzip\"\n        request = response.request\n\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_gzip_app_octetstream_contenttype(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Type\"] = \"application/octet-stream\"\n        request = response.request\n\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_gzip_binary_octetstream_contenttype(self):\n        response = self._getresponse(\"x-gzip\")\n        response.headers[\"Content-Type\"] = \"binary/octet-stream\"\n        request = response.request\n\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_gzipped_gzip_file(self):\n        \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n        only once by the middleware, leaving gunzipping of the file\n        to upper layers.\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/gzip\",\n            \"Content-Encoding\": \"gzip\",\n        }\n        # build a gzipped file (here, a sitemap)\n        f = BytesIO()\n        plainbody = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n  <url>\n    <loc>http://www.example.com/</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>daily</changefreq>\n    <priority>1</priority>\n  </url>\n  <url>\n    <loc>http://www.example.com/Special-Offers.html</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>weekly</changefreq>\n    <priority>0.8</priority>\n  </url>\n</urlset>\"\"\"\n        gz_file = GzipFile(fileobj=f, mode=\"wb\")\n        gz_file.write(plainbody)\n        gz_file.close()\n\n        # build a gzipped response body containing this gzipped file\n        r = BytesIO()\n        gz_resp = GzipFile(fileobj=r, mode=\"wb\")\n        gz_resp.write(f.getvalue())\n        gz_resp.close()\n\n        response = Response(\n            \"http;//www.example.com/\", headers=headers, body=r.getvalue()\n        )\n        request = Request(\"http://www.example.com/\")\n\n        newresponse = self.mw.process_response(request, response)\n        assert gunzip(newresponse.body) == plainbody\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 230)\n\n    def test_process_response_head_request_no_decode_required(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Type\"] = \"application/gzip\"\n        request = response.request\n        request.method = \"HEAD\"\n        response = response.replace(body=None)\n        newresponse = self.mw.process_response(request, response)\n        assert newresponse is response\n        assert response.body == b\"\"\n        self.assertStatsEqual(\"httpcompression/response_count\", None)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n\n    def _test_compression_bomb_setting(self, compression_id):\n        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n        crawler = get_crawler(Spider, settings_dict=settings)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        with pytest.raises(IgnoreRequest):\n            mw.process_response(response.request, response)\n", "n_tokens": 1196, "byte_len": 5394, "file_sha1": "775aabfb1d82b75b9dad21a1537f4e8f44b2548f", "start_line": 396, "end_line": 523}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py", "rel_path": "tests/test_downloadermiddleware_httpcompression.py", "module": "tests.test_downloadermiddleware_httpcompression", "ext": "py", "chunk_number": 5, "symbols": ["test_compression_bomb_setting_br", "test_compression_bomb_setting_deflate", "test_compression_bomb_setting_gzip", "test_compression_bomb_setting_zstd", "_test_compression_bomb_spider_attr", "test_compression_bomb_spider_attr_br", "test_compression_bomb_spider_attr_deflate", "test_compression_bomb_spider_attr_gzip", "test_compression_bomb_spider_attr_zstd", "_test_compression_bomb_request_meta", "test_compression_bomb_request_meta_br", "test_compression_bomb_request_meta_deflate", "test_compression_bomb_request_meta_gzip", "test_compression_bomb_request_meta_zstd", "_test_download_warnsize_setting", "test_download_warnsize_setting_br", "test_download_warnsize_setting_deflate", "test_download_warnsize_setting_gzip", "test_download_warnsize_setting_zstd", "_test_download_warnsize_spider_attr", "DownloadMaxSizeSpider", "DownloadWarnSizeSpider", "test", "compression", "after", "spider", "download", "get", "crawler", "pytest", "_skip_if_no_br", "_skip_if_no_zstd", "setup_method", "_getresponse", "assertStatsEqual", "test_setting_false_compression_enabled", "test_setting_default_compression_enabled", "test_setting_true_compression_enabled", "test_process_request", "test_process_response_gzip", "test_process_response_br", "test_process_response_br_unsupported", "test_process_response_zstd", "test_process_response_zstd_unsupported", "test_process_response_rawdeflate", "test_process_response_zlibdelate", "test_process_response_plain", "test_multipleencodings", "test_multi_compression_single_header", "test_multi_compression_single_header_invalid_compression"], "ast_kind": "class_or_type", "text": "    def test_compression_bomb_setting_br(self):\n        _skip_if_no_br()\n\n        self._test_compression_bomb_setting(\"br\")\n\n    def test_compression_bomb_setting_deflate(self):\n        self._test_compression_bomb_setting(\"deflate\")\n\n    def test_compression_bomb_setting_gzip(self):\n        self._test_compression_bomb_setting(\"gzip\")\n\n    def test_compression_bomb_setting_zstd(self):\n        _skip_if_no_zstd()\n\n        self._test_compression_bomb_setting(\"zstd\")\n\n    def _test_compression_bomb_spider_attr(self, compression_id):\n        class DownloadMaxSizeSpider(Spider):\n            download_maxsize = 10_000_000\n\n        crawler = get_crawler(DownloadMaxSizeSpider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        with pytest.raises(IgnoreRequest):\n            mw.process_response(response.request, response)\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_compression_bomb_spider_attr_br(self):\n        _skip_if_no_br()\n\n        self._test_compression_bomb_spider_attr(\"br\")\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_compression_bomb_spider_attr_deflate(self):\n        self._test_compression_bomb_spider_attr(\"deflate\")\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_compression_bomb_spider_attr_gzip(self):\n        self._test_compression_bomb_spider_attr(\"gzip\")\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_compression_bomb_spider_attr_zstd(self):\n        _skip_if_no_zstd()\n\n        self._test_compression_bomb_spider_attr(\"zstd\")\n\n    def _test_compression_bomb_request_meta(self, compression_id):\n        crawler = get_crawler(Spider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        response.meta[\"download_maxsize\"] = 10_000_000\n        with pytest.raises(IgnoreRequest):\n            mw.process_response(response.request, response)\n\n    def test_compression_bomb_request_meta_br(self):\n        _skip_if_no_br()\n\n        self._test_compression_bomb_request_meta(\"br\")\n\n    def test_compression_bomb_request_meta_deflate(self):\n        self._test_compression_bomb_request_meta(\"deflate\")\n\n    def test_compression_bomb_request_meta_gzip(self):\n        self._test_compression_bomb_request_meta(\"gzip\")\n\n    def test_compression_bomb_request_meta_zstd(self):\n        _skip_if_no_zstd()\n\n        self._test_compression_bomb_request_meta(\"zstd\")\n\n    def _test_download_warnsize_setting(self, compression_id):\n        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n        crawler = get_crawler(Spider, settings_dict=settings)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n        response = self._getresponse(f\"bomb-{compression_id}\")\n\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            mw.process_response(response.request, response)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"<200 http://scrapytest.org/> body size after \"\n                    \"decompression (11511612 B) is larger than the download \"\n                    \"warning size (10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_setting_br(self):\n        _skip_if_no_br()\n\n        self._test_download_warnsize_setting(\"br\")\n\n    def test_download_warnsize_setting_deflate(self):\n        self._test_download_warnsize_setting(\"deflate\")\n\n    def test_download_warnsize_setting_gzip(self):\n        self._test_download_warnsize_setting(\"gzip\")\n\n    def test_download_warnsize_setting_zstd(self):\n        _skip_if_no_zstd()\n\n        self._test_download_warnsize_setting(\"zstd\")\n\n    def _test_download_warnsize_spider_attr(self, compression_id):\n        class DownloadWarnSizeSpider(Spider):\n            download_warnsize = 10_000_000\n\n        crawler = get_crawler(DownloadWarnSizeSpider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n        response = self._getresponse(f\"bomb-{compression_id}\")\n\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            mw.process_response(response.request, response)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"<200 http://scrapytest.org/> body size after \"\n                    \"decompression (11511612 B) is larger than the download \"\n                    \"warning size (10000000 B).\"\n                ),\n            ),\n        )\n", "n_tokens": 1164, "byte_len": 5317, "file_sha1": "775aabfb1d82b75b9dad21a1537f4e8f44b2548f", "start_line": 524, "end_line": 669}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcompression.py", "rel_path": "tests/test_downloadermiddleware_httpcompression.py", "module": "tests.test_downloadermiddleware_httpcompression", "ext": "py", "chunk_number": 6, "symbols": ["test_download_warnsize_spider_attr_br", "test_download_warnsize_spider_attr_deflate", "test_download_warnsize_spider_attr_gzip", "test_download_warnsize_spider_attr_zstd", "_test_download_warnsize_request_meta", "test_download_warnsize_request_meta_br", "test_download_warnsize_request_meta_deflate", "test_download_warnsize_request_meta_gzip", "test_download_warnsize_request_meta_zstd", "warning", "test", "download", "gzip", "false", "open", "spider", "scrapy", "deprecation", "bomb", "compression", "after", "check", "mark", "meta", "with", "downloadermiddlewares", "ignore", "size", "log", "capture", "_skip_if_no_br", "_skip_if_no_zstd", "setup_method", "_getresponse", "assertStatsEqual", "test_setting_false_compression_enabled", "test_setting_default_compression_enabled", "test_setting_true_compression_enabled", "test_process_request", "test_process_response_gzip", "test_process_response_br", "test_process_response_br_unsupported", "test_process_response_zstd", "test_process_response_zstd_unsupported", "test_process_response_rawdeflate", "test_process_response_zlibdelate", "test_process_response_plain", "test_multipleencodings", "test_multi_compression_single_header", "test_multi_compression_single_header_invalid_compression"], "ast_kind": "function_or_method", "text": "    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_download_warnsize_spider_attr_br(self):\n        _skip_if_no_br()\n\n        self._test_download_warnsize_spider_attr(\"br\")\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_download_warnsize_spider_attr_deflate(self):\n        self._test_download_warnsize_spider_attr(\"deflate\")\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_download_warnsize_spider_attr_gzip(self):\n        self._test_download_warnsize_spider_attr(\"gzip\")\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_download_warnsize_spider_attr_zstd(self):\n        _skip_if_no_zstd()\n\n        self._test_download_warnsize_spider_attr(\"zstd\")\n\n    def _test_download_warnsize_request_meta(self, compression_id):\n        crawler = get_crawler(Spider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        response.meta[\"download_warnsize\"] = 10_000_000\n\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            mw.process_response(response.request, response)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"<200 http://scrapytest.org/> body size after \"\n                    \"decompression (11511612 B) is larger than the download \"\n                    \"warning size (10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_request_meta_br(self):\n        _skip_if_no_br()\n\n        self._test_download_warnsize_request_meta(\"br\")\n\n    def test_download_warnsize_request_meta_deflate(self):\n        self._test_download_warnsize_request_meta(\"deflate\")\n\n    def test_download_warnsize_request_meta_gzip(self):\n        self._test_download_warnsize_request_meta(\"gzip\")\n\n    def test_download_warnsize_request_meta_zstd(self):\n        _skip_if_no_zstd()\n\n        self._test_download_warnsize_request_meta(\"zstd\")\n", "n_tokens": 503, "byte_len": 2344, "file_sha1": "775aabfb1d82b75b9dad21a1537f4e8f44b2548f", "start_line": 670, "end_line": 731}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_deprecate.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_deprecate.py", "rel_path": "tests/test_utils_deprecate.py", "module": "tests.test_utils_deprecate", "ext": "py", "chunk_number": 1, "symbols": ["_mywarnings", "test_no_warning_on_definition", "test_subclassing_warning_message", "test_custom_class_paths", "test_subclassing_warns_only_on_direct_children", "test_subclassing_warns_once_by_default", "test_warning_on_instance", "test_warning_auto_message", "test_issubclass", "MyWarning", "SomeBaseClass", "NewName", "TestWarnWhenSubclassed", "UserClass", "NoWarnOnMe", "FooClass", "BarClass", "UserClass2", "UpdatedUserClass1", "UpdatedUserClass1a", "OutdatedUserClass1", "OutdatedUserClass1a", "UnrelatedClass", "currentframe", "subclass", "deprecated", "new", "class", "name", "mock", "test_isinstance", "test_clsdict", "test_deprecate_a_class_with_custom_metaclass", "test_deprecate_subclass_of_deprecated_class", "test_inspect_stack", "test_old_path_gets_fixed", "test_sorted_replacement", "test_unmatched_path_stays_the_same", "test_returns_nonstring", "OldStyleClass", "UpdatedUserClass2", "UpdatedUserClass2a", "OutdatedUserClass2", "OutdatedUserClass2a", "SubClass", "TestUpdateClassPath", "sub", "error", "path", "deprecatio"], "ast_kind": "class_or_type", "text": "import inspect\nimport warnings\nfrom unittest import mock\nfrom warnings import WarningMessage\n\nimport pytest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.deprecate import create_deprecated_class, update_classpath\n\n\nclass MyWarning(UserWarning):\n    pass\n\n\nclass SomeBaseClass:\n    pass\n\n\nclass NewName(SomeBaseClass):\n    pass\n\n\nclass TestWarnWhenSubclassed:\n    def _mywarnings(\n        self, w: list[WarningMessage], category: type[Warning] = MyWarning\n    ) -> list[WarningMessage]:\n        return [x for x in w if x.category is MyWarning]\n\n    def test_no_warning_on_definition(self):\n        with warnings.catch_warnings(record=True) as w:\n            create_deprecated_class(\"Deprecated\", NewName)\n\n        w = self._mywarnings(w)\n        assert w == []\n\n    def test_subclassing_warning_message(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_category=MyWarning\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n        w = self._mywarnings(w)\n        assert len(w) == 1\n        assert (\n            str(w[0].message) == \"tests.test_utils_deprecate.UserClass inherits from \"\n            \"deprecated class tests.test_utils_deprecate.Deprecated, \"\n            \"please inherit from tests.test_utils_deprecate.NewName.\"\n            \" (warning only on first subclass, there may be others)\"\n        )\n        assert w[0].lineno == inspect.getsourcelines(UserClass)[1]\n\n    def test_custom_class_paths(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\",\n            NewName,\n            new_class_path=\"foo.NewClass\",\n            old_class_path=\"bar.OldClass\",\n            warn_category=MyWarning,\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n            _ = Deprecated()\n\n        w = self._mywarnings(w)\n        assert len(w) == 2\n        assert \"foo.NewClass\" in str(w[0].message)\n        assert \"bar.OldClass\" in str(w[0].message)\n        assert \"foo.NewClass\" in str(w[1].message)\n        assert \"bar.OldClass\" in str(w[1].message)\n\n    def test_subclassing_warns_only_on_direct_children(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_once=False, warn_category=MyWarning\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n            class NoWarnOnMe(UserClass):\n                pass\n\n        w = self._mywarnings(w)\n        assert len(w) == 1\n        assert \"UserClass\" in str(w[0].message)\n\n    def test_subclassing_warns_once_by_default(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_category=MyWarning\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n            class FooClass(Deprecated):\n                pass\n\n            class BarClass(Deprecated):\n                pass\n\n        w = self._mywarnings(w)\n        assert len(w) == 1\n        assert \"UserClass\" in str(w[0].message)\n\n    def test_warning_on_instance(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_category=MyWarning\n        )\n\n        # ignore subclassing warnings\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", MyWarning)\n\n            class UserClass(Deprecated):\n                pass\n\n        with warnings.catch_warnings(record=True) as w:\n            _, lineno = Deprecated(), inspect.getlineno(inspect.currentframe())\n            _ = UserClass()  # subclass instances don't warn\n\n        w = self._mywarnings(w)\n        assert len(w) == 1\n        assert (\n            str(w[0].message) == \"tests.test_utils_deprecate.Deprecated is deprecated, \"\n            \"instantiate tests.test_utils_deprecate.NewName instead.\"\n        )\n        assert w[0].lineno == lineno\n\n    def test_warning_auto_message(self):\n        with warnings.catch_warnings(record=True) as w:\n            Deprecated = create_deprecated_class(\"Deprecated\", NewName)\n\n            class UserClass2(Deprecated):\n                pass\n\n        msg = str(w[0].message)\n        assert \"tests.test_utils_deprecate.NewName\" in msg\n        assert \"tests.test_utils_deprecate.Deprecated\" in msg\n\n    def test_issubclass(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n\n            class UpdatedUserClass1(NewName):\n                pass\n\n            class UpdatedUserClass1a(NewName):\n                pass\n\n            class OutdatedUserClass1(DeprecatedName):\n                pass\n\n            class OutdatedUserClass1a(DeprecatedName):\n                pass\n\n            class UnrelatedClass:\n                pass\n", "n_tokens": 1079, "byte_len": 5001, "file_sha1": "4fe48859a64b019da3869c9a53980f6e04ba2894", "start_line": 1, "end_line": 171}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_deprecate.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_deprecate.py", "rel_path": "tests/test_utils_deprecate.py", "module": "tests.test_utils_deprecate", "ext": "py", "chunk_number": 2, "symbols": ["test_isinstance", "test_clsdict", "test_deprecate_a_class_with_custom_metaclass", "test_deprecate_subclass_of_deprecated_class", "test_inspect_stack", "test_old_path_gets_fixed", "test_sorted_replacement", "test_unmatched_path_stays_the_same", "test_returns_nonstring", "OldStyleClass", "UpdatedUserClass2", "UpdatedUserClass2a", "OutdatedUserClass2", "OutdatedUserClass2a", "UnrelatedClass", "UserClass", "SubClass", "TestUpdateClassPath", "sub", "class", "error", "new", "path", "name", "mock", "deprecatio", "rules", "notastring", "outdated", "user", "_mywarnings", "test_no_warning_on_definition", "test_subclassing_warning_message", "test_custom_class_paths", "test_subclassing_warns_only_on_direct_children", "test_subclassing_warns_once_by_default", "test_warning_on_instance", "test_warning_auto_message", "test_issubclass", "MyWarning", "SomeBaseClass", "NewName", "TestWarnWhenSubclassed", "NoWarnOnMe", "FooClass", "BarClass", "UserClass2", "UpdatedUserClass1", "UpdatedUserClass1a", "OutdatedUserClass1"], "ast_kind": "class_or_type", "text": "            class OldStyleClass:\n                pass\n\n        assert issubclass(UpdatedUserClass1, NewName)\n        assert issubclass(UpdatedUserClass1a, NewName)\n        assert issubclass(UpdatedUserClass1, DeprecatedName)\n        assert issubclass(UpdatedUserClass1a, DeprecatedName)\n        assert issubclass(OutdatedUserClass1, DeprecatedName)\n        assert not issubclass(UnrelatedClass, DeprecatedName)\n        assert not issubclass(OldStyleClass, DeprecatedName)\n        assert not issubclass(OldStyleClass, DeprecatedName)\n        assert not issubclass(OutdatedUserClass1, OutdatedUserClass1a)\n        assert not issubclass(OutdatedUserClass1a, OutdatedUserClass1)\n\n        with pytest.raises(TypeError):\n            issubclass(object(), DeprecatedName)\n\n    def test_isinstance(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n\n            class UpdatedUserClass2(NewName):\n                pass\n\n            class UpdatedUserClass2a(NewName):\n                pass\n\n            class OutdatedUserClass2(DeprecatedName):\n                pass\n\n            class OutdatedUserClass2a(DeprecatedName):\n                pass\n\n            class UnrelatedClass:\n                pass\n\n            class OldStyleClass:\n                pass\n\n        assert isinstance(UpdatedUserClass2(), NewName)\n        assert isinstance(UpdatedUserClass2a(), NewName)\n        assert isinstance(UpdatedUserClass2(), DeprecatedName)\n        assert isinstance(UpdatedUserClass2a(), DeprecatedName)\n        assert isinstance(OutdatedUserClass2(), DeprecatedName)\n        assert isinstance(OutdatedUserClass2a(), DeprecatedName)\n        assert not isinstance(OutdatedUserClass2a(), OutdatedUserClass2)\n        assert not isinstance(OutdatedUserClass2(), OutdatedUserClass2a)\n        assert not isinstance(UnrelatedClass(), DeprecatedName)\n        assert not isinstance(OldStyleClass(), DeprecatedName)\n\n    def test_clsdict(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            Deprecated = create_deprecated_class(\"Deprecated\", NewName, {\"foo\": \"bar\"})\n\n        assert Deprecated.foo == \"bar\"\n\n    def test_deprecate_a_class_with_custom_metaclass(self):\n        Meta1 = type(\"Meta1\", (type,), {})\n        New = Meta1(\"New\", (), {})\n        create_deprecated_class(\"Deprecated\", New)\n\n    def test_deprecate_subclass_of_deprecated_class(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            Deprecated = create_deprecated_class(\n                \"Deprecated\", NewName, warn_category=MyWarning\n            )\n            AlsoDeprecated = create_deprecated_class(\n                \"AlsoDeprecated\",\n                Deprecated,\n                new_class_path=\"foo.Bar\",\n                warn_category=MyWarning,\n            )\n\n        w = self._mywarnings(w)\n        assert len(w) == 0, [str(warning) for warning in w]\n\n        with warnings.catch_warnings(record=True) as w:\n            AlsoDeprecated()\n\n            class UserClass(AlsoDeprecated):\n                pass\n\n        w = self._mywarnings(w)\n        assert len(w) == 2\n        assert \"AlsoDeprecated\" in str(w[0].message)\n        assert \"foo.Bar\" in str(w[0].message)\n        assert \"AlsoDeprecated\" in str(w[1].message)\n        assert \"foo.Bar\" in str(w[1].message)\n\n    def test_inspect_stack(self):\n        with (\n            mock.patch(\"inspect.stack\", side_effect=IndexError),\n            warnings.catch_warnings(record=True) as w,\n        ):\n            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n\n            class SubClass(DeprecatedName):\n                pass\n\n        assert \"Error detecting parent module\" in str(w[0].message)\n\n\n@mock.patch(\n    \"scrapy.utils.deprecate.DEPRECATION_RULES\",\n    [\n        (\"scrapy.contrib.pipeline.\", \"scrapy.pipelines.\"),\n        (\"scrapy.contrib.\", \"scrapy.extensions.\"),\n    ],\n)\nclass TestUpdateClassPath:\n    def test_old_path_gets_fixed(self):\n        with warnings.catch_warnings(record=True) as w:\n            output = update_classpath(\"scrapy.contrib.debug.Debug\")\n        assert output == \"scrapy.extensions.debug.Debug\"\n        assert len(w) == 1\n        assert \"scrapy.contrib.debug.Debug\" in str(w[0].message)\n        assert \"scrapy.extensions.debug.Debug\" in str(w[0].message)\n\n    def test_sorted_replacement(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            output = update_classpath(\"scrapy.contrib.pipeline.Pipeline\")\n        assert output == \"scrapy.pipelines.Pipeline\"\n\n    def test_unmatched_path_stays_the_same(self):\n        with warnings.catch_warnings(record=True) as w:\n            output = update_classpath(\"scrapy.unmatched.Path\")\n        assert output == \"scrapy.unmatched.Path\"\n        assert len(w) == 0\n\n    def test_returns_nonstring(self):\n        for notastring in [None, True, [1, 2, 3], object()]:\n            assert update_classpath(notastring) == notastring\n", "n_tokens": 1112, "byte_len": 5168, "file_sha1": "4fe48859a64b019da3869c9a53980f6e04ba2894", "start_line": 172, "end_line": 308}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py", "rel_path": "tests/test_utils_url.py", "module": "tests.test_utils_url", "ext": "py", "chunk_number": 1, "symbols": ["test_url_is_from_any_domain", "test_url_is_from_spider", "test_url_is_from_spider_class_attributes", "test_url_is_from_spider_with_allowed_domains", "MySpider", "MySpider2", "MySpider3", "wheele", "archive", "expected", "false", "allowed", "domains", "spider", "myspider2", "test", "url", "orderform", "name", "mark", "class", "parametrize", "testdomain", "strip", "import", "module", "mode", "attr", "ignore", "spiders", "test_url_has_any_extension", "test_add_http_if_no_scheme", "test_guess_scheme", "test_guess_scheme_skipped", "test_noop", "test_fragments", "test_path", "test_credentials", "test_default_ports_creds_off", "test_default_ports", "test_default_ports_keep", "test_origin_only", "test__is_filesystem_path", "test_deprecated_imports_from_w3lib", "TestStripUrl", "default", "bool", "greatgrandparent", "origin", "guess"], "ast_kind": "class_or_type", "text": "import warnings\nfrom importlib import import_module\n\nimport pytest\n\nfrom scrapy.linkextractors import IGNORED_EXTENSIONS\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.url import (  # type: ignore[attr-defined]\n    _is_filesystem_path,\n    _public_w3lib_objects,\n    add_http_if_no_scheme,\n    guess_scheme,\n    strip_url,\n    url_has_any_extension,\n    url_is_from_any_domain,\n    url_is_from_spider,\n)\n\n\ndef test_url_is_from_any_domain():\n    url = \"http://www.wheele-bin-art.co.uk/get/product/123\"\n    assert url_is_from_any_domain(url, [\"wheele-bin-art.co.uk\"])\n    assert not url_is_from_any_domain(url, [\"art.co.uk\"])\n\n    url = \"http://wheele-bin-art.co.uk/get/product/123\"\n    assert url_is_from_any_domain(url, [\"wheele-bin-art.co.uk\"])\n    assert not url_is_from_any_domain(url, [\"art.co.uk\"])\n\n    url = \"http://www.Wheele-Bin-Art.co.uk/get/product/123\"\n    assert url_is_from_any_domain(url, [\"wheele-bin-art.CO.UK\"])\n    assert url_is_from_any_domain(url, [\"WHEELE-BIN-ART.CO.UK\"])\n\n    url = \"http://192.169.0.15:8080/mypage.html\"\n    assert url_is_from_any_domain(url, [\"192.169.0.15:8080\"])\n    assert not url_is_from_any_domain(url, [\"192.169.0.15\"])\n\n    url = (\n        \"javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20\"\n        \"javascript:%20document.orderform_2581_1190810811.submit%28%29\"\n    )\n    assert not url_is_from_any_domain(url, [\"testdomain.com\"])\n    assert not url_is_from_any_domain(url + \".testdomain.com\", [\"testdomain.com\"])\n\n\ndef test_url_is_from_spider():\n    class MySpider(Spider):\n        name = \"example.com\"\n\n    assert url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n    assert url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n    assert not url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n    assert not url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n\n\ndef test_url_is_from_spider_class_attributes():\n    class MySpider(Spider):\n        name = \"example.com\"\n\n    assert url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n    assert url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n    assert not url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n    assert not url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n\n\ndef test_url_is_from_spider_with_allowed_domains():\n    class MySpider(Spider):\n        name = \"example.com\"\n        allowed_domains = [\"example.org\", \"example.net\"]\n\n    assert url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n    assert url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n    assert url_is_from_spider(\"http://example.com/some/page.html\", MySpider)\n    assert url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n    assert url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n    assert not url_is_from_spider(\"http://www.example.us/some/page.html\", MySpider)\n\n    class MySpider2(Spider):\n        name = \"example.com\"\n        allowed_domains = {\"example.com\", \"example.net\"}\n\n    assert url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider2)\n\n    class MySpider3(Spider):\n        name = \"example.com\"\n        allowed_domains = (\"example.com\", \"example.net\")\n\n    assert url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider3)\n\n\n@pytest.mark.parametrize(\n    (\"url\", \"expected\"),\n    [\n        (\"http://www.example.com/archive.tar.gz\", True),\n        (\"http://www.example.com/page.doc\", True),\n        (\"http://www.example.com/page.pdf\", True),\n        (\"http://www.example.com/page.htm\", False),\n        (\"http://www.example.com/\", False),\n        (\"http://www.example.com/page.doc.html\", False),\n    ],\n)", "n_tokens": 991, "byte_len": 3788, "file_sha1": "d496e9992b210fcbc8823a8dbaaa8cbcafcfae72", "start_line": 1, "end_line": 100}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py", "rel_path": "tests/test_utils_url.py", "module": "tests.test_utils_url", "ext": "py", "chunk_number": 2, "symbols": ["test_url_has_any_extension", "test_add_http_if_no_scheme", "test_guess_scheme", "test_guess_scheme_skipped", "test_noop", "TestStripUrl", "cases", "test", "strip", "add", "expected", "user", "bool", "url", "guess", "shell", "password", "windows", "supported", "absolute", "mark", "class", "parametrize", "startswith", "path", "hidden", "noop", "scrapy", "https", "example", "test_url_is_from_any_domain", "test_url_is_from_spider", "test_url_is_from_spider_class_attributes", "test_url_is_from_spider_with_allowed_domains", "test_fragments", "test_path", "test_credentials", "test_default_ports_creds_off", "test_default_ports", "test_default_ports_keep", "test_origin_only", "test__is_filesystem_path", "test_deprecated_imports_from_w3lib", "MySpider", "MySpider2", "MySpider3", "default", "wheele", "greatgrandparent", "origin"], "ast_kind": "class_or_type", "text": "def test_url_has_any_extension(url: str, expected: bool) -> None:\n    deny_extensions = {\".\" + e for e in IGNORED_EXTENSIONS}\n    assert url_has_any_extension(url, deny_extensions) is expected\n\n\n@pytest.mark.parametrize(\n    (\"url\", \"expected\"),\n    [\n        (\"www.example.com\", \"http://www.example.com\"),\n        (\"example.com\", \"http://example.com\"),\n        (\"www.example.com/some/page.html\", \"http://www.example.com/some/page.html\"),\n        (\"www.example.com:80\", \"http://www.example.com:80\"),\n        (\"www.example.com/some/page#frag\", \"http://www.example.com/some/page#frag\"),\n        (\"www.example.com/do?a=1&b=2&c=3\", \"http://www.example.com/do?a=1&b=2&c=3\"),\n        (\n            \"username:password@www.example.com\",\n            \"http://username:password@www.example.com\",\n        ),\n        (\n            \"username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n        ),\n        (\"http://www.example.com\", \"http://www.example.com\"),\n        (\"http://example.com\", \"http://example.com\"),\n        (\n            \"http://www.example.com/some/page.html\",\n            \"http://www.example.com/some/page.html\",\n        ),\n        (\"http://www.example.com:80\", \"http://www.example.com:80\"),\n        (\n            \"http://www.example.com/some/page#frag\",\n            \"http://www.example.com/some/page#frag\",\n        ),\n        (\n            \"http://www.example.com/do?a=1&b=2&c=3\",\n            \"http://www.example.com/do?a=1&b=2&c=3\",\n        ),\n        (\n            \"http://username:password@www.example.com\",\n            \"http://username:password@www.example.com\",\n        ),\n        (\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n        ),\n        (\"//www.example.com\", \"http://www.example.com\"),\n        (\"//example.com\", \"http://example.com\"),\n        (\"//www.example.com/some/page.html\", \"http://www.example.com/some/page.html\"),\n        (\"//www.example.com:80\", \"http://www.example.com:80\"),\n        (\"//www.example.com/some/page#frag\", \"http://www.example.com/some/page#frag\"),\n        (\"//www.example.com/do?a=1&b=2&c=3\", \"http://www.example.com/do?a=1&b=2&c=3\"),\n        (\n            \"//username:password@www.example.com\",\n            \"http://username:password@www.example.com\",\n        ),\n        (\n            \"//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n        ),\n        (\"https://www.example.com\", \"https://www.example.com\"),\n        (\"ftp://www.example.com\", \"ftp://www.example.com\"),\n    ],\n)\ndef test_add_http_if_no_scheme(url: str, expected: str) -> None:\n    assert add_http_if_no_scheme(url) == expected\n\n\n@pytest.mark.parametrize(\n    (\"url\", \"expected\"),\n    [\n        (\"/index\", \"file://\"),\n        (\"/index.html\", \"file://\"),\n        (\"./index.html\", \"file://\"),\n        (\"../index.html\", \"file://\"),\n        (\"../../index.html\", \"file://\"),\n        (\"./data/index.html\", \"file://\"),\n        (\".hidden/data/index.html\", \"file://\"),\n        (\"/home/user/www/index.html\", \"file://\"),\n        (\"//home/user/www/index.html\", \"file://\"),\n        (\"file:///home/user/www/index.html\", \"file://\"),\n        (\"index.html\", \"http://\"),\n        (\"example.com\", \"http://\"),\n        (\"www.example.com\", \"http://\"),\n        (\"www.example.com/index.html\", \"http://\"),\n        (\"http://example.com\", \"http://\"),\n        (\"http://example.com/index.html\", \"http://\"),\n        (\"localhost\", \"http://\"),\n        (\"localhost/index.html\", \"http://\"),\n        # some corner cases (default to http://)\n        (\"/\", \"http://\"),\n        (\".../test\", \"http://\"),\n    ],\n)\ndef test_guess_scheme(url: str, expected: str):\n    assert guess_scheme(url).startswith(expected)\n\n\n@pytest.mark.parametrize(\n    (\"url\", \"expected\", \"reason\"),\n    [\n        (\n            r\"C:\\absolute\\path\\to\\a\\file.html\",\n            \"file://\",\n            \"Windows filepath are not supported for scrapy shell\",\n        ),\n    ],\n)\ndef test_guess_scheme_skipped(url: str, expected: str, reason: str):\n    pytest.skip(reason)\n\n\nclass TestStripUrl:\n    @pytest.mark.parametrize(\n        \"url\",\n        [\n            \"http://www.example.com/index.html\",\n            \"http://www.example.com/index.html?somekey=somevalue\",\n        ],\n    )\n    def test_noop(self, url: str) -> None:\n        assert strip_url(url) == url\n", "n_tokens": 1197, "byte_len": 4548, "file_sha1": "d496e9992b210fcbc8823a8dbaaa8cbcafcfae72", "start_line": 101, "end_line": 222}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py", "rel_path": "tests/test_utils_url.py", "module": "tests.test_utils_url", "ext": "py", "chunk_number": 3, "symbols": ["test_fragments", "test_path", "test_credentials", "test_default_ports_creds_off", "expected", "user", "false", "bool", "pass", "origin", "password", "domain", "mark", "parametrize", "strip", "url", "none", "test", "credentials", "https", "example", "fragments", "somevalue", "section", "index", "username", "true", "pytest", "somekey", "assert", "test_url_is_from_any_domain", "test_url_is_from_spider", "test_url_is_from_spider_class_attributes", "test_url_is_from_spider_with_allowed_domains", "test_url_has_any_extension", "test_add_http_if_no_scheme", "test_guess_scheme", "test_guess_scheme_skipped", "test_noop", "test_default_ports", "test_default_ports_keep", "test_origin_only", "test__is_filesystem_path", "test_deprecated_imports_from_w3lib", "MySpider", "MySpider2", "MySpider3", "TestStripUrl", "default", "wheele"], "ast_kind": "function_or_method", "text": "    def test_fragments(self):\n        assert (\n            strip_url(\n                \"http://www.example.com/index.html?somekey=somevalue#section\",\n                strip_fragment=False,\n            )\n            == \"http://www.example.com/index.html?somekey=somevalue#section\"\n        )\n\n    @pytest.mark.parametrize(\n        (\"url\", \"origin\", \"expected\"),\n        [\n            (\"http://www.example.com/\", False, \"http://www.example.com/\"),\n            (\"http://www.example.com\", False, \"http://www.example.com\"),\n            (\"http://www.example.com\", True, \"http://www.example.com/\"),\n        ],\n    )\n    def test_path(self, url: str, origin: bool, expected: str) -> None:\n        assert strip_url(url, origin_only=origin) == expected\n\n    @pytest.mark.parametrize(\n        (\"url\", \"expected\"),\n        [\n            (\n                \"http://username@www.example.com/index.html?somekey=somevalue#section\",\n                \"http://www.example.com/index.html?somekey=somevalue\",\n            ),\n            (\n                \"https://username:@www.example.com/index.html?somekey=somevalue#section\",\n                \"https://www.example.com/index.html?somekey=somevalue\",\n            ),\n            (\n                \"ftp://username:password@www.example.com/index.html?somekey=somevalue#section\",\n                \"ftp://www.example.com/index.html?somekey=somevalue\",\n            ),\n            # user: \"username@\", password: none\n            (\n                \"http://username%40@www.example.com/index.html?somekey=somevalue#section\",\n                \"http://www.example.com/index.html?somekey=somevalue\",\n            ),\n            # user: \"username:pass\", password: \"\"\n            (\n                \"https://username%3Apass:@www.example.com/index.html?somekey=somevalue#section\",\n                \"https://www.example.com/index.html?somekey=somevalue\",\n            ),\n            # user: \"me\", password: \"user@domain.com\"\n            (\n                \"ftp://me:user%40domain.com@www.example.com/index.html?somekey=somevalue#section\",\n                \"ftp://www.example.com/index.html?somekey=somevalue\",\n            ),\n        ],\n    )\n    def test_credentials(self, url: str, expected: str) -> None:\n        assert strip_url(url, strip_credentials=True) == expected\n\n    @pytest.mark.parametrize(\n        (\"url\", \"expected\"),\n        [\n            (\n                \"http://username:password@www.example.com:80/index.html?somekey=somevalue#section\",\n                \"http://www.example.com/index.html?somekey=somevalue\",\n            ),\n            (\n                \"http://username:password@www.example.com:8080/index.html#section\",\n                \"http://www.example.com:8080/index.html\",\n            ),\n            (\n                \"http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section\",\n                \"http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://www.example.com/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:442/index.html\",\n                \"https://www.example.com:442/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:80/index.html\",\n                \"https://www.example.com:80/index.html\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:21/file.txt\",\n                \"ftp://www.example.com/file.txt\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:221/file.txt\",\n                \"ftp://www.example.com:221/file.txt\",\n            ),\n        ],\n    )\n    def test_default_ports_creds_off(self, url: str, expected: str) -> None:\n        assert strip_url(url) == expected\n\n    @pytest.mark.parametrize(\n        (\"url\", \"expected\"),\n        [\n            (\n                \"http://username:password@www.example.com:80/index.html\",\n                \"http://username:password@www.example.com/index.html\",\n            ),\n            (\n                \"http://username:password@www.example.com:8080/index.html\",\n                \"http://username:password@www.example.com:8080/index.html\",\n            ),\n            (\n                \"http://username:password@www.example.com:443/index.html\",\n                \"http://username:password@www.example.com:443/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://username:password@www.example.com/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:442/index.html\",\n                \"https://username:password@www.example.com:442/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:80/index.html\",\n                \"https://username:password@www.example.com:80/index.html\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:21/file.txt\",\n                \"ftp://username:password@www.example.com/file.txt\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:221/file.txt\",\n                \"ftp://username:password@www.example.com:221/file.txt\",\n            ),\n        ],\n    )", "n_tokens": 1156, "byte_len": 5373, "file_sha1": "d496e9992b210fcbc8823a8dbaaa8cbcafcfae72", "start_line": 223, "end_line": 354}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_url.py", "rel_path": "tests/test_utils_url.py", "module": "tests.test_utils_url", "ext": "py", "chunk_number": 4, "symbols": ["test_default_ports", "test_default_ports_keep", "test_origin_only", "test__is_filesystem_path", "test_deprecated_imports_from_w3lib", "test", "default", "bool", "greatgrandparent", "lib", "w3lib", "strip", "url", "import", "module", "credentials", "deprecated", "https", "rcinfo", "username", "letter", "pytest", "public", "isinstance", "none", "docs", "html", "http", "microsoft", "unquotepath", "test_url_is_from_any_domain", "test_url_is_from_spider", "test_url_is_from_spider_class_attributes", "test_url_is_from_spider_with_allowed_domains", "test_url_has_any_extension", "test_add_http_if_no_scheme", "test_guess_scheme", "test_guess_scheme_skipped", "test_noop", "test_fragments", "test_path", "test_credentials", "test_default_ports_creds_off", "MySpider", "MySpider2", "MySpider3", "TestStripUrl", "wheele", "origin", "guess"], "ast_kind": "function_or_method", "text": "    def test_default_ports(self, url: str, expected: str) -> None:\n        assert (\n            strip_url(url, strip_default_port=True, strip_credentials=False) == expected\n        )\n\n    @pytest.mark.parametrize(\n        (\"url\", \"expected\"),\n        [\n            (\n                \"http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section\",\n                \"http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov\",\n            ),\n            (\n                \"http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section\",\n                \"http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov\",\n            ),\n            (\n                \"http://username:password@www.example.com:443/index.html\",\n                \"http://username:password@www.example.com:443/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://username:password@www.example.com:443/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:442/index.html\",\n                \"https://username:password@www.example.com:442/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:80/index.html\",\n                \"https://username:password@www.example.com:80/index.html\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:21/file.txt\",\n                \"ftp://username:password@www.example.com:21/file.txt\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:221/file.txt\",\n                \"ftp://username:password@www.example.com:221/file.txt\",\n            ),\n        ],\n    )\n    def test_default_ports_keep(self, url: str, expected: str) -> None:\n        assert (\n            strip_url(url, strip_default_port=False, strip_credentials=False)\n            == expected\n        )\n\n    @pytest.mark.parametrize(\n        (\"url\", \"expected\"),\n        [\n            (\n                \"http://username:password@www.example.com/index.html\",\n                \"http://www.example.com/\",\n            ),\n            (\n                \"http://username:password@www.example.com:80/foo/bar?query=value#somefrag\",\n                \"http://www.example.com/\",\n            ),\n            (\n                \"http://username:password@www.example.com:8008/foo/bar?query=value#somefrag\",\n                \"http://www.example.com:8008/\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://www.example.com/\",\n            ),\n        ],\n    )\n    def test_origin_only(self, url: str, expected: str) -> None:\n        assert strip_url(url, origin_only=True) == expected\n\n\n@pytest.mark.parametrize(\n    (\"path\", \"expected\"),\n    [\n        # https://en.wikipedia.org/wiki/Path_(computing)#Representations_of_paths_by_operating_system_and_shell\n        # Unix-like OS, Microsoft Windows / cmd.exe\n        (\"/home/user/docs/Letter.txt\", True),\n        (\"./inthisdir\", True),\n        (\"../../greatgrandparent\", True),\n        (\"~/.rcinfo\", True),\n        (r\"C:\\user\\docs\\Letter.txt\", True),\n        (\"/user/docs/Letter.txt\", True),\n        (r\"C:\\Letter.txt\", True),\n        (r\"\\\\Server01\\user\\docs\\Letter.txt\", True),\n        (r\"\\\\?\\UNC\\Server01\\user\\docs\\Letter.txt\", True),\n        (r\"\\\\?\\C:\\user\\docs\\Letter.txt\", True),\n        (r\"C:\\user\\docs\\somefile.ext:alternate_stream_name\", True),\n        (r\"https://example.com\", False),\n    ],\n)\ndef test__is_filesystem_path(path: str, expected: bool) -> None:\n    assert _is_filesystem_path(path) == expected\n\n\n@pytest.mark.parametrize(\n    \"obj_name\",\n    [\n        \"_unquotepath\",\n        \"_safe_chars\",\n        \"parse_url\",\n        *_public_w3lib_objects,\n    ],\n)\ndef test_deprecated_imports_from_w3lib(obj_name: str) -> None:\n    with warnings.catch_warnings(record=True) as warns:\n        obj_type = \"attribute\" if obj_name == \"_safe_chars\" else \"function\"\n        message = f\"The scrapy.utils.url.{obj_name} {obj_type} is deprecated, use w3lib.url.{obj_name} instead.\"\n\n        getattr(import_module(\"scrapy.utils.url\"), obj_name)\n\n        assert isinstance(warns[0].message, Warning)\n        assert message in warns[0].message.args\n", "n_tokens": 1011, "byte_len": 4400, "file_sha1": "d496e9992b210fcbc8823a8dbaaa8cbcafcfae72", "start_line": 355, "end_line": 469}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_attribute_binding.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_attribute_binding.py", "rel_path": "tests/test_request_attribute_binding.py", "module": "tests.test_request_attribute_binding", "ext": "py", "chunk_number": 1, "symbols": ["process_response", "process_request", "process_exception", "alt_callback", "__init__", "from_crawler", "setup_class", "teardown_class", "test_response_200", "test_response_error", "test_downloader_middleware_raise_exception", "test_downloader_middleware_override_request_in_process_response", "signal_handler", "test_downloader_middleware_override_in_process_exception", "ProcessResponseMiddleware", "RaiseExceptionRequestMiddleware", "CatchExceptionOverrideRequestMiddleware", "CatchExceptionDoNotOverrideRequestMiddleware", "AlternativeCallbacksSpider", "AlternativeCallbacksMiddleware", "TestCrawl", "new", "request", "signal", "handler", "referer", "seed", "raise", "exception", "name", "test_downloader_middleware_do_not_override_in_process_exception", "test_downloader_middleware_alternative_callback", "responses", "invoked", "spiders", "https", "mockserver", "test", "downloader", "get", "crawler", "enter", "isinstance", "alternative", "none", "localhost", "encode", "without", "caught", "http"], "ast_kind": "class_or_type", "text": "from testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy import Request, signals\nfrom scrapy.http.response import Response\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import SingleRequestSpider\n\nOVERRIDDEN_URL = \"https://example.org\"\n\n\nclass ProcessResponseMiddleware:\n    def process_response(self, request, response):\n        return response.replace(request=Request(OVERRIDDEN_URL))\n\n\nclass RaiseExceptionRequestMiddleware:\n    def process_request(self, request):\n        1 / 0\n        return request\n\n\nclass CatchExceptionOverrideRequestMiddleware:\n    def process_exception(self, request, exception):\n        return Response(\n            url=\"http://localhost/\",\n            body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n            request=Request(OVERRIDDEN_URL),\n        )\n\n\nclass CatchExceptionDoNotOverrideRequestMiddleware:\n    def process_exception(self, request, exception):\n        return Response(\n            url=\"http://localhost/\",\n            body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n        )\n\n\nclass AlternativeCallbacksSpider(SingleRequestSpider):\n    name = \"alternative_callbacks_spider\"\n\n    def alt_callback(self, response, foo=None):\n        self.logger.info(\"alt_callback was invoked with foo=%s\", foo)\n\n\nclass AlternativeCallbacksMiddleware:\n    def __init__(self, crawler):\n        self.crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def process_response(self, request, response):\n        new_request = request.replace(\n            url=OVERRIDDEN_URL,\n            callback=self.crawler.spider.alt_callback,\n            cb_kwargs={\"foo\": \"bar\"},\n        )\n        return response.replace(request=new_request)\n\n\nclass TestCrawl:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @inlineCallbacks\n    def test_response_200(self):\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        response = crawler.spider.meta[\"responses\"][0]\n        assert response.request.url == url\n\n    @inlineCallbacks\n    def test_response_error(self):\n        for status in (\"404\", \"500\"):\n            url = self.mockserver.url(f\"/status?n={status}\")\n            crawler = get_crawler(SingleRequestSpider)\n            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n            failure = crawler.spider.meta[\"failure\"]\n            response = failure.value.response\n            assert failure.request.url == url\n            assert response.request.url == url\n\n    @inlineCallbacks\n    def test_downloader_middleware_raise_exception(self):\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    RaiseExceptionRequestMiddleware: 590,\n                },\n            },\n        )\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        failure = crawler.spider.meta[\"failure\"]\n        assert failure.request.url == url\n        assert isinstance(failure.value, ZeroDivisionError)\n\n    @inlineCallbacks\n    def test_downloader_middleware_override_request_in_process_response(self):\n        \"\"\"\n        Downloader middleware which returns a response with an specific 'request' attribute.\n\n        * The spider callback should receive the overridden response.request\n        * Handlers listening to the response_received signal should receive the overridden response.request\n        * The \"crawled\" log message should show the overridden response.request\n        \"\"\"\n        signal_params = {}\n\n        def signal_handler(response, request, spider):\n            signal_params[\"response\"] = response\n            signal_params[\"request\"] = request\n\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    ProcessResponseMiddleware: 595,\n                }\n            },\n        )\n        crawler.signals.connect(signal_handler, signal=signals.response_received)\n\n        with LogCapture() as log:\n            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n\n        response = crawler.spider.meta[\"responses\"][0]\n        assert response.request.url == OVERRIDDEN_URL\n\n        assert signal_params[\"response\"].url == url\n        assert signal_params[\"request\"].url == OVERRIDDEN_URL\n\n        log.check_present(\n            (\n                \"scrapy.core.engine\",\n                \"DEBUG\",\n                f\"Crawled (200) <GET {OVERRIDDEN_URL}> (referer: None)\",\n            ),\n        )\n\n    @inlineCallbacks\n    def test_downloader_middleware_override_in_process_exception(self):\n        \"\"\"\n        An exception is raised but caught by the next middleware, which\n        returns a Response with a specific 'request' attribute.\n\n        The spider callback should receive the overridden response.request\n        \"\"\"\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    RaiseExceptionRequestMiddleware: 590,\n                    CatchExceptionOverrideRequestMiddleware: 595,\n                },\n            },\n        )\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        response = crawler.spider.meta[\"responses\"][0]\n        assert response.body == b\"Caught ZeroDivisionError\"\n        assert response.request.url == OVERRIDDEN_URL\n", "n_tokens": 1165, "byte_len": 5892, "file_sha1": "6a6d7bc4aa6200a8fac3d9d1d9e6d99764fccf16", "start_line": 1, "end_line": 175}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_attribute_binding.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_attribute_binding.py", "rel_path": "tests/test_request_attribute_binding.py", "module": "tests.test_request_attribute_binding", "ext": "py", "chunk_number": 2, "symbols": ["test_downloader_middleware_do_not_override_in_process_exception", "test_downloader_middleware_alternative_callback", "seed", "spider", "caught", "exception", "receive", "raise", "middleware", "alt", "callback", "responses", "meta", "invoked", "downloader", "with", "alternative", "callbacks", "next", "mockserver", "body", "get", "crawler", "yield", "test", "assert", "returns", "check", "present", "attribute", "process_response", "process_request", "process_exception", "alt_callback", "__init__", "from_crawler", "setup_class", "teardown_class", "test_response_200", "test_response_error", "test_downloader_middleware_raise_exception", "test_downloader_middleware_override_request_in_process_response", "signal_handler", "test_downloader_middleware_override_in_process_exception", "ProcessResponseMiddleware", "RaiseExceptionRequestMiddleware", "CatchExceptionOverrideRequestMiddleware", "CatchExceptionDoNotOverrideRequestMiddleware", "AlternativeCallbacksSpider", "AlternativeCallbacksMiddleware"], "ast_kind": "function_or_method", "text": "    @inlineCallbacks\n    def test_downloader_middleware_do_not_override_in_process_exception(self):\n        \"\"\"\n        An exception is raised but caught by the next middleware, which\n        returns a Response without a specific 'request' attribute.\n\n        The spider callback should receive the original response.request\n        \"\"\"\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    RaiseExceptionRequestMiddleware: 590,\n                    CatchExceptionDoNotOverrideRequestMiddleware: 595,\n                },\n            },\n        )\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        response = crawler.spider.meta[\"responses\"][0]\n        assert response.body == b\"Caught ZeroDivisionError\"\n        assert response.request.url == url\n\n    @inlineCallbacks\n    def test_downloader_middleware_alternative_callback(self):\n        \"\"\"\n        Downloader middleware which returns a response with a\n        specific 'request' attribute, with an alternative callback\n        \"\"\"\n        crawler = get_crawler(\n            AlternativeCallbacksSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    AlternativeCallbacksMiddleware: 595,\n                }\n            },\n        )\n\n        with LogCapture() as log:\n            url = self.mockserver.url(\"/status?n=200\")\n            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n\n        log.check_present(\n            (\n                \"alternative_callbacks_spider\",\n                \"INFO\",\n                \"alt_callback was invoked with foo=bar\",\n            ),\n        )\n", "n_tokens": 323, "byte_len": 1730, "file_sha1": "6a6d7bc4aa6200a8fac3d9d1d9e6d99764fccf16", "start_line": 176, "end_line": 225}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 1, "symbols": ["setup_class", "teardown_class", "test_follow_all", "test_timeout_success", "test_timeout_failure", "TestCrawl", "failure", "async", "crawl", "spider", "delay", "bool", "def", "bytes", "received", "after", "small", "deferred", "from", "cause", "get", "engine", "spiders", "future", "typ", "checking", "mockserver", "urls", "visited", "crawler", "test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "_on_item_scraped", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status", "cb", "test_format_engine_status", "test_open_spider_error_on_faulty_pipeline", "test_crawlerrunner_accepts_crawler", "test_crawl_multiple"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport json\nimport logging\nfrom ipaddress import IPv4Address\nfrom socket import gethostbyname\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import urlencode, urlparse\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.internet.ssl import Certificate\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.exceptions import CloseSpider, StopDownload\nfrom scrapy.http import Request\nfrom scrapy.http.response import Response\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.engine import format_engine_status, get_engine_status\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.test import get_crawler, get_reactor_settings\nfrom tests import NON_EXISTING_RESOLVABLE\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import (\n    AsyncDefAsyncioGenComplexSpider,\n    AsyncDefAsyncioGenExcSpider,\n    AsyncDefAsyncioGenLoopSpider,\n    AsyncDefAsyncioGenSpider,\n    AsyncDefAsyncioReqsReturnSpider,\n    AsyncDefAsyncioReturnSingleElementSpider,\n    AsyncDefAsyncioReturnSpider,\n    AsyncDefAsyncioSpider,\n    AsyncDefDeferredDirectSpider,\n    AsyncDefDeferredMaybeWrappedSpider,\n    AsyncDefDeferredWrappedSpider,\n    AsyncDefSpider,\n    BrokenStartSpider,\n    BytesReceivedCallbackSpider,\n    BytesReceivedErrbackSpider,\n    CrawlSpiderWithAsyncCallback,\n    CrawlSpiderWithAsyncGeneratorCallback,\n    CrawlSpiderWithErrback,\n    CrawlSpiderWithParseMethod,\n    CrawlSpiderWithProcessRequestCallbackKeywordArguments,\n    DelaySpider,\n    DuplicateStartSpider,\n    FollowAllSpider,\n    HeadersReceivedCallbackSpider,\n    HeadersReceivedErrbackSpider,\n    SimpleSpider,\n    SingleRequestSpider,\n    StartGoodAndBadOutput,\n    StartItemSpider,\n)\n\nif TYPE_CHECKING:\n    from scrapy.statscollectors import StatsCollector\n\n\nclass TestCrawl:\n    mockserver: MockServer\n\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @inlineCallbacks\n    def test_follow_all(self):\n        crawler = get_crawler(FollowAllSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert len(crawler.spider.urls_visited) == 11  # 10 + start_url\n\n    @deferred_f_from_coro_f\n    async def test_fixed_delay(self):\n        await self._test_delay(total=3, delay=0.2)\n\n    @deferred_f_from_coro_f\n    async def test_randomized_delay(self):\n        await self._test_delay(total=3, delay=0.1, randomize=True)\n\n    async def _test_delay(\n        self, total: int, delay: float, randomize: bool = False\n    ) -> None:\n        crawl_kwargs = {\n            \"maxlatency\": delay * 2,\n            \"mockserver\": self.mockserver,\n            \"total\": total,\n        }\n        tolerance = 1 - (0.6 if randomize else 0.2)\n\n        settings = {\"DOWNLOAD_DELAY\": delay, \"RANDOMIZE_DOWNLOAD_DELAY\": randomize}\n        crawler = get_crawler(FollowAllSpider, settings)\n        await maybe_deferred_to_future(crawler.crawl(**crawl_kwargs))\n        assert crawler.spider\n        assert isinstance(crawler.spider, FollowAllSpider)\n        times = crawler.spider.times\n        total_time = times[-1] - times[0]\n        average = total_time / (len(times) - 1)\n        assert average > delay * tolerance, f\"download delay too small: {average}\"\n\n        # Ensure that the same test parameters would cause a failure if no\n        # download delay is set. Otherwise, it means we are using a combination\n        # of ``total`` and ``delay`` values that are too small for the test\n        # code above to have any meaning.\n        settings[\"DOWNLOAD_DELAY\"] = 0\n        crawler = get_crawler(FollowAllSpider, settings)\n        await maybe_deferred_to_future(crawler.crawl(**crawl_kwargs))\n        assert crawler.spider\n        assert isinstance(crawler.spider, FollowAllSpider)\n        times = crawler.spider.times\n        total_time = times[-1] - times[0]\n        average = total_time / (len(times) - 1)\n        assert average <= delay / tolerance, \"test total or delay values are too small\"\n\n    @inlineCallbacks\n    def test_timeout_success(self):\n        crawler = get_crawler(DelaySpider)\n        yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n        assert crawler.spider.t1 > 0\n        assert crawler.spider.t2 > 0\n        assert crawler.spider.t2 > crawler.spider.t1\n\n    @inlineCallbacks\n    def test_timeout_failure(self):\n        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n        yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n        assert crawler.spider.t1 > 0\n        assert crawler.spider.t2 == 0\n        assert crawler.spider.t2_err > 0\n        assert crawler.spider.t2_err > crawler.spider.t1\n\n        # server hangs after receiving response headers\n        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n        yield crawler.crawl(n=0.5, b=1, mockserver=self.mockserver)\n        assert crawler.spider.t1 > 0\n        assert crawler.spider.t2 == 0\n        assert crawler.spider.t2_err > 0\n        assert crawler.spider.t2_err > crawler.spider.t1\n", "n_tokens": 1247, "byte_len": 5300, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 1, "end_line": 147}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 2, "symbols": ["test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "_on_item_scraped", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "fail", "before", "append", "connection", "after", "name", "domain", "responses", "concurren", "requests", "dont", "filter", "avoiding", "homepage", "mockserver", "dupe", "factor", "visited", "path", "setup_class", "teardown_class", "test_follow_all", "test_timeout_success", "test_timeout_failure", "test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status", "cb", "test_format_engine_status", "test_open_spider_error_on_faulty_pipeline", "test_crawlerrunner_accepts_crawler", "test_crawl_multiple", "test_crawlspider_with_parse", "test_crawlspider_with_async_callback", "test_crawlspider_with_async_generator_callback", "test_crawlspider_with_errback", "test_crawlspider_process_request_cb_kwargs", "test_async_def_parse"], "ast_kind": "function_or_method", "text": "    @inlineCallbacks\n    def test_retry_503(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @inlineCallbacks\n    def test_retry_conn_failed(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                \"http://localhost:65432/status?n=503\", mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @inlineCallbacks\n    def test_retry_dns_error(self):\n        if NON_EXISTING_RESOLVABLE:\n            pytest.skip(\"Non-existing hosts are resolvable\")\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            # try to fetch the homepage of a nonexistent domain\n            yield crawler.crawl(\n                \"http://dns.resolution.invalid./\", mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @inlineCallbacks\n    def test_start_bug_before_yield(self):\n        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n            crawler = get_crawler(BrokenStartSpider)\n            yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n\n        assert len(log.records) == 1\n        record = log.records[0]\n        assert record.exc_info is not None\n        assert record.exc_info[0] is ZeroDivisionError\n\n    @inlineCallbacks\n    def test_start_bug_yielding(self):\n        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n            crawler = get_crawler(BrokenStartSpider)\n            yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n\n        assert len(log.records) == 1\n        record = log.records[0]\n        assert record.exc_info is not None\n        assert record.exc_info[0] is ZeroDivisionError\n\n    @inlineCallbacks\n    def test_start_items(self):\n        items = []\n\n        def _on_item_scraped(item):\n            items.append(item)\n\n        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n            crawler = get_crawler(StartItemSpider)\n            crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert len(log.records) == 0\n        assert items == [{\"name\": \"test item\"}]\n\n    @inlineCallbacks\n    def test_start_unsupported_output(self):\n        \"\"\"Anything that is not a request is assumed to be an item, avoiding a\n        potentially expensive call to itemadapter.is_item(), and letting\n        instead things fail when ItemAdapter is actually used on the\n        corresponding non-item object.\"\"\"\n\n        items = []\n\n        def _on_item_scraped(item):\n            items.append(item)\n\n        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n            crawler = get_crawler(StartGoodAndBadOutput)\n            crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert len(log.records) == 0\n        assert len(items) == 3\n        assert not any(isinstance(item, Request) for item in items)\n\n    @inlineCallbacks\n    def test_start_dupes(self):\n        settings = {\"CONCURRENT_REQUESTS\": 1}\n        crawler = get_crawler(DuplicateStartSpider, settings)\n        yield crawler.crawl(\n            dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver\n        )\n        assert crawler.spider.visited == 6\n\n        crawler = get_crawler(DuplicateStartSpider, settings)\n        yield crawler.crawl(\n            dont_filter=False,\n            distinct_urls=3,\n            dupe_factor=4,\n            mockserver=self.mockserver,\n        )\n        assert crawler.spider.visited == 3\n\n    @inlineCallbacks\n    def test_unbounded_response(self):\n        # Completeness of responses without Content-Length or Transfer-Encoding\n        # can not be determined, we treat them as valid but flagged as \"partial\"\n        query = urlencode(\n            {\n                \"raw\": \"\"\"\\\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nX-Powered-By: Servlet 2.4; JBoss-4.2.3.GA (build: SVNTag=JBoss_4_2_3_GA date=200807181417)/JBossWeb-2.0\nSet-Cookie: JSESSIONID=08515F572832D0E659FD2B0D8031D75F; Path=/\nPragma: no-cache\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nCache-Control: no-cache\nCache-Control: no-store\nContent-Type: text/html;charset=UTF-8\nContent-Language: en\nDate: Tue, 27 Aug 2013 13:05:05 GMT\nConnection: close\n\nfoo body\nwith multiples lines\n\"\"\"\n            }\n        )\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver\n            )\n        assert str(log).count(\"Got response 200\") == 1\n\n    @inlineCallbacks\n    def test_retry_conn_lost(self):\n        # connection lost after receiving data\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n", "n_tokens": 1221, "byte_len": 5178, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 148, "end_line": 295}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 3, "symbols": ["test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status", "cb", "test_format_engine_status", "test_open_spider_error_on_faulty_pipeline", "test_crawlerrunner_accepts_crawler", "test_crawl_multiple", "setup_class", "teardown_class", "TestCrawlSpider", "async", "loads", "seed", "test", "retry", "weird", "append", "case", "connection", "spider", "name", "responses", "doesn", "crawlerrunner", "dont", "filter", "get", "engine", "test_follow_all", "test_timeout_success", "test_timeout_failure", "test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "_on_item_scraped", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "test_crawlspider_with_parse", "test_crawlspider_with_async_callback", "test_crawlspider_with_async_generator_callback", "test_crawlspider_with_errback", "test_crawlspider_process_request_cb_kwargs", "test_async_def_parse"], "ast_kind": "class_or_type", "text": "    @inlineCallbacks\n    def test_retry_conn_aborted(self):\n        # connection lost before receiving data\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    def _assert_retried(self, log):\n        assert str(log).count(\"Retrying\") == 2\n        assert str(log).count(\"Gave up retrying\") == 1\n\n    @inlineCallbacks\n    def test_referer_header(self):\n        \"\"\"Referer header is set by RefererMiddleware unless it is already set\"\"\"\n        req0 = Request(self.mockserver.url(\"/echo?headers=1&body=0\"), dont_filter=1)\n        req1 = req0.replace()\n        req2 = req0.replace(headers={\"Referer\": None})\n        req3 = req0.replace(headers={\"Referer\": \"http://example.com\"})\n        req0.meta[\"next\"] = req1\n        req1.meta[\"next\"] = req2\n        req2.meta[\"next\"] = req3\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(seed=req0, mockserver=self.mockserver)\n        # basic asserts in case of weird communication errors\n        assert \"responses\" in crawler.spider.meta\n        assert \"failures\" not in crawler.spider.meta\n        # start() doesn't set Referer header\n        echo0 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n        assert \"Referer\" not in echo0[\"headers\"]\n        # following request sets Referer to the source request url\n        echo1 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][1].body))\n        assert echo1[\"headers\"].get(\"Referer\") == [req0.url]\n        # next request avoids Referer header\n        echo2 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n        assert \"Referer\" not in echo2[\"headers\"]\n        # last request explicitly sets a Referer header\n        echo3 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][3].body))\n        assert echo3[\"headers\"].get(\"Referer\") == [\"http://example.com\"]\n\n    @inlineCallbacks\n    def test_engine_status(self):\n        est = []\n\n        def cb(response):\n            est.append(get_engine_status(crawler.engine))\n\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(\n            seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n        )\n        assert len(est) == 1, est\n        s = dict(est[0])\n        assert s[\"engine.spider.name\"] == crawler.spider.name\n        assert s[\"len(engine.scraper.slot.active)\"] == 1\n\n    @inlineCallbacks\n    def test_format_engine_status(self):\n        est = []\n\n        def cb(response):\n            est.append(format_engine_status(crawler.engine))\n\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(\n            seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n        )\n        assert len(est) == 1, est\n        est = est[0].split(\"\\n\")[2:-2]  # remove header & footer\n        # convert to dict\n        est = [x.split(\":\") for x in est]\n        est = [x for sublist in est for x in sublist]  # flatten\n        est = [x.lstrip().rstrip() for x in est]\n        it = iter(est)\n        s = dict(zip(it, it))\n\n        assert s[\"engine.spider.name\"] == crawler.spider.name\n        assert s[\"len(engine.scraper.slot.active)\"] == \"1\"\n\n    @inlineCallbacks\n    def test_open_spider_error_on_faulty_pipeline(self):\n        settings = {\n            \"ITEM_PIPELINES\": {\n                \"tests.pipelines.ZeroDivisionErrorPipeline\": 300,\n            }\n        }\n        crawler = get_crawler(SimpleSpider, settings)\n        with pytest.raises(ZeroDivisionError):\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        assert not crawler.crawling\n\n    @inlineCallbacks\n    def test_crawlerrunner_accepts_crawler(self):\n        crawler = get_crawler(SimpleSpider)\n        runner = CrawlerRunner()\n        with LogCapture() as log:\n            yield runner.crawl(\n                crawler,\n                self.mockserver.url(\"/status?n=200\"),\n                mockserver=self.mockserver,\n            )\n        assert \"Got response 200\" in str(log)\n\n    @inlineCallbacks\n    def test_crawl_multiple(self):\n        runner = CrawlerRunner(get_reactor_settings())\n        runner.crawl(\n            SimpleSpider,\n            self.mockserver.url(\"/status?n=200\"),\n            mockserver=self.mockserver,\n        )\n        runner.crawl(\n            SimpleSpider,\n            self.mockserver.url(\"/status?n=503\"),\n            mockserver=self.mockserver,\n        )\n\n        with LogCapture() as log:\n            yield runner.join()\n\n        self._assert_retried(log)\n        assert \"Got response 200\" in str(log)\n\n\nclass TestCrawlSpider:\n    mockserver: MockServer\n\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    async def _run_spider(\n        self, spider_cls: type[Spider]\n    ) -> tuple[LogCapture, list[Any], StatsCollector]:\n        items = []\n", "n_tokens": 1215, "byte_len": 5184, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 296, "end_line": 440}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 4, "symbols": ["_on_item_scraped", "test_crawlspider_with_parse", "test_crawlspider_with_async_callback", "test_crawlspider_with_async_generator_callback", "test_crawlspider_with_errback", "test_crawlspider_process_request_cb_kwargs", "test_async_def_parse", "test_async_def_asyncio_parse", "test_async_def_asyncio_parse_items_single_element", "test", "async", "def", "crawl", "spider", "await", "internet", "append", "crawlspider", "stats", "twisted", "return", "run", "item", "scraped", "maybe", "deferred", "mark", "only", "asyncio", "from", "setup_class", "teardown_class", "test_follow_all", "test_timeout_success", "test_timeout_failure", "test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status", "cb"], "ast_kind": "function_or_method", "text": "        def _on_item_scraped(item):\n            items.append(item)\n\n        crawler = get_crawler(spider_cls)\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        with LogCapture() as log:\n            await maybe_deferred_to_future(\n                crawler.crawl(\n                    self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n                )\n            )\n        assert crawler.stats\n        return log, items, crawler.stats\n\n    @inlineCallbacks\n    def test_crawlspider_with_parse(self):\n        crawler = get_crawler(CrawlSpiderWithParseMethod)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert \"[parse] status 200 (foo: None)\" in str(log)\n        assert \"[parse] status 201 (foo: None)\" in str(log)\n        assert \"[parse] status 202 (foo: bar)\" in str(log)\n\n    @inlineCallbacks\n    def test_crawlspider_with_async_callback(self):\n        crawler = get_crawler(CrawlSpiderWithAsyncCallback)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert \"[parse_async] status 200 (foo: None)\" in str(log)\n        assert \"[parse_async] status 201 (foo: None)\" in str(log)\n        assert \"[parse_async] status 202 (foo: bar)\" in str(log)\n\n    @inlineCallbacks\n    def test_crawlspider_with_async_generator_callback(self):\n        crawler = get_crawler(CrawlSpiderWithAsyncGeneratorCallback)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert \"[parse_async_gen] status 200 (foo: None)\" in str(log)\n        assert \"[parse_async_gen] status 201 (foo: None)\" in str(log)\n        assert \"[parse_async_gen] status 202 (foo: bar)\" in str(log)\n\n    @inlineCallbacks\n    def test_crawlspider_with_errback(self):\n        crawler = get_crawler(CrawlSpiderWithErrback)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert \"[parse] status 200 (foo: None)\" in str(log)\n        assert \"[parse] status 201 (foo: None)\" in str(log)\n        assert \"[parse] status 202 (foo: bar)\" in str(log)\n        assert \"[errback] status 404\" in str(log)\n        assert \"[errback] status 500\" in str(log)\n        assert \"[errback] status 501\" in str(log)\n\n    @inlineCallbacks\n    def test_crawlspider_process_request_cb_kwargs(self):\n        crawler = get_crawler(CrawlSpiderWithProcessRequestCallbackKeywordArguments)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        assert \"[parse] status 200 (foo: process_request)\" in str(log)\n        assert \"[parse] status 201 (foo: process_request)\" in str(log)\n        assert \"[parse] status 202 (foo: bar)\" in str(log)\n\n    @inlineCallbacks\n    def test_async_def_parse(self):\n        crawler = get_crawler(AsyncDefSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        assert \"Got response 200\" in str(log)\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_async_def_asyncio_parse(self):\n        crawler = get_crawler(\n            AsyncDefAsyncioSpider,\n            {\n                \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            },\n        )\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        assert \"Got response 200\" in str(log)\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_asyncio_parse_items_list(self):\n        log, items, _ = await self._run_spider(AsyncDefAsyncioReturnSpider)\n        assert \"Got response 200\" in str(log)\n        assert {\"id\": 1} in items\n        assert {\"id\": 2} in items\n\n    @pytest.mark.only_asyncio\n    @inlineCallbacks\n    def test_async_def_asyncio_parse_items_single_element(self):\n        items = []\n\n        def _on_item_scraped(item):\n            items.append(item)\n\n        crawler = get_crawler(AsyncDefAsyncioReturnSingleElementSpider)\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        assert \"Got response 200\" in str(log)\n        assert {\"foo\": 42} in items\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_asyncgen_parse(self):\n        log, _, stats = await self._run_spider(AsyncDefAsyncioGenSpider)\n        assert \"Got response 200\" in str(log)\n        itemcount = stats.get_value(\"item_scraped_count\")\n        assert itemcount == 1\n", "n_tokens": 1143, "byte_len": 4797, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 441, "end_line": 564}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 5, "symbols": ["test_response_ssl_certificate_none", "test_response_ssl_certificate", "test_response_ssl_certificate_empty_response", "test_dns_server_ip_address_none", "test_dns_server_ip_address", "test_bytes_received_stop_download_callback", "async", "test", "seed", "def", "netloc", "expected", "spider", "deferred", "from", "responses", "mockserver", "cert", "req", "get", "crawler", "pytest", "items", "isinstance", "none", "localhost", "code", "gethostbyname", "reason", "value", "setup_class", "teardown_class", "test_follow_all", "test_timeout_success", "test_timeout_failure", "test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "_on_item_scraped", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status"], "ast_kind": "function_or_method", "text": "    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_asyncgen_parse_loop(self):\n        log, items, stats = await self._run_spider(AsyncDefAsyncioGenLoopSpider)\n        assert \"Got response 200\" in str(log)\n        itemcount = stats.get_value(\"item_scraped_count\")\n        assert itemcount == 10\n        for i in range(10):\n            assert {\"foo\": i} in items\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_asyncgen_parse_exc(self):\n        log, items, stats = await self._run_spider(AsyncDefAsyncioGenExcSpider)\n        log = str(log)\n        assert \"Spider error processing\" in log\n        assert \"ValueError\" in log\n        itemcount = stats.get_value(\"item_scraped_count\")\n        assert itemcount == 7\n        for i in range(7):\n            assert {\"foo\": i} in items\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_asyncgen_parse_complex(self):\n        _, items, stats = await self._run_spider(AsyncDefAsyncioGenComplexSpider)\n        itemcount = stats.get_value(\"item_scraped_count\")\n        assert itemcount == 156\n        # some random items\n        for i in [1, 4, 21, 22, 207, 311]:\n            assert {\"index\": i} in items\n        for i in [10, 30, 122]:\n            assert {\"index2\": i} in items\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_asyncio_parse_reqs_list(self):\n        log, *_ = await self._run_spider(AsyncDefAsyncioReqsReturnSpider)\n        for req_id in range(3):\n            assert f\"Got response 200, req_id {req_id}\" in str(log)\n\n    @pytest.mark.only_not_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_deferred_direct(self):\n        _, items, _ = await self._run_spider(AsyncDefDeferredDirectSpider)\n        assert items == [{\"code\": 200}]\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_async_def_deferred_wrapped(self):\n        log, items, _ = await self._run_spider(AsyncDefDeferredWrappedSpider)\n        assert items == [{\"code\": 200}]\n\n    @deferred_f_from_coro_f\n    async def test_async_def_deferred_maybe_wrapped(self):\n        _, items, _ = await self._run_spider(AsyncDefDeferredMaybeWrappedSpider)\n        assert items == [{\"code\": 200}]\n\n    @inlineCallbacks\n    def test_response_ssl_certificate_none(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        assert crawler.spider.meta[\"responses\"][0].certificate is None\n\n    @inlineCallbacks\n    def test_response_ssl_certificate(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        cert = crawler.spider.meta[\"responses\"][0].certificate\n        assert isinstance(cert, Certificate)\n        assert cert.getSubject().commonName == b\"localhost\"\n        assert cert.getIssuer().commonName == b\"localhost\"\n\n    @pytest.mark.xfail(\n        reason=\"Responses with no body return early and contain no certificate\"\n    )\n    @inlineCallbacks\n    def test_response_ssl_certificate_empty_response(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        cert = crawler.spider.meta[\"responses\"][0].certificate\n        assert isinstance(cert, Certificate)\n        assert cert.getSubject().commonName == b\"localhost\"\n        assert cert.getIssuer().commonName == b\"localhost\"\n\n    @inlineCallbacks\n    def test_dns_server_ip_address_none(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/status?n=200\")\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n        assert ip_address is None\n\n    @inlineCallbacks\n    def test_dns_server_ip_address(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/echo?body=test\")\n        expected_netloc, _ = urlparse(url).netloc.split(\":\")\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n        assert isinstance(ip_address, IPv4Address)\n        assert str(ip_address) == gethostbyname(expected_netloc)\n\n    @inlineCallbacks\n    def test_bytes_received_stop_download_callback(self):\n        crawler = get_crawler(BytesReceivedCallbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert crawler.spider.meta.get(\"failure\") is None\n        assert isinstance(crawler.spider.meta[\"response\"], Response)\n        assert crawler.spider.meta[\"response\"].body == crawler.spider.meta.get(\n            \"bytes_received\"\n        )\n        assert (\n            len(crawler.spider.meta[\"response\"].body)\n            < crawler.spider.full_response_length\n        )\n", "n_tokens": 1214, "byte_len": 5092, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 565, "end_line": 684}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 6, "symbols": ["test_bytes_received_stop_download_errback", "test_headers_received_stop_download_callback", "test_headers_received_stop_download_errback", "test_spider_errback", "eb", "test_spider_errback_silence", "test_spider_errback_exception", "test_spider_errback_item", "test_spider_errback_request", "test_spider_errback_downloader_error", "test_spider_errback_downloader_error_exception", "test_spider_errback_downloader_error_item", "allowed", "failure", "test", "spider", "failures", "seed", "bytes", "append", "processing", "received", "return", "handled", "value", "error", "http", "drop", "headers", "meta", "setup_class", "teardown_class", "test_follow_all", "test_timeout_success", "test_timeout_failure", "test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "_on_item_scraped", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status"], "ast_kind": "function_or_method", "text": "    @inlineCallbacks\n    def test_bytes_received_stop_download_errback(self):\n        crawler = get_crawler(BytesReceivedErrbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert crawler.spider.meta.get(\"response\") is None\n        assert isinstance(crawler.spider.meta[\"failure\"], Failure)\n        assert isinstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n        assert isinstance(crawler.spider.meta[\"failure\"].value.response, Response)\n        assert crawler.spider.meta[\n            \"failure\"\n        ].value.response.body == crawler.spider.meta.get(\"bytes_received\")\n        assert (\n            len(crawler.spider.meta[\"failure\"].value.response.body)\n            < crawler.spider.full_response_length\n        )\n\n    @inlineCallbacks\n    def test_headers_received_stop_download_callback(self):\n        crawler = get_crawler(HeadersReceivedCallbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert crawler.spider.meta.get(\"failure\") is None\n        assert isinstance(crawler.spider.meta[\"response\"], Response)\n        assert crawler.spider.meta[\"response\"].headers == crawler.spider.meta.get(\n            \"headers_received\"\n        )\n\n    @inlineCallbacks\n    def test_headers_received_stop_download_errback(self):\n        crawler = get_crawler(HeadersReceivedErrbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert crawler.spider.meta.get(\"response\") is None\n        assert isinstance(crawler.spider.meta[\"failure\"], Failure)\n        assert isinstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n        assert isinstance(crawler.spider.meta[\"failure\"].value.response, Response)\n        assert crawler.spider.meta[\n            \"failure\"\n        ].value.response.headers == crawler.spider.meta.get(\"headers_received\")\n\n    @inlineCallbacks\n    def test_spider_errback(self):\n        failures = []\n\n        def eb(failure: Failure) -> Failure:\n            failures.append(failure)\n            return failure\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n            )\n        assert len(failures) == 1\n        assert \"HTTP status code is not handled or not allowed\" in str(log)\n        assert \"Spider error processing\" not in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_silence(self):\n        failures = []\n\n        def eb(failure: Failure) -> None:\n            failures.append(failure)\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n            )\n        assert len(failures) == 1\n        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n        assert \"Spider error processing\" not in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_exception(self):\n        def eb(failure: Failure) -> None:\n            raise ValueError(\"foo\")\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n            )\n        assert \"Spider error processing\" in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_item(self):\n        def eb(failure: Failure) -> Any:\n            return {\"foo\": \"bar\"}\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n            )\n        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n        assert \"Spider error processing\" not in str(log)\n        assert \"'item_scraped_count': 1\" in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_request(self):\n        def eb(failure: Failure) -> Request:\n            return Request(self.mockserver.url(\"/\"))\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n            )\n        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n        assert \"Spider error processing\" not in str(log)\n        assert \"Crawled (200)\" in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_downloader_error(self):\n        failures = []\n\n        def eb(failure: Failure) -> Failure:\n            failures.append(failure)\n            return failure\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n            )\n        assert len(failures) == 1\n        assert \"Error downloading\" in str(log)\n        assert \"Spider error processing\" not in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_downloader_error_exception(self):\n        def eb(failure: Failure) -> None:\n            raise ValueError(\"foo\")\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n            )\n        assert \"Error downloading\" in str(log)\n        assert \"Spider error processing\" in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_downloader_error_item(self):", "n_tokens": 1193, "byte_len": 5531, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 685, "end_line": 827}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawl.py", "rel_path": "tests/test_crawl.py", "module": "tests.test_crawl", "ext": "py", "chunk_number": 7, "symbols": ["eb", "test_spider_errback_downloader_error_request", "test_raise_closespider", "cb", "test_raise_closespider_reason", "allowed", "failure", "cancelled", "close", "spider", "seed", "processing", "test", "drop", "return", "handled", "http", "with", "raise", "mockserver", "get", "crawler", "yield", "assert", "crawled", "reason", "item", "scraped", "request", "callback", "setup_class", "teardown_class", "test_follow_all", "test_timeout_success", "test_timeout_failure", "test_retry_503", "test_retry_conn_failed", "test_retry_dns_error", "test_start_bug_before_yield", "test_start_bug_yielding", "test_start_items", "_on_item_scraped", "test_start_unsupported_output", "test_start_dupes", "test_unbounded_response", "test_retry_conn_lost", "test_retry_conn_aborted", "_assert_retried", "test_referer_header", "test_engine_status"], "ast_kind": "function_or_method", "text": "        def eb(failure: Failure) -> Any:\n            return {\"foo\": \"bar\"}\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n            )\n        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n        assert \"Spider error processing\" not in str(log)\n        assert \"'item_scraped_count': 1\" in str(log)\n\n    @inlineCallbacks\n    def test_spider_errback_downloader_error_request(self):\n        def eb(failure: Failure) -> Request:\n            return Request(self.mockserver.url(\"/\"))\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n            )\n        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n        assert \"Spider error processing\" not in str(log)\n        assert \"Crawled (200)\" in str(log)\n\n    @inlineCallbacks\n    def test_raise_closespider(self):\n        def cb(response):\n            raise CloseSpider\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(seed=self.mockserver.url(\"/\"), callback_func=cb)\n        assert \"Closing spider (cancelled)\" in str(log)\n        assert \"Spider error processing\" not in str(log)\n\n    @inlineCallbacks\n    def test_raise_closespider_reason(self):\n        def cb(response):\n            raise CloseSpider(\"my_reason\")\n\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(seed=self.mockserver.url(\"/\"), callback_func=cb)\n        assert \"Closing spider (my_reason)\" in str(log)\n        assert \"Spider error processing\" not in str(log)\n", "n_tokens": 413, "byte_len": 1853, "file_sha1": "e3a737cd2a9569edaa6b765ff8b56ece9ec45fed", "start_line": 828, "end_line": 875}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_statsmailer.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_statsmailer.py", "rel_path": "tests/test_extension_statsmailer.py", "module": "tests.test_extension_statsmailer", "ext": "py", "chunk_number": 1, "symbols": ["dummy_stats", "__init__", "get_stats", "test_from_crawler_without_recipients_raises_notconfigured", "test_from_crawler_with_recipients_initializes_extension", "test_from_crawler_connects_spider_closed_signal", "test_spider_closed_sends_email", "DummyStats", "default", "spider", "stats", "test", "from", "statscollectors", "statsmailer", "recipients", "dummy", "magic", "mock", "spec", "return", "lambda", "name", "subject", "class", "getlist", "with", "signal", "manager", "scrapy", "not", "configured", "mail", "example", "signalmanager", "pylint", "closed", "body", "init", "crawler", "send", "catch", "pytest", "collector", "settings", "assert", "signals", "kwargs", "mailer", "sender"], "ast_kind": "class_or_type", "text": "from unittest.mock import MagicMock\n\nimport pytest\n\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions import statsmailer\nfrom scrapy.mail import MailSender\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.spider import DefaultSpider\n\n\n@pytest.fixture\ndef dummy_stats():\n    class DummyStats(StatsCollector):\n        def __init__(self):\n            # pylint: disable=super-init-not-called\n            self._stats = {\"global_item_scraped_count\": 42}\n\n        def get_stats(self):\n            return {\"item_scraped_count\": 10, **self._stats}\n\n    return DummyStats()\n\n\ndef test_from_crawler_without_recipients_raises_notconfigured():\n    crawler = MagicMock()\n    crawler.settings.getlist.return_value = []\n    crawler.stats = MagicMock()\n\n    with pytest.raises(NotConfigured):\n        statsmailer.StatsMailer.from_crawler(crawler)\n\n\ndef test_from_crawler_with_recipients_initializes_extension(dummy_stats, monkeypatch):\n    crawler = MagicMock()\n    crawler.settings.getlist.return_value = [\"test@example.com\"]\n    crawler.stats = dummy_stats\n    crawler.signals = SignalManager(crawler)\n\n    mailer = MagicMock(spec=MailSender)\n    monkeypatch.setattr(statsmailer.MailSender, \"from_crawler\", lambda _: mailer)\n\n    ext = statsmailer.StatsMailer.from_crawler(crawler)\n\n    assert isinstance(ext, statsmailer.StatsMailer)\n    assert ext.recipients == [\"test@example.com\"]\n    assert ext.mail is mailer\n\n\ndef test_from_crawler_connects_spider_closed_signal(dummy_stats, monkeypatch):\n    crawler = MagicMock()\n    crawler.settings.getlist.return_value = [\"test@example.com\"]\n    crawler.stats = dummy_stats\n    crawler.signals = SignalManager(crawler)\n\n    mailer = MagicMock(spec=MailSender)\n    monkeypatch.setattr(statsmailer.MailSender, \"from_crawler\", lambda _: mailer)\n\n    statsmailer.StatsMailer.from_crawler(crawler)\n\n    connected = crawler.signals.send_catch_log(\n        signals.spider_closed, spider=DefaultSpider(name=\"dummy\")\n    )\n    assert connected is not None\n\n\ndef test_spider_closed_sends_email(dummy_stats):\n    recipients = [\"test@example.com\"]\n    mail = MagicMock(spec=MailSender)\n    ext = statsmailer.StatsMailer(dummy_stats, recipients, mail)\n\n    spider = DefaultSpider(name=\"dummy\")\n    ext.spider_closed(spider)\n\n    args, kwargs = mail.send.call_args\n    to, subject, body = args\n    assert to == recipients\n    assert \"Scrapy stats for: dummy\" in subject\n    assert \"global_item_scraped_count\" in body\n    assert \"item_scraped_count\" in body\n", "n_tokens": 566, "byte_len": 2583, "file_sha1": "b4c104a9d6f1a7a54c78b4e18edcba6ddd2b7049", "start_line": 1, "end_line": 83}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers.py", "rel_path": "tests/test_downloader_handlers.py", "module": "tests.test_downloader_handlers", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "test_enabled_handler", "test_not_configured_handler", "test_disabled_handler", "test_lazy_handlers", "setup_method", "teardown_method", "download_request", "test_anon_request", "DummyDH", "DummyLazyDH", "OffDH", "TestLoad", "TestFile", "HttpDownloadHandlerMock", "TestS3Anon", "TestS3", "encoding", "async", "protocol", "manual", "lib", "w3lib", "test", "tests3", "tmpname", "amazonaws", "deferred", "from", "_mocked_date", "test_extra_kw", "test_request_signing1", "test_request_signing2", "test_request_signing3", "test_request_signing4", "test_request_signing6", "test_request_signing7", "TestDataURI", "checksum", "algorithm", "dgy", "kxnwqr", "method", "unidiomatic", "u03a3", "quoted", "hcicp", "akvxqm", "agent"], "ast_kind": "class_or_type", "text": "\"\"\"Tests for DownloadHandlers and for specific non-HTTP download handlers.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport os\nfrom pathlib import Path\nfrom tempfile import mkdtemp, mkstemp\nfrom unittest import mock\n\nimport pytest\nfrom w3lib.url import path_to_file_uri\n\nfrom scrapy.core.downloader.handlers import DownloadHandlers\nfrom scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler\nfrom scrapy.core.downloader.handlers.file import FileDownloadHandler\nfrom scrapy.core.downloader.handlers.s3 import S3DownloadHandler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\n\nclass DummyDH:\n    lazy = False\n\n\nclass DummyLazyDH:\n    # Default is lazy for backward compatibility\n    pass\n\n\nclass OffDH:\n    lazy = False\n\n    def __init__(self, crawler):\n        raise NotConfigured\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n\nclass TestLoad:\n    def test_enabled_handler(self):\n        handlers = {\"scheme\": DummyDH}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        assert \"scheme\" in dh._schemes\n        assert \"scheme\" in dh._handlers\n        assert \"scheme\" not in dh._notconfigured\n\n    def test_not_configured_handler(self):\n        handlers = {\"scheme\": OffDH}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        assert \"scheme\" in dh._schemes\n        assert \"scheme\" not in dh._handlers\n        assert \"scheme\" in dh._notconfigured\n\n    def test_disabled_handler(self):\n        handlers = {\"scheme\": None}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        assert \"scheme\" not in dh._schemes\n        for scheme in handlers:  # force load handlers\n            dh._get_handler(scheme)\n        assert \"scheme\" not in dh._handlers\n        assert \"scheme\" in dh._notconfigured\n\n    def test_lazy_handlers(self):\n        handlers = {\"scheme\": DummyLazyDH}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        assert \"scheme\" in dh._schemes\n        assert \"scheme\" not in dh._handlers\n        for scheme in handlers:  # force load lazy handler\n            dh._get_handler(scheme)\n        assert \"scheme\" in dh._handlers\n        assert \"scheme\" not in dh._notconfigured\n\n\nclass TestFile:\n    def setup_method(self):\n        # add a special char to check that they are handled correctly\n        self.fd, self.tmpname = mkstemp(suffix=\"^\")\n        Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n        self.download_handler = build_from_crawler(FileDownloadHandler, get_crawler())\n\n    def teardown_method(self):\n        os.close(self.fd)\n        Path(self.tmpname).unlink()\n\n    async def download_request(self, request: Request) -> Response:\n        return await maybe_deferred_to_future(\n            self.download_handler.download_request(request, DefaultSpider())\n        )\n\n    @deferred_f_from_coro_f\n    async def test_download(self):\n        request = Request(path_to_file_uri(self.tmpname))\n        assert request.url.upper().endswith(\"%5E\")\n        response = await self.download_request(request)\n        assert response.url == request.url\n        assert response.status == 200\n        assert response.body == b\"0123456789\"\n        assert response.protocol is None\n\n    @deferred_f_from_coro_f\n    async def test_non_existent(self):\n        request = Request(path_to_file_uri(mkdtemp()))\n        # the specific exception differs between platforms\n        with pytest.raises(OSError):  # noqa: PT011\n            await self.download_request(request)\n\n\nclass HttpDownloadHandlerMock:\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def download_request(self, request, spider):\n        return request\n\n\n@pytest.mark.requires_botocore\nclass TestS3Anon:\n    def setup_method(self):\n        crawler = get_crawler()\n        self.s3reqh = build_from_crawler(\n            S3DownloadHandler,\n            crawler,\n            httpdownloadhandler=HttpDownloadHandlerMock,\n            # anon=True, # implicit\n        )\n        self.download_request = self.s3reqh.download_request\n        self.spider = DefaultSpider()\n\n    def test_anon_request(self):\n        req = Request(\"s3://aws-publicdatasets/\")\n        httpreq = self.download_request(req, self.spider)\n        assert hasattr(self.s3reqh, \"anon\")\n        assert self.s3reqh.anon\n        assert httpreq.url == \"http://aws-publicdatasets.s3.amazonaws.com/\"\n\n\n@pytest.mark.requires_botocore\nclass TestS3:\n    download_handler_cls: type = S3DownloadHandler\n\n    # test use same example keys than amazon developer guide\n    # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n    # and the tests described here are the examples from that manual\n\n    AWS_ACCESS_KEY_ID = \"0PN5J17HBGZHT7JJ3X82\"\n    AWS_SECRET_ACCESS_KEY = \"uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o\"\n", "n_tokens": 1223, "byte_len": 5312, "file_sha1": "155aa9fb3904007f4c49ac85c30f221175093a05", "start_line": 1, "end_line": 159}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers.py", "rel_path": "tests/test_downloader_handlers.py", "module": "tests.test_downloader_handlers", "ext": "py", "chunk_number": 2, "symbols": ["setup_method", "_mocked_date", "test_extra_kw", "test_request_signing1", "test_request_signing2", "test_request_signing3", "test_request_signing4", "dgy", "kxnwqr", "method", "hcicp", "akvxqm", "agent", "each", "jpeg", "httpdownloadhandler", "test", "extra", "photos", "contextlib", "mozilla", "mock", "get", "crawler", "date", "pytest", "johnsmith", "object", "auth", "return", "__init__", "from_crawler", "test_enabled_handler", "test_not_configured_handler", "test_disabled_handler", "test_lazy_handlers", "teardown_method", "download_request", "test_anon_request", "test_request_signing6", "test_request_signing7", "DummyDH", "DummyLazyDH", "OffDH", "TestLoad", "TestFile", "HttpDownloadHandlerMock", "TestS3Anon", "TestS3", "TestDataURI"], "ast_kind": "function_or_method", "text": "    def setup_method(self):\n        crawler = get_crawler()\n        s3reqh = build_from_crawler(\n            S3DownloadHandler,\n            crawler,\n            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n            httpdownloadhandler=HttpDownloadHandlerMock,\n        )\n        self.download_request = s3reqh.download_request\n        self.spider = DefaultSpider()\n\n    @contextlib.contextmanager\n    def _mocked_date(self, date):\n        try:\n            import botocore.auth  # noqa: F401,PLC0415\n        except ImportError:\n            yield\n        else:\n            # We need to mock botocore.auth.formatdate, because otherwise\n            # botocore overrides Date header with current date and time\n            # and Authorization header is different each time\n            with mock.patch(\"botocore.auth.formatdate\") as mock_formatdate:\n                mock_formatdate.return_value = date\n                yield\n\n    def test_extra_kw(self):\n        crawler = get_crawler()\n        with pytest.raises((TypeError, NotConfigured)):\n            build_from_crawler(\n                S3DownloadHandler,\n                crawler,\n                extra_kw=True,\n            )\n\n    def test_request_signing1(self):\n        # gets an object from the johnsmith bucket.\n        date = \"Tue, 27 Mar 2007 19:36:42 +0000\"\n        req = Request(\"s3://johnsmith/photos/puppy.jpg\", headers={\"Date\": date})\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        assert (\n            httpreq.headers[\"Authorization\"]\n            == b\"AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=\"\n        )\n\n    def test_request_signing2(self):\n        # puts an object into the johnsmith bucket.\n        date = \"Tue, 27 Mar 2007 21:15:45 +0000\"\n        req = Request(\n            \"s3://johnsmith/photos/puppy.jpg\",\n            method=\"PUT\",\n            headers={\n                \"Content-Type\": \"image/jpeg\",\n                \"Date\": date,\n                \"Content-Length\": \"94328\",\n            },\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        assert (\n            httpreq.headers[\"Authorization\"]\n            == b\"AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=\"\n        )\n\n    def test_request_signing3(self):\n        # lists the content of the johnsmith bucket.\n        date = \"Tue, 27 Mar 2007 19:42:41 +0000\"\n        req = Request(\n            \"s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy\",\n            method=\"GET\",\n            headers={\n                \"User-Agent\": \"Mozilla/5.0\",\n                \"Date\": date,\n            },\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        assert (\n            httpreq.headers[\"Authorization\"]\n            == b\"AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=\"\n        )\n\n    def test_request_signing4(self):\n        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n        date = \"Tue, 27 Mar 2007 19:44:46 +0000\"\n        req = Request(\"s3://johnsmith/?acl\", method=\"GET\", headers={\"Date\": date})\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        assert (\n            httpreq.headers[\"Authorization\"]\n            == b\"AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=\"\n        )\n", "n_tokens": 881, "byte_len": 3466, "file_sha1": "155aa9fb3904007f4c49ac85c30f221175093a05", "start_line": 160, "end_line": 253}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers.py", "rel_path": "tests/test_downloader_handlers.py", "module": "tests.test_downloader_handlers", "ext": "py", "chunk_number": 3, "symbols": ["test_request_signing6", "test_request_signing7", "setup_method", "TestDataURI", "checksum", "algorithm", "encoding", "method", "unidiomatic", "async", "protocol", "quoted", "agent", "cfv", "ycc", "deferred", "from", "jane", "download", "handler", "world", "photos", "spaces", "date", "get", "crawler", "test", "johnsmith", "object", "request", "__init__", "from_crawler", "test_enabled_handler", "test_not_configured_handler", "test_disabled_handler", "test_lazy_handlers", "teardown_method", "download_request", "test_anon_request", "_mocked_date", "test_extra_kw", "test_request_signing1", "test_request_signing2", "test_request_signing3", "test_request_signing4", "DummyDH", "DummyLazyDH", "OffDH", "TestLoad", "TestFile"], "ast_kind": "class_or_type", "text": "    def test_request_signing6(self):\n        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n        date = \"Tue, 27 Mar 2007 21:06:08 +0000\"\n        req = Request(\n            \"s3://static.johnsmith.net:8080/db-backup.dat.gz\",\n            method=\"PUT\",\n            headers={\n                \"User-Agent\": \"curl/7.15.5\",\n                \"Host\": \"static.johnsmith.net:8080\",\n                \"Date\": date,\n                \"x-amz-acl\": \"public-read\",\n                \"content-type\": \"application/x-download\",\n                \"Content-MD5\": \"4gJE4saaMU4BqNR0kLY+lw==\",\n                \"X-Amz-Meta-ReviewedBy\": \"joe@johnsmith.net,jane@johnsmith.net\",\n                \"X-Amz-Meta-FileChecksum\": \"0x02661779\",\n                \"X-Amz-Meta-ChecksumAlgorithm\": \"crc32\",\n                \"Content-Disposition\": \"attachment; filename=database.dat\",\n                \"Content-Encoding\": \"gzip\",\n                \"Content-Length\": \"5913339\",\n            },\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        assert (\n            httpreq.headers[\"Authorization\"]\n            == b\"AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=\"\n        )\n\n    def test_request_signing7(self):\n        # ensure that spaces are quoted properly before signing\n        date = \"Tue, 27 Mar 2007 19:42:41 +0000\"\n        req = Request(\n            \"s3://johnsmith/photos/my puppy.jpg?response-content-disposition=my puppy.jpg\",\n            method=\"GET\",\n            headers={\"Date\": date},\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        assert (\n            httpreq.headers[\"Authorization\"]\n            == b\"AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=\"\n        )\n\n\nclass TestDataURI:\n    def setup_method(self):\n        crawler = get_crawler()\n        self.download_handler = build_from_crawler(DataURIDownloadHandler, crawler)\n\n    async def download_request(self, request: Request) -> Response:\n        return await maybe_deferred_to_future(\n            self.download_handler.download_request(request, DefaultSpider())\n        )\n\n    @deferred_f_from_coro_f\n    async def test_response_attrs(self):\n        uri = \"data:,A%20brief%20note\"\n        request = Request(uri)\n        response = await self.download_request(request)\n        assert response.url == uri\n        assert not response.headers\n\n    @deferred_f_from_coro_f\n    async def test_default_mediatype_encoding(self):\n        request = Request(\"data:,A%20brief%20note\")\n        response = await self.download_request(request)\n        assert response.text == \"A brief note\"\n        assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n        assert response.encoding == \"US-ASCII\"\n\n    @deferred_f_from_coro_f\n    async def test_default_mediatype(self):\n        request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n        response = await self.download_request(request)\n        assert response.text == \"\\u038e\\u03a3\\u038e\"\n        assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n        assert response.encoding == \"iso-8859-7\"\n\n    @deferred_f_from_coro_f\n    async def test_text_charset(self):\n        request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n        response = await self.download_request(request)\n        assert response.text == \"\\u038e\\u03a3\\u038e\"\n        assert response.body == b\"\\xbe\\xd3\\xbe\"\n        assert response.encoding == \"iso-8859-7\"\n\n    @deferred_f_from_coro_f\n    async def test_mediatype_parameters(self):\n        request = Request(\n            \"data:text/plain;foo=%22foo;bar%5C%22%22;\"\n            \"charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22\"\n            \",%CE%8E%CE%A3%CE%8E\"\n        )\n        response = await self.download_request(request)\n        assert response.text == \"\\u038e\\u03a3\\u038e\"\n        assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n        assert response.encoding == \"utf-8\"\n\n    @deferred_f_from_coro_f\n    async def test_base64(self):\n        request = Request(\"data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D\")\n        response = await self.download_request(request)\n        assert response.text == \"Hello, world.\"\n\n    @deferred_f_from_coro_f\n    async def test_protocol(self):\n        request = Request(\"data:,\")\n        response = await self.download_request(request)\n        assert response.protocol is None\n", "n_tokens": 1176, "byte_len": 4564, "file_sha1": "155aa9fb3904007f4c49ac85c30f221175093a05", "start_line": 254, "end_line": 363}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py", "rel_path": "tests/test_downloadermiddleware_httpproxy.py", "module": "tests.test_downloadermiddleware_httpproxy", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "teardown_method", "test_not_enabled", "test_no_environment_proxies", "test_environment_proxies", "test_proxy_precedence_meta", "test_proxy_auth", "test_proxy_auth_empty_passwd", "test_proxy_auth_encoding", "TestHttpProxyMiddleware", "encoding", "auth", "test", "http", "spider", "nlcm", "dxnlcm5hbwu6", "fcser", "u00fcser", "spiders", "proxy", "https", "get", "crawler", "username", "pytest", "none", "type", "default", "bhc", "test_proxy_already_seted", "test_no_proxy", "test_no_proxy_invalid_values", "test_add_proxy_without_credentials", "test_add_proxy_with_credentials", "test_remove_proxy_without_credentials", "test_remove_proxy_with_credentials", "test_add_credentials", "test_change_credentials", "test_remove_credentials", "test_change_proxy_add_credentials", "test_change_proxy_keep_credentials", "test_change_proxy_change_credentials", "test_change_proxy_remove_credentials", "test_change_proxy_remove_credentials_preremoved_header", "test_proxy_authentication_header_undefined_proxy", "test_proxy_authentication_header_disabled_proxy", "test_proxy_authentication_header_proxy_without_credentials", "test_proxy_authentication_header_proxy_with_same_credentials", "test_proxy_authentication_header_proxy_with_different_credentials"], "ast_kind": "class_or_type", "text": "import os\n\nimport pytest\n\nfrom scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestHttpProxyMiddleware:\n    failureException = AssertionError  # type: ignore[assignment]\n\n    def setup_method(self):\n        self._oldenv = os.environ.copy()\n\n    def teardown_method(self):\n        os.environ = self._oldenv\n\n    def test_not_enabled(self):\n        crawler = get_crawler(Spider, {\"HTTPPROXY_ENABLED\": False})\n        with pytest.raises(NotConfigured):\n            HttpProxyMiddleware.from_crawler(crawler)\n\n    def test_no_environment_proxies(self):\n        os.environ = {\"dummy_proxy\": \"reset_env_and_do_not_raise\"}\n        mw = HttpProxyMiddleware()\n\n        for url in (\"http://e.com\", \"https://e.com\", \"file:///tmp/a\"):\n            req = Request(url)\n            assert mw.process_request(req) is None\n            assert req.url == url\n            assert req.meta == {}\n\n    def test_environment_proxies(self):\n        os.environ[\"http_proxy\"] = http_proxy = \"https://proxy.for.http:3128\"\n        os.environ[\"https_proxy\"] = https_proxy = \"http://proxy.for.https:8080\"\n        os.environ.pop(\"file_proxy\", None)\n        mw = HttpProxyMiddleware()\n\n        for url, proxy in [\n            (\"http://e.com\", http_proxy),\n            (\"https://e.com\", https_proxy),\n            (\"file://tmp/a\", None),\n        ]:\n            req = Request(url)\n            assert mw.process_request(req) is None\n            assert req.url == url\n            assert req.meta.get(\"proxy\") == proxy\n\n    def test_proxy_precedence_meta(self):\n        os.environ[\"http_proxy\"] = \"https://proxy.com\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://scrapytest.org\", meta={\"proxy\": \"https://new.proxy:3128\"})\n        assert mw.process_request(req) is None\n        assert req.meta == {\"proxy\": \"https://new.proxy:3128\"}\n\n    def test_proxy_auth(self):\n        os.environ[\"http_proxy\"] = \"https://user:pass@proxy:3128\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic dXNlcjpwYXNz\"\n        # proxy from request.meta\n        req = Request(\n            \"http://scrapytest.org\",\n            meta={\"proxy\": \"https://username:password@proxy:3128\"},\n        )\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert (\n            req.headers.get(\"Proxy-Authorization\") == b\"Basic dXNlcm5hbWU6cGFzc3dvcmQ=\"\n        )\n\n    def test_proxy_auth_empty_passwd(self):\n        os.environ[\"http_proxy\"] = \"https://user:@proxy:3128\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic dXNlcjo=\"\n        # proxy from request.meta\n        req = Request(\n            \"http://scrapytest.org\", meta={\"proxy\": \"https://username:@proxy:3128\"}\n        )\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic dXNlcm5hbWU6\"\n\n    def test_proxy_auth_encoding(self):\n        # utf-8 encoding\n        os.environ[\"http_proxy\"] = \"https://m\\u00e1n:pass@proxy:3128\"\n        mw = HttpProxyMiddleware(auth_encoding=\"utf-8\")\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic bcOhbjpwYXNz\"\n\n        # proxy from request.meta\n        req = Request(\n            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00fcser:pass@proxy:3128\"}\n        )\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic w7xzZXI6cGFzcw==\"\n\n        # default latin-1 encoding\n        mw = HttpProxyMiddleware(auth_encoding=\"latin-1\")\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic beFuOnBhc3M=\"\n\n        # proxy from request.meta, latin-1 encoding\n        req = Request(\n            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00fcser:pass@proxy:3128\"}\n        )\n        assert mw.process_request(req) is None\n        assert req.meta[\"proxy\"] == \"https://proxy:3128\"\n        assert req.headers.get(\"Proxy-Authorization\") == b\"Basic /HNlcjpwYXNz\"\n", "n_tokens": 1228, "byte_len": 4891, "file_sha1": "148819d3b527b92b7389f1174dfb2e7109d7f6e8", "start_line": 1, "end_line": 123}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py", "rel_path": "tests/test_downloadermiddleware_httpproxy.py", "module": "tests.test_downloadermiddleware_httpproxy", "ext": "py", "chunk_number": 2, "symbols": ["test_proxy_already_seted", "test_no_proxy", "test_no_proxy_invalid_values", "test_add_proxy_without_credentials", "test_add_proxy_with_credentials", "test_remove_proxy_without_credentials", "test_remove_proxy_with_credentials", "test_add_credentials", "test_change_credentials", "used", "test", "change", "proxy", "takes", "user", "those", "password", "password1", "were", "authorization", "add", "credentials", "sock", "middleware", "meta", "password2", "parseable", "with", "before", "environ", "setup_method", "teardown_method", "test_not_enabled", "test_no_environment_proxies", "test_environment_proxies", "test_proxy_precedence_meta", "test_proxy_auth", "test_proxy_auth_empty_passwd", "test_proxy_auth_encoding", "test_remove_credentials", "test_change_proxy_add_credentials", "test_change_proxy_keep_credentials", "test_change_proxy_change_credentials", "test_change_proxy_remove_credentials", "test_change_proxy_remove_credentials_preremoved_header", "test_proxy_authentication_header_undefined_proxy", "test_proxy_authentication_header_disabled_proxy", "test_proxy_authentication_header_proxy_without_credentials", "test_proxy_authentication_header_proxy_with_same_credentials", "test_proxy_authentication_header_proxy_with_different_credentials"], "ast_kind": "function_or_method", "text": "    def test_proxy_already_seted(self):\n        os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://noproxy.com\", meta={\"proxy\": None})\n        assert mw.process_request(req) is None\n        assert \"proxy\" in req.meta\n        assert req.meta[\"proxy\"] is None\n\n    def test_no_proxy(self):\n        os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n        mw = HttpProxyMiddleware()\n\n        os.environ[\"no_proxy\"] = \"*\"\n        req = Request(\"http://noproxy.com\")\n        assert mw.process_request(req) is None\n        assert \"proxy\" not in req.meta\n\n        os.environ[\"no_proxy\"] = \"other.com\"\n        req = Request(\"http://noproxy.com\")\n        assert mw.process_request(req) is None\n        assert \"proxy\" in req.meta\n\n        os.environ[\"no_proxy\"] = \"other.com,noproxy.com\"\n        req = Request(\"http://noproxy.com\")\n        assert mw.process_request(req) is None\n        assert \"proxy\" not in req.meta\n\n        # proxy from meta['proxy'] takes precedence\n        os.environ[\"no_proxy\"] = \"*\"\n        req = Request(\"http://noproxy.com\", meta={\"proxy\": \"http://proxy.com\"})\n        assert mw.process_request(req) is None\n        assert req.meta == {\"proxy\": \"http://proxy.com\"}\n\n    def test_no_proxy_invalid_values(self):\n        os.environ[\"no_proxy\"] = \"/var/run/docker.sock\"\n        mw = HttpProxyMiddleware()\n        # '/var/run/docker.sock' may be used by the user for\n        # no_proxy value but is not parseable and should be skipped\n        assert \"no\" not in mw.proxies\n\n    def test_add_proxy_without_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\"https://example.com\")\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = \"https://example.com\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_add_proxy_with_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\"https://example.com\")\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = \"https://user1:password1@example.com\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n    def test_remove_proxy_without_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = None\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] is None\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_remove_proxy_with_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = None\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] is None\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_add_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with the same\n        proxy and adds credentials (there were no credentials before), the new\n        credentials must be used.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request) is None\n\n        request.meta[\"proxy\"] = \"https://user1:password1@example.com\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n    def test_change_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with different\n        credentials, those new credentials must be used.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = \"https://user2:password2@example.com\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user2\",\n            \"password2\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n", "n_tokens": 1115, "byte_len": 5238, "file_sha1": "148819d3b527b92b7389f1174dfb2e7109d7f6e8", "start_line": 124, "end_line": 247}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py", "rel_path": "tests/test_downloadermiddleware_httpproxy.py", "module": "tests.test_downloadermiddleware_httpproxy", "ext": "py", "chunk_number": 3, "symbols": ["test_remove_credentials", "test_change_proxy_add_credentials", "test_change_proxy_keep_credentials", "test_change_proxy_change_credentials", "test_change_proxy_remove_credentials", "test_change_proxy_remove_credentials_preremoved_header", "test_proxy_authentication_header_undefined_proxy", "test_proxy_authentication_header_disabled_proxy", "used", "been", "switch", "implementation", "does", "delete", "keeping", "test", "proxy", "beforehand", "change", "password", "password1", "still", "authorization", "case", "credentials", "middleware", "corner", "meta", "password2", "make", "setup_method", "teardown_method", "test_not_enabled", "test_no_environment_proxies", "test_environment_proxies", "test_proxy_precedence_meta", "test_proxy_auth", "test_proxy_auth_empty_passwd", "test_proxy_auth_encoding", "test_proxy_already_seted", "test_no_proxy", "test_no_proxy_invalid_values", "test_add_proxy_without_credentials", "test_add_proxy_with_credentials", "test_remove_proxy_without_credentials", "test_remove_proxy_with_credentials", "test_add_credentials", "test_change_credentials", "test_proxy_authentication_header_proxy_without_credentials", "test_proxy_authentication_header_proxy_with_same_credentials"], "ast_kind": "function_or_method", "text": "    def test_remove_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with the same\n        proxy but no credentials, the original credentials must be still\n        used.\n\n        To remove credentials while keeping the same proxy URL, users must\n        delete the Proxy-Authorization header.\n        \"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n\n        request.meta[\"proxy\"] = \"https://example.com\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n        request.meta[\"proxy\"] = \"https://example.com\"\n        del request.headers[b\"Proxy-Authorization\"]\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_change_proxy_add_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request) is None\n\n        request.meta[\"proxy\"] = \"https://user1:password1@example.org\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.org\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n    def test_change_proxy_keep_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n\n        request.meta[\"proxy\"] = \"https://user1:password1@example.org\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.org\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n        # Make sure, indirectly, that _auth_proxy is updated.\n        request.meta[\"proxy\"] = \"https://example.com\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_change_proxy_change_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n\n        request.meta[\"proxy\"] = \"https://user2:password2@example.org\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.org\"\n        encoded_credentials = middleware._basic_auth_header(\n            \"user2\",\n            \"password2\",\n        )\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n    def test_change_proxy_remove_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with a different\n        proxy and no credentials, no credentials must be used.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = \"https://example.org\"\n        assert middleware.process_request(request) is None\n        assert request.meta == {\"proxy\": \"https://example.org\"}\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_change_proxy_remove_credentials_preremoved_header(self):\n        \"\"\"Corner case of proxy switch with credentials removal where the\n        credentials have been removed beforehand.\n\n        It ensures that our implementation does not assume that the credentials\n        header exists when trying to remove it.\n        \"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        request.meta[\"proxy\"] = \"https://example.org\"\n        del request.headers[b\"Proxy-Authorization\"]\n        assert middleware.process_request(request) is None\n        assert request.meta == {\"proxy\": \"https://example.org\"}\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_proxy_authentication_header_undefined_proxy(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": \"Basic foo\"},\n        )\n        assert middleware.process_request(request) is None\n        assert \"proxy\" not in request.meta\n        assert b\"Proxy-Authorization\" not in request.headers\n\n    def test_proxy_authentication_header_disabled_proxy(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": \"Basic foo\"},\n            meta={\"proxy\": None},\n        )\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] is None\n        assert b\"Proxy-Authorization\" not in request.headers\n", "n_tokens": 1192, "byte_len": 5982, "file_sha1": "148819d3b527b92b7389f1174dfb2e7109d7f6e8", "start_line": 248, "end_line": 388}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpproxy.py", "rel_path": "tests/test_downloadermiddleware_httpproxy.py", "module": "tests.test_downloadermiddleware_httpproxy", "ext": "py", "chunk_number": 4, "symbols": ["test_proxy_authentication_header_proxy_without_credentials", "test_proxy_authentication_header_proxy_with_same_credentials", "test_proxy_authentication_header_proxy_with_different_credentials", "used", "test", "proxy", "long", "password", "password1", "authorization", "middleware", "user", "user1", "meta", "password2", "headers", "https", "example", "encoded", "credentials", "assert", "even", "request", "process", "basic", "none", "same", "remains", "user2", "header", "setup_method", "teardown_method", "test_not_enabled", "test_no_environment_proxies", "test_environment_proxies", "test_proxy_precedence_meta", "test_proxy_auth", "test_proxy_auth_empty_passwd", "test_proxy_auth_encoding", "test_proxy_already_seted", "test_no_proxy", "test_no_proxy_invalid_values", "test_add_proxy_without_credentials", "test_add_proxy_with_credentials", "test_remove_proxy_without_credentials", "test_remove_proxy_with_credentials", "test_add_credentials", "test_change_credentials", "test_remove_credentials", "test_change_proxy_add_credentials"], "ast_kind": "function_or_method", "text": "    def test_proxy_authentication_header_proxy_without_credentials(self):\n        \"\"\"As long as the proxy URL in request metadata remains the same, the\n        Proxy-Authorization header is used and kept, and may even be\n        changed.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": \"Basic foo\"},\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic foo\"\n\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic foo\"\n\n        request.headers[\"Proxy-Authorization\"] = b\"Basic bar\"\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic bar\"\n\n    def test_proxy_authentication_header_proxy_with_same_credentials(self):\n        middleware = HttpProxyMiddleware()\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": b\"Basic \" + encoded_credentials},\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        assert request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials\n\n    def test_proxy_authentication_header_proxy_with_different_credentials(self):\n        middleware = HttpProxyMiddleware()\n        encoded_credentials1 = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": b\"Basic \" + encoded_credentials1},\n            meta={\"proxy\": \"https://user2:password2@example.com\"},\n        )\n        assert middleware.process_request(request) is None\n        assert request.meta[\"proxy\"] == \"https://example.com\"\n        encoded_credentials2 = middleware._basic_auth_header(\n            \"user2\",\n            \"password2\",\n        )\n        assert (\n            request.headers[\"Proxy-Authorization\"] == b\"Basic \" + encoded_credentials2\n        )\n", "n_tokens": 516, "byte_len": 2580, "file_sha1": "148819d3b527b92b7389f1174dfb2e7109d7f6e8", "start_line": 389, "end_line": 447}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py", "rel_path": "tests/test_downloader_handlers_http_base.py", "module": "tests.test_downloader_handlers_http_base", "ext": "py", "chunk_number": 1, "symbols": ["download_handler_cls", "TestHttpBase", "test", "redirect", "method", "async", "call", "later", "ticket", "works", "after", "spider", "coroutines", "deferred", "from", "download", "handler", "spiders", "future", "typ", "checking", "https", "mockserver", "mock", "interface", "get", "crawler", "existin", "resolvable", "newclient", "check", "simple_mockserver", "url", "settings_dict", "proxy_mockserver", "TestHttp11Base", "TestHttps11Base", "TestSimpleHttpsBase", "TestHttpsWrongHostnameBase", "TestHttpsInvalidDNSIdBase", "TestHttpsInvalidDNSPatternBase", "TestHttpsCustomCiphersBase", "TestHttpWithCrawlerBase", "TestHttpProxyBase", "bool", "case", "connection", "data", "received", "simple"], "ast_kind": "class_or_type", "text": "\"\"\"Base classes for HTTP download handler tests.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Any\nfrom unittest import mock\n\nimport pytest\nfrom pytest_twisted import async_yield_fixture\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer, error\nfrom twisted.web._newclient import ResponseFailed\nfrom twisted.web.http import _DataLoss\n\nfrom scrapy.http import Headers, HtmlResponse, Request, Response, TextResponse\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests import NON_EXISTING_RESOLVABLE\nfrom tests.mockserver.proxy_echo import ProxyEchoMockServer\nfrom tests.mockserver.simple_https import SimpleMockServer\nfrom tests.spiders import SingleRequestSpider\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Generator\n\n    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n    from tests.mockserver.http import MockServer\n\n\nasync def download_request(\n    download_handler: DownloadHandlerProtocol,\n    request: Request,\n    spider: Spider = DefaultSpider(),\n) -> Response:\n    return await maybe_deferred_to_future(\n        download_handler.download_request(request, spider)\n    )\n\n\nasync def close_dh(dh: DownloadHandlerProtocol) -> None:\n    # needed because the interface of close() is not clearly defined\n    if not hasattr(dh, \"close\"):\n        return\n    c = dh.close()\n    if c is None:\n        return\n    # covers coroutines and Deferreds; won't work if close() uses Futures inside\n    await c\n\n\nclass TestHttpBase(ABC):\n    is_secure = False\n\n    @property\n    @abstractmethod\n    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n        raise NotImplementedError\n\n    @async_yield_fixture\n    async def download_handler(self) -> AsyncGenerator[DownloadHandlerProtocol]:\n        dh = build_from_crawler(self.download_handler_cls, get_crawler())\n\n        yield dh\n\n        await close_dh(dh)\n\n    @deferred_f_from_coro_f\n    async def test_download(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n        response = await download_request(download_handler, request)\n        assert response.body == b\"Works\"\n\n    @deferred_f_from_coro_f\n    async def test_download_head(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/text\", is_secure=self.is_secure), method=\"HEAD\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.body == b\"\"\n\n    @deferred_f_from_coro_f\n    async def test_redirect_status(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/redirect\", is_secure=self.is_secure))\n        response = await download_request(download_handler, request)\n        assert response.status == 302\n\n    @deferred_f_from_coro_f\n    async def test_redirect_status_head(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/redirect\", is_secure=self.is_secure), method=\"HEAD\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.status == 302\n\n    @deferred_f_from_coro_f\n    async def test_timeout_download_from_spider_nodata_rcvd(\n        self,\n        mockserver: MockServer,\n        download_handler: DownloadHandlerProtocol,\n        reactor_pytest: str,\n    ) -> None:\n        if reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n            # https://twistedmatrix.com/trac/ticket/10279\n            pytest.skip(\n                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n            )\n\n        # client connects but no data is received\n        meta = {\"download_timeout\": 0.5}\n        request = Request(mockserver.url(\"/wait\", is_secure=self.is_secure), meta=meta)\n        d = deferred_from_coro(download_request(download_handler, request))\n        with pytest.raises((defer.TimeoutError, error.TimeoutError)):\n            await maybe_deferred_to_future(d)\n\n    @deferred_f_from_coro_f\n    async def test_timeout_download_from_spider_server_hangs(\n        self,\n        mockserver: MockServer,\n        download_handler: DownloadHandlerProtocol,\n        reactor_pytest: str,\n    ) -> None:\n        if reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n            # https://twistedmatrix.com/trac/ticket/10279\n            pytest.skip(\n                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n            )\n        # client connects, server send headers and some body bytes but hangs\n        meta = {\"download_timeout\": 0.5}\n        request = Request(\n            mockserver.url(\"/hang-after-headers\", is_secure=self.is_secure), meta=meta\n        )\n        d = deferred_from_coro(download_request(download_handler, request))\n        with pytest.raises((defer.TimeoutError, error.TimeoutError)):\n            await maybe_deferred_to_future(d)\n", "n_tokens": 1176, "byte_len": 5484, "file_sha1": "2a963574d6d3e922acce09b736ebb824df61eba5", "start_line": 1, "end_line": 154}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py", "rel_path": "tests/test_downloader_handlers_http_base.py", "module": "tests.test_downloader_handlers_http_base", "ext": "py", "chunk_number": 2, "symbols": ["TestHttp11Base", "method", "unidiomatic", "async", "bool", "loads", "works", "case", "payload", "deferred", "from", "download", "handler", "cause", "test", "content", "contentlengths", "port", "https", "length", "mockserver", "kennethreitz", "correct", "sent", "pytest", "issues", "none", "encode", "html", "type", "download_handler_cls", "check", "simple_mockserver", "url", "settings_dict", "proxy_mockserver", "TestHttpBase", "TestHttps11Base", "TestSimpleHttpsBase", "TestHttpsWrongHostnameBase", "TestHttpsInvalidDNSIdBase", "TestHttpsInvalidDNSPatternBase", "TestHttpsCustomCiphersBase", "TestHttpWithCrawlerBase", "TestHttpProxyBase", "mock", "connection", "spider", "data", "received"], "ast_kind": "class_or_type", "text": "    @pytest.mark.parametrize(\"send_header\", [True, False])\n    @deferred_f_from_coro_f\n    async def test_host_header(\n        self,\n        send_header: bool,\n        mockserver: MockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        host_port = f\"{mockserver.host}:{mockserver.port(is_secure=self.is_secure)}\"\n        request = Request(\n            mockserver.url(\"/host\", is_secure=self.is_secure),\n            headers={\"Host\": host_port} if send_header else {},\n        )\n        response = await download_request(download_handler, request)\n        assert response.body == host_port.encode()\n        if send_header:\n            assert request.headers.get(\"Host\") == host_port.encode()\n        else:\n            assert not request.headers\n\n    @deferred_f_from_coro_f\n    async def test_content_length_zero_bodyless_post_request_headers(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        \"\"\"Tests if \"Content-Length: 0\" is sent for bodyless POST requests.\n\n        This is not strictly required by HTTP RFCs but can cause trouble\n        for some web servers.\n        See:\n        https://github.com/scrapy/scrapy/issues/823\n        https://issues.apache.org/jira/browse/TS-2902\n        https://github.com/kennethreitz/requests/issues/405\n        https://bugs.python.org/issue14721\n        \"\"\"\n        request = Request(\n            mockserver.url(\"/contentlength\", is_secure=self.is_secure), method=\"POST\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.body == b\"0\"\n\n    @deferred_f_from_coro_f\n    async def test_content_length_zero_bodyless_post_only_one(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/echo\", is_secure=self.is_secure), method=\"POST\"\n        )\n        response = await download_request(download_handler, request)\n        headers = Headers(json.loads(response.text)[\"headers\"])\n        contentlengths = headers.getlist(\"Content-Length\")\n        assert len(contentlengths) == 1\n        assert contentlengths == [b\"0\"]\n\n    @deferred_f_from_coro_f\n    async def test_payload(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n        request = Request(\n            mockserver.url(\"/payload\", is_secure=self.is_secure),\n            method=\"POST\",\n            body=body,\n        )\n        response = await download_request(download_handler, request)\n        assert response.body == body\n\n    @deferred_f_from_coro_f\n    async def test_response_header_content_length(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/text\", is_secure=self.is_secure), method=\"GET\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.headers[b\"content-length\"] == b\"5\"\n\n    @pytest.mark.parametrize(\n        (\"filename\", \"body\", \"response_class\"),\n        [\n            (\"foo.html\", b\"\", HtmlResponse),\n            (\"foo\", b\"<!DOCTYPE html>\\n<title>.</title>\", HtmlResponse),\n        ],\n    )\n    @deferred_f_from_coro_f\n    async def test_response_class(\n        self,\n        filename: str,\n        body: bytes,\n        response_class: type[Response],\n        mockserver: MockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        request = Request(\n            mockserver.url(f\"/{filename}\", is_secure=self.is_secure), body=body\n        )\n        response = await download_request(download_handler, request)\n        assert type(response) is response_class  # pylint: disable=unidiomatic-typecheck\n\n    @deferred_f_from_coro_f\n    async def test_get_duplicate_header(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/duplicate-header\", is_secure=self.is_secure))\n        response = await download_request(download_handler, request)\n        assert response.headers.getlist(b\"Set-Cookie\") == [b\"a=b\", b\"c=d\"]\n\n\nclass TestHttp11Base(TestHttpBase):\n    \"\"\"HTTP 1.1 test case\"\"\"\n\n    @deferred_f_from_coro_f\n    async def test_download_without_maxsize_limit(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n        response = await download_request(download_handler, request)\n        assert response.body == b\"Works\"\n\n    @deferred_f_from_coro_f\n    async def test_response_class_choosing_request(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        \"\"\"Tests choosing of correct response type\n        in case of Content-Type is empty but body contains text.\n        \"\"\"\n        body = b\"Some plain text\\ndata with tabs\\t and null bytes\\0\"\n        request = Request(\n            mockserver.url(\"/nocontenttype\", is_secure=self.is_secure), body=body\n        )\n        response = await download_request(download_handler, request)\n        assert type(response) is TextResponse  # pylint: disable=unidiomatic-typecheck\n", "n_tokens": 1161, "byte_len": 5319, "file_sha1": "2a963574d6d3e922acce09b736ebb824df61eba5", "start_line": 155, "end_line": 286}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py", "rel_path": "tests/test_downloader_handlers_http_base.py", "module": "tests.test_downloader_handlers_http_base", "ext": "py", "chunk_number": 3, "symbols": ["check", "async", "call", "later", "works", "mock", "connection", "after", "spider", "data", "received", "deferred", "from", "test", "download", "handler", "mockserver", "get", "crawler", "dataloss", "pytest", "process", "closing", "none", "handlers", "type", "exc", "info", "secure", "reasons", "download_handler_cls", "simple_mockserver", "url", "settings_dict", "proxy_mockserver", "TestHttpBase", "TestHttp11Base", "TestHttps11Base", "TestSimpleHttpsBase", "TestHttpsWrongHostnameBase", "TestHttpsInvalidDNSIdBase", "TestHttpsInvalidDNSPatternBase", "TestHttpsCustomCiphersBase", "TestHttpWithCrawlerBase", "TestHttpProxyBase", "method", "bool", "case", "https", "simple"], "ast_kind": "function_or_method", "text": "    @deferred_f_from_coro_f\n    async def test_download_with_maxsize(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n\n        # 10 is minimal size for this request and the limit is only counted on\n        # response body. (regardless of headers)\n        response = await download_request(\n            download_handler, request, Spider(\"foo\", download_maxsize=5)\n        )\n        assert response.body == b\"Works\"\n\n        with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n            await download_request(\n                download_handler, request, Spider(\"foo\", download_maxsize=4)\n            )\n\n    @deferred_f_from_coro_f\n    async def test_download_with_maxsize_very_large_file(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        # TODO: the logger check is specific to scrapy.core.downloader.handlers.http11\n        with mock.patch(\"scrapy.core.downloader.handlers.http11.logger\") as logger:\n            request = Request(\n                mockserver.url(\"/largechunkedfile\", is_secure=self.is_secure)\n            )\n\n            def check(logger: mock.Mock) -> None:\n                logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n\n            with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n                await download_request(\n                    download_handler, request, Spider(\"foo\", download_maxsize=1500)\n                )\n\n            # As the error message is logged in the dataReceived callback, we\n            # have to give a bit of time to the reactor to process the queue\n            # after closing the connection.\n            d: defer.Deferred[mock.Mock] = defer.Deferred()\n            d.addCallback(check)\n            call_later(0.1, d.callback, logger)\n            await maybe_deferred_to_future(d)\n\n    @deferred_f_from_coro_f\n    async def test_download_with_maxsize_per_req(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        meta = {\"download_maxsize\": 2}\n        request = Request(mockserver.url(\"/text\", is_secure=self.is_secure), meta=meta)\n        with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n            await download_request(download_handler, request)\n\n    @deferred_f_from_coro_f\n    async def test_download_with_small_maxsize_per_spider(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n        with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n            await download_request(\n                download_handler, request, Spider(\"foo\", download_maxsize=2)\n            )\n\n    @deferred_f_from_coro_f\n    async def test_download_with_large_maxsize_per_spider(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n        response = await download_request(\n            download_handler, request, Spider(\"foo\", download_maxsize=100)\n        )\n        assert response.body == b\"Works\"\n\n    @deferred_f_from_coro_f\n    async def test_download_chunked_content(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(mockserver.url(\"/chunked\", is_secure=self.is_secure))\n        response = await download_request(download_handler, request)\n        assert response.body == b\"chunked content\\n\"\n\n    @pytest.mark.parametrize(\"url\", [\"broken\", \"broken-chunked\"])\n    @deferred_f_from_coro_f\n    async def test_download_cause_data_loss(\n        self,\n        url: str,\n        mockserver: MockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        # TODO: this one checks for Twisted-specific exceptions\n        request = Request(mockserver.url(f\"/{url}\", is_secure=self.is_secure))\n        with pytest.raises(ResponseFailed) as exc_info:\n            await download_request(download_handler, request)\n        assert any(r.check(_DataLoss) for r in exc_info.value.reasons)\n\n    @pytest.mark.parametrize(\"url\", [\"broken\", \"broken-chunked\"])\n    @deferred_f_from_coro_f\n    async def test_download_allow_data_loss(\n        self,\n        url: str,\n        mockserver: MockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        request = Request(\n            mockserver.url(f\"/{url}\", is_secure=self.is_secure),\n            meta={\"download_fail_on_dataloss\": False},\n        )\n        response = await download_request(download_handler, request)\n        assert response.flags == [\"dataloss\"]\n\n    @pytest.mark.parametrize(\"url\", [\"broken\", \"broken-chunked\"])\n    @deferred_f_from_coro_f\n    async def test_download_allow_data_loss_via_setting(\n        self, url: str, mockserver: MockServer\n    ) -> None:\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n        request = Request(mockserver.url(f\"/{url}\", is_secure=self.is_secure))\n        try:\n            response = await maybe_deferred_to_future(\n                download_handler.download_request(request, DefaultSpider())\n            )\n        finally:\n            d = download_handler.close()  # type: ignore[attr-defined]\n            if d is not None:\n                await maybe_deferred_to_future(d)\n        assert response.flags == [\"dataloss\"]\n", "n_tokens": 1209, "byte_len": 5633, "file_sha1": "2a963574d6d3e922acce09b736ebb824df61eba5", "start_line": 287, "end_line": 414}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py", "rel_path": "tests/test_downloader_handlers_http_base.py", "module": "tests.test_downloader_handlers_http_base", "ext": "py", "chunk_number": 4, "symbols": ["simple_mockserver", "url", "download_handler_cls", "settings_dict", "TestHttps11Base", "TestSimpleHttpsBase", "TestHttpsWrongHostnameBase", "TestHttpsInvalidDNSIdBase", "TestHttpsInvalidDNSPatternBase", "TestHttpsCustomCiphersBase", "TestHttpWithCrawlerBase", "method", "issued", "async", "protocol", "seed", "works", "case", "connection", "domain", "deferred", "from", "https", "download", "handler", "simple", "mockserver", "port", "cert", "get", "check", "proxy_mockserver", "TestHttpBase", "TestHttp11Base", "TestHttpProxyBase", "bool", "mock", "spider", "data", "received", "test", "contentlengths", "nosuch", "sent", "process", "closing", "here", "encode", "server", "connect"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_protocol(\n        self, mockserver: MockServer, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(\n            mockserver.url(\"/host\", is_secure=self.is_secure), method=\"GET\"\n        )\n        response = await download_request(download_handler, request)\n        assert response.protocol == \"HTTP/1.1\"\n\n\nclass TestHttps11Base(TestHttp11Base):\n    is_secure = True\n\n    tls_log_message = (\n        'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=localhost\", '\n        'subject \"/C=IE/O=Scrapy/CN=localhost\"'\n    )\n\n    @deferred_f_from_coro_f\n    async def test_tls_logging(self, mockserver: MockServer) -> None:\n        crawler = get_crawler(\n            settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n        )\n        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n        try:\n            with LogCapture() as log_capture:\n                request = Request(mockserver.url(\"/text\", is_secure=self.is_secure))\n                response = await maybe_deferred_to_future(\n                    download_handler.download_request(request, DefaultSpider())\n                )\n                assert response.body == b\"Works\"\n                log_capture.check_present(\n                    (\"scrapy.core.downloader.tls\", \"DEBUG\", self.tls_log_message)\n                )\n        finally:\n            d = download_handler.close()  # type: ignore[attr-defined]\n            if d is not None:\n                await maybe_deferred_to_future(d)\n\n\nclass TestSimpleHttpsBase(ABC):\n    \"\"\"Base class for special cases tested with just one simple request\"\"\"\n\n    keyfile = \"keys/localhost.key\"\n    certfile = \"keys/localhost.crt\"\n    host = \"localhost\"\n    cipher_string: str | None = None\n\n    @pytest.fixture(scope=\"class\")\n    def simple_mockserver(self) -> Generator[SimpleMockServer]:\n        with SimpleMockServer(\n            self.keyfile, self.certfile, self.cipher_string\n        ) as simple_mockserver:\n            yield simple_mockserver\n\n    @pytest.fixture(scope=\"class\")\n    def url(self, simple_mockserver: SimpleMockServer) -> str:\n        # need to use self.host instead of what mockserver returns\n        return f\"https://{self.host}:{simple_mockserver.port(is_secure=True)}/file\"\n\n    @property\n    @abstractmethod\n    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n        raise NotImplementedError\n\n    @async_yield_fixture\n    async def download_handler(self) -> AsyncGenerator[DownloadHandlerProtocol]:\n        if self.cipher_string is not None:\n            settings_dict = {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.cipher_string}\n        else:\n            settings_dict = None\n        crawler = get_crawler(settings_dict=settings_dict)\n        dh = build_from_crawler(self.download_handler_cls, crawler)\n\n        yield dh\n\n        await close_dh(dh)\n\n    @deferred_f_from_coro_f\n    async def test_download(\n        self, url: str, download_handler: DownloadHandlerProtocol\n    ) -> None:\n        request = Request(url)\n        response = await download_request(download_handler, request)\n        assert response.body == b\"0123456789\"\n\n\nclass TestHttpsWrongHostnameBase(TestSimpleHttpsBase):\n    # above tests use a server certificate for \"localhost\",\n    # client connection to \"localhost\" too.\n    # here we test that even if the server certificate is for another domain,\n    # \"www.example.com\" in this case,\n    # the tests still pass\n    keyfile = \"keys/example-com.key.pem\"\n    certfile = \"keys/example-com.cert.pem\"\n\n\nclass TestHttpsInvalidDNSIdBase(TestSimpleHttpsBase):\n    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n\n    host = \"127.0.0.1\"\n\n\nclass TestHttpsInvalidDNSPatternBase(TestSimpleHttpsBase):\n    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n\n    keyfile = \"keys/localhost.ip.key\"\n    certfile = \"keys/localhost.ip.crt\"\n\n\nclass TestHttpsCustomCiphersBase(TestSimpleHttpsBase):\n    cipher_string = \"CAMELLIA256-SHA\"\n\n\nclass TestHttpWithCrawlerBase(ABC):\n    @property\n    @abstractmethod\n    def settings_dict(self) -> dict[str, Any] | None:\n        raise NotImplementedError\n\n    is_secure = False\n\n    @deferred_f_from_coro_f\n    async def test_download_with_content_length(self, mockserver: MockServer) -> None:\n        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n        # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n        # download it\n        await maybe_deferred_to_future(\n            crawler.crawl(\n                seed=Request(\n                    url=mockserver.url(\"/partial\", is_secure=self.is_secure),\n                    meta={\"download_maxsize\": 1000},\n                )\n            )\n        )\n        assert crawler.spider\n        failure = crawler.spider.meta[\"failure\"]  # type: ignore[attr-defined]\n        assert isinstance(failure.value, defer.CancelledError)\n", "n_tokens": 1100, "byte_len": 4999, "file_sha1": "2a963574d6d3e922acce09b736ebb824df61eba5", "start_line": 415, "end_line": 554}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handlers_http_base.py", "rel_path": "tests/test_downloader_handlers_http_base.py", "module": "tests.test_downloader_handlers_http_base", "ext": "py", "chunk_number": 5, "symbols": ["download_handler_cls", "proxy_mockserver", "TestHttpProxyBase", "test", "download", "async", "seed", "domain", "deferred", "from", "handler", "http", "proxy", "https", "port", "mockserver", "nosuch", "get", "crawler", "existin", "resolvable", "pytest", "none", "fixture", "echo", "error", "oserror", "type", "reason", "single", "check", "simple_mockserver", "url", "settings_dict", "TestHttpBase", "TestHttp11Base", "TestHttps11Base", "TestSimpleHttpsBase", "TestHttpsWrongHostnameBase", "TestHttpsInvalidDNSIdBase", "TestHttpsInvalidDNSPatternBase", "TestHttpsCustomCiphersBase", "TestHttpWithCrawlerBase", "method", "bool", "works", "mock", "case", "connection", "spider"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_download(self, mockserver: MockServer) -> None:\n        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n        await maybe_deferred_to_future(\n            crawler.crawl(\n                seed=Request(url=mockserver.url(\"\", is_secure=self.is_secure))\n            )\n        )\n        assert crawler.spider\n        failure = crawler.spider.meta.get(\"failure\")  # type: ignore[attr-defined]\n        assert failure is None\n        reason = crawler.spider.meta[\"close_reason\"]  # type: ignore[attr-defined]\n        assert reason == \"finished\"\n\n\nclass TestHttpProxyBase(ABC):\n    is_secure = False\n    expected_http_proxy_request_body = b\"http://example.com\"\n\n    @property\n    @abstractmethod\n    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n        raise NotImplementedError\n\n    @pytest.fixture(scope=\"session\")\n    def proxy_mockserver(self) -> Generator[ProxyEchoMockServer]:\n        with ProxyEchoMockServer() as proxy:\n            yield proxy\n\n    @async_yield_fixture\n    async def download_handler(self) -> AsyncGenerator[DownloadHandlerProtocol]:\n        dh = build_from_crawler(self.download_handler_cls, get_crawler())\n\n        yield dh\n\n        await close_dh(dh)\n\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy(\n        self,\n        proxy_mockserver: ProxyEchoMockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        http_proxy = proxy_mockserver.url(\"\", is_secure=self.is_secure)\n        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n        response = await download_request(download_handler, request)\n        assert response.status == 200\n        assert response.url == request.url\n        assert response.body == self.expected_http_proxy_request_body\n\n    @deferred_f_from_coro_f\n    async def test_download_without_proxy(\n        self,\n        proxy_mockserver: ProxyEchoMockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        request = Request(\n            proxy_mockserver.url(\"/path/to/resource\", is_secure=self.is_secure)\n        )\n        response = await download_request(download_handler, request)\n        assert response.status == 200\n        assert response.url == request.url\n        assert response.body == b\"/path/to/resource\"\n\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy_https_timeout(\n        self,\n        proxy_mockserver: ProxyEchoMockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        if NON_EXISTING_RESOLVABLE:\n            pytest.skip(\"Non-existing hosts are resolvable\")\n        http_proxy = proxy_mockserver.url(\"\", is_secure=self.is_secure)\n        domain = \"https://no-such-domain.nosuch\"\n        request = Request(domain, meta={\"proxy\": http_proxy, \"download_timeout\": 0.2})\n        with pytest.raises(error.TimeoutError) as exc_info:\n            await download_request(download_handler, request)\n        assert domain in exc_info.value.osError\n\n    @deferred_f_from_coro_f\n    async def test_download_with_proxy_without_http_scheme(\n        self,\n        proxy_mockserver: ProxyEchoMockServer,\n        download_handler: DownloadHandlerProtocol,\n    ) -> None:\n        http_proxy = f\"{proxy_mockserver.host}:{proxy_mockserver.port()}\"\n        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n        response = await download_request(download_handler, request)\n        assert response.status == 200\n        assert response.url == request.url\n        assert response.body == self.expected_http_proxy_request_body\n", "n_tokens": 760, "byte_len": 3606, "file_sha1": "2a963574d6d3e922acce09b736ebb824df61eba5", "start_line": 555, "end_line": 646}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_python.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_python.py", "rel_path": "tests/test_utils_python.py", "module": "tests.test_utils_python", "ext": "py", "chunk_number": 1, "symbols": ["test_mutablechain", "test_converting_an_utf8_encoded_string_to_unicode", "test_converting_a_latin_1_encoded_string_to_unicode", "test_converting_a_unicode_to_unicode_should_return_the_same_object", "test_converting_a_strange_object_should_raise_type_error", "test_errors_argument", "test_converting_a_unicode_object_to_an_utf_8_encoded_string", "test_converting_a_unicode_object_to_a_latin_1_encoded_string", "test_converting_a_regular_bytes_to_bytes_should_return_the_same_object", "test_memoizemethod_noargs", "cached", "noncached", "test_binaryistext", "test_equal_attributes", "TestMutableAsyncChain", "TestToUnicode", "TestToBytes", "A", "async", "test", "unicode", "bool", "errors", "deferred", "from", "anext", "future", "typ", "checking", "three", "compare_z", "test_get_func_args", "f1", "f2", "f3", "__init__", "method", "__call__", "test_without_none_values", "Obj", "Callable", "compare", "partial", "elif", "https", "four", "correct", "pytest", "hello", "implicit"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport functools\nimport operator\nimport platform\nimport sys\nfrom typing import TYPE_CHECKING, TypeVar\n\nimport pytest\n\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.defer import aiter_errback, deferred_f_from_coro_f\nfrom scrapy.utils.python import (\n    MutableAsyncChain,\n    MutableChain,\n    binary_is_text,\n    equal_attributes,\n    get_func_args,\n    memoizemethod_noargs,\n    to_bytes,\n    to_unicode,\n    without_none_values,\n)\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Mapping\n\n\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\ndef test_mutablechain():\n    m = MutableChain(range(2), [2, 3], (4, 5))\n    m.extend(range(6, 7))\n    m.extend([7, 8])\n    m.extend([9, 10], (11, 12))\n    assert next(m) == 0\n    assert m.__next__() == 1\n    assert list(m) == list(range(2, 13))\n\n\nclass TestMutableAsyncChain:\n    @staticmethod\n    async def g1():\n        for i in range(3):\n            yield i\n\n    @staticmethod\n    async def g2():\n        return\n        yield\n\n    @staticmethod\n    async def g3():\n        for i in range(7, 10):\n            yield i\n\n    @staticmethod\n    async def g4():\n        for i in range(3, 5):\n            yield i\n        1 / 0\n        for i in range(5, 7):\n            yield i\n\n    @deferred_f_from_coro_f\n    async def test_mutableasyncchain(self):\n        m = MutableAsyncChain(self.g1(), as_async_generator(range(3, 7)))\n        m.extend(self.g2())\n        m.extend(self.g3())\n\n        assert await m.__anext__() == 0\n        results = await collect_asyncgen(m)\n        assert results == list(range(1, 10))\n\n    @deferred_f_from_coro_f\n    async def test_mutableasyncchain_exc(self):\n        m = MutableAsyncChain(self.g1())\n        m.extend(self.g4())\n        m.extend(self.g3())\n\n        results = await collect_asyncgen(aiter_errback(m, lambda _: None))\n        assert results == list(range(5))\n\n\nclass TestToUnicode:\n    def test_converting_an_utf8_encoded_string_to_unicode(self):\n        assert to_unicode(b\"lel\\xc3\\xb1e\") == \"lel\\xf1e\"\n\n    def test_converting_a_latin_1_encoded_string_to_unicode(self):\n        assert to_unicode(b\"lel\\xf1e\", \"latin-1\") == \"lel\\xf1e\"\n\n    def test_converting_a_unicode_to_unicode_should_return_the_same_object(self):\n        assert to_unicode(\"\\xf1e\\xf1e\\xf1e\") == \"\\xf1e\\xf1e\\xf1e\"\n\n    def test_converting_a_strange_object_should_raise_type_error(self):\n        with pytest.raises(TypeError):\n            to_unicode(423)\n\n    def test_errors_argument(self):\n        assert to_unicode(b\"a\\xedb\", \"utf-8\", errors=\"replace\") == \"a\\ufffdb\"\n\n\nclass TestToBytes:\n    def test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):\n        assert to_bytes(\"\\xa3 49\") == b\"\\xc2\\xa3 49\"\n\n    def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):\n        assert to_bytes(\"\\xa3 49\", \"latin-1\") == b\"\\xa3 49\"\n\n    def test_converting_a_regular_bytes_to_bytes_should_return_the_same_object(self):\n        assert to_bytes(b\"lel\\xf1e\") == b\"lel\\xf1e\"\n\n    def test_converting_a_strange_object_should_raise_type_error(self):\n        with pytest.raises(TypeError):\n            to_bytes(pytest)\n\n    def test_errors_argument(self):\n        assert to_bytes(\"a\\ufffdb\", \"latin-1\", errors=\"replace\") == b\"a?b\"\n\n\ndef test_memoizemethod_noargs():\n    class A:\n        @memoizemethod_noargs\n        def cached(self):\n            return object()\n\n        def noncached(self):\n            return object()\n\n    a = A()\n    one = a.cached()\n    two = a.cached()\n    three = a.noncached()\n    assert one is two\n    assert one is not three\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    [\n        (b\"hello\", True),\n        (\"hello\".encode(\"utf-16\"), True),\n        (b\"<div>Price \\xa3</div>\", True),\n        (b\"\\x02\\xa3\", False),\n    ],\n)\ndef test_binaryistext(value: bytes, expected: bool) -> None:\n    assert binary_is_text(value) is expected\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\ndef test_equal_attributes():", "n_tokens": 1067, "byte_len": 4032, "file_sha1": "0735447d5ef861784f5b50a82fe12e94b5a7c915", "start_line": 1, "end_line": 154}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_python.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_python.py", "rel_path": "tests/test_utils_python.py", "module": "tests.test_utils_python", "ext": "py", "chunk_number": 2, "symbols": ["compare_z", "test_get_func_args", "f1", "f2", "f3", "__init__", "method", "__call__", "test_without_none_values", "Obj", "A", "Callable", "compare", "partial", "elif", "https", "four", "three", "correct", "pytest", "implicit", "test", "without", "object", "older", "issues", "none", "join", "fail", "callable", "test_mutablechain", "test_converting_an_utf8_encoded_string_to_unicode", "test_converting_a_latin_1_encoded_string_to_unicode", "test_converting_a_unicode_to_unicode_should_return_the_same_object", "test_converting_a_strange_object_should_raise_type_error", "test_errors_argument", "test_converting_a_unicode_object_to_an_utf_8_encoded_string", "test_converting_a_unicode_object_to_a_latin_1_encoded_string", "test_converting_a_regular_bytes_to_bytes_should_return_the_same_object", "test_memoizemethod_noargs", "cached", "noncached", "test_binaryistext", "test_equal_attributes", "TestMutableAsyncChain", "TestToUnicode", "TestToBytes", "async", "unicode", "bool"], "ast_kind": "class_or_type", "text": "    class Obj:\n        pass\n\n    a = Obj()\n    b = Obj()\n    # no attributes given return False\n    assert not equal_attributes(a, b, [])\n    # nonexistent attributes\n    assert not equal_attributes(a, b, [\"x\", \"y\"])\n\n    a.x = 1\n    b.x = 1\n    # equal attribute\n    assert equal_attributes(a, b, [\"x\"])\n\n    b.y = 2\n    # obj1 has no attribute y\n    assert not equal_attributes(a, b, [\"x\", \"y\"])\n\n    a.y = 2\n    # equal attributes\n    assert equal_attributes(a, b, [\"x\", \"y\"])\n\n    a.y = 1\n    # different attributes\n    assert not equal_attributes(a, b, [\"x\", \"y\"])\n\n    # test callable\n    a.meta = {}\n    b.meta = {}\n    assert equal_attributes(a, b, [\"meta\"])\n\n    # compare ['meta']['a']\n    a.meta[\"z\"] = 1\n    b.meta[\"z\"] = 1\n\n    get_z = operator.itemgetter(\"z\")\n    get_meta = operator.attrgetter(\"meta\")\n\n    def compare_z(obj):\n        return get_z(get_meta(obj))\n\n    assert equal_attributes(a, b, [compare_z, \"x\"])\n    # fail z equality\n    a.meta[\"z\"] = 2\n    assert not equal_attributes(a, b, [compare_z, \"x\"])\n\n\ndef test_get_func_args():\n    def f1(a, b, c):\n        pass\n\n    def f2(a, b=None, c=None):\n        pass\n\n    def f3(a, b=None, *, c=None):\n        pass\n\n    class A:\n        def __init__(self, a, b, c):\n            pass\n\n        def method(self, a, b, c):\n            pass\n\n    class Callable:\n        def __call__(self, a, b, c):\n            pass\n\n    a = A(1, 2, 3)\n    cal = Callable()\n    partial_f1 = functools.partial(f1, None)\n    partial_f2 = functools.partial(f1, b=None)\n    partial_f3 = functools.partial(partial_f2, None)\n\n    assert get_func_args(f1) == [\"a\", \"b\", \"c\"]\n    assert get_func_args(f2) == [\"a\", \"b\", \"c\"]\n    assert get_func_args(f3) == [\"a\", \"b\", \"c\"]\n    assert get_func_args(A) == [\"a\", \"b\", \"c\"]\n    assert get_func_args(a.method) == [\"a\", \"b\", \"c\"]\n    assert get_func_args(partial_f1) == [\"b\", \"c\"]\n    assert get_func_args(partial_f2) == [\"a\", \"c\"]\n    assert get_func_args(partial_f3) == [\"c\"]\n    assert get_func_args(cal) == [\"a\", \"b\", \"c\"]\n    assert get_func_args(object) == []  # pylint: disable=use-implicit-booleaness-not-comparison\n    assert get_func_args(str.split, stripself=True) == [\"sep\", \"maxsplit\"]\n    assert get_func_args(\" \".join, stripself=True) == [\"iterable\"]\n\n    if sys.version_info >= (3, 13) or platform.python_implementation() == \"PyPy\":\n        # the correct and correctly extracted signature\n        assert get_func_args(operator.itemgetter(2), stripself=True) == [\"obj\"]\n    elif platform.python_implementation() == \"CPython\":\n        # [\"args\", \"kwargs\"] is a correct result for the pre-3.13 incorrect function signature\n        # [] is an incorrect result on even older CPython (https://github.com/python/cpython/issues/86951)\n        assert get_func_args(operator.itemgetter(2), stripself=True) in [\n            [],\n            [\"args\", \"kwargs\"],\n        ]\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    [\n        ([1, None, 3, 4], [1, 3, 4]),\n        ((1, None, 3, 4), (1, 3, 4)),\n        (\n            {\"one\": 1, \"none\": None, \"three\": 3, \"four\": 4},\n            {\"one\": 1, \"three\": 3, \"four\": 4},\n        ),\n    ],\n)\ndef test_without_none_values(\n    value: Mapping[_KT, _VT] | Iterable[_KT], expected: dict[_KT, _VT] | Iterable[_KT]\n) -> None:\n    assert without_none_values(value) == expected\n", "n_tokens": 976, "byte_len": 3310, "file_sha1": "0735447d5ef861784f5b50a82fe12e94b5a7c915", "start_line": 155, "end_line": 270}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_version.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_version.py", "rel_path": "tests/test_command_version.py", "module": "tests.test_command_version", "ext": "py", "chunk_number": 1, "symbols": ["test_output", "test_verbose_output", "TestVersionCommand", "partition", "test", "verbose", "version", "output", "splitlines", "lib", "w3lib", "python", "class", "libxml", "libxml2", "cryptography", "scrapy", "headers", "lxml", "project", "commands", "from", "twisted", "assert", "parsel", "cssselect", "strip", "open", "ssl", "line", "proc", "import", "platform", "self", "tests"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom tests.test_commands import TestProjectBase\n\n\nclass TestVersionCommand(TestProjectBase):\n    def test_output(self):\n        _, out, _ = self.proc(\"version\")\n        assert out.strip() == f\"Scrapy {scrapy.__version__}\"\n\n    def test_verbose_output(self):\n        _, out, _ = self.proc(\"version\", \"-v\")\n        headers = [line.partition(\":\")[0].strip() for line in out.strip().splitlines()]\n        assert headers == [\n            \"Scrapy\",\n            \"lxml\",\n            \"libxml2\",\n            \"cssselect\",\n            \"parsel\",\n            \"w3lib\",\n            \"Twisted\",\n            \"Python\",\n            \"pyOpenSSL\",\n            \"cryptography\",\n            \"Platform\",\n        ]\n", "n_tokens": 159, "byte_len": 700, "file_sha1": "892c491152416969c217294e2a8ea0edd5318869", "start_line": 1, "end_line": 26}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_shell.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_shell.py", "rel_path": "tests/test_command_shell.py", "module": "tests.test_command_shell", "ext": "py", "chunk_number": 1, "symbols": ["setup_class", "teardown_class", "test_empty", "test_response_body", "test_response_type_text", "test_response_type_html", "test_response_selector_html", "test_response_encoding_gb18030", "test_redirect", "test_redirect_follow_302", "test_redirect_not_follow_302", "test_fetch_redirect_follow_302", "test_fetch_redirect_not_follow_302", "test_request_replace", "test_scrapy_import", "test_local_file", "test_local_nofile", "test_dns_failures", "test_shell_fetch_async", "TestShellCommand", "TestInteractiveShell", "encoding", "method", "test", "scrapy", "gb18030", "xpath", "works", "automatic", "response", "test_fetch", "empty", "thread", "path", "mockserver", "existin", "resolvable", "enter", "failed", "pytest", "loop", "traceback", "redirecting", "none", "type", "html", "code", "twiste", "reactor", "event"], "ast_kind": "class_or_type", "text": "import os\nimport sys\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport pytest\nfrom pexpect.popen_spawn import PopenSpawn\n\nfrom scrapy.utils.reactor import _asyncio_reactor_path\nfrom tests import NON_EXISTING_RESOLVABLE, tests_datadir\nfrom tests.mockserver.http import MockServer\nfrom tests.test_commands import TestProjectBase\n\n\nclass TestShellCommand(TestProjectBase):\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def test_empty(self):\n        _, out, _ = self.proc(\"shell\", \"-c\", \"item\")\n        assert \"{}\" in out\n\n    def test_response_body(self):\n        _, out, _ = self.proc(\n            \"shell\", self.mockserver.url(\"/text\"), \"-c\", \"response.body\"\n        )\n        assert \"Works\" in out\n\n    def test_response_type_text(self):\n        _, out, _ = self.proc(\n            \"shell\", self.mockserver.url(\"/text\"), \"-c\", \"type(response)\"\n        )\n        assert \"TextResponse\" in out\n\n    def test_response_type_html(self):\n        _, out, _ = self.proc(\n            \"shell\", self.mockserver.url(\"/html\"), \"-c\", \"type(response)\"\n        )\n        assert \"HtmlResponse\" in out\n\n    def test_response_selector_html(self):\n        xpath = \"response.xpath(\\\"//p[@class='one']/text()\\\").get()\"\n        _, out, _ = self.proc(\"shell\", self.mockserver.url(\"/html\"), \"-c\", xpath)\n        assert out.strip() == \"Works\"\n\n    def test_response_encoding_gb18030(self):\n        _, out, _ = self.proc(\n            \"shell\", self.mockserver.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"\n        )\n        assert out.strip() == \"gb18030\"\n\n    def test_redirect(self):\n        _, out, _ = self.proc(\n            \"shell\", self.mockserver.url(\"/redirect\"), \"-c\", \"response.url\"\n        )\n        assert out.strip().endswith(\"/redirected\")\n\n    def test_redirect_follow_302(self):\n        _, out, _ = self.proc(\n            \"shell\",\n            self.mockserver.url(\"/redirect-no-meta-refresh\"),\n            \"-c\",\n            \"response.status\",\n        )\n        assert out.strip().endswith(\"200\")\n\n    def test_redirect_not_follow_302(self):\n        _, out, _ = self.proc(\n            \"shell\",\n            \"--no-redirect\",\n            self.mockserver.url(\"/redirect-no-meta-refresh\"),\n            \"-c\",\n            \"response.status\",\n        )\n        assert out.strip().endswith(\"302\")\n\n    def test_fetch_redirect_follow_302(self):\n        \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n        url = self.mockserver.url(\"/redirect-no-meta-refresh\")\n        code = f\"fetch('{url}')\"\n        p, out, errout = self.proc(\"shell\", \"-c\", code)\n        assert p.returncode == 0, out\n        assert \"Redirecting (302)\" in errout\n        assert \"Crawled (200)\" in errout\n\n    def test_fetch_redirect_not_follow_302(self):\n        \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n        url = self.mockserver.url(\"/redirect-no-meta-refresh\")\n        code = f\"fetch('{url}', redirect=False)\"\n        p, out, errout = self.proc(\"shell\", \"-c\", code)\n        assert p.returncode == 0, out\n        assert \"Crawled (302)\" in errout\n\n    def test_request_replace(self):\n        url = self.mockserver.url(\"/text\")\n        code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n        p, out, _ = self.proc(\"shell\", \"-c\", code)\n        assert p.returncode == 0, out\n\n    def test_scrapy_import(self):\n        url = self.mockserver.url(\"/text\")\n        code = f\"fetch(scrapy.Request('{url}'))\"\n        p, out, _ = self.proc(\"shell\", \"-c\", code)\n        assert p.returncode == 0, out\n\n    def test_local_file(self):\n        filepath = Path(tests_datadir, \"test_site\", \"index.html\")\n        _, out, _ = self.proc(\"shell\", str(filepath), \"-c\", \"item\")\n        assert \"{}\" in out\n\n    def test_local_nofile(self):\n        filepath = \"file:///tests/sample_data/test_site/nothinghere.html\"\n        p, out, err = self.proc(\"shell\", filepath, \"-c\", \"item\")\n        assert p.returncode == 1, out or err\n        assert \"No such file or directory\" in err\n\n    def test_dns_failures(self):\n        if NON_EXISTING_RESOLVABLE:\n            pytest.skip(\"Non-existing hosts are resolvable\")\n        url = \"www.somedomainthatdoesntexi.st\"\n        p, out, err = self.proc(\"shell\", url, \"-c\", \"item\")\n        assert p.returncode == 1, out or err\n        assert \"DNS lookup failed\" in err\n\n    def test_shell_fetch_async(self):\n        url = self.mockserver.url(\"/html\")\n        code = f\"fetch('{url}')\"\n        p, _, err = self.proc(\n            \"shell\", \"-c\", code, \"--set\", f\"TWISTED_REACTOR={_asyncio_reactor_path}\"\n        )\n        assert p.returncode == 0, err\n        assert \"RuntimeError: There is no current event loop in thread\" not in err\n\n\nclass TestInteractiveShell:", "n_tokens": 1204, "byte_len": 4890, "file_sha1": "012276008e0cd9f06491b86a75329b5a05f2cd2a", "start_line": 1, "end_line": 141}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_shell.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_shell.py", "rel_path": "tests/test_command_shell.py", "module": "tests.test_command_shell", "ext": "py", "chunk_number": 2, "symbols": ["test_fetch", "logfile", "seek", "python", "shell", "scrap", "pytho", "objects", "with", "read", "scrapy", "environ", "mockserver", "fetch", "sendeof", "decode", "expect", "exact", "assert", "copy", "traceback", "mock", "server", "sendline", "html", "response", "executable", "popen", "spawn", "cmdline", "setup_class", "teardown_class", "test_empty", "test_response_body", "test_response_type_text", "test_response_type_html", "test_response_selector_html", "test_response_encoding_gb18030", "test_redirect", "test_redirect_follow_302", "test_redirect_not_follow_302", "test_fetch_redirect_follow_302", "test_fetch_redirect_not_follow_302", "test_request_replace", "test_scrapy_import", "test_local_file", "test_local_nofile", "test_dns_failures", "test_shell_fetch_async", "TestShellCommand"], "ast_kind": "function_or_method", "text": "    def test_fetch(self):\n        args = (\n            sys.executable,\n            \"-m\",\n            \"scrapy.cmdline\",\n            \"shell\",\n        )\n        env = os.environ.copy()\n        env[\"SCRAPY_PYTHON_SHELL\"] = \"python\"\n        logfile = BytesIO()\n        p = PopenSpawn(args, env=env, timeout=5)\n        p.logfile_read = logfile\n        p.expect_exact(\"Available Scrapy objects\")\n        with MockServer() as mockserver:\n            p.sendline(f\"fetch('{mockserver.url('/')}')\")\n            p.sendline(\"type(response)\")\n            p.expect_exact(\"HtmlResponse\")\n        p.sendeof()\n        p.wait()\n        logfile.seek(0)\n        assert \"Traceback\" not in logfile.read().decode()\n", "n_tokens": 161, "byte_len": 691, "file_sha1": "012276008e0cd9f06491b86a75329b5a05f2cd2a", "start_line": 142, "end_line": 163}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py", "rel_path": "tests/test_pipeline_images.py", "module": "tests.test_pipeline_images", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "teardown_method", "test_file_path", "test_thumbnail_name", "test_thumbnail_name_from_item", "thumb_path", "TestImagesPipeline", "CustomImagesPipeline", "image", "exception", "dorma", "python", "small", "name", "future", "https", "allow", "module", "get", "prod", "crawler", "pytest", "thumb", "path", "test", "thumbnail", "items", "object", "item", "none", "test_get_images_exception", "test_get_images", "test_convert_image", "test_rejects_non_list_image_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline_subclass", "test_different_settings_for_different_instances", "test_subclass_attrs_preserved_default_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses", "test_cls_attrs_with_DEFAULT_prefix", "test_user_defined_subclass_default_key_names", "_create_image", "TestImagesPipelineFieldsMixin"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport dataclasses\nimport io\nimport random\nfrom abc import ABC, abstractmethod\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\nfrom typing import Any\n\nimport attr\nimport pytest\nfrom itemadapter import ItemAdapter\n\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.pipelines.images import ImageException, ImagesPipeline\nfrom scrapy.utils.test import get_crawler\n\ntry:\n    from PIL import Image\nexcept ImportError:\n    pytest.skip(\n        \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\",\n        allow_module_level=True,\n    )\nelse:\n    encoders = {\"jpeg_encoder\", \"jpeg_decoder\"}\n    if not encoders.issubset(set(Image.core.__dict__)):  # type: ignore[attr-defined]\n        pytest.skip(\"Missing JPEG encoders\", allow_module_level=True)\n\n\nclass TestImagesPipeline:\n    def setup_method(self):\n        self.tempdir = mkdtemp()\n        crawler = get_crawler()\n        self.pipeline = ImagesPipeline(self.tempdir, crawler=crawler)\n\n    def teardown_method(self):\n        rmtree(self.tempdir)\n\n    def test_file_path(self):\n        file_path = self.pipeline.file_path\n        assert (\n            file_path(Request(\"https://dev.mydeco.com/mydeco.gif\"))\n            == \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n        )\n        assert (\n            file_path(\n                Request(\n                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"\n                )\n            )\n            == \"full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg\"\n        )\n        assert (\n            file_path(\n                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")\n            )\n            == \"full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg\"\n        )\n        assert (\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n                )\n            )\n            == \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\"\n        )\n        assert (\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\"))\n            == \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg\"\n        )\n        assert (\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"))\n            == \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg\"\n        )\n        assert (\n            file_path(\n                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                info=object(),\n            )\n            == \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg\"\n        )\n\n    def test_thumbnail_name(self):\n        thumb_path = self.pipeline.thumb_path\n        name = \"50\"\n        assert (\n            thumb_path(Request(\"file:///tmp/foo.jpg\"), name)\n            == \"thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg\"\n        )\n        assert (\n            thumb_path(Request(\"file://foo.png\"), name)\n            == \"thumbs/50/e55b765eba0ec7348e50a1df496040449071b96a.jpg\"\n        )\n        assert (\n            thumb_path(Request(\"file:///tmp/foo\"), name)\n            == \"thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg\"\n        )\n        assert (\n            thumb_path(Request(\"file:///tmp/some.name/foo\"), name)\n            == \"thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg\"\n        )\n        assert (\n            thumb_path(\n                Request(\"file:///tmp/some.name/foo\"),\n                name,\n                response=Response(\"file:///tmp/some.name/foo\"),\n                info=object(),\n            )\n            == \"thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg\"\n        )\n\n    def test_thumbnail_name_from_item(self):\n        \"\"\"\n        Custom thumbnail name based on item data, overriding default implementation\n        \"\"\"\n\n        class CustomImagesPipeline(ImagesPipeline):\n            def thumb_path(\n                self, request, thumb_id, response=None, info=None, item=None\n            ):\n                return f\"thumb/{thumb_id}/{item.get('path')}\"\n\n        thumb_path = CustomImagesPipeline.from_crawler(\n            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n        ).thumb_path\n        item = {\"path\": \"path-to-store-file\"}\n        request = Request(\"http://example.com\")\n        assert (\n            thumb_path(request, \"small\", item=item) == \"thumb/small/path-to-store-file\"\n        )\n", "n_tokens": 1160, "byte_len": 4559, "file_sha1": "780384002e35448fac067582fb97ede8849a7b69", "start_line": 1, "end_line": 135}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py", "rel_path": "tests/test_pipeline_images.py", "module": "tests.test_pipeline_images", "ext": "py", "chunk_number": 2, "symbols": ["test_get_images_exception", "test_get_images", "test_convert_image", "test_rejects_non_list_image_urls", "item_class", "TestImagesPipelineFieldsMixin", "image", "exception", "get", "images", "case", "resp", "resp3", "min", "height", "ratio", "small", "convert", "buf", "buf1", "https", "crawler", "pytest", "thumb", "path", "rgba", "object", "none", "resp1", "http", "setup_method", "teardown_method", "test_file_path", "test_thumbnail_name", "test_thumbnail_name_from_item", "thumb_path", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline_subclass", "test_different_settings_for_different_instances", "test_subclass_attrs_preserved_default_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses", "test_cls_attrs_with_DEFAULT_prefix", "test_user_defined_subclass_default_key_names", "_create_image"], "ast_kind": "class_or_type", "text": "    def test_get_images_exception(self):\n        self.pipeline.min_width = 100\n        self.pipeline.min_height = 100\n\n        _, buf1 = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n        _, buf2 = _create_image(\"JPEG\", \"RGB\", (150, 50), (0, 0, 0))\n        _, buf3 = _create_image(\"JPEG\", \"RGB\", (50, 150), (0, 0, 0))\n\n        resp1 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf1.getvalue())\n        resp2 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf2.getvalue())\n        resp3 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf3.getvalue())\n        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n\n        with pytest.raises(ImageException):\n            next(self.pipeline.get_images(response=resp1, request=req, info=object()))\n        with pytest.raises(ImageException):\n            next(self.pipeline.get_images(response=resp2, request=req, info=object()))\n        with pytest.raises(ImageException):\n            next(self.pipeline.get_images(response=resp3, request=req, info=object()))\n\n    def test_get_images(self):\n        self.pipeline.min_width = 0\n        self.pipeline.min_height = 0\n        self.pipeline.thumbs = {\"small\": (20, 20)}\n\n        orig_im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n        orig_thumb, orig_thumb_buf = _create_image(\"JPEG\", \"RGB\", (20, 20), (0, 0, 0))\n        resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n\n        get_images_gen = self.pipeline.get_images(\n            response=resp, request=req, info=object()\n        )\n\n        path, new_im, new_buf = next(get_images_gen)\n        assert path == \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n        assert orig_im == new_im\n        assert buf.getvalue() == new_buf.getvalue()\n\n        thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n        assert thumb_path == \"thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n        assert orig_thumb_buf.getvalue() == thumb_buf.getvalue()\n\n    def test_convert_image(self):\n        SIZE = (100, 100)\n        # straight forward case: RGB and JPEG\n        COLOUR = (0, 127, 255)\n        im, buf = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n        converted, converted_buf = self.pipeline.convert_image(im, response_body=buf)\n        assert converted.mode == \"RGB\"\n        assert converted.getcolors() == [(10000, COLOUR)]\n        # check that we don't convert JPEGs again\n        assert converted_buf == buf\n\n        # check that thumbnail keep image ratio\n        thumbnail, _ = self.pipeline.convert_image(\n            converted, size=(10, 25), response_body=converted_buf\n        )\n        assert thumbnail.mode == \"RGB\"\n        assert thumbnail.size == (10, 10)\n\n        # transparency case: RGBA and PNG\n        COLOUR = (0, 127, 255, 50)\n        im, buf = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n        converted, _ = self.pipeline.convert_image(im, response_body=buf)\n        assert converted.mode == \"RGB\"\n        assert converted.getcolors() == [(10000, (205, 230, 255))]\n\n        # transparency case with palette: P and PNG\n        COLOUR = (0, 127, 255, 50)\n        im, buf = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n        im = im.convert(\"P\")\n        converted, _ = self.pipeline.convert_image(im, response_body=buf)\n        assert converted.mode == \"RGB\"\n        assert converted.getcolors() == [(10000, (205, 230, 255))]\n\n    @pytest.mark.parametrize(\n        \"bad_type\",\n        [\n            \"http://example.com/file.jpg\",\n            (\"http://example.com/file.jpg\",),\n            {\"url\": \"http://example.com/file.jpg\"},\n            123,\n            None,\n        ],\n    )\n    def test_rejects_non_list_image_urls(self, tmp_path, bad_type):\n        pipeline = ImagesPipeline.from_crawler(\n            get_crawler(None, {\"IMAGES_STORE\": str(tmp_path)})\n        )\n        item = ImagesPipelineTestItem()\n        item[\"image_urls\"] = bad_type\n\n        with pytest.raises(TypeError, match=\"image_urls must be a list of URLs\"):\n            list(pipeline.get_media_requests(item, None))\n\n\nclass TestImagesPipelineFieldsMixin(ABC):\n    @property\n    @abstractmethod\n    def item_class(self) -> Any:\n        raise NotImplementedError\n", "n_tokens": 1110, "byte_len": 4272, "file_sha1": "780384002e35448fac067582fb97ede8849a7b69", "start_line": 136, "end_line": 238}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py", "rel_path": "tests/test_pipeline_images.py", "module": "tests.test_pipeline_images", "ext": "py", "chunk_number": 3, "symbols": ["test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline_subclass", "TestImagesPipelineFieldsDict", "ImagesPipelineTestItem", "TestImagesPipelineFieldsItem", "ImagesPipelineTestDataClass", "TestImagesPipelineFieldsDataClass", "ImagesPipelineTestAttrsItem", "TestImagesPipelineFieldsAttrsItem", "TestImagesPipelineCustomSettings", "generate", "fake", "random", "string", "small", "name", "image", "expires", "get", "crawler", "thumbs", "settings", "isinstance", "items", "test", "images", "results", "setup_method", "teardown_method", "test_file_path", "test_thumbnail_name", "test_thumbnail_name_from_item", "thumb_path", "test_get_images_exception", "test_get_images", "test_convert_image", "test_rejects_non_list_image_urls", "item_class", "test_different_settings_for_different_instances", "test_subclass_attrs_preserved_default_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses", "test_cls_attrs_with_DEFAULT_prefix", "test_user_defined_subclass_default_key_names", "_create_image"], "ast_kind": "class_or_type", "text": "    def test_item_fields_default(self):\n        url = \"http://www.example.com/images/1.jpg\"\n        item = self.item_class(name=\"item1\", image_urls=[url])\n        pipeline = ImagesPipeline.from_crawler(\n            get_crawler(None, {\"IMAGES_STORE\": \"s3://example/images/\"})\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        assert requests[0].url == url\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        images = ItemAdapter(item).get(\"images\")\n        assert images == [results[0][1]]\n        assert isinstance(item, self.item_class)\n\n    def test_item_fields_override_settings(self):\n        url = \"http://www.example.com/images/1.jpg\"\n        item = self.item_class(name=\"item1\", custom_image_urls=[url])\n        pipeline = ImagesPipeline.from_crawler(\n            get_crawler(\n                None,\n                {\n                    \"IMAGES_STORE\": \"s3://example/images/\",\n                    \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n                    \"IMAGES_RESULT_FIELD\": \"custom_images\",\n                },\n            )\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        assert requests[0].url == url\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        custom_images = ItemAdapter(item).get(\"custom_images\")\n        assert custom_images == [results[0][1]]\n        assert isinstance(item, self.item_class)\n\n\nclass TestImagesPipelineFieldsDict(TestImagesPipelineFieldsMixin):\n    item_class = dict\n\n\nclass ImagesPipelineTestItem(Item):\n    name = Field()\n    # default fields\n    image_urls = Field()\n    images = Field()\n    # overridden fields\n    custom_image_urls = Field()\n    custom_images = Field()\n\n\nclass TestImagesPipelineFieldsItem(TestImagesPipelineFieldsMixin):\n    item_class = ImagesPipelineTestItem\n\n\n@dataclasses.dataclass\nclass ImagesPipelineTestDataClass:\n    name: str\n    # default fields\n    image_urls: list = dataclasses.field(default_factory=list)\n    images: list = dataclasses.field(default_factory=list)\n    # overridden fields\n    custom_image_urls: list = dataclasses.field(default_factory=list)\n    custom_images: list = dataclasses.field(default_factory=list)\n\n\nclass TestImagesPipelineFieldsDataClass(TestImagesPipelineFieldsMixin):\n    item_class = ImagesPipelineTestDataClass\n\n\n@attr.s\nclass ImagesPipelineTestAttrsItem:\n    name = attr.ib(default=\"\")\n    # default fields\n    image_urls: list[str] = attr.ib(default=list)\n    images: list[dict[str, str]] = attr.ib(default=list)\n    # overridden fields\n    custom_image_urls: list[str] = attr.ib(default=list)\n    custom_images: list[dict[str, str]] = attr.ib(default=list)\n\n\nclass TestImagesPipelineFieldsAttrsItem(TestImagesPipelineFieldsMixin):\n    item_class = ImagesPipelineTestAttrsItem\n\n\nclass TestImagesPipelineCustomSettings:\n    img_cls_attribute_names = [\n        # Pipeline attribute names with corresponding setting names.\n        (\"EXPIRES\", \"IMAGES_EXPIRES\"),\n        (\"MIN_WIDTH\", \"IMAGES_MIN_WIDTH\"),\n        (\"MIN_HEIGHT\", \"IMAGES_MIN_HEIGHT\"),\n        (\"IMAGES_URLS_FIELD\", \"IMAGES_URLS_FIELD\"),\n        (\"IMAGES_RESULT_FIELD\", \"IMAGES_RESULT_FIELD\"),\n        (\"THUMBS\", \"IMAGES_THUMBS\"),\n    ]\n\n    # This should match what is defined in ImagesPipeline.\n    default_pipeline_settings = {\n        \"MIN_WIDTH\": 0,\n        \"MIN_HEIGHT\": 0,\n        \"EXPIRES\": 90,\n        \"THUMBS\": {},\n        \"IMAGES_URLS_FIELD\": \"image_urls\",\n        \"IMAGES_RESULT_FIELD\": \"images\",\n    }\n\n    def _generate_fake_settings(self, tmp_path, prefix=None):\n        \"\"\"\n        :param prefix: string for setting keys\n        :return: dictionary of image pipeline settings\n        \"\"\"\n\n        def random_string():\n            return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n\n        settings = {\n            \"IMAGES_EXPIRES\": random.randint(100, 1000),\n            \"IMAGES_STORE\": tmp_path,\n            \"IMAGES_RESULT_FIELD\": random_string(),\n            \"IMAGES_URLS_FIELD\": random_string(),\n            \"IMAGES_MIN_WIDTH\": random.randint(1, 1000),\n            \"IMAGES_MIN_HEIGHT\": random.randint(1, 1000),\n            \"IMAGES_THUMBS\": {\n                \"small\": (random.randint(1, 1000), random.randint(1, 1000)),\n                \"big\": (random.randint(1, 1000), random.randint(1, 1000)),\n            },\n        }\n        if not prefix:\n            return settings\n\n        return {\n            prefix.upper() + \"_\" + k if k != \"IMAGES_STORE\" else k: v\n            for k, v in settings.items()\n        }\n\n    def _generate_fake_pipeline_subclass(self):\n        \"\"\"\n        :return: ImagePipeline class will all uppercase attributes set.\n        \"\"\"\n", "n_tokens": 1077, "byte_len": 4748, "file_sha1": "780384002e35448fac067582fb97ede8849a7b69", "start_line": 239, "end_line": 377}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py", "rel_path": "tests/test_pipeline_images.py", "module": "tests.test_pipeline_images", "ext": "py", "chunk_number": 4, "symbols": ["test_different_settings_for_different_instances", "test_subclass_attrs_preserved_default_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses", "test_cls_attrs_with_DEFAULT_prefix", "UserDefinedImagePipeline", "takes", "test", "custom", "generate", "fake", "field", "two", "one", "instance", "subclass", "small", "cls", "defaul", "image", "expires", "get", "crawler", "settings", "lower", "images", "result", "than", "setup_method", "teardown_method", "test_file_path", "test_thumbnail_name", "test_thumbnail_name_from_item", "thumb_path", "test_get_images_exception", "test_get_images", "test_convert_image", "test_rejects_non_list_image_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline_subclass", "test_user_defined_subclass_default_key_names", "_create_image", "TestImagesPipeline", "CustomImagesPipeline"], "ast_kind": "class_or_type", "text": "        class UserDefinedImagePipeline(ImagesPipeline):\n            # Values should be in different range than fake_settings.\n            MIN_WIDTH = random.randint(1000, 2000)\n            MIN_HEIGHT = random.randint(1000, 2000)\n            THUMBS = {\n                \"small\": (random.randint(1000, 2000), random.randint(1000, 2000)),\n                \"big\": (random.randint(1000, 2000), random.randint(1000, 2000)),\n            }\n            EXPIRES = random.randint(1000, 2000)\n            IMAGES_URLS_FIELD = \"field_one\"\n            IMAGES_RESULT_FIELD = \"field_two\"\n\n        return UserDefinedImagePipeline\n\n    def test_different_settings_for_different_instances(self, tmp_path):\n        \"\"\"\n        If there are two instances of ImagesPipeline class with different settings, they should\n        have different settings.\n        \"\"\"\n        custom_settings = self._generate_fake_settings(tmp_path)\n        default_sts_pipe = ImagesPipeline(tmp_path, crawler=get_crawler(None))\n        user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n            custom_value = custom_settings.get(settings_attr)\n            assert expected_default_value != custom_value\n            assert (\n                getattr(default_sts_pipe, pipe_attr.lower()) == expected_default_value\n            )\n            assert getattr(user_sts_pipe, pipe_attr.lower()) == custom_value\n\n    def test_subclass_attrs_preserved_default_settings(self, tmp_path):\n        \"\"\"\n        If image settings are not defined at all subclass of ImagePipeline takes values\n        from class attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline_subclass()\n        pipeline = pipeline_cls.from_crawler(\n            get_crawler(None, {\"IMAGES_STORE\": tmp_path})\n        )\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n            attr_value = getattr(pipeline, pipe_attr.lower())\n            assert attr_value != self.default_pipeline_settings[pipe_attr]\n            assert attr_value == getattr(pipeline, pipe_attr)\n\n    def test_subclass_attrs_preserved_custom_settings(self, tmp_path):\n        \"\"\"\n        If image settings are defined but they are not defined for subclass default\n        values taken from settings should be preserved.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline_subclass()\n        settings = self._generate_fake_settings(tmp_path)\n        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Instance attribute (lowercase) must be equal to\n            # value defined in settings.\n            value = getattr(pipeline, pipe_attr.lower())\n            assert value != self.default_pipeline_settings[pipe_attr]\n            setings_value = settings.get(settings_attr)\n            assert value == setings_value\n\n    def test_no_custom_settings_for_subclasses(self, tmp_path):\n        \"\"\"\n        If there are no settings for subclass and no subclass attributes, pipeline should use\n        attributes of base class.\n        \"\"\"\n\n        class UserDefinedImagePipeline(ImagesPipeline):\n            pass\n\n        user_pipeline = UserDefinedImagePipeline.from_crawler(\n            get_crawler(None, {\"IMAGES_STORE\": tmp_path})\n        )\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = self.default_pipeline_settings.get(pipe_attr.upper())\n            assert getattr(user_pipeline, pipe_attr.lower()) == custom_value\n\n    def test_custom_settings_for_subclasses(self, tmp_path):\n        \"\"\"\n        If there are custom settings for subclass and NO class attributes, pipeline should use custom\n        settings.\n        \"\"\"\n\n        class UserDefinedImagePipeline(ImagesPipeline):\n            pass\n\n        prefix = UserDefinedImagePipeline.__name__.upper()\n        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n        user_pipeline = UserDefinedImagePipeline.from_crawler(\n            get_crawler(None, settings)\n        )\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            assert custom_value != self.default_pipeline_settings[pipe_attr]\n            assert getattr(user_pipeline, pipe_attr.lower()) == custom_value\n\n    def test_custom_settings_and_class_attrs_for_subclasses(self, tmp_path):\n        \"\"\"\n        If there are custom settings for subclass AND class attributes\n        setting keys are preferred and override attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline_subclass()\n        prefix = pipeline_cls.__name__.upper()\n        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            assert custom_value != self.default_pipeline_settings[pipe_attr]\n            assert getattr(user_pipeline, pipe_attr.lower()) == custom_value\n\n    def test_cls_attrs_with_DEFAULT_prefix(self, tmp_path):\n        class UserDefinedImagePipeline(ImagesPipeline):\n            DEFAULT_IMAGES_URLS_FIELD = \"something\"\n            DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n\n        pipeline = UserDefinedImagePipeline.from_crawler(\n            get_crawler(None, {\"IMAGES_STORE\": tmp_path})\n        )\n        assert (\n            pipeline.images_result_field\n            == UserDefinedImagePipeline.DEFAULT_IMAGES_RESULT_FIELD\n        )\n        assert (\n            pipeline.images_urls_field\n            == UserDefinedImagePipeline.DEFAULT_IMAGES_URLS_FIELD\n        )\n", "n_tokens": 1190, "byte_len": 6188, "file_sha1": "780384002e35448fac067582fb97ede8849a7b69", "start_line": 378, "end_line": 507}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_images.py", "rel_path": "tests/test_pipeline_images.py", "module": "tests.test_pipeline_images", "ext": "py", "chunk_number": 5, "symbols": ["test_user_defined_subclass_default_key_names", "_create_image", "UserPipe", "test", "user", "generate", "fake", "pipeline", "pipe", "attr", "seek", "prefixing", "pass", "image", "subclass", "return", "name", "class", "expected", "value", "with", "names", "images", "tmp", "path", "save", "get", "crawler", "cls", "defines", "setup_method", "teardown_method", "test_file_path", "test_thumbnail_name", "test_thumbnail_name_from_item", "thumb_path", "test_get_images_exception", "test_get_images", "test_convert_image", "test_rejects_non_list_image_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline_subclass", "test_different_settings_for_different_instances", "test_subclass_attrs_preserved_default_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses"], "ast_kind": "class_or_type", "text": "    def test_user_defined_subclass_default_key_names(self, tmp_path):\n        \"\"\"Test situation when user defines subclass of ImagePipeline,\n        but uses attribute names for default pipeline (without prefixing\n        them with pipeline class name).\n        \"\"\"\n        settings = self._generate_fake_settings(tmp_path)\n\n        class UserPipe(ImagesPipeline):\n            pass\n\n        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            expected_value = settings.get(settings_attr)\n            assert getattr(pipeline_cls, pipe_attr.lower()) == expected_value\n\n\ndef _create_image(format_, *a, **kw):\n    buf = io.BytesIO()\n    Image.new(*a, **kw).save(buf, format_)\n    buf.seek(0)\n    return Image.open(buf), buf\n", "n_tokens": 172, "byte_len": 817, "file_sha1": "780384002e35448fac067582fb97ede8849a7b69", "start_line": 508, "end_line": 530}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloaderslotssettings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloaderslotssettings.py", "rel_path": "tests/test_downloaderslotssettings.py", "module": "tests.test_downloaderslotssettings", "ext": "py", "chunk_number": 1, "symbols": ["parse", "not_parse", "setup_class", "teardown_class", "setup_method", "test_delay", "test_params", "DownloaderSlotsSettingsTestSpider", "TestCrawl", "books", "async", "error", "delta", "append", "name", "spiders", "https", "mockserver", "get", "crawler", "enter", "delay", "delays", "real", "settings", "items", "randomize", "none", "stop", "http", "slot", "values", "download", "runner", "prevent", "expected", "internet", "teardown", "class", "spider", "downloader", "slots", "meta", "toscrape", "classmethod", "example", "create", "list", "engine", "params"], "ast_kind": "class_or_type", "text": "import time\n\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy import Request\nfrom scrapy.core.downloader import Downloader, Slot\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import MetaSpider\n\n\nclass DownloaderSlotsSettingsTestSpider(MetaSpider):\n    name = \"downloader_slots\"\n\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 1,\n        \"RANDOMIZE_DOWNLOAD_DELAY\": False,\n        \"DOWNLOAD_SLOTS\": {\n            \"quotes.toscrape.com\": {\n                \"concurrency\": 1,\n                \"delay\": 2,\n                \"randomize_delay\": False,\n                \"throttle\": False,\n            },\n            \"books.toscrape.com\": {\"delay\": 3, \"randomize_delay\": False},\n        },\n    }\n\n    async def start(self):\n        self.times = {None: []}\n\n        slots = [*self.custom_settings.get(\"DOWNLOAD_SLOTS\", {}), None]\n\n        for slot in slots:\n            url = self.mockserver.url(f\"/?downloader_slot={slot}\")\n            self.times[slot] = []\n            yield Request(url, callback=self.parse, meta={\"download_slot\": slot})\n\n    def parse(self, response):\n        slot = response.meta.get(\"download_slot\", None)\n        self.times[slot].append(time.time())\n        url = self.mockserver.url(f\"/?downloader_slot={slot}&req=2\")\n        yield Request(url, callback=self.not_parse, meta={\"download_slot\": slot})\n\n    def not_parse(self, response):\n        slot = response.meta.get(\"download_slot\", None)\n        self.times[slot].append(time.time())\n\n\nclass TestCrawl:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        self.runner = CrawlerRunner()\n\n    @inlineCallbacks\n    def test_delay(self):\n        crawler = get_crawler(DownloaderSlotsSettingsTestSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        slots = crawler.engine.downloader.slots\n        times = crawler.spider.times\n        tolerance = 0.3\n\n        delays_real = {k: v[1] - v[0] for k, v in times.items()}\n        error_delta = {\n            k: 1 - min(delays_real[k], v.delay) / max(delays_real[k], v.delay)\n            for k, v in slots.items()\n        }\n\n        assert max(list(error_delta.values())) < tolerance\n\n\ndef test_params():\n    params = {\n        \"concurrency\": 1,\n        \"delay\": 2,\n        \"randomize_delay\": False,\n    }\n    settings = {\n        \"DOWNLOAD_SLOTS\": {\n            \"example.com\": params,\n        },\n    }\n    crawler = get_crawler(DefaultSpider, settings_dict=settings)\n    crawler.spider = crawler._create_spider()\n    downloader = Downloader(crawler)\n    downloader._slot_gc_loop.stop()  # Prevent an unclean reactor.\n    request = Request(\"https://example.com\")\n    _, actual = downloader._get_slot(request)\n    expected = Slot(**params)\n    for param in params:\n        assert getattr(expected, param) == getattr(actual, param), (\n            f\"Slot.{param}: {getattr(expected, param)!r} != {getattr(actual, param)!r}\"\n        )\n", "n_tokens": 736, "byte_len": 3213, "file_sha1": "780bd480dac41d6234935a2a92bfa69a58c0ce81", "start_line": 1, "end_line": 104}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_crawl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_crawl.py", "rel_path": "tests/test_pipeline_crawl.py", "module": "tests.test_pipeline_crawl", "ext": "py", "chunk_number": 1, "symbols": ["_process_url", "parse", "setup_class", "teardown_class", "setup_method", "teardown_method", "_on_item_scraped", "_create_crawler", "_assert_files_downloaded", "_assert_files_download_failure", "test_download_media", "MediaDownloadSpider", "BrokenLinksMediaDownloadSpider", "RedirectedMediaDownloadSpider", "TestFileDownloadCrawl", "does", "tmpmediastore", "logs", "xpath", "append", "table", "test", "file", "were", "failure", "lib", "w3lib", "media", "download", "spider", "test_download_media_wrong_urls", "test_download_media_redirected_default_failure", "test_download_media_redirected_allowed", "test_download_media_file_path_error", "file_path", "ExceptionRaisingMediaPipeline", "TestImageDownloadCrawl", "bool", "image", "python", "name", "responses", "iterdir", "error", "spiders", "future", "typ", "checking", "know", "https"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport shutil\nfrom pathlib import Path\nfrom tempfile import mkdtemp\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\nfrom w3lib.url import add_or_replace_parameter\n\nfrom scrapy import Spider, signals\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import SimpleSpider\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nclass MediaDownloadSpider(SimpleSpider):\n    name = \"mediadownload\"\n\n    def _process_url(self, url):\n        return url\n\n    def parse(self, response):\n        self.logger.info(response.headers)\n        self.logger.info(response.text)\n        item = {\n            self.media_key: [],\n            self.media_urls_key: [\n                self._process_url(response.urljoin(href))\n                for href in response.xpath(\n                    '//table[thead/tr/th=\"Filename\"]/tbody//a/@href'\n                ).getall()\n            ],\n        }\n        yield item\n\n\nclass BrokenLinksMediaDownloadSpider(MediaDownloadSpider):\n    name = \"brokenmedia\"\n\n    def _process_url(self, url):\n        return url + \".foo\"\n\n\nclass RedirectedMediaDownloadSpider(MediaDownloadSpider):\n    name = \"redirectedmedia\"\n\n    def _process_url(self, url):\n        return add_or_replace_parameter(\n            self.mockserver.url(\"/redirect-to\"), \"goto\", url\n        )\n\n\nclass TestFileDownloadCrawl:\n    pipeline_class = \"scrapy.pipelines.files.FilesPipeline\"\n    store_setting_key = \"FILES_STORE\"\n    media_key = \"files\"\n    media_urls_key = \"file_urls\"\n    expected_checksums: set[str] | None = {\n        \"5547178b89448faf0015a13f904c936e\",\n        \"c2281c83670e31d8aaab7cb642b824db\",\n        \"ed3f6538dc15d4d9179dae57319edc5f\",\n    }\n\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        # prepare a directory for storing files\n        self.tmpmediastore = Path(mkdtemp())\n        self.settings = {\n            \"ITEM_PIPELINES\": {self.pipeline_class: 1},\n            self.store_setting_key: str(self.tmpmediastore),\n        }\n        self.items = []\n\n    def teardown_method(self):\n        shutil.rmtree(self.tmpmediastore)\n        self.items = []\n\n    def _on_item_scraped(self, item):\n        self.items.append(item)\n\n    def _create_crawler(\n        self, spider_class: type[Spider], settings: dict[str, Any] | None = None\n    ) -> Crawler:\n        if settings is None:\n            settings = self.settings\n        crawler = get_crawler(spider_class, settings)\n        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n        return crawler\n\n    def _assert_files_downloaded(self, items, logs):\n        assert len(items) == 1\n        assert self.media_key in items[0]\n\n        # check that logs show the expected number of successful file downloads\n        file_dl_success = \"File (downloaded): Downloaded file from\"\n        assert logs.count(file_dl_success) == 3\n\n        # check that the images/files status is `downloaded`\n        for item in items:\n            for i in item[self.media_key]:\n                assert i[\"status\"] == \"downloaded\"\n\n        # check that the images/files checksums are what we know they should be\n        if self.expected_checksums is not None:\n            checksums = {i[\"checksum\"] for item in items for i in item[self.media_key]}\n            assert checksums == self.expected_checksums\n\n        # check that the image files where actually written to the media store\n        for item in items:\n            for i in item[self.media_key]:\n                assert (self.tmpmediastore / i[\"path\"]).exists()\n\n    def _assert_files_download_failure(self, crawler, items, code, logs):\n        # check that the item does NOT have the \"images/files\" field populated\n        assert len(items) == 1\n        assert self.media_key in items[0]\n        assert not items[0][self.media_key]\n\n        # check that there was 1 successful fetch and 3 other responses with non-200 code\n        assert crawler.stats.get_value(\"downloader/request_method_count/GET\") == 4\n        assert crawler.stats.get_value(\"downloader/response_count\") == 4\n        assert crawler.stats.get_value(\"downloader/response_status_count/200\") == 1\n        assert crawler.stats.get_value(f\"downloader/response_status_count/{code}\") == 3\n\n        # check that logs do show the failure on the file downloads\n        file_dl_failure = f\"File (code: {code}): Error downloading file from\"\n        assert logs.count(file_dl_failure) == 3\n\n        # check that no files were written to the media store\n        assert not list(self.tmpmediastore.iterdir())\n\n    @inlineCallbacks\n    def test_download_media(self):\n        crawler = self._create_crawler(MediaDownloadSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/static/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n            )\n        self._assert_files_downloaded(self.items, str(log))\n", "n_tokens": 1191, "byte_len": 5306, "file_sha1": "04fa692887cb57a54492cd56b8a4311a2224285c", "start_line": 1, "end_line": 157}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_crawl.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_crawl.py", "rel_path": "tests/test_pipeline_crawl.py", "module": "tests.test_pipeline_crawl", "ext": "py", "chunk_number": 2, "symbols": ["test_download_media_wrong_urls", "test_download_media_redirected_default_failure", "test_download_media_redirected_allowed", "test_download_media_file_path_error", "file_path", "ExceptionRaisingMediaPipeline", "TestImageDownloadCrawl", "bool", "image", "test", "file", "python", "media", "download", "https", "mockserver", "pytest", "settings", "items", "none", "reason", "get", "value", "pypi", "medi", "allo", "return", "path", "item", "class", "_process_url", "parse", "setup_class", "teardown_class", "setup_method", "teardown_method", "_on_item_scraped", "_create_crawler", "_assert_files_downloaded", "_assert_files_download_failure", "test_download_media", "MediaDownloadSpider", "BrokenLinksMediaDownloadSpider", "RedirectedMediaDownloadSpider", "TestFileDownloadCrawl", "does", "tmpmediastore", "logs", "xpath", "append"], "ast_kind": "class_or_type", "text": "    @inlineCallbacks\n    def test_download_media_wrong_urls(self):\n        crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/static/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n            )\n        self._assert_files_download_failure(crawler, self.items, 404, str(log))\n\n    @inlineCallbacks\n    def test_download_media_redirected_default_failure(self):\n        crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/static/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n                mockserver=self.mockserver,\n            )\n        self._assert_files_download_failure(crawler, self.items, 302, str(log))\n\n    @inlineCallbacks\n    def test_download_media_redirected_allowed(self):\n        settings = {\n            **self.settings,\n            \"MEDIA_ALLOW_REDIRECTS\": True,\n        }\n        crawler = self._create_crawler(RedirectedMediaDownloadSpider, settings)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/static/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n                mockserver=self.mockserver,\n            )\n        self._assert_files_downloaded(self.items, str(log))\n        assert crawler.stats.get_value(\"downloader/response_status_count/302\") == 3\n\n    @inlineCallbacks\n    def test_download_media_file_path_error(self):\n        cls = load_object(self.pipeline_class)\n\n        class ExceptionRaisingMediaPipeline(cls):\n            def file_path(self, request, response=None, info=None, *, item=None):\n                return 1 / 0\n\n        settings = {\n            **self.settings,\n            \"ITEM_PIPELINES\": {ExceptionRaisingMediaPipeline: 1},\n        }\n        crawler = self._create_crawler(MediaDownloadSpider, settings)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/static/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n                mockserver=self.mockserver,\n            )\n        assert \"ZeroDivisionError\" in str(log)\n\n\npillow_available: bool\ntry:\n    from PIL import Image  # noqa: F401\nexcept ImportError:\n    pillow_available = False\nelse:\n    pillow_available = True\n\n\n@pytest.mark.skipif(\n    not pillow_available,\n    reason=\"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\",\n)\nclass TestImageDownloadCrawl(TestFileDownloadCrawl):\n    pipeline_class = \"scrapy.pipelines.images.ImagesPipeline\"\n    store_setting_key = \"IMAGES_STORE\"\n    media_key = \"images\"\n    media_urls_key = \"image_urls\"\n\n    # somehow checksums for images are different for Python 3.3\n    expected_checksums = None\n", "n_tokens": 626, "byte_len": 3056, "file_sha1": "04fa692887cb57a54492cd56b8a4311a2224285c", "start_line": 158, "end_line": 242}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_headers.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_headers.py", "rel_path": "tests/test_http_headers.py", "module": "tests.test_http_headers", "ext": "py", "chunk_number": 1, "symbols": ["assertSortedEqual", "test_basics", "test_single_value", "test_multivalue", "test_multivalue_for_one_header", "test_encode_utf8", "test_encode_latin1", "test_encode_multiple", "test_delete_and_contains", "test_setdefault", "test_iterables", "test_update", "test_copy", "TestHeaders", "encoding", "text", "test", "multivalue", "encode", "images", "length", "setdefault", "second", "hlist", "copy", "update", "latin", "latin1", "jpeg", "type", "test_appendlist", "test_setlist", "test_setlistdefault", "test_none_value", "test_int_value", "test_invalid_value", "appendlist", "setlist", "unsupported", "invalid", "dict", "headers", "iterables", "class", "getlist", "with", "scrapy", "header", "header2", "olist"], "ast_kind": "class_or_type", "text": "import copy\n\nimport pytest\n\nfrom scrapy.http import Headers\n\n\nclass TestHeaders:\n    def assertSortedEqual(self, first, second, msg=None):\n        assert sorted(first) == sorted(second), msg\n\n    def test_basics(self):\n        h = Headers({\"Content-Type\": \"text/html\", \"Content-Length\": 1234})\n        assert h[\"Content-Type\"]\n        assert h[\"Content-Length\"]\n\n        with pytest.raises(KeyError):\n            h[\"Accept\"]\n        assert h.get(\"Accept\") is None\n        assert h.getlist(\"Accept\") == []\n\n        assert h.get(\"Accept\", \"*/*\") == b\"*/*\"\n        assert h.getlist(\"Accept\", \"*/*\") == [b\"*/*\"]\n        assert h.getlist(\"Accept\", [\"text/html\", \"images/jpeg\"]) == [\n            b\"text/html\",\n            b\"images/jpeg\",\n        ]\n\n    def test_single_value(self):\n        h = Headers()\n        h[\"Content-Type\"] = \"text/html\"\n        assert h[\"Content-Type\"] == b\"text/html\"\n        assert h.get(\"Content-Type\") == b\"text/html\"\n        assert h.getlist(\"Content-Type\") == [b\"text/html\"]\n\n    def test_multivalue(self):\n        h = Headers()\n        h[\"X-Forwarded-For\"] = hlist = [\"ip1\", \"ip2\"]\n        assert h[\"X-Forwarded-For\"] == b\"ip2\"\n        assert h.get(\"X-Forwarded-For\") == b\"ip2\"\n        assert h.getlist(\"X-Forwarded-For\") == [b\"ip1\", b\"ip2\"]\n        assert h.getlist(\"X-Forwarded-For\") is not hlist\n\n    def test_multivalue_for_one_header(self):\n        h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n        assert h[\"a\"] == b\"c\"\n        assert h.get(\"a\") == b\"c\"\n        assert h.getlist(\"a\") == [b\"b\", b\"c\"]\n\n    def test_encode_utf8(self):\n        h = Headers({\"key\": \"\\xa3\"}, encoding=\"utf-8\")\n        key, val = dict(h).popitem()\n        assert isinstance(key, bytes), key\n        assert isinstance(val[0], bytes), val[0]\n        assert val[0] == b\"\\xc2\\xa3\"\n\n    def test_encode_latin1(self):\n        h = Headers({\"key\": \"\\xa3\"}, encoding=\"latin1\")\n        key, val = dict(h).popitem()\n        assert val[0] == b\"\\xa3\"\n\n    def test_encode_multiple(self):\n        h = Headers({\"key\": [\"\\xa3\"]}, encoding=\"utf-8\")\n        key, val = dict(h).popitem()\n        assert val[0] == b\"\\xc2\\xa3\"\n\n    def test_delete_and_contains(self):\n        h = Headers()\n        h[\"Content-Type\"] = \"text/html\"\n        assert \"Content-Type\" in h\n        del h[\"Content-Type\"]\n        assert \"Content-Type\" not in h\n\n    def test_setdefault(self):\n        h = Headers()\n        hlist = [\"ip1\", \"ip2\"]\n        olist = h.setdefault(\"X-Forwarded-For\", hlist)\n        assert h.getlist(\"X-Forwarded-For\") is not hlist\n        assert h.getlist(\"X-Forwarded-For\") is olist\n\n        h = Headers()\n        olist = h.setdefault(\"X-Forwarded-For\", \"ip1\")\n        assert h.getlist(\"X-Forwarded-For\") == [b\"ip1\"]\n        assert h.getlist(\"X-Forwarded-For\") is olist\n\n    def test_iterables(self):\n        idict = {\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]}\n\n        h = Headers(idict)\n        assert dict(h) == {\n            b\"Content-Type\": [b\"text/html\"],\n            b\"X-Forwarded-For\": [b\"ip1\", b\"ip2\"],\n        }\n        self.assertSortedEqual(h.keys(), [b\"X-Forwarded-For\", b\"Content-Type\"])\n        self.assertSortedEqual(\n            h.items(),\n            [(b\"X-Forwarded-For\", [b\"ip1\", b\"ip2\"]), (b\"Content-Type\", [b\"text/html\"])],\n        )\n        self.assertSortedEqual(h.values(), [b\"ip2\", b\"text/html\"])\n\n    def test_update(self):\n        h = Headers()\n        h.update({\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]})\n        assert h.getlist(\"Content-Type\") == [b\"text/html\"]\n        assert h.getlist(\"X-Forwarded-For\") == [b\"ip1\", b\"ip2\"]\n\n    def test_copy(self):\n        h1 = Headers({\"header1\": [\"value1\", \"value2\"]})\n        h2 = copy.copy(h1)\n        assert h1 == h2\n        assert h1.getlist(\"header1\") == h2.getlist(\"header1\")\n        assert h1.getlist(\"header1\") is not h2.getlist(\"header1\")\n        assert isinstance(h2, Headers)\n", "n_tokens": 1114, "byte_len": 3884, "file_sha1": "31e740edbcbd8643c2ef74a1be2769d26f4cbe46", "start_line": 1, "end_line": 114}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_headers.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_http_headers.py", "rel_path": "tests/test_http_headers.py", "module": "tests.test_http_headers", "ext": "py", "chunk_number": 2, "symbols": ["test_appendlist", "test_setlist", "test_setlistdefault", "test_none_value", "test_int_value", "test_invalid_value", "appendlist", "setlist", "setdefault", "unsupported", "test", "invalid", "getlist", "with", "header", "header2", "headers", "value", "type", "error", "setlistdefault", "int", "value1", "pytest", "assert", "raises", "object", "header1", "none", "value3", "assertSortedEqual", "test_basics", "test_single_value", "test_multivalue", "test_multivalue_for_one_header", "test_encode_utf8", "test_encode_latin1", "test_encode_multiple", "test_delete_and_contains", "test_setdefault", "test_iterables", "test_update", "test_copy", "TestHeaders", "encoding", "text", "multivalue", "encode", "images", "length"], "ast_kind": "function_or_method", "text": "    def test_appendlist(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        h1.appendlist(\"header1\", \"value3\")\n        assert h1.getlist(\"header1\") == [b\"value1\", b\"value3\"]\n\n        h1 = Headers()\n        h1.appendlist(\"header1\", \"value1\")\n        h1.appendlist(\"header1\", \"value3\")\n        assert h1.getlist(\"header1\") == [b\"value1\", b\"value3\"]\n\n    def test_setlist(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        assert h1.getlist(\"header1\") == [b\"value1\"]\n        h1.setlist(\"header1\", [b\"value2\", b\"value3\"])\n        assert h1.getlist(\"header1\") == [b\"value2\", b\"value3\"]\n\n    def test_setlistdefault(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        h1.setlistdefault(\"header1\", [\"value2\", \"value3\"])\n        h1.setlistdefault(\"header2\", [\"value2\", \"value3\"])\n        assert h1.getlist(\"header1\") == [b\"value1\"]\n        assert h1.getlist(\"header2\") == [b\"value2\", b\"value3\"]\n\n    def test_none_value(self):\n        h1 = Headers()\n        h1[\"foo\"] = \"bar\"\n        h1[\"foo\"] = None\n        h1.setdefault(\"foo\", \"bar\")\n        assert h1.get(\"foo\") is None\n        assert h1.getlist(\"foo\") == []\n\n    def test_int_value(self):\n        h1 = Headers({\"hey\": 5})\n        h1[\"foo\"] = 1\n        h1.setdefault(\"bar\", 2)\n        h1.setlist(\"buz\", [1, \"dos\", 3])\n        assert h1.getlist(\"foo\") == [b\"1\"]\n        assert h1.getlist(\"bar\") == [b\"2\"]\n        assert h1.getlist(\"buz\") == [b\"1\", b\"dos\", b\"3\"]\n        assert h1.getlist(\"hey\") == [b\"5\"]\n\n    def test_invalid_value(self):\n        with pytest.raises(TypeError, match=\"Unsupported value type\"):\n            Headers({\"foo\": object()})\n        with pytest.raises(TypeError, match=\"Unsupported value type\"):\n            Headers()[\"foo\"] = object()\n        with pytest.raises(TypeError, match=\"Unsupported value type\"):\n            Headers().setdefault(\"foo\", object())\n        with pytest.raises(TypeError, match=\"Unsupported value type\"):\n            Headers().setlist(\"foo\", [object()])\n", "n_tokens": 552, "byte_len": 1969, "file_sha1": "31e740edbcbd8643c2ef74a1be2769d26f4cbe46", "start_line": 115, "end_line": 165}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logformatter.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logformatter.py", "rel_path": "tests/test_logformatter.py", "module": "tests.test_logformatter", "ext": "py", "chunk_number": 1, "symbols": ["__str__", "setup_method", "test_crawled_with_referer", "test_crawled_without_referer", "test_flags_in_request", "test_dropped", "test_dropitem_default_log_level", "test_dropitem_custom_log_level", "CustomItem", "TestLogFormatter", "failure", "log015", "referer", "log", "formatter", "spider", "name", "spiders", "mockserver", "get", "crawler", "pytest", "settings", "isinstance", "level", "object", "item", "none", "drop", "http", "test_item_error", "test_spider_error", "test_download_error_short", "test_download_error_long", "test_scraped", "crawled", "scraped", "dropped", "process_item", "setup_class", "teardown_class", "test_show_messages", "test_skip_messages", "LogFormatterSubclass", "TestLogformatterSubclass", "SkipMessagesLogFormatter", "DropSomeItemsPipeline", "TestShowOrSkipMessages", "traceback", "error"], "ast_kind": "class_or_type", "text": "import logging\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import ItemSpider\n\n\nclass CustomItem(Item):\n    name = Field()\n\n    def __str__(self):\n        return f\"name: {self['name']}\"\n\n\nclass TestLogFormatter:\n    def setup_method(self):\n        self.formatter = LogFormatter()\n        self.spider = Spider(\"default\")\n        self.spider.crawler = get_crawler()\n\n    def test_crawled_with_referer(self):\n        req = Request(\"http://www.example.com\")\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert logline == \"Crawled (200) <GET http://www.example.com> (referer: None)\"\n\n    def test_crawled_without_referer(self):\n        req = Request(\n            \"http://www.example.com\", headers={\"referer\": \"http://example.com\"}\n        )\n        res = Response(\"http://www.example.com\", flags=[\"cached\"])\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert (\n            logline\n            == \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\"\n        )\n\n    def test_flags_in_request(self):\n        req = Request(\"http://www.example.com\", flags=[\"test\", \"flag\"])\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert (\n            logline\n            == \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\"\n        )\n\n    def test_dropped(self):\n        item = {}\n        exception = Exception(\"\\u2018\")\n        response = Response(\"http://www.example.com\")\n        logkws = self.formatter.dropped(item, exception, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        lines = logline.splitlines()\n        assert all(isinstance(x, str) for x in lines)\n        assert lines == [\"Dropped: \\u2018\", \"{}\"]\n\n    def test_dropitem_default_log_level(self):\n        item = {}\n        exception = DropItem(\"Test drop\")\n        response = Response(\"http://www.example.com\")\n        spider = Spider(\"foo\")\n        spider.crawler = get_crawler(Spider)\n\n        logkws = self.formatter.dropped(item, exception, response, spider)\n        assert logkws[\"level\"] == logging.WARNING\n\n        spider.crawler.settings.frozen = False\n        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = logging.INFO\n        spider.crawler.settings.frozen = True\n        logkws = self.formatter.dropped(item, exception, response, spider)\n        assert logkws[\"level\"] == logging.INFO\n\n        spider.crawler.settings.frozen = False\n        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = \"INFO\"\n        spider.crawler.settings.frozen = True\n        logkws = self.formatter.dropped(item, exception, response, spider)\n        assert logkws[\"level\"] == logging.INFO\n\n        spider.crawler.settings.frozen = False\n        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 10\n        spider.crawler.settings.frozen = True\n        logkws = self.formatter.dropped(item, exception, response, spider)\n        assert logkws[\"level\"] == logging.DEBUG\n\n        spider.crawler.settings.frozen = False\n        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 0\n        spider.crawler.settings.frozen = True\n        logkws = self.formatter.dropped(item, exception, response, spider)\n        assert logkws[\"level\"] == logging.NOTSET\n\n        unsupported_value = object()\n        spider.crawler.settings.frozen = False\n        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = unsupported_value\n        spider.crawler.settings.frozen = True\n        logkws = self.formatter.dropped(item, exception, response, spider)\n        assert logkws[\"level\"] == unsupported_value\n\n        with pytest.raises(TypeError):\n            logging.log(logkws[\"level\"], \"message\")  # noqa: LOG015\n\n    def test_dropitem_custom_log_level(self):\n        item = {}\n        response = Response(\"http://www.example.com\")\n\n        exception = DropItem(\"Test drop\", log_level=\"INFO\")\n        logkws = self.formatter.dropped(item, exception, response, self.spider)\n        assert logkws[\"level\"] == logging.INFO\n\n        exception = DropItem(\"Test drop\", log_level=\"ERROR\")\n        logkws = self.formatter.dropped(item, exception, response, self.spider)\n        assert logkws[\"level\"] == logging.ERROR\n", "n_tokens": 1155, "byte_len": 4869, "file_sha1": "631a0594c9a1087366c82940b867d2d3c6a262da", "start_line": 1, "end_line": 125}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logformatter.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logformatter.py", "rel_path": "tests/test_logformatter.py", "module": "tests.test_logformatter", "ext": "py", "chunk_number": 2, "symbols": ["test_item_error", "test_spider_error", "test_download_error_short", "test_download_error_long", "test_scraped", "crawled", "setup_method", "test_crawled_with_referer", "test_crawled_without_referer", "test_flags_in_request", "scraped", "dropped", "process_item", "setup_class", "LogFormatterSubclass", "TestLogformatterSubclass", "SkipMessagesLogFormatter", "DropSomeItemsPipeline", "TestShowOrSkipMessages", "failure", "referer", "log", "formatter", "traceback", "spider", "name", "error", "passing", "mockserver", "get", "__str__", "test_dropped", "test_dropitem_default_log_level", "test_dropitem_custom_log_level", "teardown_class", "test_show_messages", "test_skip_messages", "CustomItem", "TestLogFormatter", "log015", "spiders", "test", "skip", "crawler", "enter", "pytest", "settings", "drop", "some", "isinstance"], "ast_kind": "class_or_type", "text": "    def test_item_error(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        item = {\"key\": \"value\"}\n        exception = Exception()\n        response = Response(\"http://www.example.com\")\n        logkws = self.formatter.item_error(item, exception, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert logline == \"Error processing {'key': 'value'}\"\n\n    def test_spider_error(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        failure = Failure(Exception())\n        request = Request(\n            \"http://www.example.com\", headers={\"Referer\": \"http://example.org\"}\n        )\n        response = Response(\"http://www.example.com\", request=request)\n        logkws = self.formatter.spider_error(failure, request, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert (\n            logline\n            == \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\"\n        )\n\n    def test_download_error_short(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        failure = Failure(Exception())\n        request = Request(\"http://www.example.com\")\n        logkws = self.formatter.download_error(failure, request, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert logline == \"Error downloading <GET http://www.example.com>\"\n\n    def test_download_error_long(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        failure = Failure(Exception())\n        request = Request(\"http://www.example.com\")\n        logkws = self.formatter.download_error(\n            failure, request, self.spider, \"Some message\"\n        )\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert logline == \"Error downloading <GET http://www.example.com>: Some message\"\n\n    def test_scraped(self):\n        item = CustomItem()\n        item[\"name\"] = \"\\xa3\"\n        response = Response(\"http://www.example.com\")\n        logkws = self.formatter.scraped(item, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        lines = logline.splitlines()\n        assert all(isinstance(x, str) for x in lines)\n        assert lines == [\"Scraped from <200 http://www.example.com>\", \"name: \\xa3\"]\n\n\nclass LogFormatterSubclass(LogFormatter):\n    def crawled(self, request, response, spider):\n        kwargs = super().crawled(request, response, spider)\n        CRAWLEDMSG = \"Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s\"\n        log_args = kwargs[\"args\"]\n        log_args[\"flags\"] = str(request.flags)\n        return {\n            \"level\": kwargs[\"level\"],\n            \"msg\": CRAWLEDMSG,\n            \"args\": log_args,\n        }\n\n\nclass TestLogformatterSubclass(TestLogFormatter):\n    def setup_method(self):\n        self.formatter = LogFormatterSubclass()\n        self.spider = Spider(\"default\")\n        self.spider.crawler = get_crawler(Spider)\n\n    def test_crawled_with_referer(self):\n        req = Request(\"http://www.example.com\")\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert (\n            logline == \"Crawled (200) <GET http://www.example.com> (referer: None) []\"\n        )\n\n    def test_crawled_without_referer(self):\n        req = Request(\n            \"http://www.example.com\",\n            headers={\"referer\": \"http://example.com\"},\n            flags=[\"cached\"],\n        )\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert (\n            logline\n            == \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\"\n        )\n\n    def test_flags_in_request(self):\n        req = Request(\"http://www.example.com\", flags=[\"test\", \"flag\"])\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        assert (\n            logline\n            == \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\"\n        )\n\n\nclass SkipMessagesLogFormatter(LogFormatter):\n    def crawled(self, *args, **kwargs):\n        return None\n\n    def scraped(self, *args, **kwargs):\n        return None\n\n    def dropped(self, *args, **kwargs):\n        return None\n\n\nclass DropSomeItemsPipeline:\n    drop = True\n\n    def process_item(self, item):\n        if self.drop:\n            self.drop = False\n            raise DropItem(\"Ignoring item\")\n        self.drop = True\n\n\nclass TestShowOrSkipMessages:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n", "n_tokens": 1220, "byte_len": 5072, "file_sha1": "631a0594c9a1087366c82940b867d2d3c6a262da", "start_line": 126, "end_line": 261}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logformatter.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_logformatter.py", "rel_path": "tests/test_logformatter.py", "module": "tests.test_logformatter", "ext": "py", "chunk_number": 3, "symbols": ["teardown_class", "setup_method", "test_show_messages", "test_skip_messages", "item", "spider", "skip", "messages", "ignoring", "base", "settings", "teardown", "class", "exit", "with", "classmethod", "test", "mockserver", "level", "log", "show", "dropped", "get", "crawler", "yield", "from", "drop", "some", "assert", "crawled", "__str__", "test_crawled_with_referer", "test_crawled_without_referer", "test_flags_in_request", "test_dropped", "test_dropitem_default_log_level", "test_dropitem_custom_log_level", "test_item_error", "test_spider_error", "test_download_error_short", "test_download_error_long", "test_scraped", "scraped", "process_item", "setup_class", "CustomItem", "TestLogFormatter", "LogFormatterSubclass", "TestLogformatterSubclass", "SkipMessagesLogFormatter"], "ast_kind": "function_or_method", "text": "    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        self.base_settings = {\n            \"LOG_LEVEL\": \"DEBUG\",\n            \"ITEM_PIPELINES\": {\n                DropSomeItemsPipeline: 300,\n            },\n        }\n\n    @inlineCallbacks\n    def test_show_messages(self):\n        crawler = get_crawler(ItemSpider, self.base_settings)\n        with LogCapture() as lc:\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert \"Scraped from <200 http://127.0.0.1:\" in str(lc)\n        assert \"Crawled (200) <GET http://127.0.0.1:\" in str(lc)\n        assert \"Dropped: Ignoring item\" in str(lc)\n\n    @inlineCallbacks\n    def test_skip_messages(self):\n        settings = self.base_settings.copy()\n        settings[\"LOG_FORMATTER\"] = SkipMessagesLogFormatter\n        crawler = get_crawler(ItemSpider, settings)\n        with LogCapture() as lc:\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert \"Scraped from <200 http://127.0.0.1:\" not in str(lc)\n        assert \"Crawled (200) <GET http://127.0.0.1:\" not in str(lc)\n        assert \"Dropped: Ignoring item\" not in str(lc)\n", "n_tokens": 304, "byte_len": 1181, "file_sha1": "631a0594c9a1087366c82940b867d2d3c6a262da", "start_line": 262, "end_line": 293}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_downloadtimeout.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_downloadtimeout.py", "rel_path": "tests/test_downloadermiddleware_downloadtimeout.py", "module": "tests.test_downloadermiddleware_downloadtimeout", "ext": "py", "chunk_number": 1, "symbols": ["get_request_spider_mw", "test_default_download_timeout", "test_string_download_timeout", "test_spider_has_download_timeout", "test_request_has_download_timeout", "TestDownloadTimeoutMiddleware", "test", "string", "download", "timeout", "downloadtimeout", "spider", "return", "class", "meta", "downloadermiddlewares", "spiders", "scrapy", "create", "get", "crawler", "downloa", "request", "from", "settings", "scrapytest", "assert", "default", "process", "none", "opened", "utils", "import", "http", "self"], "ast_kind": "class_or_type", "text": "from scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestDownloadTimeoutMiddleware:\n    def get_request_spider_mw(self, settings=None):\n        crawler = get_crawler(Spider, settings)\n        spider = crawler._create_spider(\"foo\")\n        request = Request(\"http://scrapytest.org/\")\n        return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n\n    def test_default_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw()\n        mw.spider_opened(spider)\n        assert mw.process_request(req) is None\n        assert req.meta.get(\"download_timeout\") == 180\n\n    def test_string_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw({\"DOWNLOAD_TIMEOUT\": \"20.1\"})\n        mw.spider_opened(spider)\n        assert mw.process_request(req) is None\n        assert req.meta.get(\"download_timeout\") == 20.1\n\n    def test_spider_has_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw()\n        spider.download_timeout = 2\n        mw.spider_opened(spider)\n        assert mw.process_request(req) is None\n        assert req.meta.get(\"download_timeout\") == 2\n\n    def test_request_has_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw()\n        spider.download_timeout = 2\n        mw.spider_opened(spider)\n        req.meta[\"download_timeout\"] = 1\n        assert mw.process_request(req) is None\n        assert req.meta.get(\"download_timeout\") == 1\n", "n_tokens": 363, "byte_len": 1597, "file_sha1": "aa22ba9ecc1ba2e39b202fe7f6bae13ddd5db01f", "start_line": 1, "end_line": 40}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_curl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_curl.py", "rel_path": "tests/test_utils_curl.py", "module": "tests.test_utils_curl", "ext": "py", "chunk_number": 1, "symbols": ["_test_command", "test_get", "test_get_without_scheme", "test_get_basic_auth", "test_get_complex", "TestCurlToRequestKwargs", "method", "cookies", "agent", "gauges", "unique", "lib", "w3lib", "khtml", "curl", "request", "linux", "chrome", "https", "requests", "test", "mozilla", "webp", "correct", "ges", "pytest", "language", "encoding", "none", "accept", "test_post", "test_post_data_raw", "test_post_data_raw_with_string_prefix", "test_explicit_get_with_data", "test_patch", "test_delete", "test_get_silent", "test_too_few_arguments_error", "test_ignore_unknown_options", "test_must_start_with_curl_error", "delete", "patch", "case", "small", "custtel", "username", "khtm", "fake", "sort", "criteria"], "ast_kind": "class_or_type", "text": "import warnings\nfrom typing import Any\n\nimport pytest\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy import Request\nfrom scrapy.utils.curl import curl_to_request_kwargs\n\n\nclass TestCurlToRequestKwargs:\n    @staticmethod\n    def _test_command(curl_command: str, expected_result: dict[str, Any]) -> None:\n        result = curl_to_request_kwargs(curl_command)\n        assert result == expected_result\n        try:\n            Request(**result)\n        except TypeError as e:\n            pytest.fail(f\"Request kwargs are not correct {e}\")\n\n    def test_get(self):\n        curl_command = \"curl http://example.org/\"\n        expected_result = {\"method\": \"GET\", \"url\": \"http://example.org/\"}\n        self._test_command(curl_command, expected_result)\n\n    def test_get_without_scheme(self):\n        curl_command = \"curl www.example.org\"\n        expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.org\"}\n        self._test_command(curl_command, expected_result)\n\n    def test_get_basic_auth(self):\n        curl_command = 'curl \"https://api.test.com/\" -u \"some_username:some_password\"'\n        expected_result = {\n            \"method\": \"GET\",\n            \"url\": \"https://api.test.com/\",\n            \"headers\": [\n                (\"Authorization\", basic_auth_header(\"some_username\", \"some_password\"))\n            ],\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_get_complex(self):\n        curl_command = (\n            \"curl 'http://httpbin.org/get' -H 'Accept-Encoding: gzip, deflate'\"\n            \" -H 'Accept-Language: en-US,en;q=0.9,ru;q=0.8,es;q=0.7' -H 'Upgra\"\n            \"de-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0 (X11; Linux \"\n            \"x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/62\"\n            \".0.3202.75 Chrome/62.0.3202.75 Safari/537.36' -H 'Accept: text/ht\"\n            \"ml,application/xhtml+xml,application/xml;q=0.9,image/webp,image/a\"\n            \"png,*/*;q=0.8' -H 'Referer: http://httpbin.org/' -H 'Cookie: _gau\"\n            \"ges_unique_year=1; _gauges_unique=1; _gauges_unique_month=1; _gau\"\n            \"ges_unique_hour=1' -H 'Connection: keep-alive' --compressed -b '_\"\n            \"gauges_unique_day=1'\"\n        )\n        expected_result = {\n            \"method\": \"GET\",\n            \"url\": \"http://httpbin.org/get\",\n            \"headers\": [\n                (\"Accept-Encoding\", \"gzip, deflate\"),\n                (\"Accept-Language\", \"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"),\n                (\"Upgrade-Insecure-Requests\", \"1\"),\n                (\n                    \"User-Agent\",\n                    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML\"\n                    \", like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.32\"\n                    \"02.75 Safari/537.36\",\n                ),\n                (\n                    \"Accept\",\n                    \"text/html,application/xhtml+xml,application/xml;q=0.9,ima\"\n                    \"ge/webp,image/apng,*/*;q=0.8\",\n                ),\n                (\"Referer\", \"http://httpbin.org/\"),\n                (\"Connection\", \"keep-alive\"),\n            ],\n            \"cookies\": {\n                \"_gauges_unique_year\": \"1\",\n                \"_gauges_unique_hour\": \"1\",\n                \"_gauges_unique_day\": \"1\",\n                \"_gauges_unique\": \"1\",\n                \"_gauges_unique_month\": \"1\",\n            },\n        }\n        self._test_command(curl_command, expected_result)\n", "n_tokens": 850, "byte_len": 3427, "file_sha1": "9027ac5b187d02b355d87d8f5fcd0f23ed5d6717", "start_line": 1, "end_line": 85}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_curl.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_curl.py", "rel_path": "tests/test_utils_curl.py", "module": "tests.test_utils_curl", "ext": "py", "chunk_number": 2, "symbols": ["test_post", "test_post_data_raw", "test_post_data_raw_with_string_prefix", "test_explicit_get_with_data", "method", "cookies", "agent", "gauges", "unique", "khtml", "small", "custtel", "linux", "chrome", "https", "requests", "mozilla", "webp", "khtm", "sort", "criteria", "functions", "language", "encoding", "anything", "afari", "hour", "ending", "accept", "html", "_test_command", "test_get", "test_get_without_scheme", "test_get_basic_auth", "test_get_complex", "test_patch", "test_delete", "test_get_silent", "test_too_few_arguments_error", "test_ignore_unknown_options", "test_must_start_with_curl_error", "TestCurlToRequestKwargs", "delete", "patch", "case", "lib", "w3lib", "curl", "request", "test"], "ast_kind": "function_or_method", "text": "    def test_post(self):\n        curl_command = (\n            \"curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique\"\n            \"_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique\"\n            \"_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H\"\n            \" 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q\"\n            \"=0.9,ru;q=0.8,es;q=0.7' -H 'Upgrade-Insecure-Requests: 1' -H 'Use\"\n            \"r-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTM\"\n            \"L, like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.3202.75 S\"\n            \"afari/537.36' -H 'Content-Type: application/x-www-form-urlencoded\"\n            \"' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0\"\n            \".9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0'\"\n            \" -H 'Referer: http://httpbin.org/forms/post' -H 'Connection: keep\"\n            \"-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith%\"\n            \"40example.org&size=small&topping=cheese&topping=onion&delivery=12\"\n            \"%3A15&comments=' --compressed\"\n        )\n        expected_result = {\n            \"method\": \"POST\",\n            \"url\": \"http://httpbin.org/post\",\n            \"body\": \"custname=John+Smith&custtel=500&custemail=jsmith%40exampl\"\n            \"e.org&size=small&topping=cheese&topping=onion&delivery=12\"\n            \"%3A15&comments=\",\n            \"cookies\": {\n                \"_gauges_unique_year\": \"1\",\n                \"_gauges_unique_hour\": \"1\",\n                \"_gauges_unique_day\": \"1\",\n                \"_gauges_unique\": \"1\",\n                \"_gauges_unique_month\": \"1\",\n            },\n            \"headers\": [\n                (\"Origin\", \"http://httpbin.org\"),\n                (\"Accept-Encoding\", \"gzip, deflate\"),\n                (\"Accept-Language\", \"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"),\n                (\"Upgrade-Insecure-Requests\", \"1\"),\n                (\n                    \"User-Agent\",\n                    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML\"\n                    \", like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.32\"\n                    \"02.75 Safari/537.36\",\n                ),\n                (\"Content-Type\", \"application/x-www-form-urlencoded\"),\n                (\n                    \"Accept\",\n                    \"text/html,application/xhtml+xml,application/xml;q=0.9,ima\"\n                    \"ge/webp,image/apng,*/*;q=0.8\",\n                ),\n                (\"Cache-Control\", \"max-age=0\"),\n                (\"Referer\", \"http://httpbin.org/forms/post\"),\n                (\"Connection\", \"keep-alive\"),\n            ],\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_post_data_raw(self):\n        curl_command = (\n            \"curl 'https://www.example.org/' --data-raw 'excerptLength=200&ena\"\n            \"bleDidYouMean=true&sortCriteria=ffirstz32xnamez32x201740686%20asc\"\n            \"ending&queryFunctions=%5B%5D&rankingFunctions=%5B%5D'\"\n        )\n        expected_result = {\n            \"method\": \"POST\",\n            \"url\": \"https://www.example.org/\",\n            \"body\": (\n                \"excerptLength=200&enableDidYouMean=true&sortCriteria=ffirstz3\"\n                \"2xnamez32x201740686%20ascending&queryFunctions=%5B%5D&ranking\"\n                \"Functions=%5B%5D\"\n            ),\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_post_data_raw_with_string_prefix(self):\n        curl_command = \"curl 'https://www.example.org/' --data-raw $'{\\\"$filters\\\":\\\"Filter\\u0021\\\"}'\"\n        expected_result = {\n            \"method\": \"POST\",\n            \"url\": \"https://www.example.org/\",\n            \"body\": '{\"$filters\":\"Filter!\"}',\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_explicit_get_with_data(self):\n        curl_command = \"curl httpbin.org/anything -X GET --data asdf\"\n        expected_result = {\n            \"method\": \"GET\",\n            \"url\": \"http://httpbin.org/anything\",\n            \"body\": \"asdf\",\n        }\n        self._test_command(curl_command, expected_result)\n", "n_tokens": 1088, "byte_len": 4119, "file_sha1": "9027ac5b187d02b355d87d8f5fcd0f23ed5d6717", "start_line": 86, "end_line": 174}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_curl.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_curl.py", "rel_path": "tests/test_utils_curl.py", "module": "tests.test_utils_curl", "ext": "py", "chunk_number": 3, "symbols": ["test_patch", "test_delete", "test_get_silent", "test_too_few_arguments_error", "test_ignore_unknown_options", "test_must_start_with_curl_error", "delete", "method", "patch", "case", "curl", "request", "linux", "https", "username", "fake", "test", "pytest", "following", "accept", "http", "ignore", "match", "command", "authorization", "options", "headers", "example", "warnings", "too", "_test_command", "test_get", "test_get_without_scheme", "test_get_basic_auth", "test_get_complex", "test_post", "test_post_data_raw", "test_post_data_raw_with_string_prefix", "test_explicit_get_with_data", "TestCurlToRequestKwargs", "cookies", "agent", "gauges", "unique", "lib", "w3lib", "khtml", "small", "custtel", "chrome"], "ast_kind": "function_or_method", "text": "    def test_patch(self):\n        curl_command = (\n            'curl \"https://example.com/api/fake\" -u \"username:password\" -H \"Ac'\n            'cept: application/vnd.go.cd.v4+json\" -H \"Content-Type: applicatio'\n            'n/json\" -X PATCH -d \\'{\"hostname\": \"agent02.example.com\",  \"agent'\n            '_config_state\": \"Enabled\", \"resources\": [\"Java\",\"Linux\"], \"enviro'\n            'nments\": [\"Dev\"]}\\''\n        )\n        expected_result = {\n            \"method\": \"PATCH\",\n            \"url\": \"https://example.com/api/fake\",\n            \"headers\": [\n                (\"Accept\", \"application/vnd.go.cd.v4+json\"),\n                (\"Content-Type\", \"application/json\"),\n                (\"Authorization\", basic_auth_header(\"username\", \"password\")),\n            ],\n            \"body\": '{\"hostname\": \"agent02.example.com\",  \"agent_config_state\"'\n            ': \"Enabled\", \"resources\": [\"Java\",\"Linux\"], \"environments'\n            '\": [\"Dev\"]}',\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_delete(self):\n        curl_command = 'curl -X \"DELETE\" https://www.url.com/page'\n        expected_result = {\"method\": \"DELETE\", \"url\": \"https://www.url.com/page\"}\n        self._test_command(curl_command, expected_result)\n\n    def test_get_silent(self):\n        curl_command = 'curl --silent \"www.example.com\"'\n        expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n        assert curl_to_request_kwargs(curl_command) == expected_result\n\n    def test_too_few_arguments_error(self):\n        with pytest.raises(\n            ValueError,\n            match=r\"too few arguments|the following arguments are required:\\s*url\",\n        ):\n            curl_to_request_kwargs(\"curl\")\n\n    def test_ignore_unknown_options(self):\n        # case 1: ignore_unknown_options=True:\n        with warnings.catch_warnings():  # avoid warning when executing tests\n            warnings.simplefilter(\"ignore\")\n            curl_command = \"curl --bar --baz http://www.example.com\"\n            expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n            assert curl_to_request_kwargs(curl_command) == expected_result\n\n        # case 2: ignore_unknown_options=False (raise exception):\n        with pytest.raises(ValueError, match=\"Unrecognized options:.*--bar.*--baz\"):\n            curl_to_request_kwargs(\n                \"curl --bar --baz http://www.example.com\", ignore_unknown_options=False\n            )\n\n    def test_must_start_with_curl_error(self):\n        with pytest.raises(ValueError, match=\"A curl command must start\"):\n            curl_to_request_kwargs(\"carl -X POST http://example.org\")\n", "n_tokens": 583, "byte_len": 2633, "file_sha1": "9027ac5b187d02b355d87d8f5fcd0f23ed5d6717", "start_line": 175, "end_line": 231}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_commands.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_commands.py", "rel_path": "tests/test_commands.py", "module": "tests.test_commands", "ext": "py", "chunk_number": 1, "symbols": ["short_desc", "run", "setup_method", "test_settings_json_string", "test_help_formatter", "teardown_method", "call", "proc", "find_in_file", "_append_settings", "EmptyCommand", "TestCommandSettings", "TestProjectBase", "TestCommandBase", "TestCommandCrawlerProcess", "MySpider", "encoding", "async", "loads", "spider", "name", "domain", "sleep", "norma", "msg", "command", "spiders", "future", "find", "file", "_replace_custom_settings", "_assert_spider_works", "_assert_spider_asyncio_fail", "test_project_settings", "test_cmdline_asyncio", "test_project_settings_explicit_asyncio", "test_cmdline_empty", "test_project_settings_empty", "test_spider_settings_asyncio", "test_spider_settings_asyncio_cmdline_empty", "test_project_empty_spider_settings_asyncio", "test_project_asyncio_spider_settings_select", "test_project_asyncio_spider_settings_select_forced", "test_list", "test_command_not_found", "test_run", "test_methods", "test_help_messages", "test_valid_command", "test_no_command"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport argparse\nimport json\nimport re\nimport subprocess\nimport sys\nfrom io import StringIO\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom tempfile import TemporaryFile, mkdtemp\nfrom typing import Any\nfrom unittest import mock\n\nimport pytest\n\nimport scrapy\nfrom scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\nfrom scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.reactor import _asyncio_reactor_path\nfrom scrapy.utils.test import get_testenv\n\n\nclass EmptyCommand(ScrapyCommand):\n    def short_desc(self) -> str:\n        return \"\"\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        pass\n\n\nclass TestCommandSettings:\n    def setup_method(self):\n        self.command = EmptyCommand()\n        self.command.settings = Settings()\n        self.parser = argparse.ArgumentParser(\n            formatter_class=ScrapyHelpFormatter, conflict_handler=\"resolve\"\n        )\n        self.command.add_options(self.parser)\n\n    def test_settings_json_string(self):\n        feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n        opts, args = self.parser.parse_known_args(\n            args=[\"-s\", f\"FEEDS={feeds_json}\", \"spider.py\"]\n        )\n        self.command.process_options(args, opts)\n        assert isinstance(self.command.settings[\"FEEDS\"], scrapy.settings.BaseSettings)\n        assert dict(self.command.settings[\"FEEDS\"]) == json.loads(feeds_json)\n\n    def test_help_formatter(self):\n        formatter = ScrapyHelpFormatter(prog=\"scrapy\")\n        part_strings = [\n            \"usage: scrapy genspider [options] <name> <domain>\\n\\n\",\n            \"\\n\",\n            \"optional arguments:\\n\",\n            \"\\n\",\n            \"Global Options:\\n\",\n        ]\n        assert formatter._join_parts(part_strings) == (\n            \"Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n\"\n            \"Optional Arguments\\n==================\\n\\n\"\n            \"Global Options\\n--------------\\n\"\n        )\n\n\nclass TestProjectBase:\n    project_name = \"testproject\"\n\n    def setup_method(self):\n        self.temp_path = mkdtemp()\n        self.cwd = self.temp_path\n        self.proj_path = Path(self.temp_path, self.project_name)\n        self.proj_mod_path = self.proj_path / self.project_name\n        self.env = get_testenv()\n\n    def teardown_method(self):\n        rmtree(self.temp_path)\n\n    def call(self, *args: str, **popen_kwargs: Any) -> int:\n        with TemporaryFile() as out:\n            args = (sys.executable, \"-m\", \"scrapy.cmdline\", *args)\n            return subprocess.call(\n                args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **popen_kwargs\n            )\n\n    def proc(\n        self, *args: str, **popen_kwargs: Any\n    ) -> tuple[subprocess.Popen[bytes], str, str]:\n        args = (sys.executable, \"-m\", \"scrapy.cmdline\", *args)\n        p = subprocess.Popen(\n            args,\n            cwd=popen_kwargs.pop(\"cwd\", self.cwd),\n            env=self.env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            **popen_kwargs,\n        )\n\n        try:\n            stdout, stderr = p.communicate(timeout=15)\n        except subprocess.TimeoutExpired:\n            p.kill()\n            p.communicate()\n            pytest.fail(\"Command took too much time to complete\")\n\n        return p, to_unicode(stdout), to_unicode(stderr)\n\n    @staticmethod\n    def find_in_file(filename: Path, regex: str) -> re.Match | None:\n        \"\"\"Find first pattern occurrence in file\"\"\"\n        pattern = re.compile(regex)\n        with filename.open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                match = pattern.search(line)\n                if match is not None:\n                    return match\n        return None\n\n\nclass TestCommandBase(TestProjectBase):\n    def setup_method(self):\n        super().setup_method()\n        self.call(\"startproject\", self.project_name)\n        self.cwd = self.proj_path\n        self.env[\"SCRAPY_SETTINGS_MODULE\"] = f\"{self.project_name}.settings\"\n\n\nclass TestCommandCrawlerProcess(TestCommandBase):\n    \"\"\"Test that the command uses the expected kind of *CrawlerProcess\n    and produces expected errors when needed.\"\"\"\n\n    name = \"crawl\"\n    NORMAL_MSG = \"Using CrawlerProcess\"\n    ASYNC_MSG = \"Using AsyncCrawlerProcess\"\n\n    def setup_method(self):\n        super().setup_method()\n        (self.cwd / self.project_name / \"spiders\" / \"sp.py\").write_text(\"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'sp'\n\n    custom_settings = {}\n\n    async def start(self):\n        self.logger.debug('It works!')\n        return\n        yield\n\"\"\")\n\n        (self.cwd / self.project_name / \"spiders\" / \"aiosp.py\").write_text(\"\"\"\nimport asyncio\n\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'aiosp'\n\n    custom_settings = {}\n\n    async def start(self):\n        await asyncio.sleep(0.01)\n        self.logger.debug('It works!')\n        return\n        yield\n\"\"\")\n\n        self._append_settings(\"LOG_LEVEL = 'DEBUG'\\n\")\n\n    def _append_settings(self, text: str) -> None:\n        \"\"\"Add text to the end of the project settings.py.\"\"\"\n        with (self.cwd / self.project_name / \"settings.py\").open(\n            \"a\", encoding=\"utf-8\"\n        ) as f:\n            f.write(text)\n", "n_tokens": 1212, "byte_len": 5399, "file_sha1": "0feca5e784d3eaad10186e77cb3b45b9a4b935d4", "start_line": 1, "end_line": 179}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_commands.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_commands.py", "rel_path": "tests/test_commands.py", "module": "tests.test_commands", "ext": "py", "chunk_number": 2, "symbols": ["_replace_custom_settings", "_assert_spider_works", "_assert_spider_asyncio_fail", "test_project_settings", "test_cmdline_asyncio", "test_project_settings_explicit_asyncio", "test_cmdline_empty", "test_project_settings_empty", "test_spider_settings_asyncio", "test_spider_settings_asyncio_cmdline_empty", "test_project_empty_spider_settings_asyncio", "encoding", "test", "project", "takes", "works", "expected", "chosen", "text", "seek", "based", "spider", "path", "append", "settings", "command", "assert", "crawler", "process", "replace", "short_desc", "run", "setup_method", "test_settings_json_string", "test_help_formatter", "teardown_method", "call", "proc", "find_in_file", "_append_settings", "test_project_asyncio_spider_settings_select", "test_project_asyncio_spider_settings_select_forced", "test_list", "test_command_not_found", "test_run", "test_methods", "test_help_messages", "test_valid_command", "test_no_command", "test_option_before_command"], "ast_kind": "function_or_method", "text": "    def _replace_custom_settings(self, spider_name: str, text: str) -> None:\n        \"\"\"Replace custom_settings in the given spider file with the given text.\"\"\"\n        spider_path = self.cwd / self.project_name / \"spiders\" / f\"{spider_name}.py\"\n        with spider_path.open(\"r+\", encoding=\"utf-8\") as f:\n            content = f.read()\n            content = content.replace(\n                \"custom_settings = {}\", f\"custom_settings = {text}\"\n            )\n            f.seek(0)\n            f.write(content)\n            f.truncate()\n\n    def _assert_spider_works(self, msg: str, *args: str) -> None:\n        \"\"\"The command uses the expected *CrawlerProcess, the spider works.\"\"\"\n        _, _, err = self.proc(self.name, *args)\n        assert msg in err\n        assert \"It works!\" in err\n        assert \"Spider closed (finished)\" in err\n\n    def _assert_spider_asyncio_fail(self, msg: str, *args: str) -> None:\n        \"\"\"The command uses the expected *CrawlerProcess, the spider fails to use asyncio.\"\"\"\n        _, _, err = self.proc(self.name, *args)\n        assert msg in err\n        assert \"no running event loop\" in err\n\n    def test_project_settings(self):\n        \"\"\"The reactor is set via the project default settings (to the asyncio value).\n\n        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n        for spider in [\"sp\", \"aiosp\"]:\n            self._assert_spider_works(self.ASYNC_MSG, spider)\n\n    def test_cmdline_asyncio(self):\n        \"\"\"The reactor is set via the command line to the asyncio value.\n        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n        for spider in [\"sp\", \"aiosp\"]:\n            self._assert_spider_works(\n                self.ASYNC_MSG, spider, \"-s\", f\"TWISTED_REACTOR={_asyncio_reactor_path}\"\n            )\n\n    def test_project_settings_explicit_asyncio(self):\n        \"\"\"The reactor explicitly is set via the project settings to the asyncio value.\n\n        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n        self._append_settings(f\"TWISTED_REACTOR = '{_asyncio_reactor_path}'\\n\")\n\n        for spider in [\"sp\", \"aiosp\"]:\n            self._assert_spider_works(self.ASYNC_MSG, spider)\n\n    def test_cmdline_empty(self):\n        \"\"\"The reactor is set via the command line to the empty value.\n\n        CrawlerProcess, the default reactor, only the normal spider works.\"\"\"\n        self._assert_spider_works(self.NORMAL_MSG, \"sp\", \"-s\", \"TWISTED_REACTOR=\")\n        self._assert_spider_asyncio_fail(\n            self.NORMAL_MSG, \"aiosp\", \"-s\", \"TWISTED_REACTOR=\"\n        )\n\n    def test_project_settings_empty(self):\n        \"\"\"The reactor is set via the project settings to the empty value.\n\n        CrawlerProcess, the default reactor, only the normal spider works.\"\"\"\n        self._append_settings(\"TWISTED_REACTOR = None\\n\")\n\n        self._assert_spider_works(self.NORMAL_MSG, \"sp\")\n        self._assert_spider_asyncio_fail(\n            self.NORMAL_MSG, \"aiosp\", \"-s\", \"TWISTED_REACTOR=\"\n        )\n\n    def test_spider_settings_asyncio(self):\n        \"\"\"The reactor is set via the spider settings to the asyncio value.\n\n        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n        for spider in [\"sp\", \"aiosp\"]:\n            self._replace_custom_settings(\n                spider, f\"{{'TWISTED_REACTOR': '{_asyncio_reactor_path}'}}\"\n            )\n            self._assert_spider_works(self.ASYNC_MSG, spider)\n\n    def test_spider_settings_asyncio_cmdline_empty(self):\n        \"\"\"The reactor is set via the spider settings to the asyncio value\n        and via command line to the empty value. The command line value takes\n        precedence so the spider settings don't matter.\n\n        CrawlerProcess, the default reactor, only the normal spider works.\"\"\"\n        for spider in [\"sp\", \"aiosp\"]:\n            self._replace_custom_settings(\n                spider, f\"{{'TWISTED_REACTOR': '{_asyncio_reactor_path}'}}\"\n            )\n\n        self._assert_spider_works(self.NORMAL_MSG, \"sp\", \"-s\", \"TWISTED_REACTOR=\")\n        self._assert_spider_asyncio_fail(\n            self.NORMAL_MSG, \"aiosp\", \"-s\", \"TWISTED_REACTOR=\"\n        )\n\n    def test_project_empty_spider_settings_asyncio(self):\n        \"\"\"The reactor is set via the project settings to the empty value\n        and via the spider settings to the asyncio value. CrawlerProcess is\n        chosen based on the project settings, but the asyncio reactor is chosen\n        based on the spider settings.\n\n        CrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n        self._append_settings(\"TWISTED_REACTOR = None\\n\")\n        for spider in [\"sp\", \"aiosp\"]:\n            self._replace_custom_settings(\n                spider, f\"{{'TWISTED_REACTOR': '{_asyncio_reactor_path}'}}\"\n            )\n            self._assert_spider_works(self.NORMAL_MSG, spider)\n", "n_tokens": 1129, "byte_len": 4831, "file_sha1": "0feca5e784d3eaad10186e77cb3b45b9a4b935d4", "start_line": 180, "end_line": 288}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_commands.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_commands.py", "rel_path": "tests/test_commands.py", "module": "tests.test_commands", "ext": "py", "chunk_number": 3, "symbols": ["test_project_asyncio_spider_settings_select", "test_project_asyncio_spider_settings_select_forced", "test_list", "test_command_not_found", "setup_method", "test_run", "test_methods", "test_help_messages", "test_valid_command", "test_no_command", "test_option_before_command", "test_option_after_command", "TestMiscCommands", "TestProjectSubdir", "TestBenchCommand", "TestViewCommand", "TestHelpMessage", "TestPopCommandName", "does", "browser", "seen", "inproject", "name", "norma", "msg", "error", "these", "spiders", "command", "usage", "short_desc", "run", "test_settings_json_string", "test_help_formatter", "teardown_method", "call", "proc", "find_in_file", "_append_settings", "_replace_custom_settings", "_assert_spider_works", "_assert_spider_asyncio_fail", "test_project_settings", "test_cmdline_asyncio", "test_project_settings_explicit_asyncio", "test_cmdline_empty", "test_project_settings_empty", "test_spider_settings_asyncio", "test_spider_settings_asyncio_cmdline_empty", "test_project_empty_spider_settings_asyncio"], "ast_kind": "class_or_type", "text": "    def test_project_asyncio_spider_settings_select(self):\n        \"\"\"The reactor is set via the project settings to the asyncio value\n        and via the spider settings to the select value. AsyncCrawlerProcess\n        is chosen based on the project settings, and the conflicting reactor\n        setting in the spider settings causes an exception.\n\n        AsyncCrawlerProcess, the asyncio reactor, both spiders produce a\n        mismatched reactor exception.\"\"\"\n        self._append_settings(f\"TWISTED_REACTOR = '{_asyncio_reactor_path}'\\n\")\n        for spider in [\"sp\", \"aiosp\"]:\n            self._replace_custom_settings(\n                spider,\n                \"{'TWISTED_REACTOR': 'twisted.internet.selectreactor.SelectReactor'}\",\n            )\n            _, _, err = self.proc(self.name, spider)\n            assert self.ASYNC_MSG in err\n            assert (\n                \"The installed reactor (twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n                \" does not match the requested one\"\n                \" (twisted.internet.selectreactor.SelectReactor)\"\n            ) in err\n\n    def test_project_asyncio_spider_settings_select_forced(self):\n        \"\"\"The reactor is set via the project settings to the asyncio value\n        and via the spider settings to the select value, CrawlerProcess is\n        forced via the project settings. The reactor is chosen based on the\n        spider settings.\n\n        CrawlerProcess, the select reactor, only the normal spider works.\"\"\"\n        self._append_settings(\"FORCE_CRAWLER_PROCESS = True\\n\")\n        for spider in [\"sp\", \"aiosp\"]:\n            self._replace_custom_settings(\n                spider,\n                \"{'TWISTED_REACTOR': 'twisted.internet.selectreactor.SelectReactor'}\",\n            )\n\n        self._assert_spider_works(self.NORMAL_MSG, \"sp\")\n        self._assert_spider_asyncio_fail(self.NORMAL_MSG, \"aiosp\")\n\n\nclass TestMiscCommands(TestCommandBase):\n    def test_list(self):\n        assert self.call(\"list\") == 0\n\n    def test_command_not_found(self):\n        na_msg = \"\"\"\nThe list command is not available from this location.\nThese commands are only available from within a project: check, crawl, edit, list, parse.\n\"\"\"\n        not_found_msg = \"\"\"\nUnknown command: abc\n\"\"\"\n        params = [\n            (\"list\", 0, na_msg),\n            (\"abc\", 0, not_found_msg),\n            (\"abc\", 1, not_found_msg),\n        ]\n        for cmdname, inproject, message in params:\n            with mock.patch(\"sys.stdout\", new=StringIO()) as out:\n                _print_unknown_command_msg(Settings(), cmdname, inproject)\n                assert out.getvalue().strip() == message.strip()\n\n\nclass TestProjectSubdir(TestProjectBase):\n    \"\"\"Test that commands work in a subdirectory of the project.\"\"\"\n\n    def setup_method(self):\n        super().setup_method()\n        self.call(\"startproject\", self.project_name)\n        self.cwd = self.proj_path / \"subdir\"\n        self.cwd.mkdir(exist_ok=True)\n\n    def test_list(self):\n        assert self.call(\"list\") == 0\n\n\nclass TestBenchCommand(TestCommandBase):\n    def test_run(self):\n        _, _, log = self.proc(\n            \"bench\", \"-s\", \"LOGSTATS_INTERVAL=0.001\", \"-s\", \"CLOSESPIDER_TIMEOUT=0.01\"\n        )\n        assert \"INFO: Crawled\" in log\n        assert \"Unhandled Error\" not in log\n        assert \"log_count/ERROR\" not in log\n\n\nclass TestViewCommand(TestCommandBase):\n    def test_methods(self):\n        command = view.Command()\n        command.settings = Settings()\n        parser = argparse.ArgumentParser(\n            prog=\"scrapy\",\n            prefix_chars=\"-\",\n            formatter_class=ScrapyHelpFormatter,\n            conflict_handler=\"resolve\",\n        )\n        command.add_options(parser)\n        assert command.short_desc() == \"Open URL in browser, as seen by Scrapy\"\n        assert \"URL using the Scrapy downloader and show its\" in command.long_desc()\n\n\nclass TestHelpMessage(TestCommandBase):\n    def setup_method(self):\n        super().setup_method()\n        self.commands = [\n            \"parse\",\n            \"startproject\",\n            \"view\",\n            \"crawl\",\n            \"edit\",\n            \"list\",\n            \"fetch\",\n            \"settings\",\n            \"shell\",\n            \"runspider\",\n            \"version\",\n            \"genspider\",\n            \"check\",\n            \"bench\",\n        ]\n\n    def test_help_messages(self):\n        for command in self.commands:\n            _, out, _ = self.proc(command, \"-h\")\n            assert \"Usage\" in out\n\n\nclass TestPopCommandName:\n    def test_valid_command(self):\n        argv = [\"scrapy\", \"crawl\", \"my_spider\"]\n        command = _pop_command_name(argv)\n        assert command == \"crawl\"\n        assert argv == [\"scrapy\", \"my_spider\"]\n\n    def test_no_command(self):\n        argv = [\"scrapy\"]\n        command = _pop_command_name(argv)\n        assert command is None\n        assert argv == [\"scrapy\"]\n\n    def test_option_before_command(self):\n        argv = [\"scrapy\", \"-h\", \"crawl\"]\n        command = _pop_command_name(argv)\n        assert command == \"crawl\"\n        assert argv == [\"scrapy\", \"-h\"]\n\n    def test_option_after_command(self):\n        argv = [\"scrapy\", \"crawl\", \"-h\"]\n        command = _pop_command_name(argv)\n        assert command == \"crawl\"\n        assert argv == [\"scrapy\", \"-h\"]\n", "n_tokens": 1158, "byte_len": 5293, "file_sha1": "0feca5e784d3eaad10186e77cb3b45b9a4b935d4", "start_line": 289, "end_line": 440}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_console.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_console.py", "rel_path": "tests/test_utils_console.py", "module": "tests.test_utils_console", "ext": "py", "chunk_number": 1, "symbols": ["test_get_shell_embed_func", "test_get_shell_embed_func_bpython", "test_get_shell_embed_func_ipython", "false", "except", "python", "shell", "test", "get", "name", "mark", "ipython", "skipif", "import", "error", "scrapy", "invalid", "embed", "console", "true", "pytest", "from", "bpython", "assert", "callable", "standard", "none", "utils", "testenv", "reason", "should", "default", "available"], "ast_kind": "function_or_method", "text": "import pytest\n\nfrom scrapy.utils.console import get_shell_embed_func\n\ntry:\n    import bpython\n\n    bpy = True\n    del bpython\nexcept ImportError:\n    bpy = False\ntry:\n    import IPython\n\n    ipy = True\n    del IPython\nexcept ImportError:\n    ipy = False\n\n\ndef test_get_shell_embed_func():\n    shell = get_shell_embed_func([\"invalid\"])\n    assert shell is None\n\n    shell = get_shell_embed_func([\"invalid\", \"python\"])\n    assert callable(shell)\n    assert shell.__name__ == \"_embed_standard_shell\"\n\n\n@pytest.mark.skipif(not bpy, reason=\"bpython not available in testenv\")\ndef test_get_shell_embed_func_bpython():\n    shell = get_shell_embed_func([\"bpython\"])\n    assert callable(shell)\n    assert shell.__name__ == \"_embed_bpython_shell\"\n\n\n@pytest.mark.skipif(not ipy, reason=\"IPython not available in testenv\")\ndef test_get_shell_embed_func_ipython():\n    # default shell should be 'ipython'\n    shell = get_shell_embed_func()\n    assert shell.__name__ == \"_embed_ipython_shell\"\n", "n_tokens": 231, "byte_len": 979, "file_sha1": "5d40940febc3f07be5276e8177d2fdd1e6b0c8a7", "start_line": 1, "end_line": 42}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_proxy_connect.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_proxy_connect.py", "rel_path": "tests/test_proxy_connect.py", "module": "tests.test_proxy_connect", "ext": "py", "chunk_number": 1, "symbols": ["start", "stop", "_wrong_credentials", "setup_class", "teardown_class", "setup_method", "teardown_method", "test_https_connect_tunnel", "test_https_tunnel_auth_error", "test_https_tunnel_without_leak_proxy_authorization_header", "_assert_got_response_code", "_assert_got_tunnel_error", "MitmProxy", "TestProxyConnect", "does", "assert", "got", "confdir", "loads", "seed", "file", "responses", "sees", "spiders", "http", "proxy", "port", "main", "path", "mockserver", "get", "crawler", "enter", "pytest", "simple", "spider", "none", "listen", "urlsplit", "line", "code", "group", "parse", "script", "bad", "auth", "single", "request", "echo", "secure"], "ast_kind": "class_or_type", "text": "import json\nimport os\nimport re\nimport sys\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\nfrom urllib.parse import urlsplit, urlunsplit\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.http import Request\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import SimpleSpider, SingleRequestSpider\n\n\nclass MitmProxy:\n    auth_user = \"scrapy\"\n    auth_pass = \"scrapy\"\n\n    def start(self):\n        script = \"\"\"\nimport sys\nfrom mitmproxy.tools.main import mitmdump\nsys.argv[0] = \"mitmdump\"\nsys.exit(mitmdump())\n        \"\"\"\n        cert_path = Path(__file__).parent.resolve() / \"keys\"\n        self.proc = Popen(\n            [\n                sys.executable,\n                \"-u\",\n                \"-c\",\n                script,\n                \"--listen-host\",\n                \"127.0.0.1\",\n                \"--listen-port\",\n                \"0\",\n                \"--proxyauth\",\n                f\"{self.auth_user}:{self.auth_pass}\",\n                \"--set\",\n                f\"confdir={cert_path}\",\n                \"--ssl-insecure\",\n            ],\n            stdout=PIPE,\n        )\n        line = self.proc.stdout.readline().decode(\"utf-8\")\n        host_port = re.search(r\"listening at (?:http://)?([^:]+:\\d+)\", line).group(1)\n        return f\"http://{self.auth_user}:{self.auth_pass}@{host_port}\"\n\n    def stop(self):\n        self.proc.kill()\n        self.proc.communicate()\n\n\ndef _wrong_credentials(proxy_url):\n    bad_auth_proxy = list(urlsplit(proxy_url))\n    bad_auth_proxy[1] = bad_auth_proxy[1].replace(\"scrapy:scrapy@\", \"wrong:wronger@\")\n    return urlunsplit(bad_auth_proxy)\n\n\nclass TestProxyConnect:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        try:\n            import mitmproxy  # noqa: F401,PLC0415\n        except ImportError:\n            pytest.skip(\"mitmproxy is not installed\")\n\n        self._oldenv = os.environ.copy()\n\n        self._proxy = MitmProxy()\n        proxy_url = self._proxy.start()\n        os.environ[\"https_proxy\"] = proxy_url\n        os.environ[\"http_proxy\"] = proxy_url\n\n    def teardown_method(self):\n        self._proxy.stop()\n        os.environ = self._oldenv\n\n    @inlineCallbacks\n    def test_https_connect_tunnel(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n        self._assert_got_response_code(200, log)\n\n    @inlineCallbacks\n    def test_https_tunnel_auth_error(self):\n        os.environ[\"https_proxy\"] = _wrong_credentials(os.environ[\"https_proxy\"])\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n        # The proxy returns a 407 error code but it does not reach the client;\n        # he just sees a TunnelError.\n        self._assert_got_tunnel_error(log)\n\n    @inlineCallbacks\n    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(seed=request)\n        self._assert_got_response_code(200, log)\n        echo = json.loads(crawler.spider.meta[\"responses\"][0].text)\n        assert \"Proxy-Authorization\" not in echo[\"headers\"]\n\n    def _assert_got_response_code(self, code, log):\n        print(log)\n        assert str(log).count(f\"Crawled ({code})\") == 1\n\n    def _assert_got_tunnel_error(self, log):\n        print(log)\n        assert \"TunnelError\" in str(log)\n", "n_tokens": 897, "byte_len": 3880, "file_sha1": "729176eaf93b99c121b989c43eea749e13bd4437", "start_line": 1, "end_line": 125}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_telnet.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_extension_telnet.py", "rel_path": "tests/test_extension_telnet.py", "module": "tests.test_extension_telnet", "ext": "py", "chunk_number": 1, "symbols": ["_get_console_and_portal", "test_bad_credentials", "test_good_credentials", "test_custom_credentials", "TestTelnetExtension", "utf", "utf8", "settings", "dict", "protocol", "test", "custom", "user", "pass", "internet", "telnet", "bad", "start", "listening", "cred", "password", "twisted", "get", "return", "credentials", "value", "error", "effects", "class", "telnetconsol", "with", "scrapy", "defer", "some", "console", "args", "crawler", "username", "yield", "pytest", "from", "function", "invalid", "good", "side", "raises", "need", "none", "stop", "encode"], "ast_kind": "class_or_type", "text": "import pytest\nfrom twisted.conch.telnet import ITelnetProtocol\nfrom twisted.cred import credentials\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.extensions.telnet import TelnetConsole\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestTelnetExtension:\n    def _get_console_and_portal(self, settings=None):\n        crawler = get_crawler(settings_dict=settings)\n        console = TelnetConsole(crawler)\n\n        # This function has some side effects we don't need for this test\n        console._get_telnet_vars = dict\n\n        console.start_listening()\n        protocol = console.protocol()\n        portal = protocol.protocolArgs[0]\n\n        return console, portal\n\n    @inlineCallbacks\n    def test_bad_credentials(self):\n        console, portal = self._get_console_and_portal()\n        creds = credentials.UsernamePassword(b\"username\", b\"password\")\n        d = portal.login(creds, None, ITelnetProtocol)\n        with pytest.raises(ValueError, match=\"Invalid credentials\"):\n            yield d\n        console.stop_listening()\n\n    @inlineCallbacks\n    def test_good_credentials(self):\n        console, portal = self._get_console_and_portal()\n        creds = credentials.UsernamePassword(\n            console.username.encode(\"utf8\"), console.password.encode(\"utf8\")\n        )\n        d = portal.login(creds, None, ITelnetProtocol)\n        yield d\n        console.stop_listening()\n\n    @inlineCallbacks\n    def test_custom_credentials(self):\n        settings = {\n            \"TELNETCONSOLE_USERNAME\": \"user\",\n            \"TELNETCONSOLE_PASSWORD\": \"pass\",\n        }\n        console, portal = self._get_console_and_portal(settings=settings)\n        creds = credentials.UsernamePassword(b\"user\", b\"pass\")\n        d = portal.login(creds, None, ITelnetProtocol)\n        yield d\n        console.stop_listening()\n", "n_tokens": 378, "byte_len": 1826, "file_sha1": "85ba9ae6a9fe8bd3023781cd054cc43db9e01c97", "start_line": 1, "end_line": 54}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_depth.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_depth.py", "rel_path": "tests/test_spidermiddleware_depth.py", "module": "tests.test_spidermiddleware_depth", "ext": "py", "chunk_number": 1, "symbols": ["crawler", "stats", "mw", "test_process_spider_output", "statscollectors", "dept", "stat", "result", "open", "spider", "request", "depth", "typing", "return", "annotations", "meta", "close", "spiders", "scrapy", "resp", "future", "typ", "checking", "collections", "middleware", "get", "test", "process", "from", "true", "pytest", "yield", "collector", "list", "out", "out2", "assert", "scrapytest", "spidermiddlewares", "none", "fixture", "utils", "import", "http", "value", "generator", "limit", "response"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport pytest\n\nfrom scrapy.http import Request, Response\nfrom scrapy.spidermiddlewares.depth import DepthMiddleware\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from collections.abc import Generator\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider, {\"DEPTH_LIMIT\": 1, \"DEPTH_STATS_VERBOSE\": True})\n\n\n@pytest.fixture\ndef stats(crawler: Crawler) -> Generator[StatsCollector]:\n    assert crawler.stats is not None\n    crawler.stats.open_spider()\n\n    yield crawler.stats\n\n    crawler.stats.close_spider()\n\n\n@pytest.fixture\ndef mw(crawler: Crawler) -> DepthMiddleware:\n    return DepthMiddleware.from_crawler(crawler)\n\n\ndef test_process_spider_output(mw: DepthMiddleware, stats: StatsCollector) -> None:\n    req = Request(\"http://scrapytest.org\")\n    resp = Response(\"http://scrapytest.org\")\n    resp.request = req\n    result = [Request(\"http://scrapytest.org\")]\n\n    out = list(mw.process_spider_output(resp, result))\n    assert out == result\n\n    rdc = stats.get_value(\"request_depth_count/1\")\n    assert rdc == 1\n\n    req.meta[\"depth\"] = 1\n\n    out2 = list(mw.process_spider_output(resp, result))\n    assert not out2\n\n    rdm = stats.get_value(\"request_depth_max\")\n    assert rdm == 1\n", "n_tokens": 341, "byte_len": 1427, "file_sha1": "bc4fbc24413d39adcd95a505f1f023467ba1c872", "start_line": 1, "end_line": 58}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 1, "symbols": ["test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "Base", "Test", "method", "spider", "test", "ttl", "spiders", "length", "get", "crawler", "meta", "refresh", "pytest", "isinstance", "dont", "reason", "misc", "http", "proxy", "empty", "response", "present", "ignore", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "test_system_proxy_unproxied_http_to_unproxied_https", "test_system_proxy_proxied_https_to_proxied_http", "test_system_proxy_proxied_https_to_unproxied_http", "test_system_proxy_unproxied_https_to_proxied_http", "test_system_proxy_unproxied_https_to_unproxied_http", "setup_method"], "ast_kind": "class_or_type", "text": "from itertools import chain, product\n\nimport pytest\n\nfrom scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\nfrom scrapy.downloadermiddlewares.redirect import (\n    MetaRefreshMiddleware,\n    RedirectMiddleware,\n)\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import set_environ\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\n\nclass Base:\n    class Test:\n        def test_priority_adjust(self):\n            req = Request(\"http://a.com\")\n            rsp = self.get_response(req, \"http://a.com/redirected\")\n            req2 = self.mw.process_response(req, rsp)\n            assert req2.priority > req.priority\n\n        def test_dont_redirect(self):\n            url = \"http://www.example.com/301\"\n            url2 = \"http://www.example.com/redirected\"\n            req = Request(url, meta={\"dont_redirect\": True})\n            rsp = self.get_response(req, url2)\n\n            r = self.mw.process_response(req, rsp)\n            assert isinstance(r, Response)\n            assert r is rsp\n\n            # Test that it redirects when dont_redirect is False\n            req = Request(url, meta={\"dont_redirect\": False})\n            rsp = self.get_response(req, url2)\n\n            r = self.mw.process_response(req, rsp)\n            assert isinstance(r, Request)\n\n        def test_post(self):\n            url = \"http://www.example.com/302\"\n            url2 = \"http://www.example.com/redirected2\"\n            req = Request(\n                url,\n                method=\"POST\",\n                body=\"test\",\n                headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n            )\n            rsp = self.get_response(req, url2)\n\n            req2 = self.mw.process_response(req, rsp)\n            assert isinstance(req2, Request)\n            assert req2.url == url2\n            assert req2.method == \"GET\"\n            assert \"Content-Type\" not in req2.headers, (\n                \"Content-Type header must not be present in redirected request\"\n            )\n            assert \"Content-Length\" not in req2.headers, (\n                \"Content-Length header must not be present in redirected request\"\n            )\n            assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n\n        def test_max_redirect_times(self):\n            self.mw.max_redirect_times = 1\n            req = Request(\"http://scrapytest.org/302\")\n            rsp = self.get_response(req, \"/redirected\")\n\n            req = self.mw.process_response(req, rsp)\n            assert isinstance(req, Request)\n            assert \"redirect_times\" in req.meta\n            assert req.meta[\"redirect_times\"] == 1\n            with pytest.raises(IgnoreRequest):\n                self.mw.process_response(req, rsp)\n\n        def test_ttl(self):\n            self.mw.max_redirect_times = 100\n            req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n            rsp = self.get_response(req, \"/a\")\n\n            req = self.mw.process_response(req, rsp)\n            assert isinstance(req, Request)\n            with pytest.raises(IgnoreRequest):\n                self.mw.process_response(req, rsp)\n\n        def test_redirect_urls(self):\n            req1 = Request(\"http://scrapytest.org/first\")\n            rsp1 = self.get_response(req1, \"/redirected\")\n            req2 = self.mw.process_response(req1, rsp1)\n            rsp2 = self.get_response(req1, \"/redirected2\")\n            req3 = self.mw.process_response(req2, rsp2)\n\n            assert req2.url == \"http://scrapytest.org/redirected\"\n            assert req2.meta[\"redirect_urls\"] == [\"http://scrapytest.org/first\"]\n            assert req3.url == \"http://scrapytest.org/redirected2\"\n            assert req3.meta[\"redirect_urls\"] == [\n                \"http://scrapytest.org/first\",\n                \"http://scrapytest.org/redirected\",\n            ]\n\n        def test_redirect_reasons(self):\n            req1 = Request(\"http://scrapytest.org/first\")\n            rsp1 = self.get_response(req1, \"/redirected1\")\n            req2 = self.mw.process_response(req1, rsp1)\n            rsp2 = self.get_response(req2, \"/redirected2\")\n            req3 = self.mw.process_response(req2, rsp2)\n            assert req2.meta[\"redirect_reasons\"] == [self.reason]\n            assert req3.meta[\"redirect_reasons\"] == [self.reason, self.reason]\n", "n_tokens": 974, "byte_len": 4436, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 1, "end_line": 111}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 2, "symbols": ["test_cross_origin_header_dropping", "does", "origin", "cookies", "were", "domain", "implicit", "port", "https", "unicode", "dict", "explicit", "upgrade", "changes", "redirect", "isinstance", "safe", "headers", "internal", "docs", "http", "default", "note", "cookie", "test", "cross", "authorization", "keep", "appropriate", "middleware", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https"], "ast_kind": "function_or_method", "text": "        def test_cross_origin_header_dropping(self):\n            safe_headers = {\"A\": \"B\"}\n            cookie_header = {\"Cookie\": \"a=b\"}\n            authorization_header = {\"Authorization\": \"Bearer 123456\"}\n\n            original_request = Request(\n                \"https://example.com\",\n                headers={**safe_headers, **cookie_header, **authorization_header},\n            )\n\n            # Redirects to the same origin (same scheme, same domain, same port)\n            # keep all headers.\n            internal_response = self.get_response(\n                original_request, \"https://example.com/a\"\n            )\n            internal_redirect_request = self.mw.process_response(\n                original_request, internal_response\n            )\n            assert isinstance(internal_redirect_request, Request)\n            assert original_request.headers == internal_redirect_request.headers\n\n            # Redirects to the same origin (same scheme, same domain, same port)\n            # keep all headers also when the scheme is http.\n            http_request = Request(\n                \"http://example.com\",\n                headers={**safe_headers, **cookie_header, **authorization_header},\n            )\n            http_response = self.get_response(http_request, \"http://example.com/a\")\n            http_redirect_request = self.mw.process_response(\n                http_request, http_response\n            )\n            assert isinstance(http_redirect_request, Request)\n            assert http_request.headers == http_redirect_request.headers\n\n            # For default ports, whether the port is explicit or implicit does not\n            # affect the outcome, it is still the same origin.\n            to_explicit_port_response = self.get_response(\n                original_request, \"https://example.com:443/a\"\n            )\n            to_explicit_port_redirect_request = self.mw.process_response(\n                original_request, to_explicit_port_response\n            )\n            assert isinstance(to_explicit_port_redirect_request, Request)\n            assert original_request.headers == to_explicit_port_redirect_request.headers\n\n            # For default ports, whether the port is explicit or implicit does not\n            # affect the outcome, it is still the same origin.\n            to_implicit_port_response = self.get_response(\n                original_request, \"https://example.com/a\"\n            )\n            to_implicit_port_redirect_request = self.mw.process_response(\n                original_request, to_implicit_port_response\n            )\n            assert isinstance(to_implicit_port_redirect_request, Request)\n            assert original_request.headers == to_implicit_port_redirect_request.headers\n\n            # A port change drops the Authorization header because the origin\n            # changes, but keeps the Cookie header because the domain remains the\n            # same.\n            different_port_response = self.get_response(\n                original_request, \"https://example.com:8080/a\"\n            )\n            different_port_redirect_request = self.mw.process_response(\n                original_request, different_port_response\n            )\n            assert isinstance(different_port_redirect_request, Request)\n            assert {\n                **safe_headers,\n                **cookie_header,\n            } == different_port_redirect_request.headers.to_unicode_dict()\n\n            # A domain change drops both the Authorization and the Cookie header.\n            external_response = self.get_response(\n                original_request, \"https://example.org/a\"\n            )\n            external_redirect_request = self.mw.process_response(\n                original_request, external_response\n            )\n            assert isinstance(external_redirect_request, Request)\n            assert safe_headers == external_redirect_request.headers.to_unicode_dict()\n\n            # A scheme upgrade (http → https) drops the Authorization header\n            # because the origin changes, but keeps the Cookie header because the\n            # domain remains the same.\n            upgrade_response = self.get_response(http_request, \"https://example.com/a\")\n            upgrade_redirect_request = self.mw.process_response(\n                http_request, upgrade_response\n            )\n            assert isinstance(upgrade_redirect_request, Request)\n            assert {\n                **safe_headers,\n                **cookie_header,\n            } == upgrade_redirect_request.headers.to_unicode_dict()\n\n            # A scheme downgrade (https → http) drops the Authorization header\n            # because the origin changes, and the Cookie header because its value\n            # cannot indicate whether the cookies were secure (HTTPS-only) or not.\n            #\n            # Note: If the Cookie header is set by the cookie management\n            # middleware, as recommended in the docs, the dropping of Cookie on\n            # scheme downgrade is not an issue, because the cookie management\n            # middleware will add again the Cookie header to the new request if\n            # appropriate.\n            downgrade_response = self.get_response(\n                original_request, \"http://example.com/a\"\n            )\n            downgrade_redirect_request = self.mw.process_response(\n                original_request, downgrade_response\n            )\n            assert isinstance(downgrade_redirect_request, Request)\n            assert safe_headers == downgrade_redirect_request.headers.to_unicode_dict()\n", "n_tokens": 965, "byte_len": 5563, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 112, "end_line": 223}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 3, "symbols": ["test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "redirect", "authorization", "proxy", "test", "meta", "headers", "https", "example", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "assert", "response2", "isinstance", "request1", "response1", "basic", "self", "http", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "test_system_proxy_unproxied_http_to_unproxied_https", "test_system_proxy_proxied_https_to_proxied_http", "test_system_proxy_proxied_https_to_unproxied_http"], "ast_kind": "function_or_method", "text": "        def test_meta_proxy_http_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"http://example.com\", meta=meta)\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_meta_proxy_http_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"http://example.com\", meta=meta)\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n", "n_tokens": 842, "byte_len": 3827, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 224, "end_line": 305}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 4, "symbols": ["test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "redirect", "authorization", "proxy", "meta", "test", "headers", "https", "example", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "assert", "response2", "isinstance", "request1", "response1", "basic", "self", "http", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "test_system_proxy_unproxied_http_to_unproxied_https", "test_system_proxy_proxied_https_to_proxied_http", "test_system_proxy_proxied_https_to_unproxied_http"], "ast_kind": "function_or_method", "text": "        def test_meta_proxy_https_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"https://example.com\", meta=meta)\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_meta_proxy_https_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"https://example.com\", meta=meta)\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n", "n_tokens": 842, "byte_len": 3833, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 306, "end_line": 387}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 5, "symbols": ["test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "redirect", "authorization", "proxy", "meta", "test", "headers", "https", "example", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "assert", "response2", "isinstance", "request1", "response1", "basic", "self", "http", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "test_system_proxy_unproxied_http_to_unproxied_https", "test_system_proxy_proxied_https_to_proxied_http", "test_system_proxy_proxied_https_to_unproxied_http"], "ast_kind": "function_or_method", "text": "        def test_meta_proxy_http_to_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"http://example.com\", meta=meta)\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_meta_proxy_https_to_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"https://example.com\", meta=meta)\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n", "n_tokens": 850, "byte_len": 3862, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 388, "end_line": 469}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 6, "symbols": ["test_system_proxy_http_absolute", "test_system_proxy_http_relative", "redirect", "authorization", "proxy", "test", "system", "meta", "with", "http", "headers", "https", "example", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "set", "environ", "assert", "response2", "isinstance", "request1", "response1", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "test_system_proxy_unproxied_http_to_unproxied_https"], "ast_kind": "function_or_method", "text": "        def test_system_proxy_http_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_system_proxy_http_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n", "n_tokens": 860, "byte_len": 3961, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 470, "end_line": 557}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 7, "symbols": ["test_system_proxy_https_absolute", "test_system_proxy_https_relative", "https", "proxy", "redirect", "test", "system", "authorization", "meta", "with", "headers", "example", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "set", "environ", "assert", "response2", "isinstance", "request1", "response1", "basic", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "test_system_proxy_unproxied_http_to_unproxied_https"], "ast_kind": "function_or_method", "text": "        def test_system_proxy_https_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_system_proxy_https_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n", "n_tokens": 860, "byte_len": 3969, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 558, "end_line": 645}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 8, "symbols": ["test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https", "test_system_proxy_unproxied_http_to_proxied_https", "https", "proxy", "redirect", "authorization", "test", "system", "meta", "with", "http", "example", "headers", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "set", "environ", "assert", "response2", "isinstance", "request1", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_unproxied_http_to_unproxied_https", "test_system_proxy_proxied_https_to_proxied_http"], "ast_kind": "function_or_method", "text": "        def test_system_proxy_proxied_http_to_proxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic Yjo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://b.example\"\n            assert request2.meta[\"proxy\"] == \"https://b.example\"\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_system_proxy_proxied_http_to_unproxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request1.meta[\"proxy\"] == \"https://a.example\"\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request3.meta[\"proxy\"] == \"https://a.example\"\n\n        def test_system_proxy_unproxied_http_to_proxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert \"Proxy-Authorization\" not in request1.headers\n            assert \"_auth_proxy\" not in request1.meta\n            assert \"proxy\" not in request1.meta\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic Yjo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://b.example\"\n            assert request2.meta[\"proxy\"] == \"https://b.example\"\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n", "n_tokens": 1225, "byte_len": 5696, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 646, "end_line": 778}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#9", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 9, "symbols": ["test_system_proxy_unproxied_http_to_unproxied_https", "test_system_proxy_proxied_https_to_proxied_http", "test_system_proxy_proxied_https_to_unproxied_http", "https", "proxy", "test", "system", "redirect", "authorization", "meta", "with", "http", "headers", "example", "auth", "request", "request3", "get", "crawler", "from", "mwcls", "process", "response", "request2", "set", "environ", "assert", "response2", "isinstance", "request1", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https"], "ast_kind": "function_or_method", "text": "        def test_system_proxy_unproxied_http_to_unproxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert \"Proxy-Authorization\" not in request1.headers\n            assert \"_auth_proxy\" not in request1.meta\n            assert \"proxy\" not in request1.meta\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n        def test_system_proxy_proxied_https_to_proxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic Yjo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://b.example\"\n            assert request1.meta[\"proxy\"] == \"https://b.example\"\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic Yjo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://b.example\"\n            assert request3.meta[\"proxy\"] == \"https://b.example\"\n\n        def test_system_proxy_proxied_https_to_unproxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert request1.headers[\"Proxy-Authorization\"] == b\"Basic Yjo=\"\n            assert request1.meta[\"_auth_proxy\"] == \"https://b.example\"\n            assert request1.meta[\"proxy\"] == \"https://b.example\"\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert request3.headers[\"Proxy-Authorization\"] == b\"Basic Yjo=\"\n            assert request3.meta[\"_auth_proxy\"] == \"https://b.example\"\n            assert request3.meta[\"proxy\"] == \"https://b.example\"\n", "n_tokens": 1186, "byte_len": 5526, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 779, "end_line": 907}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#10", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 10, "symbols": ["test_system_proxy_unproxied_https_to_proxied_http", "test_system_proxy_unproxied_https_to_unproxied_http", "setup_method", "get_response", "test_redirect_3xx_permanent", "_test", "test_redirect_302_head", "TestRedirectMiddleware", "default", "spider", "test", "method", "post", "redirect", "authorization", "proxy", "req", "req2", "return", "system", "meta", "class", "base", "ignored", "with", "http", "headers", "https", "example", "url", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https"], "ast_kind": "class_or_type", "text": "        def test_system_proxy_unproxied_https_to_proxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert \"Proxy-Authorization\" not in request1.headers\n            assert \"_auth_proxy\" not in request1.meta\n            assert \"proxy\" not in request1.meta\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert request2.headers[\"Proxy-Authorization\"] == b\"Basic YTo=\"\n            assert request2.meta[\"_auth_proxy\"] == \"https://a.example\"\n            assert request2.meta[\"proxy\"] == \"https://a.example\"\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n        def test_system_proxy_unproxied_https_to_unproxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            proxy_mw.process_request(request1)\n\n            assert \"Proxy-Authorization\" not in request1.headers\n            assert \"_auth_proxy\" not in request1.meta\n            assert \"proxy\" not in request1.meta\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1)\n\n            assert isinstance(request2, Request)\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            proxy_mw.process_request(request2)\n\n            assert \"Proxy-Authorization\" not in request2.headers\n            assert \"_auth_proxy\" not in request2.meta\n            assert \"proxy\" not in request2.meta\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2)\n\n            assert isinstance(request3, Request)\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n            proxy_mw.process_request(request3)\n\n            assert \"Proxy-Authorization\" not in request3.headers\n            assert \"_auth_proxy\" not in request3.meta\n            assert \"proxy\" not in request3.meta\n\n\nclass TestRedirectMiddleware(Base.Test):\n    mwcls = RedirectMiddleware\n    reason = 302\n\n    def setup_method(self):\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        self.mw = self.mwcls.from_crawler(crawler)\n\n    def get_response(self, request, location, status=302):\n        headers = {\"Location\": location}\n        return Response(request.url, status=status, headers=headers)\n\n    def test_redirect_3xx_permanent(self):\n        def _test(method, status=301):\n            url = f\"http://www.example.com/{status}\"\n            url2 = \"http://www.example.com/redirected\"\n            req = Request(url, method=method)\n            rsp = Response(url, headers={\"Location\": url2}, status=status)\n\n            req2 = self.mw.process_response(req, rsp)\n            assert isinstance(req2, Request)\n            assert req2.url == url2\n            assert req2.method == method\n\n            # response without Location header but with status code is 3XX should be ignored\n            del rsp.headers[\"Location\"]\n            assert self.mw.process_response(req, rsp) is rsp\n\n        _test(\"GET\")\n        _test(\"POST\")\n        _test(\"HEAD\")\n\n        _test(\"GET\", status=307)\n        _test(\"POST\", status=307)\n        _test(\"HEAD\", status=307)\n\n        _test(\"GET\", status=308)\n        _test(\"POST\", status=308)\n        _test(\"HEAD\", status=308)\n\n    def test_redirect_302_head(self):\n        url = \"http://www.example.com/302\"\n        url2 = \"http://www.example.com/redirected2\"\n        req = Request(url, method=\"HEAD\")\n        rsp = Response(url, headers={\"Location\": url2}, status=302)\n\n        req2 = self.mw.process_response(req, rsp)\n        assert isinstance(req2, Request)\n        assert req2.url == url2\n        assert req2.method == \"HEAD\"\n", "n_tokens": 1171, "byte_len": 5296, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 908, "end_line": 1044}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#11", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 11, "symbols": ["test_redirect_302_relative", "test_spider_handling", "test_request_meta_handling", "_test_passthrough", "test_latin1_location", "test_utf8_location", "test_no_location", "test_redirect_schemes", "meta_refresh_body", "setup_method", "_body", "get_response", "TestMetaRefreshMiddleware", "encoding", "method", "does", "body", "latin", "latin1", "spider", "newpage", "initial", "target", "handle", "httpstatus", "https", "get", "crawler", "meta", "refresh", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https"], "ast_kind": "class_or_type", "text": "    def test_redirect_302_relative(self):\n        url = \"http://www.example.com/302\"\n        url2 = \"///i8n.example2.com/302\"\n        url3 = \"http://i8n.example2.com/302\"\n        req = Request(url, method=\"HEAD\")\n        rsp = Response(url, headers={\"Location\": url2}, status=302)\n\n        req2 = self.mw.process_response(req, rsp)\n        assert isinstance(req2, Request)\n        assert req2.url == url3\n        assert req2.method == \"HEAD\"\n\n    def test_spider_handling(self):\n        self.mw.crawler.spider.handle_httpstatus_list = [404, 301, 302]\n        url = \"http://www.example.com/301\"\n        url2 = \"http://www.example.com/redirected\"\n        req = Request(url)\n        rsp = Response(url, headers={\"Location\": url2}, status=301)\n        r = self.mw.process_response(req, rsp)\n        assert r is rsp\n\n    def test_request_meta_handling(self):\n        url = \"http://www.example.com/301\"\n        url2 = \"http://www.example.com/redirected\"\n\n        def _test_passthrough(req):\n            rsp = Response(url, headers={\"Location\": url2}, status=301, request=req)\n            r = self.mw.process_response(req, rsp)\n            assert r is rsp\n\n        _test_passthrough(\n            Request(url, meta={\"handle_httpstatus_list\": [404, 301, 302]})\n        )\n        _test_passthrough(Request(url, meta={\"handle_httpstatus_all\": True}))\n\n    def test_latin1_location(self):\n        req = Request(\"http://scrapytest.org/first\")\n        latin1_location = \"/ação\".encode(\"latin1\")  # HTTP historically supports latin1\n        resp = Response(\n            \"http://scrapytest.org/first\",\n            headers={\"Location\": latin1_location},\n            status=302,\n        )\n        req_result = self.mw.process_response(req, resp)\n        perc_encoded_utf8_url = \"http://scrapytest.org/a%E7%E3o\"\n        assert perc_encoded_utf8_url == req_result.url\n\n    def test_utf8_location(self):\n        req = Request(\"http://scrapytest.org/first\")\n        utf8_location = \"/ação\".encode()  # header using UTF-8 encoding\n        resp = Response(\n            \"http://scrapytest.org/first\",\n            headers={\"Location\": utf8_location},\n            status=302,\n        )\n        req_result = self.mw.process_response(req, resp)\n        perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"\n        assert perc_encoded_utf8_url == req_result.url\n\n    def test_no_location(self):\n        request = Request(\"https://example.com\")\n        response = Response(request.url, status=302)\n        assert self.mw.process_response(request, response) is response\n\n\nSCHEME_PARAMS = (\"url\", \"location\", \"target\")\nHTTP_SCHEMES = (\"http\", \"https\")\nNON_HTTP_SCHEMES = (\"data\", \"file\", \"ftp\", \"s3\", \"foo\")\nREDIRECT_SCHEME_CASES = (\n    # http/https → http/https redirects\n    *(\n        (\n            f\"{input_scheme}://example.com/a\",\n            f\"{output_scheme}://example.com/b\",\n            f\"{output_scheme}://example.com/b\",\n        )\n        for input_scheme, output_scheme in product(HTTP_SCHEMES, repeat=2)\n    ),\n    # http/https → data/file/ftp/s3/foo does not redirect\n    *(\n        (\n            f\"{input_scheme}://example.com/a\",\n            f\"{output_scheme}://example.com/b\",\n            None,\n        )\n        for input_scheme in HTTP_SCHEMES\n        for output_scheme in NON_HTTP_SCHEMES\n    ),\n    # http/https → relative redirects\n    *(\n        (\n            f\"{scheme}://example.com/a\",\n            location,\n            f\"{scheme}://example.com/b\",\n        )\n        for scheme in HTTP_SCHEMES\n        for location in (\"//example.com/b\", \"/b\")\n    ),\n    # Note: We do not test data/file/ftp/s3 schemes for the initial URL\n    # because their download handlers cannot return a status code of 3xx.\n)\n\n\n@pytest.mark.parametrize(SCHEME_PARAMS, REDIRECT_SCHEME_CASES)\ndef test_redirect_schemes(url, location, target):\n    crawler = get_crawler(Spider)\n    mw = RedirectMiddleware.from_crawler(crawler)\n    request = Request(url)\n    response = Response(url, headers={\"Location\": location}, status=301)\n    redirect = mw.process_response(request, response)\n    if target is None:\n        assert redirect == response\n    else:\n        assert isinstance(redirect, Request)\n        assert redirect.url == target\n\n\ndef meta_refresh_body(url, interval=5):\n    html = f\"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{interval};url={url}\"/></head></html>\"\"\"\n    return html.encode(\"utf-8\")\n\n\nclass TestMetaRefreshMiddleware(Base.Test):\n    mwcls = MetaRefreshMiddleware\n    reason = \"meta refresh\"\n\n    def setup_method(self):\n        crawler = get_crawler(Spider)\n        self.mw = self.mwcls.from_crawler(crawler)\n\n    def _body(self, interval=5, url=\"http://example.org/newpage\"):\n        return meta_refresh_body(url, interval)\n\n    def get_response(self, request, location):\n        return HtmlResponse(request.url, body=self._body(url=location))\n", "n_tokens": 1188, "byte_len": 4859, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 1045, "end_line": 1180}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py#12", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_redirect.py", "rel_path": "tests/test_downloadermiddleware_redirect.py", "module": "tests.test_downloadermiddleware_redirect", "ext": "py", "chunk_number": 12, "symbols": ["test_meta_refresh", "test_meta_refresh_with_high_interval", "test_meta_refresh_trough_posted_request", "test_ignore_tags_default", "test_ignore_tags_1_x_list", "test_meta_refresh_schemes", "encoding", "method", "does", "body", "possible", "newpage", "spider", "target", "length", "get", "crawler", "meta", "refresh", "test", "pytest", "ignore", "settings", "equiv", "isinstance", "none", "encode", "output", "scheme", "script", "test_priority_adjust", "test_dont_redirect", "test_post", "test_max_redirect_times", "test_ttl", "test_redirect_urls", "test_redirect_reasons", "test_cross_origin_header_dropping", "test_meta_proxy_http_absolute", "test_meta_proxy_http_relative", "test_meta_proxy_https_absolute", "test_meta_proxy_https_relative", "test_meta_proxy_http_to_https", "test_meta_proxy_https_to_http", "test_system_proxy_http_absolute", "test_system_proxy_http_relative", "test_system_proxy_https_absolute", "test_system_proxy_https_relative", "test_system_proxy_proxied_http_to_proxied_https", "test_system_proxy_proxied_http_to_unproxied_https"], "ast_kind": "function_or_method", "text": "    def test_meta_refresh(self):\n        req = Request(url=\"http://example.org\")\n        rsp = HtmlResponse(req.url, body=self._body())\n        req2 = self.mw.process_response(req, rsp)\n        assert isinstance(req2, Request)\n        assert req2.url == \"http://example.org/newpage\"\n\n    def test_meta_refresh_with_high_interval(self):\n        # meta-refresh with high intervals don't trigger redirects\n        req = Request(url=\"http://example.org\")\n        rsp = HtmlResponse(\n            url=\"http://example.org\", body=self._body(interval=1000), encoding=\"utf-8\"\n        )\n        rsp2 = self.mw.process_response(req, rsp)\n        assert rsp is rsp2\n\n    def test_meta_refresh_trough_posted_request(self):\n        req = Request(\n            url=\"http://example.org\",\n            method=\"POST\",\n            body=\"test\",\n            headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n        )\n        rsp = HtmlResponse(req.url, body=self._body())\n        req2 = self.mw.process_response(req, rsp)\n\n        assert isinstance(req2, Request)\n        assert req2.url == \"http://example.org/newpage\"\n        assert req2.method == \"GET\"\n        assert \"Content-Type\" not in req2.headers, (\n            \"Content-Type header must not be present in redirected request\"\n        )\n        assert \"Content-Length\" not in req2.headers, (\n            \"Content-Length header must not be present in redirected request\"\n        )\n        assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n\n    def test_ignore_tags_default(self):\n        req = Request(url=\"http://example.org\")\n        body = (\n            \"\"\"<noscript><meta http-equiv=\"refresh\" \"\"\"\n            \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n        )\n        rsp = HtmlResponse(req.url, body=body.encode())\n        response = self.mw.process_response(req, rsp)\n        assert isinstance(response, Response)\n\n    def test_ignore_tags_1_x_list(self):\n        \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n        settings = {\"METAREFRESH_IGNORE_TAGS\": [\"script\", \"noscript\"]}\n        crawler = get_crawler(Spider, settings)\n        mw = MetaRefreshMiddleware.from_crawler(crawler)\n        req = Request(url=\"http://example.org\")\n        body = (\n            \"\"\"<noscript><meta http-equiv=\"refresh\" \"\"\"\n            \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n        )\n        rsp = HtmlResponse(req.url, body=body.encode())\n        response = mw.process_response(req, rsp)\n        assert isinstance(response, Response)\n\n\n@pytest.mark.parametrize(\n    SCHEME_PARAMS,\n    [\n        *REDIRECT_SCHEME_CASES,\n        # data/file/ftp/s3/foo → * does not redirect\n        *(\n            (\n                f\"{input_scheme}://example.com/a\",\n                f\"{output_scheme}://example.com/b\",\n                None,\n            )\n            for input_scheme in NON_HTTP_SCHEMES\n            for output_scheme in chain(HTTP_SCHEMES, NON_HTTP_SCHEMES)\n        ),\n        # data/file/ftp/s3/foo → relative does not redirect\n        *(\n            (\n                f\"{scheme}://example.com/a\",\n                location,\n                None,\n            )\n            for scheme in NON_HTTP_SCHEMES\n            for location in (\"//example.com/b\", \"/b\")\n        ),\n    ],\n)\ndef test_meta_refresh_schemes(url, location, target):\n    crawler = get_crawler(Spider)\n    mw = MetaRefreshMiddleware.from_crawler(crawler)\n    request = Request(url)\n    response = HtmlResponse(url, body=meta_refresh_body(location))\n    redirect = mw.process_response(request, response)\n    if target is None:\n        assert redirect == response\n    else:\n        assert isinstance(redirect, Request)\n        assert redirect.url == target\n", "n_tokens": 833, "byte_len": 3736, "file_sha1": "75cfff1bc3e3eaa0445b9c9a6075881492ff2ec0", "start_line": 1181, "end_line": 1280}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py", "rel_path": "tests/test_pipeline_files.py", "module": "tests.test_pipeline_files", "ext": "py", "chunk_number": 1, "symbols": ["get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "setup_method", "teardown_method", "TestFilesPipeline", "delete", "loads", "bool", "append", "ftp", "data", "mocked", "download", "port", "path", "mockserver", "mock", "get", "crawler", "username", "pytest", "settings", "google", "item", "none", "join", "test", "pipeline", "adapter", "test_file_path", "test_fs_store", "test_file_not_expired", "test_file_expired", "test_file_cached", "test_file_path_from_item", "file_path", "test_rejects_non_list_file_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline", "test_different_settings_for_different_instances", "test_subclass_attributes_preserved_if_no_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses"], "ast_kind": "class_or_type", "text": "import dataclasses\nimport os\nimport random\nimport time\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom ftplib import FTP\nfrom io import BytesIO\nfrom pathlib import Path\nfrom posixpath import split\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\nfrom typing import Any\nfrom unittest import mock\nfrom urllib.parse import urlparse\n\nimport attr\nimport pytest\nfrom itemadapter import ItemAdapter\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.pipelines.files import (\n    FilesPipeline,\n    FSFilesStore,\n    FTPFilesStore,\n    GCSFilesStore,\n    S3FilesStore,\n)\nfrom scrapy.settings import Settings\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.ftp import MockFTPServer\n\nfrom .test_pipeline_media import _mocked_download_func\n\n\ndef get_gcs_content_and_delete(\n    bucket: Any, path: str\n) -> tuple[bytes, list[dict[str, str]], Any]:\n    from google.cloud import storage  # noqa: PLC0415\n\n    client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob\n\n\ndef get_ftp_content_and_delete(\n    path: str,\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n) -> bytes:\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data: list[bytes] = []\n\n    def buffer_data(data: bytes) -> None:\n        ftp_data.append(data)\n\n    ftp.retrbinary(f\"RETR {path}\", buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return b\"\".join(ftp_data)\n\n\nclass TestFilesPipeline:\n    def setup_method(self):\n        self.tempdir = mkdtemp()\n        settings_dict = {\"FILES_STORE\": self.tempdir}\n        crawler = get_crawler(DefaultSpider, settings_dict=settings_dict)\n        crawler.spider = crawler._create_spider()\n        self.pipeline = FilesPipeline.from_crawler(crawler)\n        self.pipeline.download_func = _mocked_download_func\n        self.pipeline.open_spider()\n\n    def teardown_method(self):\n        rmtree(self.tempdir)\n", "n_tokens": 573, "byte_len": 2480, "file_sha1": "b4a8154ffa4bd11d428ddb0d02792eebb6a041c8", "start_line": 1, "end_line": 92}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py", "rel_path": "tests/test_pipeline_files.py", "module": "tests.test_pipeline_files", "ext": "py", "chunk_number": 2, "symbols": ["test_file_path", "test_fs_store", "test_file_not_expired", "test_file_expired", "full", "pipeline", "image", "images", "result", "dorma", "base", "base64", "patch", "downloaded", "tempdir", "fullpath", "maddiebrown", "status", "mdm", "file", "path", "basedir", "item", "prepare", "request", "test", "with", "inc", "stats", "time", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "setup_method", "teardown_method", "test_file_cached", "test_file_path_from_item", "file_path", "test_rejects_non_list_file_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline", "test_different_settings_for_different_instances", "test_subclass_attributes_preserved_if_no_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses"], "ast_kind": "function_or_method", "text": "    def test_file_path(self):\n        file_path = self.pipeline.file_path\n        assert (\n            file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\"))\n            == \"full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf\"\n        )\n        assert (\n            file_path(\n                Request(\n                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\"\n                )\n            )\n            == \"full/4ce274dd83db0368bafd7e406f382ae088e39219.txt\"\n        )\n        assert (\n            file_path(\n                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")\n            )\n            == \"full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc\"\n        )\n        assert (\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n                )\n            )\n            == \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\"\n        )\n        assert (\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\"))\n            == \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2\"\n        )\n        assert (\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"))\n            == \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\"\n        )\n        assert (\n            file_path(\n                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                info=object(),\n            )\n            == \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\"\n        )\n        assert (\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\"\n                )\n            )\n            == \"full/76c00cef2ef669ae65052661f68d451162829507\"\n        )\n        assert (\n            file_path(\n                Request(\n                    \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\"\n                )\n            )\n            == \"full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png\"\n        )\n\n    def test_fs_store(self):\n        assert isinstance(self.pipeline.store, FSFilesStore)\n        assert self.pipeline.store.basedir == self.tempdir\n\n        path = \"some/image/key.jpg\"\n        fullpath = Path(self.tempdir, \"some\", \"image\", \"key.jpg\")\n        assert self.pipeline.store._get_filesystem_path(path) == fullpath\n\n    @inlineCallbacks\n    def test_file_not_expired(self):\n        item_url = \"http://example.com/file.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\"checksum\": \"abc\", \"last_modified\": time.time()},\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url)],\n            ),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item)\n        assert result[\"files\"][0][\"checksum\"] == \"abc\"\n        assert result[\"files\"][0][\"status\"] == \"uptodate\"\n\n        for p in patchers:\n            p.stop()\n\n    @inlineCallbacks\n    def test_file_expired(self):\n        item_url = \"http://example.com/file2.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\n                    \"checksum\": \"abc\",\n                    \"last_modified\": time.time()\n                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n                },\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url)],\n            ),\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item)\n        assert result[\"files\"][0][\"checksum\"] != \"abc\"\n        assert result[\"files\"][0][\"status\"] == \"downloaded\"\n\n        for p in patchers:\n            p.stop()\n", "n_tokens": 1131, "byte_len": 4558, "file_sha1": "b4a8154ffa4bd11d428ddb0d02792eebb6a041c8", "start_line": 93, "end_line": 220}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py", "rel_path": "tests/test_pipeline_files.py", "module": "tests.test_pipeline_files", "ext": "py", "chunk_number": 3, "symbols": ["test_file_cached", "test_file_path_from_item", "file_path", "test_rejects_non_list_file_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "CustomFilesPipeline", "TestFilesPipelineFieldsMixin", "TestFilesPipelineFieldsDict", "FilesPipelineTestItem", "TestFilesPipelineFieldsItem", "FilesPipelineTestDataClass", "TestFilesPipelineFieldsDataClass", "FilesPipelineTestAttrsItem", "TestFilesPipelineFieldsAttrsItem", "item", "with", "name", "inc", "stats", "mock", "get", "crawler", "pytest", "file", "resul", "isinstance", "files", "pipeline", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "setup_method", "teardown_method", "test_file_path", "test_fs_store", "test_file_not_expired", "test_file_expired", "_generate_fake_settings", "random_string", "_generate_fake_pipeline", "test_different_settings_for_different_instances", "test_subclass_attributes_preserved_if_no_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses", "test_cls_attrs_with_DEFAULT_prefix", "test_user_defined_subclass_default_key_names"], "ast_kind": "class_or_type", "text": "    @inlineCallbacks\n    def test_file_cached(self):\n        item_url = \"http://example.com/file3.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\n                    \"checksum\": \"abc\",\n                    \"last_modified\": time.time()\n                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n                },\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url, flags=[\"cached\"])],\n            ),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item)\n        assert result[\"files\"][0][\"checksum\"] != \"abc\"\n        assert result[\"files\"][0][\"status\"] == \"cached\"\n\n        for p in patchers:\n            p.stop()\n\n    def test_file_path_from_item(self):\n        \"\"\"\n        Custom file path based on item data, overriding default implementation\n        \"\"\"\n\n        class CustomFilesPipeline(FilesPipeline):\n            def file_path(self, request, response=None, info=None, item=None):\n                return f\"full/{item.get('path')}\"\n\n        file_path = CustomFilesPipeline.from_crawler(\n            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n        ).file_path\n        item = {\"path\": \"path-to-store-file\"}\n        request = Request(\"http://example.com\")\n        assert file_path(request, item=item) == \"full/path-to-store-file\"\n\n    @pytest.mark.parametrize(\n        \"bad_type\",\n        [\n            \"http://example.com/file.pdf\",\n            (\"http://example.com/file.pdf\",),\n            {\"url\": \"http://example.com/file.pdf\"},\n            123,\n            None,\n        ],\n    )\n    def test_rejects_non_list_file_urls(self, tmp_path, bad_type):\n        pipeline = FilesPipeline.from_crawler(\n            get_crawler(None, {\"FILES_STORE\": str(tmp_path)})\n        )\n        item = ItemWithFiles()\n        item[\"file_urls\"] = bad_type\n\n        with pytest.raises(TypeError, match=\"file_urls must be a list of URLs\"):\n            list(pipeline.get_media_requests(item, None))\n\n\nclass TestFilesPipelineFieldsMixin(ABC):\n    @property\n    @abstractmethod\n    def item_class(self) -> Any:\n        raise NotImplementedError\n\n    def test_item_fields_default(self, tmp_path):\n        url = \"http://www.example.com/files/1.txt\"\n        item = self.item_class(name=\"item1\", file_urls=[url])\n        pipeline = FilesPipeline.from_crawler(\n            get_crawler(None, {\"FILES_STORE\": tmp_path})\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        assert requests[0].url == url\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        files = ItemAdapter(item).get(\"files\")\n        assert files == [results[0][1]]\n        assert isinstance(item, self.item_class)\n\n    def test_item_fields_override_settings(self, tmp_path):\n        url = \"http://www.example.com/files/1.txt\"\n        item = self.item_class(name=\"item1\", custom_file_urls=[url])\n        pipeline = FilesPipeline.from_crawler(\n            get_crawler(\n                None,\n                {\n                    \"FILES_STORE\": tmp_path,\n                    \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                    \"FILES_RESULT_FIELD\": \"custom_files\",\n                },\n            )\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        assert requests[0].url == url\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        custom_files = ItemAdapter(item).get(\"custom_files\")\n        assert custom_files == [results[0][1]]\n        assert isinstance(item, self.item_class)\n\n\nclass TestFilesPipelineFieldsDict(TestFilesPipelineFieldsMixin):\n    item_class = dict\n\n\nclass FilesPipelineTestItem(Item):\n    name = Field()\n    # default fields\n    file_urls = Field()\n    files = Field()\n    # overridden fields\n    custom_file_urls = Field()\n    custom_files = Field()\n\n\nclass TestFilesPipelineFieldsItem(TestFilesPipelineFieldsMixin):\n    item_class = FilesPipelineTestItem\n\n\n@dataclasses.dataclass\nclass FilesPipelineTestDataClass:\n    name: str\n    # default fields\n    file_urls: list = dataclasses.field(default_factory=list)\n    files: list = dataclasses.field(default_factory=list)\n    # overridden fields\n    custom_file_urls: list = dataclasses.field(default_factory=list)\n    custom_files: list = dataclasses.field(default_factory=list)\n\n\nclass TestFilesPipelineFieldsDataClass(TestFilesPipelineFieldsMixin):\n    item_class = FilesPipelineTestDataClass\n\n\n@attr.s\nclass FilesPipelineTestAttrsItem:\n    name = attr.ib(default=\"\")\n    # default fields\n    file_urls: list[str] = attr.ib(default=list)\n    files: list[dict[str, str]] = attr.ib(default=list)\n    # overridden fields\n    custom_file_urls: list[str] = attr.ib(default=list)\n    custom_files: list[dict[str, str]] = attr.ib(default=list)\n\n\nclass TestFilesPipelineFieldsAttrsItem(TestFilesPipelineFieldsMixin):\n    item_class = FilesPipelineTestAttrsItem\n\n", "n_tokens": 1134, "byte_len": 5280, "file_sha1": "b4a8154ffa4bd11d428ddb0d02792eebb6a041c8", "start_line": 221, "end_line": 378}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py", "rel_path": "tests/test_pipeline_files.py", "module": "tests.test_pipeline_files", "ext": "py", "chunk_number": 4, "symbols": ["_generate_fake_settings", "random_string", "_generate_fake_pipeline", "test_different_settings_for_different_instances", "test_subclass_attributes_preserved_if_no_settings", "test_subclass_attrs_preserved_custom_settings", "test_no_custom_settings_for_subclasses", "test_custom_settings_for_subclasses", "test_custom_settings_and_class_attrs_for_subclasses", "test_cls_attrs_with_DEFAULT_prefix", "TestFilesPipelineCustomSettings", "UserDefinedFilePipeline", "UserDefinedFilesPipeline", "pipe", "ins", "test", "custom", "generate", "fake", "those", "subclasses", "instance", "subclass", "random", "string", "cls", "user", "defined", "files", "result", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "setup_method", "teardown_method", "test_file_path", "test_fs_store", "test_file_not_expired", "test_file_expired", "test_file_cached", "test_file_path_from_item", "file_path", "test_rejects_non_list_file_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "test_user_defined_subclass_default_key_names", "test_file_pipeline_using_pathlike_objects", "test_files_store_constructor_with_pathlike_object", "test_persist"], "ast_kind": "class_or_type", "text": "class TestFilesPipelineCustomSettings:\n    default_cls_settings = {\n        \"EXPIRES\": 90,\n        \"FILES_URLS_FIELD\": \"file_urls\",\n        \"FILES_RESULT_FIELD\": \"files\",\n    }\n    file_cls_attr_settings_map = {\n        (\"EXPIRES\", \"FILES_EXPIRES\", \"expires\"),\n        (\"FILES_URLS_FIELD\", \"FILES_URLS_FIELD\", \"files_urls_field\"),\n        (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\"),\n    }\n\n    def _generate_fake_settings(self, tmp_path, prefix=None):\n        def random_string():\n            return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n\n        settings = {\n            \"FILES_EXPIRES\": random.randint(100, 1000),\n            \"FILES_URLS_FIELD\": random_string(),\n            \"FILES_RESULT_FIELD\": random_string(),\n            \"FILES_STORE\": tmp_path,\n        }\n        if not prefix:\n            return settings\n\n        return {\n            prefix.upper() + \"_\" + k if k != \"FILES_STORE\" else k: v\n            for k, v in settings.items()\n        }\n\n    def _generate_fake_pipeline(self):\n        class UserDefinedFilePipeline(FilesPipeline):\n            EXPIRES = 1001\n            FILES_URLS_FIELD = \"alfa\"\n            FILES_RESULT_FIELD = \"beta\"\n\n        return UserDefinedFilePipeline\n\n    def test_different_settings_for_different_instances(self, tmp_path):\n        \"\"\"\n        If there are different instances with different settings they should keep\n        different settings.\n        \"\"\"\n        custom_settings = self._generate_fake_settings(tmp_path)\n        another_pipeline = FilesPipeline.from_crawler(\n            get_crawler(None, custom_settings)\n        )\n        one_pipeline = FilesPipeline(tmp_path, crawler=get_crawler(None))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            default_value = self.default_cls_settings[pipe_attr]\n            assert getattr(one_pipeline, pipe_attr) == default_value\n            custom_value = custom_settings[settings_attr]\n            assert default_value != custom_value\n            assert getattr(another_pipeline, pipe_ins_attr) == custom_value\n\n    def test_subclass_attributes_preserved_if_no_settings(self, tmp_path):\n        \"\"\"\n        If subclasses override class attributes and there are no special settings those values should be kept.\n        \"\"\"\n        pipe_cls = self._generate_fake_pipeline()\n        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": tmp_path}))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            custom_value = getattr(pipe, pipe_ins_attr)\n            assert custom_value != self.default_cls_settings[pipe_attr]\n            assert getattr(pipe, pipe_ins_attr) == getattr(pipe, pipe_attr)\n\n    def test_subclass_attrs_preserved_custom_settings(self, tmp_path):\n        \"\"\"\n        If file settings are defined but they are not defined for subclass\n        settings should be preserved.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        settings = self._generate_fake_settings(tmp_path)\n        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            value = getattr(pipeline, pipe_ins_attr)\n            setting_value = settings.get(settings_attr)\n            assert value != self.default_cls_settings[pipe_attr]\n            assert value == setting_value\n\n    def test_no_custom_settings_for_subclasses(self, tmp_path):\n        \"\"\"\n        If there are no settings for subclass and no subclass attributes, pipeline should use\n        attributes of base class.\n        \"\"\"\n\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n            get_crawler(None, {\"FILES_STORE\": tmp_path})\n        )\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = self.default_cls_settings.get(pipe_attr.upper())\n            assert getattr(user_pipeline, pipe_ins_attr) == custom_value\n\n    def test_custom_settings_for_subclasses(self, tmp_path):\n        \"\"\"\n        If there are custom settings for subclass and NO class attributes, pipeline should use custom\n        settings.\n        \"\"\"\n\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        prefix = UserDefinedFilesPipeline.__name__.upper()\n        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n            get_crawler(None, settings)\n        )\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            assert custom_value != self.default_cls_settings[pipe_attr]\n            assert getattr(user_pipeline, pipe_inst_attr) == custom_value\n\n    def test_custom_settings_and_class_attrs_for_subclasses(self, tmp_path):\n        \"\"\"\n        If there are custom settings for subclass AND class attributes\n        setting keys are preferred and override attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        prefix = pipeline_cls.__name__.upper()\n        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n        for (\n            pipe_cls_attr,\n            settings_attr,\n            pipe_inst_attr,\n        ) in self.file_cls_attr_settings_map:\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            assert custom_value != self.default_cls_settings[pipe_cls_attr]\n            assert getattr(user_pipeline, pipe_inst_attr) == custom_value\n\n    def test_cls_attrs_with_DEFAULT_prefix(self, tmp_path):", "n_tokens": 1192, "byte_len": 6013, "file_sha1": "b4a8154ffa4bd11d428ddb0d02792eebb6a041c8", "start_line": 379, "end_line": 514}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py", "rel_path": "tests/test_pipeline_files.py", "module": "tests.test_pipeline_files", "ext": "py", "chunk_number": 5, "symbols": ["test_user_defined_subclass_default_key_names", "test_file_pipeline_using_pathlike_objects", "file_path", "test_files_store_constructor_with_pathlike_object", "test_persist", "test_stat", "test_blob_path_consistency", "UserDefinedFilesPipeline", "UserPipe", "CustomFilesPipelineWithPathLikeDir", "TestS3FilesStore", "TestGCSFilesStore", "does", "generate", "fake", "method", "calls", "headers", "subclass", "stream", "name", "user", "defined", "pipe", "files", "result", "cache", "control", "test", "blob", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "setup_method", "teardown_method", "test_file_path", "test_fs_store", "test_file_not_expired", "test_file_expired", "test_file_cached", "test_file_path_from_item", "test_rejects_non_list_file_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline", "test_different_settings_for_different_instances", "test_subclass_attributes_preserved_if_no_settings"], "ast_kind": "class_or_type", "text": "        class UserDefinedFilesPipeline(FilesPipeline):\n            DEFAULT_FILES_RESULT_FIELD = \"this\"\n            DEFAULT_FILES_URLS_FIELD = \"that\"\n\n        pipeline = UserDefinedFilesPipeline.from_crawler(\n            get_crawler(None, {\"FILES_STORE\": tmp_path})\n        )\n        assert (\n            pipeline.files_result_field\n            == UserDefinedFilesPipeline.DEFAULT_FILES_RESULT_FIELD\n        )\n        assert (\n            pipeline.files_urls_field\n            == UserDefinedFilesPipeline.DEFAULT_FILES_URLS_FIELD\n        )\n\n    def test_user_defined_subclass_default_key_names(self, tmp_path):\n        \"\"\"Test situation when user defines subclass of FilesPipeline,\n        but uses attribute names for default pipeline (without prefixing\n        them with pipeline class name).\n        \"\"\"\n        settings = self._generate_fake_settings(tmp_path)\n\n        class UserPipe(FilesPipeline):\n            pass\n\n        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            expected_value = settings.get(settings_attr)\n            assert getattr(pipeline_cls, pipe_inst_attr) == expected_value\n\n    def test_file_pipeline_using_pathlike_objects(self, tmp_path):\n        class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n            def file_path(self, request, response=None, info=None, *, item=None):\n                return Path(\"subdir\") / Path(request.url).name\n\n        pipeline = CustomFilesPipelineWithPathLikeDir.from_crawler(\n            get_crawler(None, {\"FILES_STORE\": tmp_path})\n        )\n        request = Request(\"http://example.com/image01.jpg\")\n        assert pipeline.file_path(request) == Path(\"subdir/image01.jpg\")\n\n    def test_files_store_constructor_with_pathlike_object(self, tmp_path):\n        fs_store = FSFilesStore(tmp_path)\n        assert fs_store.basedir == str(tmp_path)\n\n\n@pytest.mark.requires_botocore\nclass TestS3FilesStore:\n    @inlineCallbacks\n    def test_persist(self):\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}\"\n        buffer = mock.MagicMock()\n        meta = {\"foo\": \"bar\"}\n        path = \"\"\n        content_type = \"image/png\"\n\n        store = S3FilesStore(uri)\n        from botocore.stub import Stubber  # noqa: PLC0415\n\n        with Stubber(store.s3_client) as stub:\n            stub.add_response(\n                \"put_object\",\n                expected_params={\n                    \"ACL\": S3FilesStore.POLICY,\n                    \"Body\": buffer,\n                    \"Bucket\": bucket,\n                    \"CacheControl\": S3FilesStore.HEADERS[\"Cache-Control\"],\n                    \"ContentType\": content_type,\n                    \"Key\": key,\n                    \"Metadata\": meta,\n                },\n                service_response={},\n            )\n\n            yield store.persist_file(\n                path,\n                buffer,\n                info=None,\n                meta=meta,\n                headers={\"Content-Type\": content_type},\n            )\n\n            stub.assert_no_pending_responses()\n            # The call to read does not happen with Stubber\n            assert buffer.method_calls == [mock.call.seek(0)]\n\n    @inlineCallbacks\n    def test_stat(self):\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}\"\n        checksum = \"3187896a9657a28163abb31667df64c8\"\n        last_modified = datetime(2019, 12, 1)\n\n        store = S3FilesStore(uri)\n        from botocore.stub import Stubber  # noqa: PLC0415\n\n        with Stubber(store.s3_client) as stub:\n            stub.add_response(\n                \"head_object\",\n                expected_params={\n                    \"Bucket\": bucket,\n                    \"Key\": key,\n                },\n                service_response={\n                    \"ETag\": f'\"{checksum}\"',\n                    \"LastModified\": last_modified,\n                },\n            )\n\n            file_stats = yield store.stat_file(\"\", info=None)\n            assert file_stats == {\n                \"checksum\": checksum,\n                \"last_modified\": last_modified.timestamp(),\n            }\n\n            stub.assert_no_pending_responses()\n\n\n@pytest.mark.skipif(\n    \"GCS_PROJECT_ID\" not in os.environ, reason=\"GCS_PROJECT_ID not found\"\n)\nclass TestGCSFilesStore:\n    @inlineCallbacks\n    def test_persist(self):\n        uri = os.environ.get(\"GCS_TEST_FILE_URI\")\n        if not uri:\n            pytest.skip(\"No GCS URI available for testing\")\n        data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {\"foo\": \"bar\"}\n        path = \"full/filename\"\n        store = GCSFilesStore(uri)\n        store.POLICY = \"authenticatedRead\"\n        expected_policy = {\"role\": \"READER\", \"entity\": \"allAuthenticatedUsers\"}\n        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n        s = yield store.stat_file(path, info=None)\n        assert \"last_modified\" in s\n        assert \"checksum\" in s\n        assert s[\"checksum\"] == \"cdcda85605e46d0af6110752770dce3c\"\n        u = urlparse(uri)\n        content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:] + path)\n        assert content == data\n        assert blob.metadata == {\"foo\": \"bar\"}\n        assert blob.cache_control == GCSFilesStore.CACHE_CONTROL\n        assert blob.content_type == \"application/octet-stream\"\n        assert expected_policy in acl\n\n    @inlineCallbacks\n    def test_blob_path_consistency(self):\n        \"\"\"Test to make sure that paths used to store files is the same as the one used to get\n        already uploaded files.\n        \"\"\"\n        try:", "n_tokens": 1209, "byte_len": 5674, "file_sha1": "b4a8154ffa4bd11d428ddb0d02792eebb6a041c8", "start_line": 515, "end_line": 672}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_files.py", "rel_path": "tests/test_pipeline_files.py", "module": "tests.test_pipeline_files", "ext": "py", "chunk_number": 6, "symbols": ["test_persist", "_create_item_with_files", "_prepare_request_object", "setup_method", "teardown_method", "test_simple", "test_has_old_init", "__init__", "test_has_from_settings", "from_settings", "test_has_from_crawler_and_init", "from_crawler", "test_files_pipeline_raises_notconfigured_when_files_store_invalid", "TestFTPFileStore", "ItemWithFiles", "TestBuildFromCrawler", "Pipeline", "item", "with", "subclasses", "separate", "stat", "mock", "test", "has", "empty", "dict", "bucket", "ftp", "server", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "test_file_path", "test_fs_store", "test_file_not_expired", "test_file_expired", "test_file_cached", "test_file_path_from_item", "file_path", "test_rejects_non_list_file_urls", "item_class", "test_item_fields_default", "test_item_fields_override_settings", "_generate_fake_settings", "random_string", "_generate_fake_pipeline", "test_different_settings_for_different_instances", "test_subclass_attributes_preserved_if_no_settings", "test_subclass_attrs_preserved_custom_settings"], "ast_kind": "class_or_type", "text": "            import google.cloud.storage  # noqa: F401,PLC0415\n        except ModuleNotFoundError:\n            pytest.skip(\"google-cloud-storage is not installed\")\n        with (\n            mock.patch(\"google.cloud.storage\"),\n            mock.patch(\"scrapy.pipelines.files.time\"),\n        ):\n            uri = \"gs://my_bucket/my_prefix/\"\n            store = GCSFilesStore(uri)\n            store.bucket = mock.Mock()\n            path = \"full/my_data.txt\"\n            yield store.persist_file(\n                path, mock.Mock(), info=None, meta=None, headers=None\n            )\n            yield store.stat_file(path, info=None)\n            expected_blob_path = store.prefix + path\n            store.bucket.blob.assert_called_with(expected_blob_path)\n            store.bucket.get_blob.assert_called_with(expected_blob_path)\n\n\nclass TestFTPFileStore:\n    @inlineCallbacks\n    def test_persist(self):\n        data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {\"foo\": \"bar\"}\n        path = \"full/filename\"\n        with MockFTPServer() as ftp_server:\n            store = FTPFilesStore(ftp_server.url(\"/\"))\n            empty_dict = yield store.stat_file(path, info=None)\n            assert empty_dict == {}\n            yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n            stat = yield store.stat_file(path, info=None)\n            assert \"last_modified\" in stat\n            assert \"checksum\" in stat\n            assert stat[\"checksum\"] == \"d113d66b2ec7258724a268bd88eef6b6\"\n            path = f\"{store.basedir}/{path}\"\n            content = get_ftp_content_and_delete(\n                path,\n                store.host,\n                store.port,\n                store.username,\n                store.password,\n                store.USE_ACTIVE_MODE,\n            )\n        assert data == content\n\n\nclass ItemWithFiles(Item):\n    file_urls = Field()\n    files = Field()\n\n\ndef _create_item_with_files(*files):\n    item = ItemWithFiles()\n    item[\"file_urls\"] = files\n    return item\n\n\ndef _prepare_request_object(item_url, flags=None):\n    return Request(\n        item_url,\n        meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n    )\n\n\n# this is separate from the one in test_pipeline_media.py to specifically test FilesPipeline subclasses\nclass TestBuildFromCrawler:\n    def setup_method(self):\n        self.tempdir = mkdtemp()\n        self.crawler = get_crawler(None, {\"FILES_STORE\": self.tempdir})\n\n    def teardown_method(self):\n        rmtree(self.tempdir)\n\n    def test_simple(self):\n        class Pipeline(FilesPipeline):\n            pass\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 0\n            assert pipe.store\n\n    def test_has_old_init(self):\n        class Pipeline(FilesPipeline):\n            def __init__(self, store_uri, download_func=None, settings=None):\n                super().__init__(store_uri, download_func, settings)\n                self._init_called = True\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 2\n            assert pipe._init_called\n\n    def test_has_from_settings(self):\n        class Pipeline(FilesPipeline):\n            _from_settings_called = False\n\n            @classmethod\n            def from_settings(cls, settings):\n                o = super().from_settings(settings)\n                o._from_settings_called = True\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 3\n            assert pipe.store\n            assert pipe._from_settings_called\n\n    def test_has_from_crawler_and_init(self):\n        class Pipeline(FilesPipeline):\n            _from_crawler_called = False\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                settings = crawler.settings\n                store_uri = settings[\"FILES_STORE\"]\n                o = cls(store_uri, crawler=crawler)\n                o._from_crawler_called = True\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 0\n            assert pipe.store\n            assert pipe._from_crawler_called\n\n\n@pytest.mark.parametrize(\"store\", [None, \"\"])\ndef test_files_pipeline_raises_notconfigured_when_files_store_invalid(store):\n    settings = Settings()\n    settings.clear()\n    settings.set(\"FILES_STORE\", store, priority=\"cmdline\")\n    crawler = get_crawler(settings_dict=settings)\n\n    with pytest.raises(NotConfigured):\n        FilesPipeline.from_crawler(crawler)\n", "n_tokens": 1069, "byte_len": 5130, "file_sha1": "b4a8154ffa4bd11d428ddb0d02792eebb6a041c8", "start_line": 673, "end_line": 820}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py", "rel_path": "tests/test_downloadermiddleware_httpcache.py", "module": "tests.test_downloadermiddleware_httpcache", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "teardown_method", "_get_settings", "_get_crawler", "_storage", "_middleware", "assertEqualResponse", "assertEqualRequest", "assertEqualRequestButWithCacheValidators", "test_storage", "test_storage_never_expire", "test_storage_no_content_type_header", "TestBase", "StorageTestMixin", "PolicyTestMixin", "containing", "agent", "test", "storage", "spider", "assert", "equal", "sleep", "httpcach", "enabled", "spiders", "future", "typ", "checking", "contextlib", "test_dont_cache", "test_middleware", "test_different_request_response_urls", "test_middleware_ignore_missing", "test_middleware_ignore_schemes", "test_middleware_ignore_http_codes", "_process_requestresponse", "test_request_cacheability", "test_response_cacheability", "test_cached_and_fresh", "test_cached_and_stale", "test_process_exception", "test_ignore_response_cache_controls", "test_custom_dbm_module_loaded", "DummyPolicyTestMixin", "RFC2616PolicyTestMixin", "TestFilesystemStorageWithDummyPolicy", "TestFilesystemStorageWithRFC2616Policy", "TestDbmStorageWithDummyPolicy", "TestDbmStorageWithRFC2616Policy"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport email.utils\nimport shutil\nimport tempfile\nimport time\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\n\nfrom scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from collections.abc import Generator\n\n    from scrapy.crawler import Crawler\n\n\nclass TestBase:\n    \"\"\"Base class with common setup and helper methods.\"\"\"\n\n    policy_class: str\n    storage_class: str\n\n    def setup_method(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n        self.response = Response(\n            \"http://www.example.com\",\n            headers={\"Content-Type\": \"text/html\"},\n            body=b\"test body\",\n            status=202,\n        )\n\n    def teardown_method(self):\n        shutil.rmtree(self.tmpdir)\n\n    def _get_settings(self, **new_settings: Any) -> dict[str, Any]:\n        settings = {\n            \"HTTPCACHE_ENABLED\": True,\n            \"HTTPCACHE_DIR\": self.tmpdir,\n            \"HTTPCACHE_EXPIRATION_SECS\": 1,\n            \"HTTPCACHE_IGNORE_HTTP_CODES\": [],\n            \"HTTPCACHE_POLICY\": self.policy_class,\n            \"HTTPCACHE_STORAGE\": self.storage_class,\n        }\n        settings.update(new_settings)\n        return settings\n\n    @contextmanager\n    def _get_crawler(self, **new_settings: Any) -> Generator[Crawler]:\n        settings = self._get_settings(**new_settings)\n        crawler = get_crawler(Spider, settings)\n        crawler.spider = crawler._create_spider(\"example.com\")\n        assert crawler.stats\n        crawler.stats.open_spider()\n        try:\n            yield crawler\n        finally:\n            crawler.stats.close_spider()\n\n    @contextmanager\n    def _storage(self, **new_settings: Any):\n        with self._middleware(**new_settings) as mw:\n            yield mw.storage, mw.crawler\n\n    @contextmanager\n    def _middleware(self, **new_settings: Any) -> Generator[HttpCacheMiddleware]:\n        with self._get_crawler(**new_settings) as crawler:\n            assert crawler.spider\n            mw = HttpCacheMiddleware.from_crawler(crawler)\n            mw.spider_opened(crawler.spider)\n            try:\n                yield mw\n            finally:\n                mw.spider_closed(crawler.spider)\n\n    def assertEqualResponse(self, response1, response2):\n        assert response1.url == response2.url\n        assert response1.status == response2.status\n        assert response1.headers == response2.headers\n        assert response1.body == response2.body\n\n    def assertEqualRequest(self, request1, request2):\n        assert request1.url == request2.url\n        assert request1.headers == request2.headers\n        assert request1.body == request2.body\n\n    def assertEqualRequestButWithCacheValidators(self, request1, request2):\n        assert request1.url == request2.url\n        assert b\"If-None-Match\" not in request1.headers\n        assert b\"If-Modified-Since\" not in request1.headers\n        assert any(\n            h in request2.headers for h in (b\"If-None-Match\", b\"If-Modified-Since\")\n        )\n        assert request1.body == request2.body\n\n\nclass StorageTestMixin:\n    \"\"\"Mixin containing storage-specific test methods.\"\"\"\n\n    def test_storage(self):\n        with self._storage() as (storage, crawler):\n            request2 = self.request.copy()\n            assert storage.retrieve_response(crawler.spider, request2) is None\n\n            storage.store_response(crawler.spider, self.request, self.response)\n            response2 = storage.retrieve_response(crawler.spider, request2)\n            assert isinstance(response2, HtmlResponse)  # content-type header\n            self.assertEqualResponse(self.response, response2)\n\n            time.sleep(2)  # wait for cache to expire\n            assert storage.retrieve_response(crawler.spider, request2) is None\n\n    def test_storage_never_expire(self):\n        with self._storage(HTTPCACHE_EXPIRATION_SECS=0) as (storage, crawler):\n            assert storage.retrieve_response(crawler.spider, self.request) is None\n            storage.store_response(crawler.spider, self.request, self.response)\n            time.sleep(0.5)  # give the chance to expire\n            assert storage.retrieve_response(crawler.spider, self.request)\n\n    def test_storage_no_content_type_header(self):\n        \"\"\"Test that the response body is used to get the right response class\n        even if there is no Content-Type header\"\"\"\n        with self._storage() as (storage, crawler):\n            assert storage.retrieve_response(crawler.spider, self.request) is None\n            response = Response(\n                \"http://www.example.com\",\n                body=b\"<!DOCTYPE html>\\n<title>.</title>\",\n                status=202,\n            )\n            storage.store_response(crawler.spider, self.request, response)\n            cached_response = storage.retrieve_response(crawler.spider, self.request)\n            assert isinstance(cached_response, HtmlResponse)\n            self.assertEqualResponse(response, cached_response)\n\n\nclass PolicyTestMixin:\n    \"\"\"Mixin containing policy-specific test methods.\"\"\"\n", "n_tokens": 1155, "byte_len": 5534, "file_sha1": "61d05a15e758858138473fa90df6a0a0cf131087", "start_line": 1, "end_line": 148}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py", "rel_path": "tests/test_downloadermiddleware_httpcache.py", "module": "tests.test_downloadermiddleware_httpcache", "ext": "py", "chunk_number": 2, "symbols": ["test_dont_cache", "test_middleware", "test_different_request_response_urls", "test_middleware_ignore_missing", "test_middleware_ignore_schemes", "test_middleware_ignore_http_codes", "_process_requestresponse", "DummyPolicyTestMixin", "RFC2616PolicyTestMixin", "host", "host2", "policy", "test", "middleware", "false", "containing", "result", "except", "storage", "ignore", "request", "spider", "dummy", "return", "assert", "equal", "responses", "meta", "class", "httpcach", "setup_method", "teardown_method", "_get_settings", "_get_crawler", "_storage", "_middleware", "assertEqualResponse", "assertEqualRequest", "assertEqualRequestButWithCacheValidators", "test_storage", "test_storage_never_expire", "test_storage_no_content_type_header", "test_request_cacheability", "test_response_cacheability", "test_cached_and_fresh", "test_cached_and_stale", "test_process_exception", "test_ignore_response_cache_controls", "test_custom_dbm_module_loaded", "TestBase"], "ast_kind": "class_or_type", "text": "    def test_dont_cache(self):\n        with self._middleware() as mw:\n            self.request.meta[\"dont_cache\"] = True\n            mw.process_response(self.request, self.response)\n            assert mw.storage.retrieve_response(mw.crawler.spider, self.request) is None\n\n        with self._middleware() as mw:\n            self.request.meta[\"dont_cache\"] = False\n            mw.process_response(self.request, self.response)\n            if mw.policy.should_cache_response(self.response, self.request):\n                assert isinstance(\n                    mw.storage.retrieve_response(mw.crawler.spider, self.request),\n                    self.response.__class__,\n                )\n\n\nclass DummyPolicyTestMixin(PolicyTestMixin):\n    \"\"\"Mixin containing dummy policy specific test methods.\"\"\"\n\n    def test_middleware(self):\n        with self._middleware() as mw:\n            assert mw.process_request(self.request) is None\n            mw.process_response(self.request, self.response)\n            response = mw.process_request(self.request)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert \"cached\" in response.flags\n\n    def test_different_request_response_urls(self):\n        with self._middleware() as mw:\n            req = Request(\"http://host.com/path\")\n            res = Response(\"http://host2.net/test.html\")\n            assert mw.process_request(req) is None\n            mw.process_response(req, res)\n            cached = mw.process_request(req)\n            assert isinstance(cached, Response)\n            self.assertEqualResponse(res, cached)\n            assert \"cached\" in cached.flags\n\n    def test_middleware_ignore_missing(self):\n        with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:\n            with pytest.raises(IgnoreRequest):\n                mw.process_request(self.request)\n            mw.process_response(self.request, self.response)\n            response = mw.process_request(self.request)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert \"cached\" in response.flags\n\n    def test_middleware_ignore_schemes(self):\n        # http responses are cached by default\n        req, res = Request(\"http://test.com/\"), Response(\"http://test.com/\")\n        with self._middleware() as mw:\n            assert mw.process_request(req) is None\n            mw.process_response(req, res)\n\n            cached = mw.process_request(req)\n            assert isinstance(cached, Response), type(cached)\n            self.assertEqualResponse(res, cached)\n            assert \"cached\" in cached.flags\n\n        # file response is not cached by default\n        req, res = Request(\"file:///tmp/t.txt\"), Response(\"file:///tmp/t.txt\")\n        with self._middleware() as mw:\n            assert mw.process_request(req) is None\n            mw.process_response(req, res)\n\n            assert mw.storage.retrieve_response(mw.crawler.spider, req) is None\n            assert mw.process_request(req) is None\n\n        # s3 scheme response is cached by default\n        req, res = Request(\"s3://bucket/key\"), Response(\"http://bucket/key\")\n        with self._middleware() as mw:\n            assert mw.process_request(req) is None\n            mw.process_response(req, res)\n\n            cached = mw.process_request(req)\n            assert isinstance(cached, Response), type(cached)\n            self.assertEqualResponse(res, cached)\n            assert \"cached\" in cached.flags\n\n        # ignore s3 scheme\n        req, res = Request(\"s3://bucket/key2\"), Response(\"http://bucket/key2\")\n        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=[\"s3\"]) as mw:\n            assert mw.process_request(req) is None\n            mw.process_response(req, res)\n\n            assert mw.storage.retrieve_response(mw.crawler.spider, req) is None\n            assert mw.process_request(req) is None\n\n    def test_middleware_ignore_http_codes(self):\n        # test response is not cached\n        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202]) as mw:\n            assert mw.process_request(self.request) is None\n            mw.process_response(self.request, self.response)\n\n            assert mw.storage.retrieve_response(mw.crawler.spider, self.request) is None\n            assert mw.process_request(self.request) is None\n\n        # test response is cached\n        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203]) as mw:\n            mw.process_response(self.request, self.response)\n            response = mw.process_request(self.request)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert \"cached\" in response.flags\n\n\nclass RFC2616PolicyTestMixin(PolicyTestMixin):\n    \"\"\"Mixin containing RFC2616 policy specific test methods.\"\"\"\n\n    @staticmethod\n    def _process_requestresponse(\n        mw: HttpCacheMiddleware, request: Request, response: Response | None\n    ) -> Response | Request:\n        result = None\n        try:\n            result = mw.process_request(request)\n            if result:\n                assert isinstance(result, (Request, Response))\n                return result\n            assert response is not None\n            result = mw.process_response(request, response)\n            assert isinstance(result, Response)\n            return result\n        except Exception:\n            print(\"Request\", request)\n            print(\"Response\", response)\n            print(\"Result\", result)\n            raise\n", "n_tokens": 1074, "byte_len": 5555, "file_sha1": "61d05a15e758858138473fa90df6a0a0cf131087", "start_line": 149, "end_line": 279}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py", "rel_path": "tests/test_downloadermiddleware_httpcache.py", "module": "tests.test_downloadermiddleware_httpcache", "ext": "py", "chunk_number": 3, "symbols": ["test_request_cacheability", "test_response_cacheability", "test", "response", "validation", "resc", "assert", "equal", "responses", "res", "res0b", "missing", "expiration", "expires", "none", "without", "http", "always", "res304", "obey", "sends", "res5", "cache", "spider", "return", "servers", "headers", "example", "expect", "unconditionally", "setup_method", "teardown_method", "_get_settings", "_get_crawler", "_storage", "_middleware", "assertEqualResponse", "assertEqualRequest", "assertEqualRequestButWithCacheValidators", "test_storage", "test_storage_never_expire", "test_storage_no_content_type_header", "test_dont_cache", "test_middleware", "test_different_request_response_urls", "test_middleware_ignore_missing", "test_middleware_ignore_schemes", "test_middleware_ignore_http_codes", "_process_requestresponse", "test_cached_and_fresh"], "ast_kind": "function_or_method", "text": "    def test_request_cacheability(self):\n        res0 = Response(\n            self.request.url, status=200, headers={\"Expires\": self.tomorrow}\n        )\n        req0 = Request(\"http://example.com\")\n        req1 = req0.replace(headers={\"Cache-Control\": \"no-store\"})\n        req2 = req0.replace(headers={\"Cache-Control\": \"no-cache\"})\n        with self._middleware() as mw:\n            # response for a request with no-store must not be cached\n            res1 = self._process_requestresponse(mw, req1, res0)\n            self.assertEqualResponse(res1, res0)\n            assert mw.storage.retrieve_response(mw.crawler.spider, req1) is None\n            # Re-do request without no-store and expect it to be cached\n            res2 = self._process_requestresponse(mw, req0, res0)\n            assert \"cached\" not in res2.flags\n            res3 = mw.process_request(req0)\n            assert \"cached\" in res3.flags\n            self.assertEqualResponse(res2, res3)\n            # request with no-cache directive must not return cached response\n            # but it allows new response to be stored\n            res0b = res0.replace(body=b\"foo\")\n            res4 = self._process_requestresponse(mw, req2, res0b)\n            self.assertEqualResponse(res4, res0b)\n            assert \"cached\" not in res4.flags\n            res5 = self._process_requestresponse(mw, req0, None)\n            self.assertEqualResponse(res5, res0b)\n            assert \"cached\" in res5.flags\n\n    def test_response_cacheability(self):\n        responses = [\n            # 304 is not cacheable no matter what servers sends\n            (False, 304, {}),\n            (False, 304, {\"Last-Modified\": self.yesterday}),\n            (False, 304, {\"Expires\": self.tomorrow}),\n            (False, 304, {\"Etag\": \"bar\"}),\n            (False, 304, {\"Cache-Control\": \"max-age=3600\"}),\n            # Always obey no-store cache control\n            (False, 200, {\"Cache-Control\": \"no-store\"}),\n            (False, 200, {\"Cache-Control\": \"no-store, max-age=300\"}),  # invalid\n            (\n                False,\n                200,\n                {\"Cache-Control\": \"no-store\", \"Expires\": self.tomorrow},\n            ),  # invalid\n            # Ignore responses missing expiration and/or validation headers\n            (False, 200, {}),\n            (False, 302, {}),\n            (False, 307, {}),\n            (False, 404, {}),\n            # Cache responses with expiration and/or validation headers\n            (True, 200, {\"Last-Modified\": self.yesterday}),\n            (True, 203, {\"Last-Modified\": self.yesterday}),\n            (True, 300, {\"Last-Modified\": self.yesterday}),\n            (True, 301, {\"Last-Modified\": self.yesterday}),\n            (True, 308, {\"Last-Modified\": self.yesterday}),\n            (True, 401, {\"Last-Modified\": self.yesterday}),\n            (True, 404, {\"Cache-Control\": \"public, max-age=600\"}),\n            (True, 302, {\"Expires\": self.tomorrow}),\n            (True, 200, {\"Etag\": \"foo\"}),\n        ]\n        with self._middleware() as mw:\n            for idx, (shouldcache, status, headers) in enumerate(responses):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                res1 = self._process_requestresponse(mw, req0, res0)\n                res304 = res0.replace(status=304)\n                res2 = self._process_requestresponse(\n                    mw, req0, res304 if shouldcache else res0\n                )\n                self.assertEqualResponse(res1, res0)\n                self.assertEqualResponse(res2, res0)\n                resc = mw.storage.retrieve_response(mw.crawler.spider, req0)\n                if shouldcache:\n                    self.assertEqualResponse(resc, res1)\n                    assert \"cached\" in res2.flags\n                    assert res2.status != 304\n                else:\n                    assert not resc\n                    assert \"cached\" not in res2.flags\n\n        # cache unconditionally unless response contains no-store or is a 304\n        with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:\n            for idx, (_, status, headers) in enumerate(responses):\n                shouldcache = (\n                    \"no-store\" not in headers.get(\"Cache-Control\", \"\") and status != 304\n                )\n                req0 = Request(f\"http://example2-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                res1 = self._process_requestresponse(mw, req0, res0)\n                res304 = res0.replace(status=304)\n                res2 = self._process_requestresponse(\n                    mw, req0, res304 if shouldcache else res0\n                )\n                self.assertEqualResponse(res1, res0)\n                self.assertEqualResponse(res2, res0)\n                resc = mw.storage.retrieve_response(mw.crawler.spider, req0)\n                if shouldcache:\n                    self.assertEqualResponse(resc, res1)\n                    assert \"cached\" in res2.flags\n                    assert res2.status != 304\n                else:\n                    assert not resc\n                    assert \"cached\" not in res2.flags\n", "n_tokens": 1159, "byte_len": 5175, "file_sha1": "61d05a15e758858138473fa90df6a0a0cf131087", "start_line": 280, "end_line": 383}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py", "rel_path": "tests/test_downloadermiddleware_httpcache.py", "module": "tests.test_downloadermiddleware_httpcache", "ext": "py", "chunk_number": 4, "symbols": ["test_cached_and_fresh", "year", "yesterday", "present", "last", "cache", "res", "res1", "network", "greater", "response", "return", "over", "res0", "replace", "tomorrow", "assert", "equal", "fresh", "with", "default", "res3", "headers", "redirects", "date", "example", "sampledata", "today", "test", "cached", "setup_method", "teardown_method", "_get_settings", "_get_crawler", "_storage", "_middleware", "assertEqualResponse", "assertEqualRequest", "assertEqualRequestButWithCacheValidators", "test_storage", "test_storage_never_expire", "test_storage_no_content_type_header", "test_dont_cache", "test_middleware", "test_different_request_response_urls", "test_middleware_ignore_missing", "test_middleware_ignore_schemes", "test_middleware_ignore_http_codes", "_process_requestresponse", "test_request_cacheability"], "ast_kind": "function_or_method", "text": "    def test_cached_and_fresh(self):\n        sampledata = [\n            (200, {\"Date\": self.yesterday, \"Expires\": self.tomorrow}),\n            (200, {\"Date\": self.yesterday, \"Cache-Control\": \"max-age=86405\"}),\n            (200, {\"Age\": \"299\", \"Cache-Control\": \"max-age=300\"}),\n            # Obey max-age if present over any others\n            (\n                200,\n                {\n                    \"Date\": self.today,\n                    \"Age\": \"86405\",\n                    \"Cache-Control\": \"max-age=\" + str(86400 * 3),\n                    \"Expires\": self.yesterday,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            # obey Expires if max-age is not present\n            (\n                200,\n                {\n                    \"Date\": self.yesterday,\n                    \"Age\": \"86400\",\n                    \"Cache-Control\": \"public\",\n                    \"Expires\": self.tomorrow,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            # Default missing Date header to right now\n            (200, {\"Expires\": self.tomorrow}),\n            # Firefox - Expires if age is greater than 10% of (Date - Last-Modified)\n            (\n                200,\n                {\n                    \"Date\": self.today,\n                    \"Last-Modified\": self.yesterday,\n                    \"Age\": str(86400 / 10 - 1),\n                },\n            ),\n            # Firefox - Set one year maxage to permanent redirects missing expiration info\n            (300, {}),\n            (301, {}),\n            (308, {}),\n        ]\n        with self._middleware() as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                # cache fresh response\n                res1 = self._process_requestresponse(mw, req0, res0)\n                self.assertEqualResponse(res1, res0)\n                assert \"cached\" not in res1.flags\n                # return fresh cached response without network interaction\n                res2 = self._process_requestresponse(mw, req0, None)\n                self.assertEqualResponse(res1, res2)\n                assert \"cached\" in res2.flags\n                # validate cached response if request max-age set as 0\n                req1 = req0.replace(headers={\"Cache-Control\": \"max-age=0\"})\n                res304 = res0.replace(status=304)\n                assert mw.process_request(req1) is None\n                res3 = self._process_requestresponse(mw, req1, res304)\n                self.assertEqualResponse(res1, res3)\n                assert \"cached\" in res3.flags\n", "n_tokens": 569, "byte_len": 2726, "file_sha1": "61d05a15e758858138473fa90df6a0a0cf131087", "start_line": 384, "end_line": 446}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py", "rel_path": "tests/test_downloadermiddleware_httpcache.py", "module": "tests.test_downloadermiddleware_httpcache", "ext": "py", "chunk_number": 5, "symbols": ["test_cached_and_stale", "test_process_exception", "revalidation", "assert", "equal", "responses", "res", "res0b", "requests", "succeed", "expiration", "expires", "none", "server", "encountering", "http", "response", "res5", "tag", "etag", "cache", "present", "process", "exception", "res0c", "downloa", "exceptions", "stale", "download", "headers", "setup_method", "teardown_method", "_get_settings", "_get_crawler", "_storage", "_middleware", "assertEqualResponse", "assertEqualRequest", "assertEqualRequestButWithCacheValidators", "test_storage", "test_storage_never_expire", "test_storage_no_content_type_header", "test_dont_cache", "test_middleware", "test_different_request_response_urls", "test_middleware_ignore_missing", "test_middleware_ignore_schemes", "test_middleware_ignore_http_codes", "_process_requestresponse", "test_request_cacheability"], "ast_kind": "function_or_method", "text": "    def test_cached_and_stale(self):\n        sampledata = [\n            (200, {\"Date\": self.today, \"Expires\": self.yesterday}),\n            (\n                200,\n                {\n                    \"Date\": self.today,\n                    \"Expires\": self.yesterday,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            (200, {\"Expires\": self.yesterday}),\n            (200, {\"Expires\": self.yesterday, \"ETag\": \"foo\"}),\n            (200, {\"Expires\": self.yesterday, \"Last-Modified\": self.yesterday}),\n            (200, {\"Expires\": self.tomorrow, \"Age\": \"86405\"}),\n            (200, {\"Cache-Control\": \"max-age=86400\", \"Age\": \"86405\"}),\n            # no-cache forces expiration, also revalidation if validators exists\n            (200, {\"Cache-Control\": \"no-cache\"}),\n            (200, {\"Cache-Control\": \"no-cache\", \"ETag\": \"foo\"}),\n            (200, {\"Cache-Control\": \"no-cache\", \"Last-Modified\": self.yesterday}),\n            (\n                200,\n                {\n                    \"Cache-Control\": \"no-cache,must-revalidate\",\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            (\n                200,\n                {\n                    \"Cache-Control\": \"must-revalidate\",\n                    \"Expires\": self.yesterday,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            (200, {\"Cache-Control\": \"max-age=86400,must-revalidate\", \"Age\": \"86405\"}),\n        ]\n        with self._middleware() as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0a = Response(req0.url, status=status, headers=headers)\n                # cache expired response\n                res1 = self._process_requestresponse(mw, req0, res0a)\n                self.assertEqualResponse(res1, res0a)\n                assert \"cached\" not in res1.flags\n                # Same request but as cached response is stale a new response must\n                # be returned\n                res0b = res0a.replace(body=b\"bar\")\n                res2 = self._process_requestresponse(mw, req0, res0b)\n                self.assertEqualResponse(res2, res0b)\n                assert \"cached\" not in res2.flags\n                cc = headers.get(\"Cache-Control\", \"\")\n                # Previous response expired too, subsequent request to same\n                # resource must revalidate and succeed on 304 if validators\n                # are present\n                if \"ETag\" in headers or \"Last-Modified\" in headers:\n                    res0c = res0b.replace(status=304)\n                    res3 = self._process_requestresponse(mw, req0, res0c)\n                    self.assertEqualResponse(res3, res0b)\n                    assert \"cached\" in res3.flags\n                    # get cached response on server errors unless must-revalidate\n                    # in cached response\n                    res0d = res0b.replace(status=500)\n                    res4 = self._process_requestresponse(mw, req0, res0d)\n                    if \"must-revalidate\" in cc:\n                        assert \"cached\" not in res4.flags\n                        self.assertEqualResponse(res4, res0d)\n                    else:\n                        assert \"cached\" in res4.flags\n                        self.assertEqualResponse(res4, res0b)\n                # Requests with max-stale can fetch expired cached responses\n                # unless cached response has must-revalidate\n                req1 = req0.replace(headers={\"Cache-Control\": \"max-stale\"})\n                res5 = self._process_requestresponse(mw, req1, res0b)\n                self.assertEqualResponse(res5, res0b)\n                if \"no-cache\" in cc or \"must-revalidate\" in cc:\n                    assert \"cached\" not in res5.flags\n                else:\n                    assert \"cached\" in res5.flags\n\n    def test_process_exception(self):\n        with self._middleware() as mw:\n            res0 = Response(self.request.url, headers={\"Expires\": self.yesterday})\n            req0 = Request(self.request.url)\n            self._process_requestresponse(mw, req0, res0)\n            for e in mw.DOWNLOAD_EXCEPTIONS:\n                # Simulate encountering an error on download attempts\n                assert mw.process_request(req0) is None\n                res1 = mw.process_exception(req0, e(\"foo\"))\n                # Use cached response as recovery\n                assert \"cached\" in res1.flags\n                self.assertEqualResponse(res0, res1)\n            # Do not use cached response for unhandled exceptions\n            mw.process_request(req0)\n            assert mw.process_exception(req0, Exception(\"foo\")) is None\n", "n_tokens": 995, "byte_len": 4745, "file_sha1": "61d05a15e758858138473fa90df6a0a0cf131087", "start_line": 447, "end_line": 542}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpcache.py", "rel_path": "tests/test_downloadermiddleware_httpcache.py", "module": "tests.test_downloadermiddleware_httpcache", "ext": "py", "chunk_number": 6, "symbols": ["test_ignore_response_cache_controls", "_get_settings", "test_custom_dbm_module_loaded", "TestFilesystemStorageWithDummyPolicy", "TestFilesystemStorageWithRFC2616Policy", "TestDbmStorageWithDummyPolicy", "TestDbmStorageWithRFC2616Policy", "TestDbmStorageWithCustomDbmModule", "TestFilesystemStorageGzipWithDummyPolicy", "policy", "test", "assert", "equal", "class", "filesystem", "cache", "make", "concrete", "dbm", "module", "expires", "none", "without", "http", "dummy", "rfc2616policy", "response", "network", "return", "storage", "setup_method", "teardown_method", "_get_crawler", "_storage", "_middleware", "assertEqualResponse", "assertEqualRequest", "assertEqualRequestButWithCacheValidators", "test_storage", "test_storage_never_expire", "test_storage_no_content_type_header", "test_dont_cache", "test_middleware", "test_different_request_response_urls", "test_middleware_ignore_missing", "test_middleware_ignore_schemes", "test_middleware_ignore_http_codes", "_process_requestresponse", "test_request_cacheability", "test_response_cacheability"], "ast_kind": "class_or_type", "text": "    def test_ignore_response_cache_controls(self):\n        sampledata = [\n            (200, {\"Date\": self.yesterday, \"Expires\": self.tomorrow}),\n            (200, {\"Date\": self.yesterday, \"Cache-Control\": \"no-store,max-age=86405\"}),\n            (200, {\"Age\": \"299\", \"Cache-Control\": \"max-age=300,no-cache\"}),\n            (300, {\"Cache-Control\": \"no-cache\"}),\n            (200, {\"Expires\": self.tomorrow, \"Cache-Control\": \"no-store\"}),\n        ]\n        with self._middleware(\n            HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=[\"no-cache\", \"no-store\"]\n        ) as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                # cache fresh response\n                res1 = self._process_requestresponse(mw, req0, res0)\n                self.assertEqualResponse(res1, res0)\n                assert \"cached\" not in res1.flags\n                # return fresh cached response without network interaction\n                res2 = self._process_requestresponse(mw, req0, None)\n                self.assertEqualResponse(res1, res2)\n                assert \"cached\" in res2.flags\n\n\n# Concrete test classes that combine storage and policy mixins\n\n\nclass TestFilesystemStorageWithDummyPolicy(\n    TestBase, StorageTestMixin, DummyPolicyTestMixin\n):\n    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n\n\nclass TestFilesystemStorageWithRFC2616Policy(\n    TestBase, StorageTestMixin, RFC2616PolicyTestMixin\n):\n    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n\nclass TestDbmStorageWithDummyPolicy(TestBase, StorageTestMixin, DummyPolicyTestMixin):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n\n\nclass TestDbmStorageWithRFC2616Policy(\n    TestBase, StorageTestMixin, RFC2616PolicyTestMixin\n):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n\nclass TestDbmStorageWithCustomDbmModule(TestDbmStorageWithDummyPolicy):\n    dbm_module = \"tests.mocks.dummydbm\"\n\n    def _get_settings(self, **new_settings) -> dict[str, Any]:\n        new_settings.setdefault(\"HTTPCACHE_DBM_MODULE\", self.dbm_module)\n        return super()._get_settings(**new_settings)\n\n    def test_custom_dbm_module_loaded(self):\n        # make sure our dbm module has been loaded\n        with self._storage() as (storage, _):\n            assert storage.dbmodule.__name__ == self.dbm_module\n\n\nclass TestFilesystemStorageGzipWithDummyPolicy(TestFilesystemStorageWithDummyPolicy):\n    def _get_settings(self, **new_settings) -> dict[str, Any]:\n        new_settings.setdefault(\"HTTPCACHE_GZIP\", True)\n        return super()._get_settings(**new_settings)\n", "n_tokens": 681, "byte_len": 2982, "file_sha1": "61d05a15e758858138473fa90df6a0a0cf131087", "start_line": 543, "end_line": 613}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py", "rel_path": "tests/test_pipeline_media.py", "module": "tests.test_pipeline_media", "ext": "py", "chunk_number": 1, "symbols": ["_mocked_download_func", "media_to_download", "get_media_requests", "media_downloaded", "media_failed", "file_path", "setup_method", "teardown_method", "test_modify_media_request", "test_should_remove_req_res_references_before_caching_the_results", "UserDefinedPipeline", "TestBaseMediaPipeline", "test", "base", "failure", "does", "method", "async", "those", "call", "later", "scheduled", "signal", "instance", "case", "python", "modify", "name", "callback", "detect", "test_default_item_completed", "test_default_process_item", "__init__", "download", "item_completed", "_errback", "test_result_succeed", "test_result_failure", "test_mix_of_success_and_failure", "test_get_media_requests", "test_results_are_cached_across_multiple_items", "test_results_are_cached_for_requests_of_single_item", "test_wait_if_request_is_downloading", "_check_downloading", "rsp1_func", "rsp2_func", "test_use_media_to_download_result", "test_key_for_pipe", "_assert_request_no3xx", "test_subclass_standard_setting"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import signals\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.files import FileException\nfrom scrapy.pipelines.media import MediaPipeline\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.signal import disconnect_all\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\n\ndef _mocked_download_func(request, info):\n    assert request.callback is NO_CALLBACK\n    response = request.meta.get(\"response\")\n    return response() if callable(response) else response\n\n\nclass UserDefinedPipeline(MediaPipeline):\n    def media_to_download(self, request, info, *, item=None):\n        pass\n\n    def get_media_requests(self, item, info):\n        pass\n\n    def media_downloaded(self, response, request, info, *, item=None):\n        return {}\n\n    def media_failed(self, failure, request, info):\n        failure.raiseException()\n\n    def file_path(self, request, response=None, info=None, *, item=None):\n        return \"\"\n\n\nclass TestBaseMediaPipeline:\n    pipeline_class = UserDefinedPipeline\n    settings = None\n\n    def setup_method(self):\n        crawler = get_crawler(DefaultSpider, self.settings)\n        crawler.spider = crawler._create_spider()\n        self.pipe = self.pipeline_class.from_crawler(crawler)\n        self.pipe.download_func = _mocked_download_func\n        self.pipe.open_spider()\n        self.info = self.pipe.spiderinfo\n        self.fingerprint = crawler.request_fingerprinter.fingerprint\n\n    def teardown_method(self):\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                disconnect_all(signal)\n\n    def test_modify_media_request(self):\n        request = Request(\"http://url\")\n        self.pipe._modify_media_request(request)\n        assert request.meta == {\"handle_httpstatus_all\": True}\n\n    def test_should_remove_req_res_references_before_caching_the_results(self):\n        \"\"\"Regression test case to prevent a memory leak in the Media Pipeline.\n\n        The memory leak is triggered when an exception is raised when a Response\n        scheduled by the Media Pipeline is being returned. For example, when a\n        FileException('download-error') is raised because the Response status\n        code is not 200 OK.\n\n        It happens because we are keeping a reference to the Response object\n        inside the FileException context. This is caused by the way Twisted\n        return values from inline callbacks. It raises a custom exception\n        encapsulating the original return value.\n\n        The solution is to remove the exception context when this context is a\n        _DefGen_Return instance, the BaseException used by Twisted to pass the\n        returned value from those inline callbacks.\n\n        Maybe there's a better and more reliable way to test the case described\n        here, but it would be more complicated and involve running - or at least\n        mocking - some async steps from the Media Pipeline. The current test\n        case is simple and detects the problem very fast. On the other hand, it\n        would not detect another kind of leak happening due to old object\n        references being kept inside the Media Pipeline cache.\n\n        This problem does not occur in Python 2.7 since we don't have Exception\n        Chaining (https://www.python.org/dev/peps/pep-3134/).\n        \"\"\"\n        # Create sample pair of Request and Response objects\n        request = Request(\"http://url\")\n        response = Response(\"http://url\", body=b\"\", request=request)\n\n        # Simulate the Media Pipeline behavior to produce a Twisted Failure\n        try:\n            # Simulate a Twisted inline callback returning a Response\n            raise StopIteration(response)\n        except StopIteration as exc:\n            def_gen_return_exc = exc\n            try:\n                # Simulate the media_downloaded callback raising a FileException\n                # This usually happens when the status code is not 200 OK\n                raise FileException(\"download-error\")\n            except Exception as exc:\n                file_exc = exc\n                # Simulate Twisted capturing the FileException\n                # It encapsulates the exception inside a Twisted Failure\n                failure = Failure(file_exc)\n\n        # The Failure should encapsulate a FileException ...\n        assert failure.value == file_exc\n        # ... and it should have the StopIteration exception set as its context\n        assert failure.value.__context__ == def_gen_return_exc\n\n        # Let's calculate the request fingerprint and fake some runtime data...\n        fp = self.fingerprint(request)\n        info = self.pipe.spiderinfo\n        info.downloading.add(fp)\n        info.waiting[fp] = []\n\n        # When calling the method that caches the Request's result ...\n        self.pipe._cache_result_and_execute_waiters(failure, fp, info)\n        # ... it should store the Twisted Failure ...\n        assert info.downloaded[fp] == failure\n        # ... encapsulating the original FileException ...\n        assert info.downloaded[fp].value == file_exc\n        # ... but it should not store the StopIteration exception on its context\n        context = getattr(info.downloaded[fp].value, \"__context__\", None)\n        assert context is None\n", "n_tokens": 1128, "byte_len": 5640, "file_sha1": "ca0a8b7669dd9ec6ff4653e9c24793ebda5f079b", "start_line": 1, "end_line": 136}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py", "rel_path": "tests/test_pipeline_media.py", "module": "tests.test_pipeline_media", "ext": "py", "chunk_number": 2, "symbols": ["test_default_item_completed", "test_default_process_item", "__init__", "download", "media_to_download", "get_media_requests", "media_downloaded", "media_failed", "item_completed", "_errback", "test_result_succeed", "test_result_failure", "test_mix_of_success_and_failure", "MockedMediaPipeline", "TestMediaPipeline", "faile", "results", "test", "base", "failure", "append", "name", "errback", "isinstance", "none", "media", "failed", "fail", "default", "http", "_mocked_download_func", "file_path", "setup_method", "teardown_method", "test_modify_media_request", "test_should_remove_req_res_references_before_caching_the_results", "test_get_media_requests", "test_results_are_cached_across_multiple_items", "test_results_are_cached_for_requests_of_single_item", "test_wait_if_request_is_downloading", "_check_downloading", "rsp1_func", "rsp2_func", "test_use_media_to_download_result", "test_key_for_pipe", "_assert_request_no3xx", "test_subclass_standard_setting", "test_subclass_specific_setting", "test_simple", "test_has_old_init"], "ast_kind": "class_or_type", "text": "    def test_default_item_completed(self):\n        item = {\"name\": \"name\"}\n        assert self.pipe.item_completed([], item, self.info) is item\n\n        # Check that failures are logged by default\n        fail = Failure(Exception())\n        results = [(True, 1), (False, fail)]\n\n        with LogCapture() as log:\n            new_item = self.pipe.item_completed(results, item, self.info)\n\n        assert new_item is item\n        assert len(log.records) == 1\n        record = log.records[0]\n        assert record.levelname == \"ERROR\"\n        assert record.exc_info == failure_to_exc_info(fail)\n\n        # disable failure logging and check again\n        self.pipe.LOG_FAILED_RESULTS = False\n        with LogCapture() as log:\n            new_item = self.pipe.item_completed(results, item, self.info)\n        assert new_item is item\n        assert len(log.records) == 0\n\n    @inlineCallbacks\n    def test_default_process_item(self):\n        item = {\"name\": \"name\"}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item is item\n\n\nclass MockedMediaPipeline(UserDefinedPipeline):\n    def __init__(self, *args, crawler=None, **kwargs):\n        super().__init__(*args, crawler=crawler, **kwargs)\n        self._mockcalled = []\n\n    def download(self, request, info):\n        self._mockcalled.append(\"download\")\n        return super().download(request, info)\n\n    def media_to_download(self, request, info, *, item=None):\n        self._mockcalled.append(\"media_to_download\")\n        if \"result\" in request.meta:\n            return request.meta.get(\"result\")\n        return super().media_to_download(request, info)\n\n    def get_media_requests(self, item, info):\n        self._mockcalled.append(\"get_media_requests\")\n        return item.get(\"requests\")\n\n    def media_downloaded(self, response, request, info, *, item=None):\n        self._mockcalled.append(\"media_downloaded\")\n        return super().media_downloaded(response, request, info)\n\n    def media_failed(self, failure, request, info):\n        self._mockcalled.append(\"media_failed\")\n        return super().media_failed(failure, request, info)\n\n    def item_completed(self, results, item, info):\n        self._mockcalled.append(\"item_completed\")\n        item = super().item_completed(results, item, info)\n        item[\"results\"] = results\n        return item\n\n\nclass TestMediaPipeline(TestBaseMediaPipeline):\n    pipeline_class = MockedMediaPipeline\n\n    def _errback(self, result):\n        self.pipe._mockcalled.append(\"request_errback\")\n        return result\n\n    @inlineCallbacks\n    def test_result_succeed(self):\n        rsp = Response(\"http://url1\")\n        req = Request(\n            \"http://url1\",\n            meta={\"response\": rsp},\n            errback=self._errback,\n        )\n        item = {\"requests\": req}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item[\"results\"] == [(True, {})]\n        assert self.pipe._mockcalled == [\n            \"get_media_requests\",\n            \"media_to_download\",\n            \"media_downloaded\",\n            \"item_completed\",\n        ]\n\n    @inlineCallbacks\n    def test_result_failure(self):\n        self.pipe.LOG_FAILED_RESULTS = False\n        exc = Exception(\"foo\")\n        fail = Failure(exc)\n        req = Request(\n            \"http://url1\",\n            meta={\"response\": fail},\n            errback=self._errback,\n        )\n        item = {\"requests\": req}\n        new_item = yield self.pipe.process_item(item)\n        assert len(new_item[\"results\"]) == 1\n        assert new_item[\"results\"][0][0] is False\n        assert isinstance(new_item[\"results\"][0][1], Failure)\n        assert new_item[\"results\"][0][1].value == exc\n        assert self.pipe._mockcalled == [\n            \"get_media_requests\",\n            \"media_to_download\",\n            \"media_failed\",\n            \"request_errback\",\n            \"item_completed\",\n        ]\n\n    @inlineCallbacks\n    def test_mix_of_success_and_failure(self):\n        self.pipe.LOG_FAILED_RESULTS = False\n        rsp1 = Response(\"http://url1\")\n        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n        exc = Exception(\"foo\")\n        fail = Failure(exc)\n        req2 = Request(\"http://url2\", meta={\"response\": fail})\n        item = {\"requests\": [req1, req2]}\n        new_item = yield self.pipe.process_item(item)\n        assert len(new_item[\"results\"]) == 2\n        assert new_item[\"results\"][0] == (True, {})\n        assert new_item[\"results\"][1][0] is False\n        assert isinstance(new_item[\"results\"][1][1], Failure)\n        assert new_item[\"results\"][1][1].value == exc\n        m = self.pipe._mockcalled\n        # only once\n        assert m[0] == \"get_media_requests\"  # first hook called\n        assert m.count(\"get_media_requests\") == 1\n        assert m.count(\"item_completed\") == 1\n        assert m[-1] == \"item_completed\"  # last hook called\n        # twice, one per request\n        assert m.count(\"media_to_download\") == 2\n        # one to handle success and other for failure\n        assert m.count(\"media_downloaded\") == 1\n        assert m.count(\"media_failed\") == 1\n", "n_tokens": 1153, "byte_len": 5075, "file_sha1": "ca0a8b7669dd9ec6ff4653e9c24793ebda5f079b", "start_line": 137, "end_line": 277}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py", "rel_path": "tests/test_pipeline_media.py", "module": "tests.test_pipeline_media", "ext": "py", "chunk_number": 3, "symbols": ["test_get_media_requests", "test_results_are_cached_across_multiple_items", "test_results_are_cached_for_requests_of_single_item", "test_wait_if_request_is_downloading", "_check_downloading", "rsp1_func", "rsp2_func", "test_use_media_to_download_result", "test_key_for_pipe", "_assert_request_no3xx", "test_subclass_standard_setting", "test_subclass_specific_setting", "setup_method", "test_simple", "test_has_old_init", "TestMediaPipelineAllowRedirectSettings", "TestBuildFromCrawler", "Pipeline", "donot", "call", "later", "these", "requests", "rsp", "func", "check", "downloading", "get", "crawler", "test", "_mocked_download_func", "media_to_download", "get_media_requests", "media_downloaded", "media_failed", "file_path", "teardown_method", "test_modify_media_request", "test_should_remove_req_res_references_before_caching_the_results", "test_default_item_completed", "test_default_process_item", "__init__", "download", "item_completed", "_errback", "test_result_succeed", "test_result_failure", "test_mix_of_success_and_failure", "test_has_from_settings", "from_settings"], "ast_kind": "class_or_type", "text": "    @inlineCallbacks\n    def test_get_media_requests(self):\n        # returns single Request (without callback)\n        req = Request(\"http://url\")\n        item = {\"requests\": req}  # pass a single item\n        new_item = yield self.pipe.process_item(item)\n        assert new_item is item\n        assert self.fingerprint(req) in self.info.downloaded\n\n        # returns iterable of Requests\n        req1 = Request(\"http://url1\")\n        req2 = Request(\"http://url2\")\n        item = {\"requests\": iter([req1, req2])}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item is item\n        assert self.fingerprint(req1) in self.info.downloaded\n        assert self.fingerprint(req2) in self.info.downloaded\n\n    @inlineCallbacks\n    def test_results_are_cached_across_multiple_items(self):\n        rsp1 = Response(\"http://url1\")\n        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n        item = {\"requests\": req1}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item is item\n        assert new_item[\"results\"] == [(True, {})]\n\n        # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n        req2 = Request(\n            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n        )\n        item = {\"requests\": req2}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item is item\n        assert self.fingerprint(req1) == self.fingerprint(req2)\n        assert new_item[\"results\"] == [(True, {})]\n\n    @inlineCallbacks\n    def test_results_are_cached_for_requests_of_single_item(self):\n        rsp1 = Response(\"http://url1\")\n        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n        req2 = Request(\n            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n        )\n        item = {\"requests\": [req1, req2]}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item is item\n        assert new_item[\"results\"] == [(True, {}), (True, {})]\n\n    @inlineCallbacks\n    def test_wait_if_request_is_downloading(self):\n        def _check_downloading(response):\n            fp = self.fingerprint(req1)\n            assert fp in self.info.downloading\n            assert fp in self.info.waiting\n            assert fp not in self.info.downloaded\n            assert len(self.info.waiting[fp]) == 2\n            return response\n\n        rsp1 = Response(\"http://url\")\n\n        def rsp1_func():\n            dfd = Deferred().addCallback(_check_downloading)\n            call_later(0.1, dfd.callback, rsp1)\n            return dfd\n\n        def rsp2_func():\n            pytest.fail(\"it must cache rsp1 result and must not try to redownload\")\n\n        req1 = Request(\"http://url\", meta={\"response\": rsp1_func})\n        req2 = Request(req1.url, meta={\"response\": rsp2_func})\n        item = {\"requests\": [req1, req2]}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item[\"results\"] == [(True, {}), (True, {})]\n\n    @inlineCallbacks\n    def test_use_media_to_download_result(self):\n        req = Request(\"http://url\", meta={\"result\": \"ITSME\"})\n        item = {\"requests\": req}\n        new_item = yield self.pipe.process_item(item)\n        assert new_item[\"results\"] == [(True, \"ITSME\")]\n        assert self.pipe._mockcalled == [\n            \"get_media_requests\",\n            \"media_to_download\",\n            \"item_completed\",\n        ]\n\n    def test_key_for_pipe(self):\n        assert (\n            self.pipe._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n            == \"MOCKEDMEDIAPIPELINE_IMAGES\"\n        )\n\n\nclass TestMediaPipelineAllowRedirectSettings:\n    def _assert_request_no3xx(self, pipeline_class, settings):\n        pipe = pipeline_class(crawler=get_crawler(None, settings))\n        request = Request(\"http://url\")\n        pipe._modify_media_request(request)\n\n        assert \"handle_httpstatus_list\" in request.meta\n        for status, check in [\n            (200, True),\n            # These are the status codes we want\n            # the downloader to handle itself\n            (301, False),\n            (302, False),\n            (302, False),\n            (307, False),\n            (308, False),\n            # we still want to get 4xx and 5xx\n            (400, True),\n            (404, True),\n            (500, True),\n        ]:\n            if check:\n                assert status in request.meta[\"handle_httpstatus_list\"]\n            else:\n                assert status not in request.meta[\"handle_httpstatus_list\"]\n\n    def test_subclass_standard_setting(self):\n        self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n\n    def test_subclass_specific_setting(self):\n        self._assert_request_no3xx(\n            UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n        )\n\n\nclass TestBuildFromCrawler:\n    def setup_method(self):\n        self.crawler = get_crawler(None, {\"FILES_STORE\": \"/foo\"})\n\n    def test_simple(self):\n        class Pipeline(UserDefinedPipeline):\n            pass\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 0\n\n    def test_has_old_init(self):\n        class Pipeline(UserDefinedPipeline):", "n_tokens": 1214, "byte_len": 5352, "file_sha1": "ca0a8b7669dd9ec6ff4653e9c24793ebda5f079b", "start_line": 278, "end_line": 422}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_pipeline_media.py", "rel_path": "tests/test_pipeline_media.py", "module": "tests.test_pipeline_media", "ext": "py", "chunk_number": 4, "symbols": ["__init__", "test_has_from_settings", "from_settings", "test_has_from_settings_and_from_crawler", "from_crawler", "test_has_from_settings_and_init", "test_has_from_crawler_and_init", "test_has_from_crawler", "media_failed", "setup_method", "teardown_method", "_errback", "test_result_failure", "Pipeline", "MediaFailedFailurePipeline", "TestMediaFailedFailure", "faile", "results", "failure", "append", "signal", "instance", "test", "has", "name", "mocked", "download", "media", "deprecated", "get", "_mocked_download_func", "media_to_download", "get_media_requests", "media_downloaded", "file_path", "test_modify_media_request", "test_should_remove_req_res_references_before_caching_the_results", "test_default_item_completed", "test_default_process_item", "item_completed", "test_result_succeed", "test_mix_of_success_and_failure", "test_get_media_requests", "test_results_are_cached_across_multiple_items", "test_results_are_cached_for_requests_of_single_item", "test_wait_if_request_is_downloading", "_check_downloading", "rsp1_func", "rsp2_func", "test_use_media_to_download_result"], "ast_kind": "class_or_type", "text": "            def __init__(self):\n                super().__init__()\n                self._init_called = True\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 2\n            assert pipe._init_called\n\n    def test_has_from_settings(self):\n        class Pipeline(UserDefinedPipeline):\n            _from_settings_called = False\n\n            @classmethod\n            def from_settings(cls, settings):\n                o = cls()\n                o._from_settings_called = True\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 2\n            assert pipe._from_settings_called\n\n    def test_has_from_settings_and_from_crawler(self):\n        class Pipeline(UserDefinedPipeline):\n            _from_settings_called = False\n            _from_crawler_called = False\n\n            @classmethod\n            def from_settings(cls, settings):\n                o = cls()\n                o._from_settings_called = True\n                return o\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                o = super().from_crawler(crawler)\n                o._from_crawler_called = True\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 2\n            assert pipe._from_settings_called\n            assert pipe._from_crawler_called\n\n    def test_has_from_settings_and_init(self):\n        class Pipeline(UserDefinedPipeline):\n            _from_settings_called = False\n\n            def __init__(self, store_uri, settings):\n                super().__init__()\n                self._init_called = True\n\n            @classmethod\n            def from_settings(cls, settings):\n                store_uri = settings[\"FILES_STORE\"]\n                o = cls(store_uri, settings=settings)\n                o._from_settings_called = True\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 2\n            assert pipe._from_settings_called\n            assert pipe._init_called\n\n    def test_has_from_crawler_and_init(self):\n        class Pipeline(UserDefinedPipeline):\n            _from_crawler_called = False\n\n            def __init__(self, store_uri, settings, *, crawler):\n                super().__init__(crawler=crawler)\n                self._init_called = True\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                settings = crawler.settings\n                store_uri = settings[\"FILES_STORE\"]\n                o = cls(store_uri, settings=settings, crawler=crawler)\n                o._from_crawler_called = True\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 0\n            assert pipe._from_crawler_called\n            assert pipe._init_called\n\n    def test_has_from_crawler(self):\n        class Pipeline(UserDefinedPipeline):\n            _from_crawler_called = False\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                settings = crawler.settings\n                o = super().from_crawler(crawler)\n                o._from_crawler_called = True\n                o.store_uri = settings[\"FILES_STORE\"]\n                return o\n\n        with warnings.catch_warnings(record=True) as w:\n            pipe = Pipeline.from_crawler(self.crawler)\n            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n            assert pipe.crawler == self.crawler\n            assert pipe._fingerprinter\n            assert len(w) == 0\n            assert pipe._from_crawler_called\n\n\nclass MediaFailedFailurePipeline(MockedMediaPipeline):\n    def media_failed(self, failure, request, info):\n        self._mockcalled.append(\"media_failed\")\n        return failure  # deprecated\n\n\nclass TestMediaFailedFailure:\n    \"\"\"Test that media_failed() can return a failure instead of raising.\"\"\"\n\n    pipeline_class = MediaFailedFailurePipeline\n    settings = None\n\n    def setup_method(self):\n        crawler = get_crawler(DefaultSpider, self.settings)\n        crawler.spider = crawler._create_spider()\n        self.pipe = self.pipeline_class.from_crawler(crawler)\n        self.pipe.download_func = _mocked_download_func\n        self.pipe.open_spider()\n        self.info = self.pipe.spiderinfo\n        self.fingerprint = crawler.request_fingerprinter.fingerprint\n\n    def teardown_method(self):\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                disconnect_all(signal)\n\n    def _errback(self, result):\n        self.pipe._mockcalled.append(\"request_errback\")\n        return result\n\n    @inlineCallbacks\n    def test_result_failure(self):\n        self.pipe.LOG_FAILED_RESULTS = False\n        exc = Exception(\"foo\")\n        fail = Failure(exc)\n        req = Request(\n            \"http://url1\",\n            meta={\"response\": fail},\n            errback=self._errback,\n        )\n        item = {\"requests\": req}\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=\"media_failed returned a Failure instance\"\n        ):\n            new_item = yield self.pipe.process_item(item)\n        assert len(new_item[\"results\"]) == 1\n        assert new_item[\"results\"][0][0] is False\n        assert isinstance(new_item[\"results\"][0][1], Failure)\n        assert new_item[\"results\"][0][1].value == exc\n        assert self.pipe._mockcalled == [\n            \"get_media_requests\",\n            \"media_to_download\",\n            \"media_failed\",\n            \"request_errback\",\n            \"item_completed\",\n        ]\n", "n_tokens": 1265, "byte_len": 6306, "file_sha1": "ca0a8b7669dd9ec6ff4653e9c24793ebda5f079b", "start_line": 423, "end_line": 600}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_core_scraper.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_core_scraper.py", "rel_path": "tests/test_core_scraper.py", "module": "tests.test_core_scraper", "ext": "py", "chunk_number": 1, "symbols": ["async", "core", "text", "await", "processing", "handle", "spider", "typing", "lambda", "annotations", "deferred", "from", "maybe", "spiders", "scrapy", "future", "typ", "checking", "defer", "mockserver", "log", "capture", "get", "crawler", "test", "pytest", "simple", "engine", "assert", "kwargs", "scraper", "mock", "server", "none", "utils", "import", "monkey", "patch", "http", "tests", "monkeypatch", "crawl", "setattr", "args", "caplog"], "ast_kind": "imports", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import SimpleSpider\n\nif TYPE_CHECKING:\n    import pytest\n\n    from tests.mockserver.http import MockServer\n\n\n@deferred_f_from_coro_f\nasync def test_scraper_exception(\n    mockserver: MockServer,\n    caplog: pytest.LogCaptureFixture,\n    monkeypatch: pytest.MonkeyPatch,\n) -> None:\n    crawler = get_crawler(SimpleSpider)\n    monkeypatch.setattr(\n        \"scrapy.core.engine.Scraper.handle_spider_output_async\",\n        lambda *args, **kwargs: 1 / 0,\n    )\n    await maybe_deferred_to_future(crawler.crawl(url=mockserver.url(\"/\")))\n    assert \"Scraper bug processing\" in caplog.text\n", "n_tokens": 192, "byte_len": 793, "file_sha1": "3a995d57803e749550cf589ab00f1e16abb0a236", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_process_start.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_process_start.py", "rel_path": "tests/test_spidermiddleware_process_start.py", "module": "tests.test_spidermiddleware_process_start", "ext": "py", "chunk_number": 1, "symbols": ["process_start_requests", "start_requests", "track_item", "AsyncioSleepSpiderMiddleware", "NoOpSpiderMiddleware", "TwistedSleepSpiderMiddleware", "UniversalSpiderMiddleware", "ModernWrapSpider", "ModernWrapSpiderSubclass", "UniversalWrapSpider", "DeprecatedWrapSpider", "ModernWrapSpiderMiddleware", "UniversalWrapSpiderMiddleware", "DeprecatedWrapSpiderMiddleware", "TestMain", "async", "test", "deprecated", "append", "spider", "name", "deferred", "from", "middleware", "sleep", "spiders", "spide", "middlewares", "smw", "smw2", "TestSpider", "cls", "trying", "get", "crawler", "universal", "compatible", "start", "requests", "pytest", "settings", "finish", "reason", "none", "process", "modern", "actual", "items", "value", "match"], "ast_kind": "class_or_type", "text": "import warnings\nfrom asyncio import sleep\n\nimport pytest\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\nfrom tests.test_spider_start import SLEEP_SECONDS\n\nfrom .utils import twisted_sleep\n\nITEM_A = {\"id\": \"a\"}\nITEM_B = {\"id\": \"b\"}\nITEM_C = {\"id\": \"c\"}\nITEM_D = {\"id\": \"d\"}\n\n\nclass AsyncioSleepSpiderMiddleware:\n    async def process_start(self, start):\n        await sleep(SLEEP_SECONDS)\n        async for item_or_request in start:\n            yield item_or_request\n\n\nclass NoOpSpiderMiddleware:\n    async def process_start(self, start):\n        async for item_or_request in start:\n            yield item_or_request\n\n\nclass TwistedSleepSpiderMiddleware:\n    async def process_start(self, start):\n        await maybe_deferred_to_future(twisted_sleep(SLEEP_SECONDS))\n        async for item_or_request in start:\n            yield item_or_request\n\n\nclass UniversalSpiderMiddleware:\n    async def process_start(self, start):\n        async for item_or_request in start:\n            yield item_or_request\n\n    def process_start_requests(self, start_requests, spider):\n        raise NotImplementedError\n\n\n# Spiders and spider middlewares for TestMain._test_wrap\n\n\nclass ModernWrapSpider(Spider):\n    name = \"test\"\n\n    async def start(self):\n        yield ITEM_B\n\n\nclass ModernWrapSpiderSubclass(ModernWrapSpider):\n    name = \"test\"\n\n\nclass UniversalWrapSpider(Spider):\n    name = \"test\"\n\n    async def start(self):\n        yield ITEM_B\n\n    def start_requests(self):\n        yield ITEM_D\n\n\nclass DeprecatedWrapSpider(Spider):\n    name = \"test\"\n\n    def start_requests(self):\n        yield ITEM_B\n\n\nclass ModernWrapSpiderMiddleware:\n    async def process_start(self, start):\n        yield ITEM_A\n        async for item_or_request in start:\n            yield item_or_request\n        yield ITEM_C\n\n\nclass UniversalWrapSpiderMiddleware:\n    async def process_start(self, start):\n        yield ITEM_A\n        async for item_or_request in start:\n            yield item_or_request\n        yield ITEM_C\n\n    def process_start_requests(self, start, spider):\n        yield ITEM_A\n        yield from start\n        yield ITEM_C\n\n\nclass DeprecatedWrapSpiderMiddleware:\n    def process_start_requests(self, start, spider):\n        yield ITEM_A\n        yield from start\n        yield ITEM_C\n\n\nclass TestMain:\n    async def _test(self, spider_middlewares, spider_cls, expected_items):\n        actual_items = []\n\n        def track_item(item, response, spider):\n            actual_items.append(item)\n\n        settings = {\n            \"SPIDER_MIDDLEWARES\": {cls: n for n, cls in enumerate(spider_middlewares)},\n        }\n        crawler = get_crawler(spider_cls, settings_dict=settings)\n        crawler.signals.connect(track_item, signals.item_scraped)\n        await maybe_deferred_to_future(crawler.crawl())\n        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n        assert actual_items == expected_items, f\"{actual_items=} != {expected_items=}\"\n\n    async def _test_wrap(self, spider_middleware, spider_cls, expected_items=None):\n        expected_items = expected_items or [ITEM_A, ITEM_B, ITEM_C]\n        await self._test([spider_middleware], spider_cls, expected_items)\n\n    async def _test_douple_wrap(self, smw1, smw2, spider_cls, expected_items=None):\n        expected_items = expected_items or [ITEM_A, ITEM_A, ITEM_B, ITEM_C, ITEM_C]\n        await self._test([smw1, smw2], spider_cls, expected_items)\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_modern_spider(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_wrap(ModernWrapSpiderMiddleware, ModernWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_universal_spider(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_wrap(ModernWrapSpiderMiddleware, UniversalWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_deprecated_spider(self):\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n        ):\n            await self._test_wrap(ModernWrapSpiderMiddleware, DeprecatedWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_universal_mw_modern_spider(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_wrap(UniversalWrapSpiderMiddleware, ModernWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_universal_mw_universal_spider(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_wrap(UniversalWrapSpiderMiddleware, UniversalWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_universal_mw_deprecated_spider(self):\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n        ):\n            await self._test_wrap(UniversalWrapSpiderMiddleware, DeprecatedWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_deprecated_mw_modern_spider(self):\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n            ),\n            pytest.raises(\n                ValueError, match=r\"only compatible with \\(deprecated\\) spiders\"\n            ),\n        ):\n            await self._test_wrap(DeprecatedWrapSpiderMiddleware, ModernWrapSpider)\n", "n_tokens": 1225, "byte_len": 5577, "file_sha1": "c608a50df0d4f9cbc3ffb1b5219bbb0d00563a72", "start_line": 1, "end_line": 181}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_process_start.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_process_start.py", "rel_path": "tests/test_spidermiddleware_process_start.py", "module": "tests.test_spidermiddleware_process_start", "ext": "py", "chunk_number": 2, "symbols": ["TestSpider", "deprecated", "wrap", "test", "async", "ite", "item", "await", "universal", "modern", "simplefilter", "spider", "scrapy", "deprecation", "warns", "value", "error", "name", "deferred", "from", "class", "middlewares", "process", "start", "mark", "only", "asyncio", "with", "spiders", "trying", "process_start_requests", "start_requests", "track_item", "AsyncioSleepSpiderMiddleware", "NoOpSpiderMiddleware", "TwistedSleepSpiderMiddleware", "UniversalSpiderMiddleware", "ModernWrapSpider", "ModernWrapSpiderSubclass", "UniversalWrapSpider", "DeprecatedWrapSpider", "ModernWrapSpiderMiddleware", "UniversalWrapSpiderMiddleware", "DeprecatedWrapSpiderMiddleware", "TestMain", "append", "middleware", "sleep", "spide", "smw"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_deprecated_mw_modern_spider_subclass(self):\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n            ),\n            pytest.raises(\n                ValueError,\n                match=r\"^\\S+?\\.ModernWrapSpider \\(inherited by \\S+?.ModernWrapSpiderSubclass\\) .*? only compatible with \\(deprecated\\) spiders\",\n            ),\n        ):\n            await self._test_wrap(\n                DeprecatedWrapSpiderMiddleware, ModernWrapSpiderSubclass\n            )\n\n    @deferred_f_from_coro_f\n    async def test_deprecated_mw_universal_spider(self):\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n        ):\n            await self._test_wrap(\n                DeprecatedWrapSpiderMiddleware,\n                UniversalWrapSpider,\n                [ITEM_A, ITEM_D, ITEM_C],\n            )\n\n    @deferred_f_from_coro_f\n    async def test_deprecated_mw_deprecated_spider(self):\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n            ),\n        ):\n            await self._test_wrap(DeprecatedWrapSpiderMiddleware, DeprecatedWrapSpider)\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_universal_mw_modern_spider(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_douple_wrap(\n                ModernWrapSpiderMiddleware,\n                UniversalWrapSpiderMiddleware,\n                ModernWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_deprecated_mw_modern_spider(self):\n        with pytest.raises(ValueError, match=r\"trying to combine spider middlewares\"):\n            await self._test_douple_wrap(\n                ModernWrapSpiderMiddleware,\n                DeprecatedWrapSpiderMiddleware,\n                ModernWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_universal_mw_deprecated_mw_modern_spider(self):\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n            ),\n            pytest.raises(\n                ValueError, match=r\"only compatible with \\(deprecated\\) spiders\"\n            ),\n        ):\n            await self._test_douple_wrap(\n                UniversalWrapSpiderMiddleware,\n                DeprecatedWrapSpiderMiddleware,\n                ModernWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_universal_mw_universal_spider(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_douple_wrap(\n                ModernWrapSpiderMiddleware,\n                UniversalWrapSpiderMiddleware,\n                UniversalWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_deprecated_mw_universal_spider(self):\n        with pytest.raises(ValueError, match=r\"trying to combine spider middlewares\"):\n            await self._test_douple_wrap(\n                ModernWrapSpiderMiddleware,\n                DeprecatedWrapSpiderMiddleware,\n                UniversalWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_universal_mw_deprecated_mw_universal_spider(self):\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n        ):\n            await self._test_douple_wrap(\n                UniversalWrapSpiderMiddleware,\n                DeprecatedWrapSpiderMiddleware,\n                UniversalWrapSpider,\n                [ITEM_A, ITEM_A, ITEM_D, ITEM_C, ITEM_C],\n            )\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_universal_mw_deprecated_spider(self):\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n        ):\n            await self._test_douple_wrap(\n                ModernWrapSpiderMiddleware,\n                UniversalWrapSpiderMiddleware,\n                DeprecatedWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_modern_mw_deprecated_mw_deprecated_spider(self):\n        with pytest.raises(ValueError, match=r\"trying to combine spider middlewares\"):\n            await self._test_douple_wrap(\n                ModernWrapSpiderMiddleware,\n                DeprecatedWrapSpiderMiddleware,\n                DeprecatedWrapSpider,\n            )\n\n    @deferred_f_from_coro_f\n    async def test_universal_mw_deprecated_mw_deprecated_spider(self):\n        with (\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n            ),\n            pytest.warns(\n                ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n            ),\n        ):\n            await self._test_douple_wrap(\n                UniversalWrapSpiderMiddleware,\n                DeprecatedWrapSpiderMiddleware,\n                DeprecatedWrapSpider,\n            )\n\n    async def _test_sleep(self, spider_middlewares):\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                yield ITEM_A\n\n        await self._test(spider_middlewares, TestSpider, [ITEM_A])\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_asyncio_sleep_single(self):\n        await self._test_sleep([AsyncioSleepSpiderMiddleware])\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_asyncio_sleep_multiple(self):\n        await self._test_sleep(\n            [NoOpSpiderMiddleware, AsyncioSleepSpiderMiddleware, NoOpSpiderMiddleware]\n        )\n", "n_tokens": 1198, "byte_len": 5884, "file_sha1": "c608a50df0d4f9cbc3ffb1b5219bbb0d00563a72", "start_line": 182, "end_line": 342}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_process_start.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_process_start.py", "rel_path": "tests/test_spidermiddleware_process_start.py", "module": "tests.test_spidermiddleware_process_start", "ext": "py", "chunk_number": 3, "symbols": ["test", "twisted", "spider", "middleware", "deferred", "from", "async", "await", "self", "sleep", "process_start_requests", "start_requests", "track_item", "AsyncioSleepSpiderMiddleware", "NoOpSpiderMiddleware", "TwistedSleepSpiderMiddleware", "UniversalSpiderMiddleware", "ModernWrapSpider", "ModernWrapSpiderSubclass", "UniversalWrapSpider", "DeprecatedWrapSpider", "ModernWrapSpiderMiddleware", "UniversalWrapSpiderMiddleware", "DeprecatedWrapSpiderMiddleware", "TestMain", "TestSpider", "deprecated", "append", "name", "spiders", "spide", "middlewares", "smw", "smw2", "cls", "trying", "get", "crawler", "universal", "compatible", "start", "requests", "pytest", "settings", "finish", "reason", "none", "process", "modern", "actual"], "ast_kind": "unknown", "text": "    @deferred_f_from_coro_f\n    async def test_twisted_sleep_single(self):\n        await self._test_sleep([TwistedSleepSpiderMiddleware])\n\n    @deferred_f_from_coro_f\n    async def test_twisted_sleep_multiple(self):\n        await self._test_sleep(\n            [NoOpSpiderMiddleware, TwistedSleepSpiderMiddleware, NoOpSpiderMiddleware]\n        )\n", "n_tokens": 80, "byte_len": 345, "file_sha1": "c608a50df0d4f9cbc3ffb1b5219bbb0d00563a72", "start_line": 343, "end_line": 352}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_squeues.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_squeues.py", "rel_path": "tests/test_squeues.py", "module": "tests.test_squeues", "ext": "py", "chunk_number": 1, "symbols": ["_test_procesor", "nonserializable_object_test", "test_serialize", "queue", "test_serialize_item", "test_serialize_loader", "test_serialize_request_recursive", "test_non_pickable_object", "MyItem", "MyLoader", "FifoDiskQueueTestMixin", "MarshalFifoDiskQueueTest", "ChunkSize1MarshalFifoDiskQueueTest", "ChunkSize2MarshalFifoDiskQueueTest", "ChunkSize3MarshalFifoDiskQueueTest", "ChunkSize4MarshalFifoDiskQueueTest", "PickleFifoDiskQueueTest", "ChunkSize1PickleFifoDiskQueueTest", "ChunkSize2PickleFifoDiskQueueTest", "ChunkSize3PickleFifoDiskQueueTest", "ChunkSize4PickleFifoDiskQueueTest", "LifoDiskQueueTestMixin", "MarshalLifoDiskQueueTest", "PickleLifoDiskQueueTest", "selectors", "html", "element", "name", "out", "selector", "squeues", "queuelib", "pytest", "isinstance", "object", "item", "unmarshallable", "lifo", "disk", "http", "fail", "loader", "myloader", "found", "match", "exc", "info", "marshal", "myitem", "pickling"], "ast_kind": "class_or_type", "text": "import pickle\nimport sys\n\nimport pytest\nfrom queuelib.tests import test_queue as t\n\nfrom scrapy.http import Request\nfrom scrapy.item import Field, Item\nfrom scrapy.loader import ItemLoader\nfrom scrapy.selector import Selector\nfrom scrapy.squeues import (\n    _MarshalFifoSerializationDiskQueue,\n    _MarshalLifoSerializationDiskQueue,\n    _PickleFifoSerializationDiskQueue,\n    _PickleLifoSerializationDiskQueue,\n)\n\n\nclass MyItem(Item):\n    name = Field()\n\n\ndef _test_procesor(x):\n    return x + x\n\n\nclass MyLoader(ItemLoader):\n    default_item_class = MyItem\n    name_out = staticmethod(_test_procesor)\n\n\ndef nonserializable_object_test(self):\n    q = self.queue()\n    with pytest.raises(\n        ValueError,\n        match=\"unmarshallable object|Can't (get|pickle) local object|Can't pickle .*: it's not found as\",\n    ):\n        q.push(lambda x: x)\n    # Selectors should fail (lxml.html.HtmlElement objects can't be pickled)\n    sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n    with pytest.raises(\n        ValueError, match=\"unmarshallable object|can't pickle Selector objects\"\n    ):\n        q.push(sel)\n\n\nclass FifoDiskQueueTestMixin:\n    def test_serialize(self):\n        q = self.queue()\n        q.push(\"a\")\n        q.push(123)\n        q.push({\"a\": \"dict\"})\n        assert q.pop() == \"a\"\n        assert q.pop() == 123\n        assert q.pop() == {\"a\": \"dict\"}\n\n    test_nonserializable_object = nonserializable_object_test\n\n\nclass MarshalFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n    chunksize = 100000\n\n    def queue(self):\n        return _MarshalFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)\n\n\nclass ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 1\n\n\nclass ChunkSize2MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 2\n\n\nclass ChunkSize3MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 3\n\n\nclass ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 4\n\n\nclass PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n    chunksize = 100000\n\n    def queue(self):\n        return _PickleFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)\n\n    def test_serialize_item(self):\n        q = self.queue()\n        i = MyItem(name=\"foo\")\n        q.push(i)\n        i2 = q.pop()\n        assert isinstance(i2, MyItem)\n        assert i == i2\n\n    def test_serialize_loader(self):\n        q = self.queue()\n        loader = MyLoader()\n        q.push(loader)\n        loader2 = q.pop()\n        assert isinstance(loader2, MyLoader)\n        assert loader2.default_item_class is MyItem\n        assert loader2.name_out(\"x\") == \"xx\"\n\n    def test_serialize_request_recursive(self):\n        q = self.queue()\n        r = Request(\"http://www.example.com\")\n        r.meta[\"request\"] = r\n        q.push(r)\n        r2 = q.pop()\n        assert isinstance(r2, Request)\n        assert r.url == r2.url\n        assert r2.meta[\"request\"] is r2\n\n    def test_non_pickable_object(self):\n        q = self.queue()\n        with pytest.raises(\n            ValueError,\n            match=\"Can't (get|pickle) local object|Can't pickle .*: it's not found as\",\n        ) as exc_info:\n            q.push(lambda x: x)\n        if hasattr(sys, \"pypy_version_info\"):\n            assert isinstance(exc_info.value.__context__, pickle.PicklingError)\n        else:\n            assert isinstance(exc_info.value.__context__, AttributeError)\n        sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n        with pytest.raises(\n            ValueError, match=\"can't pickle Selector objects\"\n        ) as exc_info:\n            q.push(sel)\n        assert isinstance(exc_info.value.__context__, TypeError)\n        q.close()\n\n\nclass ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 1\n\n\nclass ChunkSize2PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 2\n\n\nclass ChunkSize3PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 3\n\n\nclass ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 4\n\n\nclass LifoDiskQueueTestMixin:\n    def test_serialize(self):\n        q = self.queue()\n        q.push(\"a\")\n        q.push(123)\n        q.push({\"a\": \"dict\"})\n        assert q.pop() == {\"a\": \"dict\"}\n        assert q.pop() == 123\n        assert q.pop() == \"a\"\n\n    test_nonserializable_object = nonserializable_object_test\n\n\nclass MarshalLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n    def queue(self):\n        return _MarshalLifoSerializationDiskQueue(self.qpath)\n\n\nclass PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n    def queue(self):\n        return _PickleLifoSerializationDiskQueue(self.qpath)\n", "n_tokens": 1184, "byte_len": 4751, "file_sha1": "7048e9972c71478f4657358c213a780c848bb324", "start_line": 1, "end_line": 173}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_squeues.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_squeues.py", "rel_path": "tests/test_squeues.py", "module": "tests.test_squeues", "ext": "py", "chunk_number": 2, "symbols": ["test_serialize_item", "test_serialize_loader", "test_serialize_request_recursive", "item", "myitem", "name", "out", "queue", "test", "serialize", "push", "meta", "default", "loader", "loader2", "example", "assert", "isinstance", "request", "self", "http", "myloader", "_test_procesor", "nonserializable_object_test", "test_serialize", "test_non_pickable_object", "MyItem", "MyLoader", "FifoDiskQueueTestMixin", "MarshalFifoDiskQueueTest", "ChunkSize1MarshalFifoDiskQueueTest", "ChunkSize2MarshalFifoDiskQueueTest", "ChunkSize3MarshalFifoDiskQueueTest", "ChunkSize4MarshalFifoDiskQueueTest", "PickleFifoDiskQueueTest", "ChunkSize1PickleFifoDiskQueueTest", "ChunkSize2PickleFifoDiskQueueTest", "ChunkSize3PickleFifoDiskQueueTest", "ChunkSize4PickleFifoDiskQueueTest", "LifoDiskQueueTestMixin", "MarshalLifoDiskQueueTest", "PickleLifoDiskQueueTest", "selectors", "html", "element", "selector", "squeues", "queuelib", "pytest", "object"], "ast_kind": "class_or_type", "text": "    def test_serialize_item(self):\n        q = self.queue()\n        i = MyItem(name=\"foo\")\n        q.push(i)\n        i2 = q.pop()\n        assert isinstance(i2, MyItem)\n        assert i == i2\n\n    def test_serialize_loader(self):\n        q = self.queue()\n        loader = MyLoader()\n        q.push(loader)\n        loader2 = q.pop()\n        assert isinstance(loader2, MyLoader)\n        assert loader2.default_item_class is MyItem\n        assert loader2.name_out(\"x\") == \"xx\"\n\n    def test_serialize_request_recursive(self):\n        q = self.queue()\n        r = Request(\"http://www.example.com\")\n        r.meta[\"request\"] = r\n        q.push(r)\n        r2 = q.pop()\n        assert isinstance(r2, Request)\n        assert r.url == r2.url\n        assert r2.meta[\"request\"] is r2\n", "n_tokens": 189, "byte_len": 772, "file_sha1": "7048e9972c71478f4657358c213a780c848bb324", "start_line": 174, "end_line": 200}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_crawl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_crawl.py", "rel_path": "tests/test_command_crawl.py", "module": "tests.test_command_crawl", "ext": "py", "chunk_number": 1, "symbols": ["crawl", "get_log", "test_no_output", "test_output", "test_overwrite_output", "test_output_and_overwrite_output", "test_default_reactor", "TestCrawlCommand", "MySpider", "encoding", "dumps", "async", "works", "first", "line", "test", "output", "example", "example2", "internet", "feeds", "twisted", "getdict", "return", "please", "spider", "name", "annotations", "class", "overwrite", "sort", "keys", "json", "with", "spiders", "scrapy", "logger", "future", "myspider", "default", "pathlib", "path", "asyncio", "selector", "stderr", "commands", "open", "code", "yield", "true"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom pathlib import Path\n\nfrom tests.test_commands import TestCommandBase\n\n\nclass TestCrawlCommand(TestCommandBase):\n    def crawl(self, code, args=()):\n        Path(self.proj_mod_path, \"spiders\", \"myspider.py\").write_text(\n            code, encoding=\"utf-8\"\n        )\n        return self.proc(\"crawl\", \"myspider\", *args)\n\n    def get_log(self, code, args=()):\n        _, _, stderr = self.crawl(code, args=args)\n        return stderr\n\n    def test_no_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug('It works!')\n        return\n        yield\n\"\"\"\n        log = self.get_log(spider_code)\n        assert \"[myspider] DEBUG: It works!\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Spider closed (finished)\" in log\n\n    def test_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n        return\n        yield\n\"\"\"\n        args = [\"-o\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\" in log\n\n    def test_overwrite_output(self):\n        spider_code = \"\"\"\nimport json\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug(\n            'FEEDS: {}'.format(\n                json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n            )\n        )\n        return\n        yield\n\"\"\"\n        Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n        args = [\"-O\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}'\n            in log\n        )\n        with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n            first_line = f2.readline()\n        assert first_line != \"not empty\"\n\n    def test_output_and_overwrite_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        return\n        yield\n\"\"\"\n        args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n        log = self.get_log(spider_code, args=args)\n        assert (\n            \"error: Please use only one of -o/--output and -O/--overwrite-output\" in log\n        )\n\n    def test_default_reactor(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    async def start(self):\n        self.logger.debug('It works!')\n        return\n        yield\n\"\"\"\n        log = self.get_log(spider_code, args=(\"-s\", \"TWISTED_REACTOR=\"))\n        assert \"[myspider] DEBUG: It works!\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            not in log\n        )\n        assert \"Spider closed (finished)\" in log\n", "n_tokens": 780, "byte_len": 3202, "file_sha1": "d536d41aee74d8c656c49190c746d3ce7afa488e", "start_line": 1, "end_line": 119}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_addons.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_addons.py", "rel_path": "tests/test_addons.py", "module": "tests.test_addons", "ext": "py", "chunk_number": 1, "symbols": ["update_settings", "get_addon_cls", "__init__", "from_crawler", "test_update_settings", "test_load_settings", "test_notconfigured", "test_load_settings_order", "test_build_from_crawler", "test_settings_priority", "test_fallback_workflow", "SimpleAddon", "AddonWithConfig", "CreateInstanceAddon", "TestAddon", "TestAddonManager", "NotConfiguredAddon", "AddonWithFallback", "create", "instance", "fallbac", "downloa", "append", "spider", "possible", "https", "addons", "mock", "key2", "not", "test_logging_message", "test_enable_addon_in_spider", "LoggedAddon", "MySpider", "logger", "name", "configured", "get", "crawler", "three", "settings", "test", "fallback", "addon", "isinstance", "permutations", "handlers", "none", "return", "value"], "ast_kind": "class_or_type", "text": "import itertools\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler, CrawlerRunner\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.test import get_crawler, get_reactor_settings\n\n\nclass SimpleAddon:\n    def update_settings(self, settings):\n        pass\n\n\ndef get_addon_cls(config: dict[str, Any]) -> type:\n    class AddonWithConfig:\n        def update_settings(self, settings: BaseSettings):\n            settings.update(config, priority=\"addon\")\n\n    return AddonWithConfig\n\n\nclass CreateInstanceAddon:\n    def __init__(self, crawler: Crawler) -> None:\n        super().__init__()\n        self.crawler = crawler\n        self.config = crawler.settings.getdict(\"MYADDON\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler):\n        return cls(crawler)\n\n    def update_settings(self, settings):\n        settings.update(self.config, \"addon\")\n\n\nclass TestAddon:\n    def test_update_settings(self):\n        settings = BaseSettings()\n        settings.set(\"KEY1\", \"default\", priority=\"default\")\n        settings.set(\"KEY2\", \"project\", priority=\"project\")\n        addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n        testaddon = get_addon_cls(addon_config)()\n        testaddon.update_settings(settings)\n        assert settings[\"KEY1\"] == \"addon\"\n        assert settings[\"KEY2\"] == \"project\"\n        assert settings[\"KEY3\"] == \"addon\"\n\n\nclass TestAddonManager:\n    def test_load_settings(self):\n        settings_dict = {\n            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        manager = crawler.addons\n        assert isinstance(manager.addons[0], SimpleAddon)\n\n    def test_notconfigured(self):\n        class NotConfiguredAddon:\n            def update_settings(self, settings):\n                raise NotConfigured\n\n        settings_dict = {\n            \"ADDONS\": {NotConfiguredAddon: 0},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        manager = crawler.addons\n        assert not manager.addons\n\n    def test_load_settings_order(self):\n        # Get three addons with different settings\n        addonlist = []\n        for i in range(3):\n            addon = get_addon_cls({\"KEY1\": i})\n            addon.number = i\n            addonlist.append(addon)\n        # Test for every possible ordering\n        for ordered_addons in itertools.permutations(addonlist):\n            expected_order = [a.number for a in ordered_addons]\n            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n            crawler = get_crawler(settings_dict=settings)\n            manager = crawler.addons\n            assert [a.number for a in manager.addons] == expected_order\n            assert crawler.settings.getint(\"KEY1\") == expected_order[-1]\n\n    def test_build_from_crawler(self):\n        settings_dict = {\n            \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n            \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        manager = crawler.addons\n        assert isinstance(manager.addons[0], CreateInstanceAddon)\n        assert crawler.settings.get(\"MYADDON_KEY\") == \"val\"\n\n    def test_settings_priority(self):\n        config = {\n            \"KEY\": 15,  # priority=addon\n        }\n        settings_dict = {\n            \"ADDONS\": {get_addon_cls(config): 1},\n            **get_reactor_settings(),\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        assert crawler.settings.getint(\"KEY\") == 15\n\n        settings = Settings(settings_dict)\n        settings.set(\"KEY\", 0, priority=\"default\")\n        runner = CrawlerRunner(settings)\n        crawler = runner.create_crawler(Spider)\n        crawler._apply_settings()\n        assert crawler.settings.getint(\"KEY\") == 15\n\n        settings_dict = {\n            \"KEY\": 20,  # priority=project\n            \"ADDONS\": {get_addon_cls(config): 1},\n            **get_reactor_settings(),\n        }\n        settings = Settings(settings_dict)\n        settings.set(\"KEY\", 0, priority=\"default\")\n        runner = CrawlerRunner(settings)\n        crawler = runner.create_crawler(Spider)\n        assert crawler.settings.getint(\"KEY\") == 20\n\n    def test_fallback_workflow(self):\n        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n\n        class AddonWithFallback:\n            def update_settings(self, settings):\n                if not settings.get(FALLBACK_SETTING):\n                    settings.set(\n                        FALLBACK_SETTING,\n                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n                        \"addon\",\n                    )\n                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n\n        settings_dict = {\n            \"ADDONS\": {AddonWithFallback: 1},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        assert (\n            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"] == \"AddonHandler\"\n        )\n        assert (\n            crawler.settings.get(FALLBACK_SETTING)\n            == \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\"\n        )\n\n        settings_dict = {\n            \"ADDONS\": {AddonWithFallback: 1},\n            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        assert (\n            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"] == \"AddonHandler\"\n        )\n        assert crawler.settings.get(FALLBACK_SETTING) == \"UserHandler\"\n", "n_tokens": 1225, "byte_len": 5693, "file_sha1": "0e2899b51491c601feb3ac0cacd337610808b8b0", "start_line": 1, "end_line": 164}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_addons.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_addons.py", "rel_path": "tests/test_addons.py", "module": "tests.test_addons", "ext": "py", "chunk_number": 2, "symbols": ["test_logging_message", "update_settings", "test_enable_addon_in_spider", "from_crawler", "LoggedAddon", "MySpider", "settings", "dict", "runner", "pass", "patch", "spider", "priority", "logger", "mock", "assert", "called", "extra", "addon", "config", "build", "from", "addons", "enabled", "name", "return", "class", "crawler", "with", "scrapy", "get_addon_cls", "__init__", "test_update_settings", "test_load_settings", "test_notconfigured", "test_load_settings_order", "test_build_from_crawler", "test_settings_priority", "test_fallback_workflow", "SimpleAddon", "AddonWithConfig", "CreateInstanceAddon", "TestAddon", "TestAddonManager", "NotConfiguredAddon", "AddonWithFallback", "create", "instance", "fallbac", "downloa"], "ast_kind": "class_or_type", "text": "    def test_logging_message(self):\n        class LoggedAddon:\n            def update_settings(self, settings):\n                pass\n\n        with (\n            patch(\"scrapy.addons.logger\") as logger_mock,\n            patch(\"scrapy.addons.build_from_crawler\") as build_from_crawler_mock,\n        ):\n            settings_dict = {\n                \"ADDONS\": {LoggedAddon: 1},\n            }\n            addon = LoggedAddon()\n            build_from_crawler_mock.return_value = addon\n            crawler = get_crawler(settings_dict=settings_dict)\n            logger_mock.info.assert_called_once_with(\n                \"Enabled addons:\\n%(addons)s\",\n                {\"addons\": [addon]},\n                extra={\"crawler\": crawler},\n            )\n\n    @inlineCallbacks\n    def test_enable_addon_in_spider(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler, *args, **kwargs):\n                spider = super().from_crawler(crawler, *args, **kwargs)\n                addon_config = {\"KEY\": \"addon\"}\n                addon_cls = get_addon_cls(addon_config)\n                spider.settings.set(\"ADDONS\", {addon_cls: 1}, priority=\"spider\")\n                return spider\n\n        settings = Settings()\n        settings.setdict(get_reactor_settings())\n        settings.set(\"KEY\", \"default\", priority=\"default\")\n        runner = CrawlerRunner(settings)\n        crawler = runner.create_crawler(MySpider)\n        assert crawler.settings.get(\"KEY\") == \"default\"\n        yield crawler.crawl()\n        assert crawler.settings.get(\"KEY\") == \"addon\"\n", "n_tokens": 331, "byte_len": 1609, "file_sha1": "0e2899b51491c601feb3ac0cacd337610808b8b0", "start_line": 165, "end_line": 207}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py", "rel_path": "tests/test_utils_request.py", "module": "tests.test_utils_request", "ext": "py", "chunk_number": 1, "symbols": ["test_request_authenticate", "test_request_httprepr", "test_request_httprepr_for_non_http_request", "post", "request", "curl", "method", "some", "expected", "representation", "text", "weak", "key", "bhc", "c29tzxvzzxi6c29tzxbhc3m", "python", "scrapy", "deprecation", "authorization", "type", "http", "annotations", "mark", "parametrize", "fingerprint", "cache", "json", "content", "ncontent", "ignore", "test_query_string_key_order", "test_query_string_key_without_value", "test_caching", "test_header", "test_headers", "test_fragment", "test_method_and_body", "test_request_replace", "test_part_separation", "test_hashes", "test_default_implementation", "test_deprecated_implementation", "test_include_headers", "test_dont_canonicalize", "test_meta", "test_from_crawler", "from_crawler", "__init__", "test_from_settings", "from_settings"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport json\nimport warnings\nfrom hashlib import sha1\nfrom weakref import WeakKeyDictionary\n\nimport pytest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.request import (\n    _fingerprint_cache,\n    fingerprint,\n    request_authenticate,\n    request_httprepr,\n    request_to_curl,\n)\nfrom scrapy.utils.test import get_crawler\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\ndef test_request_authenticate():\n    r = Request(\"http://www.example.com\")\n    request_authenticate(r, \"someuser\", \"somepass\")\n    assert r.headers[\"Authorization\"] == b\"Basic c29tZXVzZXI6c29tZXBhc3M=\"\n\n\n@pytest.mark.parametrize(\n    (\"r\", \"expected\"),\n    [\n        (\n            Request(\"http://www.example.com\"),\n            b\"GET / HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n\",\n        ),\n        (\n            Request(\"http://www.example.com/some/page.html?arg=1\"),\n            b\"GET /some/page.html?arg=1 HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n\",\n        ),\n        (\n            Request(\n                \"http://www.example.com\",\n                method=\"POST\",\n                headers={\"Content-type\": b\"text/html\"},\n                body=b\"Some body\",\n            ),\n            b\"POST / HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n        ),\n    ],\n)\ndef test_request_httprepr(r: Request, expected: bytes) -> None:\n    assert request_httprepr(r) == expected\n\n\n@pytest.mark.parametrize(\n    \"r\",\n    [\n        Request(\"file:///tmp/foo.txt\"),\n        Request(\"ftp://localhost/tmp/foo.txt\"),\n    ],\n)\ndef test_request_httprepr_for_non_http_request(r: Request) -> None:\n    # the representation is not important but it must not fail.\n    request_httprepr(r)\n\n", "n_tokens": 437, "byte_len": 1843, "file_sha1": "bdf80aea8d701dca7a7469b22cd3bc84998b2cd9", "start_line": 1, "end_line": 67}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py", "rel_path": "tests/test_utils_request.py", "module": "tests.test_utils_request", "ext": "py", "chunk_number": 2, "symbols": ["test_query_string_key_order", "test_query_string_key_without_value", "test_caching", "TestFingerprint", "method", "xbf", "xbfh", "bool", "x05b", "x05r", "xa0x", "xe0d", "known", "hashes", "default", "cache", "fingerprint", "xfc", "xfcw", "https", "xe3v", "test", "caching", "xde", "xdew", "none", "http", "xdcz", "xdcz8", "xf8k", "test_request_authenticate", "test_request_httprepr", "test_request_httprepr_for_non_http_request", "test_header", "test_headers", "test_fragment", "test_method_and_body", "test_request_replace", "test_part_separation", "test_hashes", "test_default_implementation", "test_deprecated_implementation", "test_include_headers", "test_dont_canonicalize", "test_meta", "test_from_crawler", "from_crawler", "__init__", "test_from_settings", "from_settings"], "ast_kind": "class_or_type", "text": "class TestFingerprint:\n    function: staticmethod = staticmethod(fingerprint)\n    cache: (\n        WeakKeyDictionary[Request, dict[tuple[tuple[bytes, ...] | None, bool], bytes]]\n        | WeakKeyDictionary[Request, dict[tuple[tuple[bytes, ...] | None, bool], str]]\n    ) = _fingerprint_cache\n    default_cache_key = (None, False)\n    known_hashes: tuple[tuple[Request, bytes | str, dict], ...] = (\n        (\n            Request(\"http://example.org\"),\n            b\"xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\"),\n            b\"\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a\"),\n            b\"G\\xad\\xb8Ck\\x19\\x1c\\xed\\x838,\\x01\\xc4\\xde;\\xee\\xa5\\x94a\\x0c\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a=b\"),\n            b\"\\x024MYb\\x8a\\xc2\\x1e\\xbc>\\xd6\\xac*\\xda\\x9cF\\xc1r\\x7f\\x17\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a=b&a\"),\n            b\"t+\\xe8*\\xfb\\x84\\xe3v\\x1a}\\x88p\\xc0\\xccB\\xd7\\x9d\\xfez\\x96\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a=b&a=c\"),\n            b\"\\xda\\x1ec\\xd0\\x9c\\x08s`\\xb4\\x9b\\xe2\\xb6R\\xf8k\\xef\\xeaQG\\xef\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\", method=\"POST\"),\n            b\"\\x9d\\xcdA\\x0fT\\x02:\\xca\\xa0}\\x90\\xda\\x05B\\xded\\x8aN7\\x1d\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\", body=b\"a\"),\n            b\"\\xc34z>\\xd8\\x99\\x8b\\xda7\\x05r\\x99I\\xa8\\xa0x;\\xa41_\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\", method=\"POST\", body=b\"a\"),\n            b\"5`\\xe2y4\\xd0\\x9d\\xee\\xe0\\xbatw\\x87Q\\xe8O\\xd78\\xfc\\xe7\",\n            {},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l\",\n            {},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"]\\xc7\\x1f\\xf2\\xafG2\\xbc\\xa4\\xfa\\x99\\n33\\xda\\x18\\x94\\x81U.\",\n            {\"include_headers\": [\"A\"]},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"<\\x1a\\xeb\\x85y\\xdeW\\xfb\\xdcq\\x88\\xee\\xaf\\x17\\xdd\\x0c\\xbfH\\x18\\x1f\",\n            {\"keep_fragments\": True},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"\\xc1\\xef~\\x94\\x9bS\\xc1\\x83\\t\\xdcz8\\x9f\\xdc{\\x11\\x16I.\\x11\",\n            {\"include_headers\": [\"A\"], \"keep_fragments\": True},\n        ),\n        (\n            Request(\"https://example.org/ab\"),\n            b\"N\\xe5l\\xb8\\x12@iw\\xe2\\xf3\\x1bp\\xea\\xffp!u\\xe2\\x8a\\xc6\",\n            {},\n        ),\n        (\n            Request(\"https://example.org/a\", body=b\"b\"),\n            b\"_NOv\\xbco$6\\xfcW\\x9f\\xb24g\\x9f\\xbb\\xdd\\xa82\\xc5\",\n            {},\n        ),\n    )\n\n    def test_query_string_key_order(self):\n        r1 = Request(\"http://www.example.com/query?id=111&cat=222\")\n        r2 = Request(\"http://www.example.com/query?cat=222&id=111\")\n        assert self.function(r1) == self.function(r1)\n        assert self.function(r1) == self.function(r2)\n\n    def test_query_string_key_without_value(self):\n        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78132,199\")\n        r2 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n        assert self.function(r1) != self.function(r2)\n\n    def test_caching(self):\n        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n        assert self.function(r1) == self.cache[r1][self.default_cache_key]\n", "n_tokens": 1139, "byte_len": 3698, "file_sha1": "bdf80aea8d701dca7a7469b22cd3bc84998b2cd9", "start_line": 68, "end_line": 167}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py", "rel_path": "tests/test_utils_request.py", "module": "tests.test_utils_request", "ext": "py", "chunk_number": 3, "symbols": ["test_header", "test_headers", "test_fragment", "test_method_and_body", "test_request_replace", "test_part_separation", "test_hashes", "test_default_implementation", "test_deprecated_implementation", "test_include_headers", "fingerprint", "test_dont_canonicalize", "test_meta", "TestRequestFingerprinter", "TestCustomRequestFingerprinter", "RequestFingerprinter", "method", "reques", "fingerprinte", "future", "serialize", "after", "test", "known", "hashes", "https", "make", "get", "crawler", "dont", "test_request_authenticate", "test_request_httprepr", "test_request_httprepr_for_non_http_request", "test_query_string_key_order", "test_query_string_key_without_value", "test_caching", "test_from_crawler", "from_crawler", "__init__", "test_from_settings", "from_settings", "test_from_crawler_and_settings", "_test_request", "test_get", "test_post", "test_cookies_dict", "test_cookies_list", "TestFingerprint", "TestRequestToCurl", "presence"], "ast_kind": "class_or_type", "text": "    def test_header(self):\n        r1 = Request(\"http://www.example.com/members/offers.html\")\n        r2 = Request(\"http://www.example.com/members/offers.html\")\n        r2.headers[\"SESSIONID\"] = b\"somehash\"\n        assert self.function(r1) == self.function(r2)\n\n    def test_headers(self):\n        r1 = Request(\"http://www.example.com/\")\n        r2 = Request(\"http://www.example.com/\")\n        r2.headers[\"Accept-Language\"] = b\"en\"\n        r3 = Request(\"http://www.example.com/\")\n        r3.headers[\"Accept-Language\"] = b\"en\"\n        r3.headers[\"SESSIONID\"] = b\"somehash\"\n\n        assert self.function(r1) == self.function(r2) == self.function(r3)\n\n        assert self.function(r1) == self.function(\n            r1, include_headers=[\"Accept-Language\"]\n        )\n\n        assert self.function(r1) != self.function(\n            r2, include_headers=[\"Accept-Language\"]\n        )\n\n        assert self.function(\n            r3, include_headers=[\"accept-language\", \"sessionid\"]\n        ) == self.function(r3, include_headers=[\"SESSIONID\", \"Accept-Language\"])\n\n    def test_fragment(self):\n        r1 = Request(\"http://www.example.com/test.html\")\n        r2 = Request(\"http://www.example.com/test.html#fragment\")\n        assert self.function(r1) == self.function(r2)\n        assert self.function(r1) == self.function(r1, keep_fragments=True)\n        assert self.function(r2) != self.function(r2, keep_fragments=True)\n        assert self.function(r1) != self.function(r2, keep_fragments=True)\n\n    def test_method_and_body(self):\n        r1 = Request(\"http://www.example.com\")\n        r2 = Request(\"http://www.example.com\", method=\"POST\")\n        r3 = Request(\"http://www.example.com\", method=\"POST\", body=b\"request body\")\n\n        assert self.function(r1) != self.function(r2)\n        assert self.function(r2) != self.function(r3)\n\n    def test_request_replace(self):\n        # cached fingerprint must be cleared on request copy\n        r1 = Request(\"http://www.example.com\")\n        fp1 = self.function(r1)\n        r2 = r1.replace(url=\"http://www.example.com/other\")\n        fp2 = self.function(r2)\n        assert fp1 != fp2\n\n    def test_part_separation(self):\n        # An old implementation used to serialize request data in a way that\n        # would put the body right after the URL.\n        r1 = Request(\"http://www.example.com/foo\")\n        fp1 = self.function(r1)\n        r2 = Request(\"http://www.example.com/f\", body=b\"oo\")\n        fp2 = self.function(r2)\n        assert fp1 != fp2\n\n    def test_hashes(self):\n        \"\"\"Test hardcoded hashes, to make sure future changes to not introduce\n        backward incompatibilities.\"\"\"\n        actual = [\n            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes\n        ]\n        expected = [_fingerprint for _, _fingerprint, _ in self.known_hashes]\n        assert actual == expected\n\n\nclass TestRequestFingerprinter:\n    def test_default_implementation(self):\n        crawler = get_crawler()\n        request = Request(\"https://example.com\")\n        assert crawler.request_fingerprinter.fingerprint(request) == fingerprint(\n            request\n        )\n\n    def test_deprecated_implementation(self):\n        settings = {\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n        }\n        with warnings.catch_warnings(record=True) as logged_warnings:\n            crawler = get_crawler(settings_dict=settings)\n        request = Request(\"https://example.com\")\n        assert crawler.request_fingerprinter.fingerprint(request) == fingerprint(\n            request\n        )\n        assert logged_warnings\n\n\nclass TestCustomRequestFingerprinter:\n    def test_include_headers(self):\n        class RequestFingerprinter:\n            def fingerprint(self, request):\n                return fingerprint(request, include_headers=[\"X-ID\"])\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        r1 = Request(\"http://www.example.com\", headers={\"X-ID\": \"1\"})\n        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n        r2 = Request(\"http://www.example.com\", headers={\"X-ID\": \"2\"})\n        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n        assert fp1 != fp2\n\n    def test_dont_canonicalize(self):\n        class RequestFingerprinter:\n            cache = WeakKeyDictionary()\n\n            def fingerprint(self, request):\n                if request not in self.cache:\n                    fp = sha1()\n                    fp.update(to_bytes(request.url))\n                    self.cache[request] = fp.digest()\n                return self.cache[request]\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        r1 = Request(\"http://www.example.com?a=1&a=2\")\n        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n        r2 = Request(\"http://www.example.com?a=2&a=1\")\n        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n        assert fp1 != fp2\n\n    def test_meta(self):\n        class RequestFingerprinter:", "n_tokens": 1147, "byte_len": 5109, "file_sha1": "bdf80aea8d701dca7a7469b22cd3bc84998b2cd9", "start_line": 168, "end_line": 300}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py", "rel_path": "tests/test_utils_request.py", "module": "tests.test_utils_request", "ext": "py", "chunk_number": 4, "symbols": ["fingerprint", "test_from_crawler", "from_crawler", "__init__", "test_from_settings", "from_settings", "test_from_crawler_and_settings", "_test_request", "test_get", "test_post", "test_headers", "test_cookies_dict", "RequestFingerprinter", "TestRequestToCurl", "post", "settings", "dict", "method", "presence", "request", "curl", "dumps", "httpbin", "application", "simplefilter", "cookies", "scrapy", "deprecation", "test", "fingerprinter", "test_request_authenticate", "test_request_httprepr", "test_request_httprepr_for_non_http_request", "test_query_string_key_order", "test_query_string_key_without_value", "test_caching", "test_header", "test_fragment", "test_method_and_body", "test_request_replace", "test_part_separation", "test_hashes", "test_default_implementation", "test_deprecated_implementation", "test_include_headers", "test_dont_canonicalize", "test_meta", "test_cookies_list", "TestFingerprint", "TestRequestFingerprinter"], "ast_kind": "class_or_type", "text": "            def fingerprint(self, request):\n                if \"fingerprint\" in request.meta:\n                    return request.meta[\"fingerprint\"]\n                return fingerprint(request)\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        r1 = Request(\"http://www.example.com\")\n        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n        r2 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"a\"})\n        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n        r3 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"a\"})\n        fp3 = crawler.request_fingerprinter.fingerprint(r3)\n        r4 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"b\"})\n        fp4 = crawler.request_fingerprinter.fingerprint(r4)\n        assert fp1 != fp2\n        assert fp1 != fp4\n        assert fp2 != fp4\n        assert fp2 == fp3\n\n    def test_from_crawler(self):\n        class RequestFingerprinter:\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler)\n\n            def __init__(self, crawler):\n                self._fingerprint = crawler.settings[\"FINGERPRINT\"]\n\n            def fingerprint(self, request):\n                return self._fingerprint\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n            \"FINGERPRINT\": b\"fingerprint\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        request = Request(\"http://www.example.com\")\n        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n        assert fingerprint == settings[\"FINGERPRINT\"]\n\n    def test_from_settings(self):\n        class RequestFingerprinter:\n            @classmethod\n            def from_settings(cls, settings):\n                return cls(settings)\n\n            def __init__(self, settings):\n                self._fingerprint = settings[\"FINGERPRINT\"]\n\n            def fingerprint(self, request):\n                return self._fingerprint\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n            \"FINGERPRINT\": b\"fingerprint\",\n        }\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            crawler = get_crawler(settings_dict=settings)\n\n        request = Request(\"http://www.example.com\")\n        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n        assert fingerprint == settings[\"FINGERPRINT\"]\n\n    def test_from_crawler_and_settings(self):\n        class RequestFingerprinter:\n            # This method is ignored due to the presence of from_crawler\n            @classmethod\n            def from_settings(cls, settings):\n                return cls(settings)\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler)\n\n            def __init__(self, crawler):\n                self._fingerprint = crawler.settings[\"FINGERPRINT\"]\n\n            def fingerprint(self, request):\n                return self._fingerprint\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n            \"FINGERPRINT\": b\"fingerprint\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        request = Request(\"http://www.example.com\")\n        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n        assert fingerprint == settings[\"FINGERPRINT\"]\n\n\nclass TestRequestToCurl:\n    def _test_request(self, request_object, expected_curl_command):\n        curl_command = request_to_curl(request_object)\n        assert curl_command == expected_curl_command\n\n    def test_get(self):\n        request_object = Request(\"https://www.example.com\")\n        expected_curl_command = \"curl -X GET https://www.example.com\"\n        self._test_request(request_object, expected_curl_command)\n\n    def test_post(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            'curl -X POST https://www.httpbin.org/post --data-raw \\'{\"foo\": \"bar\"}\\''\n        )\n        self._test_request(request_object, expected_curl_command)\n\n    def test_headers(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"},\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            \"curl -X POST https://www.httpbin.org/post\"\n            ' --data-raw \\'{\"foo\": \"bar\"}\\''\n            \" -H 'Content-Type: application/json' -H 'Accept: application/json'\"\n        )\n        self._test_request(request_object, expected_curl_command)\n\n    def test_cookies_dict(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            cookies={\"foo\": \"bar\"},\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            \"curl -X POST https://www.httpbin.org/post\"\n            \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n        )\n        self._test_request(request_object, expected_curl_command)\n", "n_tokens": 1109, "byte_len": 5336, "file_sha1": "bdf80aea8d701dca7a7469b22cd3bc84998b2cd9", "start_line": 301, "end_line": 445}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_request.py", "rel_path": "tests/test_utils_request.py", "module": "tests.test_utils_request", "ext": "py", "chunk_number": 5, "symbols": ["test_cookies_list", "post", "data", "method", "dumps", "test", "request", "expected", "curl", "httpbin", "json", "self", "object", "cookies", "https", "body", "cookie", "test_request_authenticate", "test_request_httprepr", "test_request_httprepr_for_non_http_request", "test_query_string_key_order", "test_query_string_key_without_value", "test_caching", "test_header", "test_headers", "test_fragment", "test_method_and_body", "test_request_replace", "test_part_separation", "test_hashes", "test_default_implementation", "test_deprecated_implementation", "test_include_headers", "fingerprint", "test_dont_canonicalize", "test_meta", "test_from_crawler", "from_crawler", "__init__", "test_from_settings", "from_settings", "test_from_crawler_and_settings", "_test_request", "test_get", "test_post", "test_cookies_dict", "TestFingerprint", "TestRequestFingerprinter", "TestCustomRequestFingerprinter", "RequestFingerprinter"], "ast_kind": "function_or_method", "text": "    def test_cookies_list(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            cookies=[{\"foo\": \"bar\"}],\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            \"curl -X POST https://www.httpbin.org/post\"\n            \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n        )\n        self._test_request(request_object, expected_curl_command)\n", "n_tokens": 107, "byte_len": 463, "file_sha1": "bdf80aea8d701dca7a7469b22cd3bc84998b2cd9", "start_line": 446, "end_line": 458}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_item.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_item.py", "rel_path": "tests/test_item.py", "module": "tests.test_item", "ext": "py", "chunk_number": 1, "symbols": ["assertSortedEqual", "test_simple", "test_init", "test_invalid_field", "test_repr", "test_private_attr", "test_raise_getattr", "test_raise_setattr", "test_custom_methods", "get_name", "change_name", "test_metaclass", "test_metaclass_with_fields_attribute", "test_metaclass_inheritance", "test_metaclass_multiple_inheritance_simple", "test_metaclass_multiple_inheritance_diamond", "TestItem", "ParentItem", "A", "B", "C", "D", "E", "used", "values", "repr", "text", "number", "test", "metaclass", "test_metaclass_multiple_inheritance_without_metaclass", "test_to_dict", "test_copy", "test_deepcopy", "test_new_method_propagates_classcell", "f", "test_item_meta_classcell_regression", "__init__", "TestItemMeta", "MyItem", "TestItemMetaClassCellRegression", "append", "private", "tag", "tag1", "first", "call", "mock", "name", "item"], "ast_kind": "class_or_type", "text": "from abc import ABCMeta\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.item import Field, Item, ItemMeta\n\n\nclass TestItem:\n    def assertSortedEqual(self, first, second, msg=None):\n        assert sorted(first) == sorted(second), msg\n\n    def test_simple(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"name\"\n        assert i[\"name\"] == \"name\"\n\n    def test_init(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        with pytest.raises(KeyError):\n            i[\"name\"]\n\n        i2 = TestItem(name=\"john doe\")\n        assert i2[\"name\"] == \"john doe\"\n\n        i3 = TestItem({\"name\": \"john doe\"})\n        assert i3[\"name\"] == \"john doe\"\n\n        i4 = TestItem(i3)\n        assert i4[\"name\"] == \"john doe\"\n\n        with pytest.raises(KeyError):\n            TestItem({\"name\": \"john doe\", \"other\": \"foo\"})\n\n    def test_invalid_field(self):\n        class TestItem(Item):\n            pass\n\n        i = TestItem()\n        with pytest.raises(KeyError):\n            i[\"field\"] = \"text\"\n        with pytest.raises(KeyError):\n            i[\"field\"]\n\n    def test_repr(self):\n        class TestItem(Item):\n            name = Field()\n            number = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John Doe\"\n        i[\"number\"] = 123\n        itemrepr = repr(i)\n\n        assert itemrepr == \"{'name': 'John Doe', 'number': 123}\"\n\n        i2 = eval(itemrepr)  # pylint: disable=eval-used\n        assert i2[\"name\"] == \"John Doe\"\n        assert i2[\"number\"] == 123\n\n    def test_private_attr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i._private = \"test\"\n        assert i._private == \"test\"\n\n    def test_raise_getattr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        with pytest.raises(AttributeError):\n            i.name\n\n    def test_raise_setattr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        with pytest.raises(AttributeError):\n            i.name = \"john\"\n\n    def test_custom_methods(self):\n        class TestItem(Item):\n            name = Field()\n\n            def get_name(self):\n                return self[\"name\"]\n\n            def change_name(self, name):\n                self[\"name\"] = name\n\n        i = TestItem()\n        with pytest.raises(KeyError):\n            i.get_name()\n        i[\"name\"] = \"lala\"\n        assert i.get_name() == \"lala\"\n        i.change_name(\"other\")\n        assert i.get_name() == \"other\"\n\n    def test_metaclass(self):\n        class TestItem(Item):\n            name = Field()\n            keys = Field()\n            values = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John\"\n        assert list(i.keys()) == [\"name\"]\n        assert list(i.values()) == [\"John\"]\n\n        i[\"keys\"] = \"Keys\"\n        i[\"values\"] = \"Values\"\n        self.assertSortedEqual(list(i.keys()), [\"keys\", \"values\", \"name\"])\n        self.assertSortedEqual(list(i.values()), [\"Keys\", \"Values\", \"John\"])\n\n    def test_metaclass_with_fields_attribute(self):\n        class TestItem(Item):\n            fields = {\"new\": Field(default=\"X\")}\n\n        item = TestItem(new=\"New\")\n        self.assertSortedEqual(list(item.keys()), [\"new\"])\n        self.assertSortedEqual(list(item.values()), [\"New\"])\n\n    def test_metaclass_inheritance(self):\n        class ParentItem(Item):\n            name = Field()\n            keys = Field()\n            values = Field()\n\n        class TestItem(ParentItem):\n            keys = Field()\n\n        i = TestItem()\n        i[\"keys\"] = 3\n        assert list(i.keys()) == [\"keys\"]\n        assert list(i.values()) == [3]\n\n    def test_metaclass_multiple_inheritance_simple(self):\n        class A(Item):\n            fields = {\"load\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C(Item):\n            fields = {\"load\": Field(default=\"C\")}\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            pass\n\n        item = D(save=\"X\", load=\"Y\")\n        assert item[\"save\"] == \"X\"\n        assert item[\"load\"] == \"Y\"\n        assert D.fields == {\"load\": {\"default\": \"A\"}, \"save\": {\"default\": \"A\"}}\n\n        # D class inverted\n        class E(C, B):\n            pass\n\n        assert E(save=\"X\")[\"save\"] == \"X\"\n        assert E(load=\"X\")[\"load\"] == \"X\"\n        assert E.fields == {\"load\": {\"default\": \"C\"}, \"save\": {\"default\": \"C\"}}\n\n    def test_metaclass_multiple_inheritance_diamond(self):\n        class A(Item):\n            fields = {\"update\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n            load = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C(A):\n            fields = {\"update\": Field(default=\"C\")}\n            save = Field(default=\"C\")\n", "n_tokens": 1138, "byte_len": 4843, "file_sha1": "8703865a505c18f6da71e67aa16b03df5440896e", "start_line": 1, "end_line": 187}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_item.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_item.py", "rel_path": "tests/test_item.py", "module": "tests.test_item", "ext": "py", "chunk_number": 2, "symbols": ["test_metaclass_multiple_inheritance_without_metaclass", "test_to_dict", "test_copy", "test_deepcopy", "test_new_method_propagates_classcell", "f", "test_item_meta_classcell_regression", "__init__", "D", "E", "A", "B", "C", "TestItem", "TestItemMeta", "MyItem", "TestItemMetaClassCellRegression", "append", "tag", "tag1", "first", "call", "mock", "name", "test", "item", "load", "https", "save", "pytest", "assertSortedEqual", "test_simple", "test_init", "test_invalid_field", "test_repr", "test_private_attr", "test_raise_getattr", "test_raise_setattr", "test_custom_methods", "get_name", "change_name", "test_metaclass", "test_metaclass_with_fields_attribute", "test_metaclass_inheritance", "test_metaclass_multiple_inheritance_simple", "test_metaclass_multiple_inheritance_diamond", "ParentItem", "private", "lala", "get"], "ast_kind": "class_or_type", "text": "        class D(B, C):\n            fields = {\"update\": Field(default=\"D\")}\n            load = Field(default=\"D\")\n\n        assert D(save=\"X\")[\"save\"] == \"X\"\n        assert D(load=\"X\")[\"load\"] == \"X\"\n        assert D.fields == {\n            \"save\": {\"default\": \"C\"},\n            \"load\": {\"default\": \"D\"},\n            \"update\": {\"default\": \"D\"},\n        }\n\n        # D class inverted\n        class E(C, B):\n            load = Field(default=\"E\")\n\n        assert E(save=\"X\")[\"save\"] == \"X\"\n        assert E(load=\"X\")[\"load\"] == \"X\"\n        assert E.fields == {\n            \"save\": {\"default\": \"C\"},\n            \"load\": {\"default\": \"E\"},\n            \"update\": {\"default\": \"C\"},\n        }\n\n    def test_metaclass_multiple_inheritance_without_metaclass(self):\n        class A(Item):\n            fields = {\"load\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C:\n            fields = {\"load\": Field(default=\"C\")}\n            not_allowed = Field(default=\"not_allowed\")\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            pass\n\n        with pytest.raises(KeyError):\n            D(not_allowed=\"value\")\n        assert D(save=\"X\")[\"save\"] == \"X\"\n        assert D.fields == {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}}\n\n        # D class inverted\n        class E(C, B):\n            pass\n\n        with pytest.raises(KeyError):\n            E(not_allowed=\"value\")\n        assert E(save=\"X\")[\"save\"] == \"X\"\n        assert E.fields == {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}}\n\n    def test_to_dict(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John\"\n        assert dict(i) == {\"name\": \"John\"}\n\n    def test_copy(self):\n        class TestItem(Item):\n            name = Field()\n\n        item = TestItem({\"name\": \"lower\"})\n        copied_item = item.copy()\n        assert id(item) != id(copied_item)\n        copied_item[\"name\"] = copied_item[\"name\"].upper()\n        assert item[\"name\"] != copied_item[\"name\"]\n\n    def test_deepcopy(self):\n        class TestItem(Item):\n            tags = Field()\n\n        item = TestItem({\"tags\": [\"tag1\"]})\n        copied_item = item.deepcopy()\n        item[\"tags\"].append(\"tag2\")\n        assert item[\"tags\"] != copied_item[\"tags\"]\n\n\nclass TestItemMeta:\n    def test_new_method_propagates_classcell(self):\n        new_mock = mock.Mock(side_effect=ABCMeta.__new__)\n        base = ItemMeta.__bases__[0]\n\n        with mock.patch.object(base, \"__new__\", new_mock):\n\n            class MyItem(Item):\n                def f(self):\n                    # For rationale of this see:\n                    # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n                    return __class__\n\n            MyItem()\n\n        (first_call, second_call) = new_mock.call_args_list[-2:]\n\n        mcs, class_name, bases, attrs = first_call[0]\n        assert \"__classcell__\" not in attrs\n        mcs, class_name, bases, attrs = second_call[0]\n        assert \"__classcell__\" in attrs\n\n\nclass TestItemMetaClassCellRegression:\n    def test_item_meta_classcell_regression(self):\n        class MyItem(Item, metaclass=ItemMeta):\n            def __init__(self, *args, **kwargs):  # pylint: disable=useless-parent-delegation\n                # This call to super() trigger the __classcell__ propagation\n                # requirement. When not done properly raises an error:\n                # TypeError: __class__ set to <class '__main__.MyItem'>\n                # defining 'MyItem' as <class '__main__.MyItem'>\n                super().__init__(*args, **kwargs)\n", "n_tokens": 889, "byte_len": 3671, "file_sha1": "8703865a505c18f6da71e67aa16b03df5440896e", "start_line": 188, "end_line": 302}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_webclient.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_webclient.py", "rel_path": "tests/test_webclient.py", "module": "tests.test_webclient", "ext": "py", "chunk_number": 1, "symbols": ["getPage", "_clientfactory", "test_earlyHeaders", "_test", "TestScrapyHTTPPageGetter", "contextfactory", "method", "protocol", "agent", "name", "test", "early", "forever", "taking", "useful", "future", "testvalue", "make", "connection", "mockserver", "policies", "webclient", "get", "crawler", "sent", "pytest", "unicode", "ssl", "context", "page", "test_non_standard_line_endings", "render", "_listen", "wrapper", "server_url", "testPayload", "testHostHeader", "test_getPage", "test_getPageHead", "_getPage", "test_timeoutNotTriggering", "test_timeoutTriggering", "testNotFound", "testFactoryInfo", "testRedirect", "test_encoding", "testPayloadDisabledCipher", "EncodingResource", "BrokenDownloadResource", "ErrorResource"], "ast_kind": "class_or_type", "text": "\"\"\"\nTests borrowed from the twisted.web.client tests.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom urllib.parse import urlparse\n\nimport OpenSSL.SSL\nimport pytest\nfrom pytest_twisted import async_yield_fixture\nfrom twisted.internet import defer\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.internet.testing import StringTransport\nfrom twisted.protocols.policies import WrappingFactory\nfrom twisted.web import resource, server, static, util\nfrom twisted.web.client import _makeGetterFactory\n\nfrom scrapy.core.downloader import webclient as client\nfrom scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\nfrom scrapy.http import Headers, Request\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.python import to_bytes, to_unicode\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http_resources import (\n    ForeverTakingResource,\n    HostHeaderResource,\n    PayloadResource,\n)\nfrom tests.mockserver.utils import ssl_context_factory\nfrom tests.test_core_downloader import TestContextFactoryBase\n\n\ndef getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n    \"\"\"Adapted version of twisted.web.client.getPage\"\"\"\n\n    def _clientfactory(url, *args, **kwargs):\n        url = to_unicode(url)\n        timeout = kwargs.pop(\"timeout\", 0)\n        f = client.ScrapyHTTPClientFactory(\n            Request(url, *args, **kwargs), timeout=timeout\n        )\n        f.deferred.addCallback(response_transform or (lambda r: r.body))\n        return f\n\n    return _makeGetterFactory(\n        to_bytes(url),\n        _clientfactory,\n        contextFactory=contextFactory,\n        *args,\n        **kwargs,\n    ).deferred\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestScrapyHTTPPageGetter:\n    def test_earlyHeaders(self):\n        # basic test stolen from twisted HTTPageGetter\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                url=\"http://foo/bar\",\n                body=\"some data\",\n                headers={\n                    \"Host\": \"example.net\",\n                    \"User-Agent\": \"fooble\",\n                    \"Cookie\": \"blah blah\",\n                    \"Content-Length\": \"12981\",\n                    \"Useful\": \"value\",\n                },\n            )\n        )\n\n        self._test(\n            factory,\n            b\"GET /bar HTTP/1.0\\r\\n\"\n            b\"Content-Length: 9\\r\\n\"\n            b\"Useful: value\\r\\n\"\n            b\"Connection: close\\r\\n\"\n            b\"User-Agent: fooble\\r\\n\"\n            b\"Host: example.net\\r\\n\"\n            b\"Cookie: blah blah\\r\\n\"\n            b\"\\r\\n\"\n            b\"some data\",\n        )\n\n        # test minimal sent headers\n        factory = client.ScrapyHTTPClientFactory(Request(\"http://foo/bar\"))\n        self._test(factory, b\"GET /bar HTTP/1.0\\r\\nHost: foo\\r\\n\\r\\n\")\n\n        # test a simple POST with body and content-type\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                method=\"POST\",\n                url=\"http://foo/bar\",\n                body=\"name=value\",\n                headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n            )\n        )\n\n        self._test(\n            factory,\n            b\"POST /bar HTTP/1.0\\r\\n\"\n            b\"Host: foo\\r\\n\"\n            b\"Connection: close\\r\\n\"\n            b\"Content-Type: application/x-www-form-urlencoded\\r\\n\"\n            b\"Content-Length: 10\\r\\n\"\n            b\"\\r\\n\"\n            b\"name=value\",\n        )\n\n        # test a POST method with no body provided\n        factory = client.ScrapyHTTPClientFactory(\n            Request(method=\"POST\", url=\"http://foo/bar\")\n        )\n\n        self._test(\n            factory,\n            b\"POST /bar HTTP/1.0\\r\\nHost: foo\\r\\nContent-Length: 0\\r\\n\\r\\n\",\n        )\n\n        # test with single and multivalued headers\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                url=\"http://foo/bar\",\n                headers={\n                    \"X-Meta-Single\": \"single\",\n                    \"X-Meta-Multivalued\": [\"value1\", \"value2\"],\n                },\n            )\n        )\n\n        self._test(\n            factory,\n            b\"GET /bar HTTP/1.0\\r\\n\"\n            b\"Host: foo\\r\\n\"\n            b\"X-Meta-Multivalued: value1\\r\\n\"\n            b\"X-Meta-Multivalued: value2\\r\\n\"\n            b\"X-Meta-Single: single\\r\\n\"\n            b\"\\r\\n\",\n        )\n\n        # same test with single and multivalued headers but using Headers class\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                url=\"http://foo/bar\",\n                headers=Headers(\n                    {\n                        \"X-Meta-Single\": \"single\",\n                        \"X-Meta-Multivalued\": [\"value1\", \"value2\"],\n                    }\n                ),\n            )\n        )\n\n        self._test(\n            factory,\n            b\"GET /bar HTTP/1.0\\r\\n\"\n            b\"Host: foo\\r\\n\"\n            b\"X-Meta-Multivalued: value1\\r\\n\"\n            b\"X-Meta-Multivalued: value2\\r\\n\"\n            b\"X-Meta-Single: single\\r\\n\"\n            b\"\\r\\n\",\n        )\n\n    def _test(self, factory, testvalue):\n        transport = StringTransport()\n        protocol = client.ScrapyHTTPPageGetter()\n        protocol.factory = factory\n        protocol.makeConnection(transport)\n        assert set(transport.value().splitlines()) == set(testvalue.splitlines())\n        return testvalue\n", "n_tokens": 1210, "byte_len": 5431, "file_sha1": "98718148c7565b49558beabf1e12c1e3cbdee245", "start_line": 1, "end_line": 172}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_webclient.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_webclient.py", "rel_path": "tests/test_webclient.py", "module": "tests.test_webclient", "ext": "py", "chunk_number": 2, "symbols": ["test_non_standard_line_endings", "render", "_listen", "wrapper", "server_url", "testPayload", "testHostHeader", "test_getPage", "test_getPageHead", "_getPage", "test_timeoutNotTriggering", "test_timeoutTriggering", "testNotFound", "EncodingResource", "BrokenDownloadResource", "ErrorResource", "NoLengthResource", "TestWebClient", "encoding", "method", "clean", "protocol", "async", "such", "site", "ticket", "broken", "download", "managed", "test", "getPage", "_clientfactory", "test_earlyHeaders", "_test", "testFactoryInfo", "testRedirect", "test_encoding", "testPayloadDisabledCipher", "TestScrapyHTTPPageGetter", "TestWebClientSSL", "TestWebClientCustomCiphersSSL", "contextfactory", "get", "agent", "retrieved", "cp1251", "payload", "connection", "data", "received"], "ast_kind": "class_or_type", "text": "    def test_non_standard_line_endings(self):\n        # regression test for: http://dev.scrapy.org/ticket/258\n        factory = client.ScrapyHTTPClientFactory(Request(url=\"http://foo/bar\"))\n        protocol = client.ScrapyHTTPPageGetter()\n        protocol.factory = factory\n        protocol.headers = Headers()\n        protocol.dataReceived(b\"HTTP/1.0 200 OK\\n\")\n        protocol.dataReceived(b\"Hello: World\\n\")\n        protocol.dataReceived(b\"Foo: Bar\\n\")\n        protocol.dataReceived(b\"\\n\")\n        assert protocol.headers == Headers({\"Hello\": [\"World\"], \"Foo\": [\"Bar\"]})\n\n\nclass EncodingResource(resource.Resource):\n    out_encoding = \"cp1251\"\n\n    def render(self, request):\n        body = to_unicode(request.content.read())\n        request.setHeader(b\"content-encoding\", self.out_encoding)\n        return body.encode(self.out_encoding)\n\n\nclass BrokenDownloadResource(resource.Resource):\n    def render(self, request):\n        # only sends 3 bytes even though it claims to send 5\n        request.setHeader(b\"content-length\", b\"5\")\n        request.write(b\"abc\")\n        return b\"\"\n\n\nclass ErrorResource(resource.Resource):\n    def render(self, request):\n        request.setResponseCode(401)\n        if request.args.get(b\"showlength\"):\n            request.setHeader(b\"content-length\", b\"0\")\n        return b\"\"\n\n\nclass NoLengthResource(resource.Resource):\n    def render(self, request):\n        return b\"nolength\"\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestWebClient:\n    def _listen(self, site):\n        from twisted.internet import reactor\n\n        return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n\n    @pytest.fixture\n    def wrapper(self, tmp_path):\n        (tmp_path / \"file\").write_bytes(b\"0123456789\")\n        r = static.File(str(tmp_path))\n        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n        r.putChild(b\"wait\", ForeverTakingResource())\n        r.putChild(b\"error\", ErrorResource())\n        r.putChild(b\"nolength\", NoLengthResource())\n        r.putChild(b\"host\", HostHeaderResource())\n        r.putChild(b\"payload\", PayloadResource())\n        r.putChild(b\"broken\", BrokenDownloadResource())\n        r.putChild(b\"encoding\", EncodingResource())\n        site = server.Site(r, timeout=None)\n        return WrappingFactory(site)\n\n    @async_yield_fixture\n    async def server_port(self, wrapper):\n        port = self._listen(wrapper)\n\n        yield port.getHost().port\n\n        await port.stopListening()\n\n    @pytest.fixture\n    def server_url(self, server_port):\n        return f\"http://127.0.0.1:{server_port}/\"\n\n    @inlineCallbacks\n    def testPayload(self, server_url):\n        s = \"0123456789\" * 10\n        body = yield getPage(server_url + \"payload\", body=s)\n        assert body == to_bytes(s)\n\n    @inlineCallbacks\n    def testHostHeader(self, server_port, server_url):\n        # if we pass Host header explicitly, it should be used, otherwise\n        # it should extract from url\n        body = yield getPage(server_url + \"host\")\n        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n        body = yield getPage(server_url + \"host\", headers={\"Host\": \"www.example.com\"})\n        assert body == to_bytes(\"www.example.com\")\n\n    @inlineCallbacks\n    def test_getPage(self, server_url):\n        \"\"\"\n        L{client.getPage} returns a L{Deferred} which is called back with\n        the body of the response if the default method B{GET} is used.\n        \"\"\"\n        body = yield getPage(server_url + \"file\")\n        assert body == b\"0123456789\"\n\n    @inlineCallbacks\n    def test_getPageHead(self, server_url):\n        \"\"\"\n        L{client.getPage} returns a L{Deferred} which is called back with\n        the empty string if the method is C{HEAD} and there is a successful\n        response code.\n        \"\"\"\n\n        def _getPage(method):\n            return getPage(server_url + \"file\", method=method)\n\n        body = yield _getPage(\"head\")\n        assert body == b\"\"\n        body = yield _getPage(\"HEAD\")\n        assert body == b\"\"\n\n    @inlineCallbacks\n    def test_timeoutNotTriggering(self, server_port, server_url):\n        \"\"\"\n        When a non-zero timeout is passed to L{getPage} and the page is\n        retrieved before the timeout period elapses, the L{Deferred} is\n        called back with the contents of the page.\n        \"\"\"\n        body = yield getPage(server_url + \"host\", timeout=100)\n        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n\n    @inlineCallbacks\n    def test_timeoutTriggering(self, wrapper, server_url):\n        \"\"\"\n        When a non-zero timeout is passed to L{getPage} and that many\n        seconds elapse before the server responds to the request. the\n        L{Deferred} is errbacked with a L{error.TimeoutError}.\n        \"\"\"\n        with pytest.raises(defer.TimeoutError):\n            yield getPage(server_url + \"wait\", timeout=0.000001)\n        # Clean up the server which is hanging around not doing\n        # anything.\n        connected = list(wrapper.protocols.keys())\n        # There might be nothing here if the server managed to already see\n        # that the connection was lost.\n        if connected:\n            connected[0].transport.loseConnection()\n\n    @inlineCallbacks\n    def testNotFound(self, server_url):\n        body = yield getPage(server_url + \"notsuchfile\")\n        assert b\"404 - No Such Resource\" in body\n", "n_tokens": 1221, "byte_len": 5364, "file_sha1": "98718148c7565b49558beabf1e12c1e3cbdee245", "start_line": 173, "end_line": 321}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_webclient.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_webclient.py", "rel_path": "tests/test_webclient.py", "module": "tests.test_webclient", "ext": "py", "chunk_number": 3, "symbols": ["testFactoryInfo", "testRedirect", "test_encoding", "testPayload", "testPayloadDisabledCipher", "TestWebClientSSL", "TestWebClientCustomCiphersSSL", "encoding", "payload", "enabled", "error", "port", "length", "out", "get", "crawler", "pytest", "original", "body", "test", "web", "downloade", "clien", "equiv", "href", "page", "unicode", "ssl", "context", "here", "getPage", "_clientfactory", "test_earlyHeaders", "_test", "test_non_standard_line_endings", "render", "_listen", "wrapper", "server_url", "testHostHeader", "test_getPage", "test_getPageHead", "_getPage", "test_timeoutNotTriggering", "test_timeoutTriggering", "testNotFound", "TestScrapyHTTPPageGetter", "EncodingResource", "BrokenDownloadResource", "ErrorResource"], "ast_kind": "class_or_type", "text": "    @inlineCallbacks\n    def testFactoryInfo(self, server_url):\n        from twisted.internet import reactor\n\n        url = server_url + \"file\"\n        parsed = urlparse(url)\n        factory = client.ScrapyHTTPClientFactory(Request(url))\n        reactor.connectTCP(parsed.hostname, parsed.port, factory)\n        yield factory.deferred\n        assert factory.status == b\"200\"\n        assert factory.version.startswith(b\"HTTP/\")\n        assert factory.message == b\"OK\"\n        assert factory.response_headers[b\"content-length\"] == b\"10\"\n\n    @inlineCallbacks\n    def testRedirect(self, server_url):\n        body = yield getPage(server_url + \"redirect\")\n        assert (\n            body\n            == b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n            b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n            b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n'\n        )\n\n    @inlineCallbacks\n    def test_encoding(self, server_url):\n        \"\"\"Test that non-standart body encoding matches\n        Content-Encoding header\"\"\"\n        original_body = b\"\\xd0\\x81\\xd1\\x8e\\xd0\\xaf\"\n        response = yield getPage(\n            server_url + \"encoding\", body=original_body, response_transform=lambda r: r\n        )\n        content_encoding = to_unicode(response.headers[b\"Content-Encoding\"])\n        assert content_encoding == EncodingResource.out_encoding\n        assert response.body.decode(content_encoding) == to_unicode(original_body)\n\n\n@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\nclass TestWebClientSSL(TestContextFactoryBase):\n    @inlineCallbacks\n    def testPayload(self, server_url):\n        s = \"0123456789\" * 10\n        body = yield getPage(server_url + \"payload\", body=s)\n        assert body == to_bytes(s)\n\n\nclass TestWebClientCustomCiphersSSL(TestWebClientSSL):\n    # we try to use a cipher that is not enabled by default in OpenSSL\n    custom_ciphers = \"CAMELLIA256-SHA\"\n    context_factory = ssl_context_factory(cipher_string=custom_ciphers)\n\n    @inlineCallbacks\n    def testPayload(self, server_url):\n        s = \"0123456789\" * 10\n        crawler = get_crawler(\n            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n        )\n        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n        body = yield getPage(\n            server_url + \"payload\", body=s, contextFactory=client_context_factory\n        )\n        assert body == to_bytes(s)\n\n    @inlineCallbacks\n    def testPayloadDisabledCipher(self, server_url):\n        s = \"0123456789\" * 10\n        crawler = get_crawler(\n            settings_dict={\n                \"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"\n            }\n        )\n        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n        with pytest.raises(OpenSSL.SSL.Error):\n            yield getPage(\n                server_url + \"payload\", body=s, contextFactory=client_context_factory\n            )\n", "n_tokens": 695, "byte_len": 3052, "file_sha1": "98718148c7565b49558beabf1e12c1e3cbdee245", "start_line": 322, "end_line": 398}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_spider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_spider.py", "rel_path": "tests/test_utils_spider.py", "module": "tests.test_utils_spider", "ext": "py", "chunk_number": 1, "symbols": ["test_iterate_spider_output", "test_iter_spider_classes", "MySpider1", "MySpider2", "spider", "myspider2", "test", "iterate", "name", "item", "myspider", "class", "scrapy", "plw0406", "myspider1", "pylint", "iter", "plc0415", "noqa", "from", "list", "utils", "scrapytest", "assert", "request", "object", "import", "http", "tests", "disable", "self"], "ast_kind": "class_or_type", "text": "from scrapy import Spider\nfrom scrapy.http import Request\nfrom scrapy.item import Item\nfrom scrapy.utils.spider import iter_spider_classes, iterate_spider_output\n\n\nclass MySpider1(Spider):\n    name = \"myspider1\"\n\n\nclass MySpider2(Spider):\n    name = \"myspider2\"\n\n\ndef test_iterate_spider_output():\n    i = Item()\n    r = Request(\"http://scrapytest.org\")\n    o = object()\n\n    assert list(iterate_spider_output(i)) == [i]\n    assert list(iterate_spider_output(r)) == [r]\n    assert list(iterate_spider_output(o)) == [o]\n    assert list(iterate_spider_output([r, i, o])) == [r, i, o]\n\n\ndef test_iter_spider_classes():\n    import tests.test_utils_spider  # noqa: PLW0406,PLC0415  # pylint: disable=import-self\n\n    it = iter_spider_classes(tests.test_utils_spider)\n    assert set(it) == {MySpider1, MySpider2}\n", "n_tokens": 227, "byte_len": 807, "file_sha1": "f51efc04fe9f54aeaff973ced8a95a6c884e1514", "start_line": 1, "end_line": 31}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_fetch.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_fetch.py", "rel_path": "tests/test_command_fetch.py", "module": "tests.test_command_fetch", "ext": "py", "chunk_number": 1, "symbols": ["setup_class", "teardown_class", "test_output", "test_redirect_default", "test_redirect_disabled", "test_headers", "TestFetchCommand", "downloader", "text", "teardown", "class", "exit", "works", "refresh", "plain", "test", "output", "headers", "type", "replace", "meta", "twisted", "web", "redirect", "classmethod", "mockserver", "fetch", "redirected", "project", "commands", "enter", "from", "response", "status", "assert", "mock", "server", "here", "content", "strip", "none", "setup", "proc", "import", "required", "http", "self", "tests", "win", "win32"], "ast_kind": "class_or_type", "text": "from tests.mockserver.http import MockServer\nfrom tests.test_commands import TestProjectBase\n\n\nclass TestFetchCommand(TestProjectBase):\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def test_output(self):\n        _, out, _ = self.proc(\"fetch\", self.mockserver.url(\"/text\"))\n        assert out.strip() == \"Works\"\n\n    def test_redirect_default(self):\n        _, out, _ = self.proc(\"fetch\", self.mockserver.url(\"/redirect\"))\n        assert out.strip() == \"Redirected here\"\n\n    def test_redirect_disabled(self):\n        _, _, err = self.proc(\n            \"fetch\", \"--no-redirect\", self.mockserver.url(\"/redirect-no-meta-refresh\")\n        )\n        err = err.strip()\n        assert \"downloader/response_status_count/302\" in err, err\n        assert \"downloader/response_status_count/200\" not in err, err\n\n    def test_headers(self):\n        _, out, _ = self.proc(\"fetch\", self.mockserver.url(\"/text\"), \"--headers\")\n        out = out.replace(\"\\r\", \"\")  # required on win32\n        assert \"Server: TwistedWeb\" in out, out\n        assert \"Content-Type: text/plain\" in out\n", "n_tokens": 287, "byte_len": 1237, "file_sha1": "98c098bf09332c5fdbd0b7d3a352622754c4f5c6", "start_line": 1, "end_line": 36}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpauth.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_httpauth.py", "rel_path": "tests/test_downloadermiddleware_httpauth.py", "module": "tests.test_downloadermiddleware_httpauth", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "test_auth", "teardown_method", "test_no_auth", "test_auth_domain", "test_auth_subdomain", "test_auth_already_set", "LegacySpider", "DomainSpider", "AnyDomainSpider", "TestHttpAuthMiddlewareLegacy", "TestHttpAuthMiddleware", "TestHttpAuthAnyMiddleware", "noauth", "teardown", "method", "spider", "http", "auth", "test", "authorization", "lib", "w3lib", "class", "downloadermiddlewares", "with", "spiders", "scrapy", "digest", "headers", "legacy", "example", "pytest", "from", "pass", "assert", "request", "raises", "process", "domain", "any", "none", "opened", "import", "user", "httpauth", "self", "basic", "setup", "attribute"], "ast_kind": "class_or_type", "text": "import pytest\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\n\n\nclass LegacySpider(Spider):\n    http_user = \"foo\"\n    http_pass = \"bar\"\n\n\nclass DomainSpider(Spider):\n    http_user = \"foo\"\n    http_pass = \"bar\"\n    http_auth_domain = \"example.com\"\n\n\nclass AnyDomainSpider(Spider):\n    http_user = \"foo\"\n    http_pass = \"bar\"\n    http_auth_domain = None\n\n\nclass TestHttpAuthMiddlewareLegacy:\n    def setup_method(self):\n        self.spider = LegacySpider(\"foo\")\n\n    def test_auth(self):\n        mw = HttpAuthMiddleware()\n        with pytest.raises(AttributeError):\n            mw.spider_opened(self.spider)\n\n\nclass TestHttpAuthMiddleware:\n    def setup_method(self):\n        self.mw = HttpAuthMiddleware()\n        spider = DomainSpider(\"foo\")\n        self.mw.spider_opened(spider)\n\n    def teardown_method(self):\n        del self.mw\n\n    def test_no_auth(self):\n        req = Request(\"http://example-noauth.com/\")\n        assert self.mw.process_request(req) is None\n        assert \"Authorization\" not in req.headers\n\n    def test_auth_domain(self):\n        req = Request(\"http://example.com/\")\n        assert self.mw.process_request(req) is None\n        assert req.headers[\"Authorization\"] == basic_auth_header(\"foo\", \"bar\")\n\n    def test_auth_subdomain(self):\n        req = Request(\"http://foo.example.com/\")\n        assert self.mw.process_request(req) is None\n        assert req.headers[\"Authorization\"] == basic_auth_header(\"foo\", \"bar\")\n\n    def test_auth_already_set(self):\n        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n        assert self.mw.process_request(req) is None\n        assert req.headers[\"Authorization\"] == b\"Digest 123\"\n\n\nclass TestHttpAuthAnyMiddleware:\n    def setup_method(self):\n        self.mw = HttpAuthMiddleware()\n        spider = AnyDomainSpider(\"foo\")\n        self.mw.spider_opened(spider)\n\n    def teardown_method(self):\n        del self.mw\n\n    def test_auth(self):\n        req = Request(\"http://example.com/\")\n        assert self.mw.process_request(req) is None\n        assert req.headers[\"Authorization\"] == basic_auth_header(\"foo\", \"bar\")\n\n    def test_auth_already_set(self):\n        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n        assert self.mw.process_request(req) is None\n        assert req.headers[\"Authorization\"] == b\"Digest 123\"\n", "n_tokens": 563, "byte_len": 2477, "file_sha1": "96bafc7990c48e60596fa35cc3ac0c9b33868b7c", "start_line": 1, "end_line": 84}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderstate.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderstate.py", "rel_path": "tests/test_spiderstate.py", "module": "tests.test_spiderstate", "ext": "py", "chunk_number": 1, "symbols": ["test_store_load", "test_state_attribute", "test_not_configured", "spider", "spider2", "present", "test", "store", "jobdir", "consistent", "not", "name", "spiderstate", "with", "spiders", "scrapy", "configured", "provide", "tmp", "path", "closed", "interface", "get", "crawler", "from", "pytest", "state", "assert", "attribute", "raises", "exceptions", "opened", "utils", "import", "datetime", "default", "timezone", "extensions", "must"], "ast_kind": "function_or_method", "text": "from datetime import datetime, timezone\n\nimport pytest\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions.spiderstate import SpiderState\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\ndef test_store_load(tmp_path):\n    jobdir = str(tmp_path)\n\n    spider = Spider(name=\"default\")\n    dt = datetime.now(tz=timezone.utc)\n\n    ss = SpiderState(jobdir)\n    ss.spider_opened(spider)\n    spider.state[\"one\"] = 1\n    spider.state[\"dt\"] = dt\n    ss.spider_closed(spider)\n\n    spider2 = Spider(name=\"default\")\n    ss2 = SpiderState(jobdir)\n    ss2.spider_opened(spider2)\n    assert spider.state == {\"one\": 1, \"dt\": dt}\n    ss2.spider_closed(spider2)\n\n\ndef test_state_attribute():\n    # state attribute must be present if jobdir is not set, to provide a\n    # consistent interface\n    spider = Spider(name=\"default\")\n    ss = SpiderState()\n    ss.spider_opened(spider)\n    assert spider.state == {}\n    ss.spider_closed(spider)\n\n\ndef test_not_configured():\n    crawler = get_crawler(Spider)\n    with pytest.raises(NotConfigured):\n        SpiderState.from_crawler(crawler)\n", "n_tokens": 276, "byte_len": 1109, "file_sha1": "f4dde1a420211ce9ebbaeffdcde4262d3e0a127a", "start_line": 1, "end_line": 44}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py", "rel_path": "tests/test_spidermiddleware_output_chain.py", "module": "tests.test_spidermiddleware_output_chain", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_spider_exception", "parse", "process_spider_input", "errback", "process_spider_output", "_BaseSpiderMiddleware", "LogExceptionMiddleware", "RecoveryMiddleware", "RecoverySpider", "RecoveryAsyncGenSpider", "FailProcessSpiderInputMiddleware", "ProcessSpiderInputSpiderWithoutErrback", "ProcessSpiderInputSpiderWithErrback", "GeneratorCallbackSpider", "AsyncGeneratorCallbackSpider", "GeneratorCallbackSpiderMiddlewareRightAfterSpider", "NotGeneratorCallbackSpider", "NotGeneratorCallbackSpiderMiddlewareRightAfterSpider", "_GeneratorDoNothingMiddleware", "GeneratorFailMiddleware", "failure", "method", "processed", "async", "append", "generator", "middleware", "not", "setup_class", "teardown_class", "GeneratorDoNothingAfterFailureMiddleware", "GeneratorRecoverMiddleware", "GeneratorDoNothingAfterRecoveryMiddleware", "GeneratorOutputChainSpider", "_NotGeneratorDoNothingMiddleware", "NotGeneratorFailMiddleware", "NotGeneratorDoNothingAfterFailureMiddleware", "NotGeneratorRecoverMiddleware", "NotGeneratorDoNothingAfterRecoveryMiddleware", "NotGeneratorOutputChainSpider", "TestSpiderMiddleware", "nothing", "case", "recover", "after", "log", "log1", "spider"], "ast_kind": "class_or_type", "text": "from testfixtures import LogCapture\n\nfrom scrapy import Request, Spider\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\n\n\nclass _BaseSpiderMiddleware:\n    def __init__(self, crawler):\n        self.crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n\nclass LogExceptionMiddleware(_BaseSpiderMiddleware):\n    def process_spider_exception(self, response, exception):\n        self.crawler.spider.logger.info(\n            \"Middleware: %s exception caught\", exception.__class__.__name__\n        )\n\n\n# ================================================================================\n# (0) recover from an exception on a spider callback\nclass RecoveryMiddleware(_BaseSpiderMiddleware):\n    def process_spider_exception(self, response, exception):\n        self.crawler.spider.logger.info(\n            \"Middleware: %s exception caught\", exception.__class__.__name__\n        )\n        return [\n            {\"from\": \"process_spider_exception\"},\n            Request(response.url, meta={\"dont_fail\": True}, dont_filter=True),\n        ]\n\n\nclass RecoverySpider(Spider):\n    name = \"RecoverySpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES_BASE\": {},\n        \"SPIDER_MIDDLEWARES\": {\n            RecoveryMiddleware: 10,\n        },\n    }\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        yield {\"test\": 1}\n        self.logger.info(\"DONT_FAIL: %s\", response.meta.get(\"dont_fail\"))\n        if not response.meta.get(\"dont_fail\"):\n            raise TabError\n\n\nclass RecoveryAsyncGenSpider(RecoverySpider):\n    name = \"RecoveryAsyncGenSpider\"\n\n    async def parse(self, response):\n        for r in super().parse(response):\n            yield r\n\n\n# ================================================================================\n# (1) exceptions from a spider middleware's process_spider_input method\nclass FailProcessSpiderInputMiddleware(_BaseSpiderMiddleware):\n    def process_spider_input(self, response):\n        self.crawler.spider.logger.info(\"Middleware: will raise IndexError\")\n        raise IndexError\n\n\nclass ProcessSpiderInputSpiderWithoutErrback(Spider):\n    name = \"ProcessSpiderInputSpiderWithoutErrback\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            # spider\n            FailProcessSpiderInputMiddleware: 8,\n            LogExceptionMiddleware: 6,\n            # engine\n        }\n    }\n\n    async def start(self):\n        yield Request(url=self.mockserver.url(\"/status?n=200\"), callback=self.parse)\n\n    def parse(self, response):\n        return {\"from\": \"callback\"}\n\n\nclass ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback):\n    name = \"ProcessSpiderInputSpiderWithErrback\"\n\n    async def start(self):\n        yield Request(\n            self.mockserver.url(\"/status?n=200\"), self.parse, errback=self.errback\n        )\n\n    def errback(self, failure):\n        self.logger.info(\"Got a Failure on the Request errback\")\n        return {\"from\": \"errback\"}\n\n\n# ================================================================================\n# (2) exceptions from a spider callback (generator)\nclass GeneratorCallbackSpider(Spider):\n    name = \"GeneratorCallbackSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 10,\n        },\n    }\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        yield {\"test\": 1}\n        yield {\"test\": 2}\n        raise ImportError\n\n\nclass AsyncGeneratorCallbackSpider(GeneratorCallbackSpider):\n    async def parse(self, response):\n        yield {\"test\": 1}\n        yield {\"test\": 2}\n        raise ImportError\n\n\n# ================================================================================\n# (2.1) exceptions from a spider callback (generator, middleware right after callback)\nclass GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider):\n    name = \"GeneratorCallbackSpiderMiddlewareRightAfterSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 100000,\n        },\n    }\n\n\n# ================================================================================\n# (3) exceptions from a spider callback (not a generator)\nclass NotGeneratorCallbackSpider(Spider):\n    name = \"NotGeneratorCallbackSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 10,\n        },\n    }\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        return [{\"test\": 1}, {\"test\": 1 / 0}]\n\n\n# ================================================================================\n# (3.1) exceptions from a spider callback (not a generator, middleware right after callback)\nclass NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackSpider):\n    name = \"NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 100000,\n        },\n    }\n\n\n# ================================================================================\n# (4) exceptions from a middleware process_spider_output method (generator)\nclass _GeneratorDoNothingMiddleware(_BaseSpiderMiddleware):\n    def process_spider_output(self, response, result):\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            yield r\n\n    def process_spider_exception(self, response, exception):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        self.crawler.spider.logger.info(\n            \"%s: %s caught\", method, exception.__class__.__name__\n        )\n\n\nclass GeneratorFailMiddleware(_BaseSpiderMiddleware):", "n_tokens": 1191, "byte_len": 5947, "file_sha1": "8381ce8ea9cef21aa95c065e6c4fc9553f1fb12d", "start_line": 1, "end_line": 184}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py", "rel_path": "tests/test_spidermiddleware_output_chain.py", "module": "tests.test_spidermiddleware_output_chain", "ext": "py", "chunk_number": 2, "symbols": ["process_spider_output", "process_spider_exception", "parse", "setup_class", "teardown_class", "GeneratorDoNothingAfterFailureMiddleware", "GeneratorRecoverMiddleware", "GeneratorDoNothingAfterRecoveryMiddleware", "GeneratorOutputChainSpider", "_NotGeneratorDoNothingMiddleware", "NotGeneratorFailMiddleware", "NotGeneratorDoNothingAfterFailureMiddleware", "NotGeneratorRecoverMiddleware", "NotGeneratorDoNothingAfterRecoveryMiddleware", "NotGeneratorOutputChainSpider", "TestSpiderMiddleware", "processed", "method", "async", "append", "generator", "nothing", "middleware", "recover", "spider", "name", "deferred", "from", "reference", "error", "__init__", "from_crawler", "process_spider_input", "errback", "_BaseSpiderMiddleware", "LogExceptionMiddleware", "RecoveryMiddleware", "RecoverySpider", "RecoveryAsyncGenSpider", "FailProcessSpiderInputMiddleware", "ProcessSpiderInputSpiderWithoutErrback", "ProcessSpiderInputSpiderWithErrback", "GeneratorCallbackSpider", "AsyncGeneratorCallbackSpider", "GeneratorCallbackSpiderMiddlewareRightAfterSpider", "NotGeneratorCallbackSpider", "NotGeneratorCallbackSpiderMiddlewareRightAfterSpider", "_GeneratorDoNothingMiddleware", "GeneratorFailMiddleware", "failure"], "ast_kind": "class_or_type", "text": "    def process_spider_output(self, response, result):\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            yield r\n            raise LookupError\n\n    def process_spider_exception(self, response, exception):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        self.crawler.spider.logger.info(\n            \"%s: %s caught\", method, exception.__class__.__name__\n        )\n        yield {\"processed\": [method]}\n\n\nclass GeneratorDoNothingAfterFailureMiddleware(_GeneratorDoNothingMiddleware):\n    pass\n\n\nclass GeneratorRecoverMiddleware(_BaseSpiderMiddleware):\n    def process_spider_output(self, response, result):\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            yield r\n\n    def process_spider_exception(self, response, exception):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        self.crawler.spider.logger.info(\n            \"%s: %s caught\", method, exception.__class__.__name__\n        )\n        yield {\"processed\": [method]}\n\n\nclass GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):\n    pass\n\n\nclass GeneratorOutputChainSpider(Spider):\n    name = \"GeneratorOutputChainSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            GeneratorFailMiddleware: 10,\n            GeneratorDoNothingAfterFailureMiddleware: 8,\n            GeneratorRecoverMiddleware: 5,\n            GeneratorDoNothingAfterRecoveryMiddleware: 3,\n        },\n    }\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        yield {\"processed\": [\"parse-first-item\"]}\n        yield {\"processed\": [\"parse-second-item\"]}\n\n\n# ================================================================================\n# (5) exceptions from a middleware process_spider_output method (not generator)\n\n\nclass _NotGeneratorDoNothingMiddleware(_BaseSpiderMiddleware):\n    def process_spider_output(self, response, result):\n        out = []\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            out.append(r)\n        return out\n\n    def process_spider_exception(self, response, exception):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        self.crawler.spider.logger.info(\n            \"%s: %s caught\", method, exception.__class__.__name__\n        )\n\n\nclass NotGeneratorFailMiddleware(_BaseSpiderMiddleware):\n    def process_spider_output(self, response, result):\n        out = []\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            out.append(r)\n        raise ReferenceError\n\n    def process_spider_exception(self, response, exception):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        self.crawler.spider.logger.info(\n            \"%s: %s caught\", method, exception.__class__.__name__\n        )\n        return [{\"processed\": [method]}]\n\n\nclass NotGeneratorDoNothingAfterFailureMiddleware(_NotGeneratorDoNothingMiddleware):\n    pass\n\n\nclass NotGeneratorRecoverMiddleware(_BaseSpiderMiddleware):\n    def process_spider_output(self, response, result):\n        out = []\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            out.append(r)\n        return out\n\n    def process_spider_exception(self, response, exception):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        self.crawler.spider.logger.info(\n            \"%s: %s caught\", method, exception.__class__.__name__\n        )\n        return [{\"processed\": [method]}]\n\n\nclass NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddleware):\n    pass\n\n\nclass NotGeneratorOutputChainSpider(Spider):\n    name = \"NotGeneratorOutputChainSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            NotGeneratorFailMiddleware: 10,\n            NotGeneratorDoNothingAfterFailureMiddleware: 8,\n            NotGeneratorRecoverMiddleware: 5,\n            NotGeneratorDoNothingAfterRecoveryMiddleware: 3,\n        },\n    }\n\n    async def start(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        return [\n            {\"processed\": [\"parse-first-item\"]},\n            {\"processed\": [\"parse-second-item\"]},\n        ]\n\n\n# ================================================================================\nclass TestSpiderMiddleware:\n    mockserver: MockServer\n\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    async def crawl_log(self, spider: type[Spider]) -> LogCapture:\n        crawler = get_crawler(spider)\n        with LogCapture() as log:\n            await maybe_deferred_to_future(crawler.crawl(mockserver=self.mockserver))\n        return log\n\n    @deferred_f_from_coro_f\n    async def test_recovery(self):\n        \"\"\"\n        (0) Recover from an exception in a spider callback. The final item count should be 3\n        (one yielded from the callback method before the exception is raised, one directly\n        from the recovery middleware and one from the spider when processing the request that\n        was enqueued from the recovery middleware)\n        \"\"\"\n        log = await self.crawl_log(RecoverySpider)\n        assert \"Middleware: TabError exception caught\" in str(log)\n        assert str(log).count(\"Middleware: TabError exception caught\") == 1\n        assert \"'item_scraped_count': 3\" in str(log)\n", "n_tokens": 1215, "byte_len": 5748, "file_sha1": "8381ce8ea9cef21aa95c065e6c4fc9553f1fb12d", "start_line": 185, "end_line": 351}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py", "rel_path": "tests/test_spidermiddleware_output_chain.py", "module": "tests.test_spidermiddleware_output_chain", "ext": "py", "chunk_number": 3, "symbols": ["chain", "failure", "processed", "async", "will", "log", "log21", "test", "generator", "await", "callback", "exceptions", "placed", "crawl", "spider", "middleware", "caught", "exception", "process", "items", "case", "not", "after", "log1", "deferred", "from", "returning", "with", "special", "import", "__init__", "from_crawler", "process_spider_exception", "parse", "process_spider_input", "errback", "process_spider_output", "setup_class", "teardown_class", "_BaseSpiderMiddleware", "LogExceptionMiddleware", "RecoveryMiddleware", "RecoverySpider", "RecoveryAsyncGenSpider", "FailProcessSpiderInputMiddleware", "ProcessSpiderInputSpiderWithoutErrback", "ProcessSpiderInputSpiderWithErrback", "GeneratorCallbackSpider", "AsyncGeneratorCallbackSpider", "GeneratorCallbackSpiderMiddlewareRightAfterSpider"], "ast_kind": "imports", "text": "    @deferred_f_from_coro_f\n    async def test_recovery_asyncgen(self):\n        \"\"\"\n        Same as test_recovery but with an async callback.\n        \"\"\"\n        log = await self.crawl_log(RecoveryAsyncGenSpider)\n        assert \"Middleware: TabError exception caught\" in str(log)\n        assert str(log).count(\"Middleware: TabError exception caught\") == 1\n        assert \"'item_scraped_count': 3\" in str(log)\n\n    @deferred_f_from_coro_f\n    async def test_process_spider_input_without_errback(self):\n        \"\"\"\n        (1.1) An exception from the process_spider_input chain should be caught by the\n        process_spider_exception chain from the start if the Request has no errback\n        \"\"\"\n        log1 = await self.crawl_log(ProcessSpiderInputSpiderWithoutErrback)\n        assert \"Middleware: will raise IndexError\" in str(log1)\n        assert \"Middleware: IndexError exception caught\" in str(log1)\n\n    @deferred_f_from_coro_f\n    async def test_process_spider_input_with_errback(self):\n        \"\"\"\n        (1.2) An exception from the process_spider_input chain should not be caught by the\n        process_spider_exception chain if the Request has an errback\n        \"\"\"\n        log1 = await self.crawl_log(ProcessSpiderInputSpiderWithErrback)\n        assert \"Middleware: IndexError exception caught\" not in str(log1)\n        assert \"Middleware: will raise IndexError\" in str(log1)\n        assert \"Got a Failure on the Request errback\" in str(log1)\n        assert \"{'from': 'errback'}\" in str(log1)\n        assert \"{'from': 'callback'}\" not in str(log1)\n        assert \"'item_scraped_count': 1\" in str(log1)\n\n    @deferred_f_from_coro_f\n    async def test_generator_callback(self):\n        \"\"\"\n        (2) An exception from a spider callback (returning a generator) should\n        be caught by the process_spider_exception chain. Items yielded before the\n        exception is raised should be processed normally.\n        \"\"\"\n        log2 = await self.crawl_log(GeneratorCallbackSpider)\n        assert \"Middleware: ImportError exception caught\" in str(log2)\n        assert \"'item_scraped_count': 2\" in str(log2)\n\n    @deferred_f_from_coro_f\n    async def test_async_generator_callback(self):\n        \"\"\"\n        Same as test_generator_callback but with an async callback.\n        \"\"\"\n        log2 = await self.crawl_log(AsyncGeneratorCallbackSpider)\n        assert \"Middleware: ImportError exception caught\" in str(log2)\n        assert \"'item_scraped_count': 2\" in str(log2)\n\n    @deferred_f_from_coro_f\n    async def test_generator_callback_right_after_callback(self):\n        \"\"\"\n        (2.1) Special case of (2): Exceptions should be caught\n        even if the middleware is placed right after the spider\n        \"\"\"\n        log21 = await self.crawl_log(GeneratorCallbackSpiderMiddlewareRightAfterSpider)\n        assert \"Middleware: ImportError exception caught\" in str(log21)\n        assert \"'item_scraped_count': 2\" in str(log21)\n\n    @deferred_f_from_coro_f\n    async def test_not_a_generator_callback(self):\n        \"\"\"\n        (3) An exception from a spider callback (returning a list) should\n        be caught by the process_spider_exception chain. No items should be processed.\n        \"\"\"\n        log3 = await self.crawl_log(NotGeneratorCallbackSpider)\n        assert \"Middleware: ZeroDivisionError exception caught\" in str(log3)\n        assert \"item_scraped_count\" not in str(log3)\n\n    @deferred_f_from_coro_f\n    async def test_not_a_generator_callback_right_after_callback(self):\n        \"\"\"\n        (3.1) Special case of (3): Exceptions should be caught\n        even if the middleware is placed right after the spider\n        \"\"\"\n        log31 = await self.crawl_log(\n            NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\n        )\n        assert \"Middleware: ZeroDivisionError exception caught\" in str(log31)\n        assert \"item_scraped_count\" not in str(log31)\n", "n_tokens": 890, "byte_len": 3899, "file_sha1": "8381ce8ea9cef21aa95c065e6c4fc9553f1fb12d", "start_line": 352, "end_line": 437}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_output_chain.py", "rel_path": "tests/test_spidermiddleware_output_chain.py", "module": "tests.test_spidermiddleware_output_chain", "ext": "py", "chunk_number": 4, "symbols": ["chain", "method", "processed", "async", "lookup", "error", "await", "result", "crawl", "log", "spider", "generator", "fail", "caught", "nothing", "exception", "second", "handled", "middleware", "item", "deferred", "from", "process", "reference", "next", "log5", "sent", "recovery", "first", "test", "__init__", "from_crawler", "process_spider_exception", "parse", "process_spider_input", "errback", "process_spider_output", "setup_class", "teardown_class", "_BaseSpiderMiddleware", "LogExceptionMiddleware", "RecoveryMiddleware", "RecoverySpider", "RecoveryAsyncGenSpider", "FailProcessSpiderInputMiddleware", "ProcessSpiderInputSpiderWithoutErrback", "ProcessSpiderInputSpiderWithErrback", "GeneratorCallbackSpider", "AsyncGeneratorCallbackSpider", "GeneratorCallbackSpiderMiddlewareRightAfterSpider"], "ast_kind": "imports", "text": "    @deferred_f_from_coro_f\n    async def test_generator_output_chain(self):\n        \"\"\"\n        (4) An exception from a middleware's process_spider_output method should be sent\n        to the process_spider_exception method from the next middleware in the chain.\n        The result of the recovery by the process_spider_exception method should be handled\n        by the process_spider_output method from the next middleware.\n        The final item count should be 2 (one from the spider callback and one from the\n        process_spider_exception chain)\n        \"\"\"\n        log4 = await self.crawl_log(GeneratorOutputChainSpider)\n        assert \"'item_scraped_count': 2\" in str(log4)\n        assert (\n            \"GeneratorRecoverMiddleware.process_spider_exception: LookupError caught\"\n            in str(log4)\n        )\n        assert (\n            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: LookupError caught\"\n            in str(log4)\n        )\n        assert (\n            \"GeneratorFailMiddleware.process_spider_exception: LookupError caught\"\n            not in str(log4)\n        )\n        assert (\n            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: LookupError caught\"\n            not in str(log4)\n        )\n        item_from_callback = {\n            \"processed\": [\n                \"parse-first-item\",\n                \"GeneratorFailMiddleware.process_spider_output\",\n                \"GeneratorDoNothingAfterFailureMiddleware.process_spider_output\",\n                \"GeneratorRecoverMiddleware.process_spider_output\",\n                \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n            ]\n        }\n        item_recovered = {\n            \"processed\": [\n                \"GeneratorRecoverMiddleware.process_spider_exception\",\n                \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n            ]\n        }\n        assert str(item_from_callback) in str(log4)\n        assert str(item_recovered) in str(log4)\n        assert \"parse-second-item\" not in str(log4)\n\n    @deferred_f_from_coro_f\n    async def test_not_a_generator_output_chain(self):\n        \"\"\"\n        (5) An exception from a middleware's process_spider_output method should be sent\n        to the process_spider_exception method from the next middleware in the chain.\n        The result of the recovery by the process_spider_exception method should be handled\n        by the process_spider_output method from the next middleware.\n        The final item count should be 1 (from the process_spider_exception chain, the items\n        from the spider callback are lost)\n        \"\"\"\n        log5 = await self.crawl_log(NotGeneratorOutputChainSpider)\n        assert \"'item_scraped_count': 1\" in str(log5)\n        assert (\n            \"GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught\"\n            in str(log5)\n        )\n        assert (\n            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: ReferenceError caught\"\n            in str(log5)\n        )\n        assert (\n            \"GeneratorFailMiddleware.process_spider_exception: ReferenceError caught\"\n            not in str(log5)\n        )\n        assert (\n            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: ReferenceError caught\"\n            not in str(log5)\n        )\n        item_recovered = {\n            \"processed\": [\n                \"NotGeneratorRecoverMiddleware.process_spider_exception\",\n                \"NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n            ]\n        }\n        assert str(item_recovered) in str(log5)\n        assert \"parse-first-item\" not in str(log5)\n        assert \"parse-second-item\" not in str(log5)\n", "n_tokens": 745, "byte_len": 3759, "file_sha1": "8381ce8ea9cef21aa95c065e6c4fc9553f1fb12d", "start_line": 438, "end_line": 522}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_conf.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_conf.py", "rel_path": "tests/test_utils_conf.py", "module": "tests.test_utils_conf", "ext": "py", "chunk_number": 1, "symbols": ["test_build_dict", "test_duplicate_components_in_basesettings", "test_valid_numbers", "test_arglist_to_dict", "test_feed_export_config_invalid_format", "test_feed_export_config_mismatch", "test_feed_export_config_explicit_formats", "test_feed_export_config_implicit_formats", "test_feed_export_config_stdout", "test_feed_export_config_overwrite", "test_output_and_overwrite_output", "test_feed_complete_default_values_from_settings_empty", "TestBuildComponentList", "TestFeedExportConfig", "encoding", "takes", "usage", "error", "output", "output1", "arg", "arg2", "test", "build", "item", "export", "higher", "items", "items2", "val", "test_feed_complete_default_values_from_settings_non_empty", "val2", "arg1", "feed", "process", "four", "three", "overwrite", "pytest", "settings", "lower", "object", "none", "valid", "match", "values", "fee", "expor", "paths", "arglist"], "ast_kind": "class_or_type", "text": "import pytest\n\nfrom scrapy.exceptions import UsageError\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.conf import (\n    arglist_to_dict,\n    build_component_list,\n    feed_complete_default_values_from_settings,\n    feed_process_params_from_cli,\n)\n\n\nclass TestBuildComponentList:\n    def test_build_dict(self):\n        d = {\"one\": 1, \"two\": None, \"three\": 8, \"four\": 4}\n        assert build_component_list(d, convert=lambda x: x) == [\"one\", \"four\", \"three\"]\n\n    def test_duplicate_components_in_basesettings(self):\n        # Higher priority takes precedence\n        duplicate_bs = BaseSettings({\"one\": 1, \"two\": 2}, priority=0)\n        duplicate_bs.set(\"ONE\", 4, priority=10)\n        assert build_component_list(duplicate_bs, convert=lambda x: x.lower()) == [\n            \"two\",\n            \"one\",\n        ]\n        duplicate_bs.set(\"one\", duplicate_bs[\"one\"], priority=20)\n        assert build_component_list(duplicate_bs, convert=lambda x: x.lower()) == [\n            \"one\",\n            \"two\",\n        ]\n        # Same priority raises ValueError\n        duplicate_bs.set(\"ONE\", duplicate_bs[\"ONE\"], priority=20)\n        with pytest.raises(\n            ValueError, match=\"Some paths in .* convert to the same object\"\n        ):\n            build_component_list(duplicate_bs, convert=lambda x: x.lower())\n\n    def test_valid_numbers(self):\n        # work well with None and numeric values\n        d = {\"a\": 10, \"b\": None, \"c\": 15, \"d\": 5.0}\n        assert build_component_list(d, convert=lambda x: x) == [\"d\", \"a\", \"c\"]\n        d = {\n            \"a\": 33333333333333333333,\n            \"b\": 11111111111111111111,\n            \"c\": 22222222222222222222,\n        }\n        assert build_component_list(d, convert=lambda x: x) == [\"b\", \"c\", \"a\"]\n\n\ndef test_arglist_to_dict():\n    assert arglist_to_dict([\"arg1=val1\", \"arg2=val2\"]) == {\n        \"arg1\": \"val1\",\n        \"arg2\": \"val2\",\n    }\n\n\nclass TestFeedExportConfig:\n    def test_feed_export_config_invalid_format(self):\n        settings = Settings()\n        with pytest.raises(UsageError):\n            feed_process_params_from_cli(settings, [\"items.dat\"])\n\n    def test_feed_export_config_mismatch(self):\n        settings = Settings()\n        with pytest.raises(UsageError):\n            feed_process_params_from_cli(settings, [\"items1.dat\", \"items2.dat\"])\n\n    def test_feed_export_config_explicit_formats(self):\n        settings = Settings()\n        assert {\n            \"items_1.dat\": {\"format\": \"json\"},\n            \"items_2.dat\": {\"format\": \"xml\"},\n            \"items_3.dat\": {\"format\": \"csv\"},\n        } == feed_process_params_from_cli(\n            settings, [\"items_1.dat:json\", \"items_2.dat:xml\", \"items_3.dat:csv\"]\n        )\n\n    def test_feed_export_config_implicit_formats(self):\n        settings = Settings()\n        assert {\n            \"items_1.json\": {\"format\": \"json\"},\n            \"items_2.xml\": {\"format\": \"xml\"},\n            \"items_3.csv\": {\"format\": \"csv\"},\n        } == feed_process_params_from_cli(\n            settings, [\"items_1.json\", \"items_2.xml\", \"items_3.csv\"]\n        )\n\n    def test_feed_export_config_stdout(self):\n        settings = Settings()\n        assert {\"stdout:\": {\"format\": \"pickle\"}} == feed_process_params_from_cli(\n            settings, [\"-:pickle\"]\n        )\n\n    def test_feed_export_config_overwrite(self):\n        settings = Settings()\n        assert {\n            \"output.json\": {\"format\": \"json\", \"overwrite\": True}\n        } == feed_process_params_from_cli(\n            settings, [], overwrite_output=[\"output.json\"]\n        )\n\n    def test_output_and_overwrite_output(self):\n        with pytest.raises(UsageError):\n            feed_process_params_from_cli(\n                Settings(), [\"output1.json\"], overwrite_output=[\"output2.json\"]\n            )\n\n    def test_feed_complete_default_values_from_settings_empty(self):\n        feed = {}\n        settings = Settings(\n            {\n                \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n                \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n                \"FEED_EXPORT_INDENT\": 42,\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_URI_PARAMS\": (1, 2, 3, 4),\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n            }\n        )\n        new_feed = feed_complete_default_values_from_settings(feed, settings)\n        assert new_feed == {\n            \"encoding\": \"custom encoding\",\n            \"fields\": [\"f1\", \"f2\", \"f3\"],\n            \"indent\": 42,\n            \"store_empty\": True,\n            \"uri_params\": (1, 2, 3, 4),\n            \"batch_item_count\": 2,\n            \"item_export_kwargs\": {},\n        }\n", "n_tokens": 1092, "byte_len": 4603, "file_sha1": "401c925d3ef4ccc8f8a56256c8bab4b21fbb5ee1", "start_line": 1, "end_line": 130}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_conf.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_conf.py", "rel_path": "tests/test_utils_conf.py", "module": "tests.test_utils_conf", "ext": "py", "chunk_number": 2, "symbols": ["test_feed_complete_default_values_from_settings_non_empty", "encoding", "fee", "expor", "stor", "item", "export", "other", "fields", "test", "feed", "true", "custom", "settings", "new", "store", "empty", "batch", "assert", "indent", "none", "self", "uri", "params", "complete", "test_build_dict", "test_duplicate_components_in_basesettings", "test_valid_numbers", "test_arglist_to_dict", "test_feed_export_config_invalid_format", "test_feed_export_config_mismatch", "test_feed_export_config_explicit_formats", "test_feed_export_config_implicit_formats", "test_feed_export_config_stdout", "test_feed_export_config_overwrite", "test_output_and_overwrite_output", "test_feed_complete_default_values_from_settings_empty", "TestBuildComponentList", "TestFeedExportConfig", "takes", "usage", "error", "output", "output1", "arg", "arg2", "build", "higher", "items", "items2"], "ast_kind": "function_or_method", "text": "    def test_feed_complete_default_values_from_settings_non_empty(self):\n        feed = {\n            \"encoding\": \"other encoding\",\n            \"fields\": None,\n        }\n        settings = Settings(\n            {\n                \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n                \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n                \"FEED_EXPORT_INDENT\": 42,\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n            }\n        )\n        new_feed = feed_complete_default_values_from_settings(feed, settings)\n        assert new_feed == {\n            \"encoding\": \"other encoding\",\n            \"fields\": None,\n            \"indent\": 42,\n            \"store_empty\": True,\n            \"uri_params\": None,\n            \"batch_item_count\": 2,\n            \"item_export_kwargs\": {},\n        }\n", "n_tokens": 179, "byte_len": 840, "file_sha1": "401c925d3ef4ccc8f8a56256c8bab4b21fbb5ee1", "start_line": 131, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_httpobj.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_httpobj.py", "rel_path": "tests/test_utils_httpobj.py", "module": "tests.test_utils_httpobj", "ext": "py", "chunk_number": 1, "symbols": ["test_urlparse_cached", "req", "req2", "urlp", "scrapy", "example", "req1a", "urlparse", "cached", "index", "from", "request", "request2", "assert", "request1", "urllib", "utils", "html", "import", "parse", "http", "httpobj", "test", "req1b"], "ast_kind": "function_or_method", "text": "from urllib.parse import urlparse\n\nfrom scrapy.http import Request\nfrom scrapy.utils.httpobj import urlparse_cached\n\n\ndef test_urlparse_cached():\n    url = \"http://www.example.com/index.html\"\n    request1 = Request(url)\n    request2 = Request(url)\n    req1a = urlparse_cached(request1)\n    req1b = urlparse_cached(request1)\n    req2 = urlparse_cached(request2)\n    urlp = urlparse(url)\n\n    assert req1a == req2\n    assert req1a == urlp\n    assert req1a is req1b\n    assert req1a is not req2\n    assert req1a is not req2\n", "n_tokens": 137, "byte_len": 521, "file_sha1": "c740720731e67bb3a6c3e3ec1dcc83006e25554f", "start_line": 1, "end_line": 21}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_response.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_response.py", "rel_path": "tests/test_utils_response.py", "module": "tests.test_utils_response", "ext": "py", "chunk_number": 1, "symbols": ["test_open_in_browser", "browser_open", "test_get_meta_refresh", "test_get_base_url", "test_response_status_message", "test_inject_base_url", "bool", "burl", "newpage", "domain", "target", "unexpected", "refresh", "open", "browser", "path", "script", "noscript", "pytest", "debug", "href", "equiv", "document", "unknown", "none", "html", "type", "parse", "http", "bytes", "check_base_url", "test_open_in_browser_redos_comment", "test_open_in_browser_redos_head", "test_remove_html_comments", "test", "world", "output", "body", "https", "check", "base", "comment", "inject", "standard", "get", "response", "openfunc", "endif", "return", "end"], "ast_kind": "function_or_method", "text": "from pathlib import Path\nfrom time import process_time\nfrom urllib.parse import urlparse\n\nimport pytest\n\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.response import (\n    _remove_html_comments,\n    get_base_url,\n    get_meta_refresh,\n    open_in_browser,\n    response_status_message,\n)\n\n\ndef test_open_in_browser():\n    url = \"http:///www.example.com/some/page.html\"\n    body = (\n        b\"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n    )\n\n    def browser_open(burl: str) -> bool:\n        path = urlparse(burl).path\n        if not path or not Path(path).exists():\n            path = burl.replace(\"file://\", \"\")\n        bbody = Path(path).read_bytes()\n        assert b'<base href=\"' + to_bytes(url) + b'\">' in bbody\n        return True\n\n    response = HtmlResponse(url, body=body)\n    assert open_in_browser(response, _openfunc=browser_open), \"Browser not called\"\n\n    resp = Response(url, body=body)\n    with pytest.raises(TypeError):\n        open_in_browser(resp, debug=True)  # pylint: disable=unexpected-keyword-arg\n\n\ndef test_get_meta_refresh():\n    r1 = HtmlResponse(\n        \"http://www.example.com\",\n        body=b\"\"\"\n    <html>\n    <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n    <body>blahablsdfsal&amp;</body>\n    </html>\"\"\",\n    )\n    r2 = HtmlResponse(\n        \"http://www.example.com\",\n        body=b\"\"\"\n    <html>\n    <head><title>Dummy</title><noScript>\n    <meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n    </noSCRIPT>\n    <body>blahablsdfsal&amp;</body>\n    </html>\"\"\",\n    )\n    r3 = HtmlResponse(\n        \"http://www.example.com\",\n        body=b\"\"\"\n<noscript><meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage</noscript>\n<script type=\"text/javascript\">\nif(!checkCookies()){\n    document.write('<meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage\">');\n}\n</script>\n    \"\"\",\n    )\n    r4 = HtmlResponse(\n        \"http://www.example.com\",\n        body=b\"\"\"\n    <html>\n    <head><title>Dummy</title>\n    <base href=\"http://www.another-domain.com/base/path/\">\n    <meta http-equiv=\"refresh\" content=\"5;url=target.html\"</head>\n    <body>blahablsdfsal&amp;</body>\n    </html>\"\"\",\n    )\n    assert get_meta_refresh(r1) == (5.0, \"http://example.org/newpage\")\n    assert get_meta_refresh(r2) == (None, None)\n    assert get_meta_refresh(r3) == (None, None)\n    assert get_meta_refresh(r4) == (\n        5.0,\n        \"http://www.another-domain.com/base/path/target.html\",\n    )\n\n\ndef test_get_base_url():\n    resp = HtmlResponse(\n        \"http://www.example.com\",\n        body=b\"\"\"\n    <html>\n    <head><base href=\"http://www.example.com/img/\" target=\"_blank\"></head>\n    <body>blahablsdfsal&amp;</body>\n    </html>\"\"\",\n    )\n    assert get_base_url(resp) == \"http://www.example.com/img/\"\n\n    resp2 = HtmlResponse(\n        \"http://www.example.com\",\n        body=b\"\"\"\n    <html><body>blahablsdfsal&amp;</body></html>\"\"\",\n    )\n    assert get_base_url(resp2) == \"http://www.example.com\"\n\n\ndef test_response_status_message():\n    assert response_status_message(200) == \"200 OK\"\n    assert response_status_message(404) == \"404 Not Found\"\n    assert response_status_message(573) == \"573 Unknown Status\"\n\n\ndef test_inject_base_url():\n    url = \"http://www.example.com\"\n", "n_tokens": 897, "byte_len": 3406, "file_sha1": "0a2bb611b2f44a2ee15b9abda02309223a1f7795", "start_line": 1, "end_line": 116}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_response.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_response.py", "rel_path": "tests/test_utils_response.py", "module": "tests.test_utils_response", "ext": "py", "chunk_number": 2, "symbols": ["check_base_url", "test_open_in_browser_redos_comment", "test_open_in_browser_redos_head", "test_remove_html_comments", "playground", "input", "body", "burl", "dummy", "pattern", "process", "time", "github", "return", "test", "open", "argumented", "end", "replace", "lambda", "mark", "remove", "parametrize", "world", "with", "path", "browser", "find", "output", "comments", "test_open_in_browser", "browser_open", "test_get_meta_refresh", "test_get_base_url", "test_response_status_message", "test_inject_base_url", "bool", "newpage", "domain", "target", "unexpected", "refresh", "https", "script", "noscript", "pytest", "debug", "href", "equiv", "document"], "ast_kind": "function_or_method", "text": "    def check_base_url(burl):\n        path = urlparse(burl).path\n        if not path or not Path(path).exists():\n            path = burl.replace(\"file://\", \"\")\n        bbody = Path(path).read_bytes()\n        assert bbody.count(b'<base href=\"' + to_bytes(url) + b'\">') == 1\n        return True\n\n    r1 = HtmlResponse(\n        url,\n        body=b\"\"\"\n    <html>\n        <head><title>Dummy</title></head>\n        <body><p>Hello world.</p></body>\n    </html>\"\"\",\n    )\n    r2 = HtmlResponse(\n        url,\n        body=b\"\"\"\n    <html>\n        <head id=\"foo\"><title>Dummy</title></head>\n        <body>Hello world.</body>\n    </html>\"\"\",\n    )\n    r3 = HtmlResponse(\n        url,\n        body=b\"\"\"\n    <html>\n        <head><title>Dummy</title></head>\n        <body>\n            <header>Hello header</header>\n            <p>Hello world.</p>\n        </body>\n    </html>\"\"\",\n    )\n    r4 = HtmlResponse(\n        url,\n        body=b\"\"\"\n    <html>\n        <!-- <head>Dummy comment</head> -->\n        <head><title>Dummy</title></head>\n        <body><p>Hello world.</p></body>\n    </html>\"\"\",\n    )\n    r5 = HtmlResponse(\n        url,\n        body=b\"\"\"\n    <html>\n        <!--[if IE]>\n        <head><title>IE head</title></head>\n        <![endif]-->\n        <!--[if !IE]>-->\n        <head><title>Standard head</title></head>\n        <!--<![endif]-->\n        <body><p>Hello world.</p></body>\n    </html>\"\"\",\n    )\n\n    assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n    assert open_in_browser(r2, _openfunc=check_base_url), (\n        \"Inject base url with argumented head\"\n    )\n    assert open_in_browser(r3, _openfunc=check_base_url), (\n        \"Inject unique base url with misleading tag\"\n    )\n    assert open_in_browser(r4, _openfunc=check_base_url), (\n        \"Inject unique base url with misleading comment\"\n    )\n    assert open_in_browser(r5, _openfunc=check_base_url), (\n        \"Inject unique base url with conditional comment\"\n    )\n\n\ndef test_open_in_browser_redos_comment():\n    MAX_CPU_TIME = 0.02\n\n    # Exploit input from\n    # https://makenowjust-labs.github.io/recheck/playground/\n    # for /<!--.*?-->/ (old pattern to remove comments).\n    body = b\"-><!--\\x00\" * 25_000 + b\"->\\n<!---->\"\n    response = HtmlResponse(\"https://example.com\", body=body)\n    start_time = process_time()\n    open_in_browser(response, lambda url: True)\n    end_time = process_time()\n    assert end_time - start_time < MAX_CPU_TIME\n\n\ndef test_open_in_browser_redos_head():\n    MAX_CPU_TIME = 0.02\n\n    # Exploit input from\n    # https://makenowjust-labs.github.io/recheck/playground/\n    # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).\n    body = b\"<head\\t\" * 8_000\n    response = HtmlResponse(\"https://example.com\", body=body)\n    start_time = process_time()\n    open_in_browser(response, lambda url: True)\n    end_time = process_time()\n    assert end_time - start_time < MAX_CPU_TIME\n\n\n@pytest.mark.parametrize(\n    (\"input_body\", \"output_body\"),\n    [\n        (b\"a<!--\", b\"a\"),\n        (b\"a<!---->b\", b\"ab\"),\n        (b\"a<!--b-->c\", b\"ac\"),\n        (b\"a<!--b-->c<!--\", b\"ac\"),\n        (b\"a<!--b-->c<!--d\", b\"ac\"),\n        (b\"a<!--b-->c<!---->d\", b\"acd\"),\n        (b\"a<!--b--><!--c-->d\", b\"ad\"),\n    ],\n)\ndef test_remove_html_comments(input_body, output_body):\n    assert _remove_html_comments(input_body) == output_body\n", "n_tokens": 922, "byte_len": 3348, "file_sha1": "0a2bb611b2f44a2ee15b9abda02309223a1f7795", "start_line": 117, "end_line": 232}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_cb_kwargs.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_cb_kwargs.py", "rel_path": "tests/test_request_cb_kwargs.py", "module": "tests.test_request_cb_kwargs", "ext": "py", "chunk_number": 1, "symbols": ["process_request", "process_response", "process_spider_input", "process_spider_output", "parse_first", "parse_second", "parse_general", "parse_no_kwargs", "parse_default", "parse_takes_less", "parse_takes_more", "parse_downloader_mw", "parse_spider_mw", "parse_spider_mw_2", "InjectArgumentsDownloaderMiddleware", "InjectArgumentsSpiderMiddleware", "KeywordArgumentsSpider", "TestCallbackKeywordArguments", "parse", "default", "boolean", "checks", "async", "bool", "append", "inc", "value", "name", "make", "from", "setup_class", "teardown_class", "test_callback_kwargs", "process", "unexpected", "spiders", "elif", "spide", "middlewares", "mockserver", "missing", "get", "crawler", "enter", "should", "mock", "server", "takes", "less", "implicit"], "ast_kind": "class_or_type", "text": "from testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.http import Request\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import MockServerSpider\n\n\nclass InjectArgumentsDownloaderMiddleware:\n    \"\"\"\n    Make sure downloader middlewares are able to update the keyword arguments\n    \"\"\"\n\n    def process_request(self, request):\n        if request.callback.__name__ == \"parse_downloader_mw\":\n            request.cb_kwargs[\"from_process_request\"] = True\n\n    def process_response(self, request, response):\n        if request.callback.__name__ == \"parse_downloader_mw\":\n            request.cb_kwargs[\"from_process_response\"] = True\n        return response\n\n\nclass InjectArgumentsSpiderMiddleware:\n    \"\"\"\n    Make sure spider middlewares are able to update the keyword arguments\n    \"\"\"\n\n    async def process_start(self, start):\n        async for request in start:\n            if request.callback.__name__ == \"parse_spider_mw\":\n                request.cb_kwargs[\"from_process_start\"] = True\n            yield request\n\n    def process_spider_input(self, response):\n        request = response.request\n        if request.callback.__name__ == \"parse_spider_mw\":\n            request.cb_kwargs[\"from_process_spider_input\"] = True\n\n    def process_spider_output(self, response, result):\n        for element in result:\n            if (\n                isinstance(element, Request)\n                and element.callback.__name__ == \"parse_spider_mw_2\"\n            ):\n                element.cb_kwargs[\"from_process_spider_output\"] = True\n            yield element\n\n\nclass KeywordArgumentsSpider(MockServerSpider):\n    name = \"kwargs\"\n    custom_settings = {\n        \"DOWNLOADER_MIDDLEWARES\": {\n            InjectArgumentsDownloaderMiddleware: 750,\n        },\n        \"SPIDER_MIDDLEWARES\": {\n            InjectArgumentsSpiderMiddleware: 750,\n        },\n    }\n\n    checks: list[bool] = []\n\n    async def start(self):\n        data = {\"key\": \"value\", \"number\": 123, \"callback\": \"some_callback\"}\n        yield Request(self.mockserver.url(\"/first\"), self.parse_first, cb_kwargs=data)\n        yield Request(\n            self.mockserver.url(\"/general_with\"), self.parse_general, cb_kwargs=data\n        )\n        yield Request(self.mockserver.url(\"/general_without\"), self.parse_general)\n        yield Request(self.mockserver.url(\"/no_kwargs\"), self.parse_no_kwargs)\n        yield Request(\n            self.mockserver.url(\"/default\"), self.parse_default, cb_kwargs=data\n        )\n        yield Request(\n            self.mockserver.url(\"/takes_less\"), self.parse_takes_less, cb_kwargs=data\n        )\n        yield Request(\n            self.mockserver.url(\"/takes_more\"), self.parse_takes_more, cb_kwargs=data\n        )\n        yield Request(self.mockserver.url(\"/downloader_mw\"), self.parse_downloader_mw)\n        yield Request(self.mockserver.url(\"/spider_mw\"), self.parse_spider_mw)\n\n    def parse_first(self, response, key, number):\n        self.checks.append(key == \"value\")\n        self.checks.append(number == 123)\n        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n        yield response.follow(\n            self.mockserver.url(\"/two\"),\n            self.parse_second,\n            cb_kwargs={\"new_key\": \"new_value\"},\n        )\n\n    def parse_second(self, response, new_key):\n        self.checks.append(new_key == \"new_value\")\n        self.crawler.stats.inc_value(\"boolean_checks\")\n\n    def parse_general(self, response, **kwargs):\n        if response.url.endswith(\"/general_with\"):\n            self.checks.append(kwargs[\"key\"] == \"value\")\n            self.checks.append(kwargs[\"number\"] == 123)\n            self.checks.append(kwargs[\"callback\"] == \"some_callback\")\n            self.crawler.stats.inc_value(\"boolean_checks\", 3)\n        elif response.url.endswith(\"/general_without\"):\n            self.checks.append(\n                kwargs == {}  # pylint: disable=use-implicit-booleaness-not-comparison\n            )\n            self.crawler.stats.inc_value(\"boolean_checks\")\n\n    def parse_no_kwargs(self, response):\n        self.checks.append(response.url.endswith(\"/no_kwargs\"))\n        self.crawler.stats.inc_value(\"boolean_checks\")\n\n    def parse_default(self, response, key, number=None, default=99):\n        self.checks.append(response.url.endswith(\"/default\"))\n        self.checks.append(key == \"value\")\n        self.checks.append(number == 123)\n        self.checks.append(default == 99)\n        self.crawler.stats.inc_value(\"boolean_checks\", 4)\n\n    def parse_takes_less(self, response, key, callback):\n        \"\"\"\n        Should raise\n        TypeError: parse_takes_less() got an unexpected keyword argument 'number'\n        \"\"\"\n\n    def parse_takes_more(self, response, key, number, callback, other):\n        \"\"\"\n        Should raise\n        TypeError: parse_takes_more() missing 1 required positional argument: 'other'\n        \"\"\"\n\n    def parse_downloader_mw(\n        self, response, from_process_request, from_process_response\n    ):\n        self.checks.append(bool(from_process_request))\n        self.checks.append(bool(from_process_response))\n        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n\n    def parse_spider_mw(self, response, from_process_spider_input, from_process_start):\n        self.checks.append(bool(from_process_spider_input))\n        self.checks.append(bool(from_process_start))\n        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n        return Request(self.mockserver.url(\"/spider_mw_2\"), self.parse_spider_mw_2)\n\n    def parse_spider_mw_2(self, response, from_process_spider_output):\n        self.checks.append(bool(from_process_spider_output))\n        self.crawler.stats.inc_value(\"boolean_checks\", 1)\n\n\nclass TestCallbackKeywordArguments:", "n_tokens": 1219, "byte_len": 5781, "file_sha1": "29356511aeece12e47ea90551e0081e88b1d7344", "start_line": 1, "end_line": 151}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_cb_kwargs.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_cb_kwargs.py", "rel_path": "tests/test_request_cb_kwargs.py", "module": "tests.test_request_cb_kwargs", "ext": "py", "chunk_number": 2, "symbols": ["setup_class", "teardown_class", "test_callback_kwargs", "boolean", "checks", "number", "keyword", "arguments", "teardown", "class", "test", "callback", "argument", "spider", "exit", "takes", "more", "stats", "check", "unexpected", "with", "classmethod", "mockserver", "exception", "other", "type", "error", "get", "crawler", "records", "process_request", "process_response", "process_spider_input", "process_spider_output", "parse_first", "parse_second", "parse_general", "parse_no_kwargs", "parse_default", "parse_takes_less", "parse_takes_more", "parse_downloader_mw", "parse_spider_mw", "parse_spider_mw_2", "InjectArgumentsDownloaderMiddleware", "InjectArgumentsSpiderMiddleware", "KeywordArgumentsSpider", "TestCallbackKeywordArguments", "parse", "default"], "ast_kind": "function_or_method", "text": "    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @inlineCallbacks\n    def test_callback_kwargs(self):\n        crawler = get_crawler(KeywordArgumentsSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert all(crawler.spider.checks)\n        assert len(crawler.spider.checks) == crawler.stats.get_value(\"boolean_checks\")\n        # check exceptions for argument mismatch\n        exceptions = {}\n        for line in log.records:\n            for key in (\"takes_less\", \"takes_more\"):\n                if key in line.getMessage():\n                    exceptions[key] = line\n        assert exceptions[\"takes_less\"].exc_info[0] is TypeError\n        assert str(exceptions[\"takes_less\"].exc_info[1]).endswith(\n            \"parse_takes_less() got an unexpected keyword argument 'number'\"\n        ), \"Exception message: \" + str(exceptions[\"takes_less\"].exc_info[1])\n        assert exceptions[\"takes_more\"].exc_info[0] is TypeError\n        assert str(exceptions[\"takes_more\"].exc_info[1]).endswith(\n            \"parse_takes_more() missing 1 required positional argument: 'other'\"\n        ), \"Exception message: \" + str(exceptions[\"takes_more\"].exc_info[1])\n", "n_tokens": 300, "byte_len": 1371, "file_sha1": "29356511aeece12e47ea90551e0081e88b1d7344", "start_line": 152, "end_line": 182}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_display.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_display.py", "rel_path": "tests/test_utils_display.py", "module": "tests.test_utils_display", "ext": "py", "chunk_number": 1, "symbols": ["test_pformat", "test_pformat_dont_colorize", "test_pformat_not_tty", "test_pformat_old_windows", "test_pformat_windows_no_terminal_processing", "test_pformat_windows", "test_pformat_no_pygments", "mock_import", "test_pprint", "test", "pformat", "isatty", "false", "import", "mock", "patch", "github", "globals", "return", "stdout", "display", "getvalue", "name", "enable", "windows", "pprint", "with", "error", "scrapy", "colorize", "https", "value", "plain", "string", "true", "stringio", "builtins", "from", "version", "out", "assert", "suffix", "platform", "unittest", "fromlist", "issues", "terminal", "processing", "colorized", "strings"], "ast_kind": "function_or_method", "text": "import builtins\nfrom io import StringIO\nfrom unittest import mock\n\nfrom scrapy.utils.display import pformat, pprint\n\nvalue = {\"a\": 1}\ncolorized_strings = {\n    (\n        (\n            \"{\\x1b[33m'\\x1b[39;49;00m\\x1b[33ma\\x1b[39;49;00m\\x1b[33m'\"\n            \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\"\n        )\n        + suffix\n    )\n    for suffix in (\n        # https://github.com/pygments/pygments/issues/2313\n        \"\\n\",  # pygments ≤ 2.13\n        \"\\x1b[37m\\x1b[39;49;00m\\n\",  # pygments ≥ 2.14\n    )\n}\nplain_string = \"{'a': 1}\"\n\n\n@mock.patch(\"sys.platform\", \"linux\")\n@mock.patch(\"sys.stdout.isatty\")\ndef test_pformat(isatty):\n    isatty.return_value = True\n    assert pformat(value) in colorized_strings\n\n\n@mock.patch(\"sys.stdout.isatty\")\ndef test_pformat_dont_colorize(isatty):\n    isatty.return_value = True\n    assert pformat(value, colorize=False) == plain_string\n\n\ndef test_pformat_not_tty():\n    assert pformat(value) == plain_string\n\n\n@mock.patch(\"sys.platform\", \"win32\")\n@mock.patch(\"platform.version\")\n@mock.patch(\"sys.stdout.isatty\")\ndef test_pformat_old_windows(isatty, version):\n    isatty.return_value = True\n    version.return_value = \"10.0.14392\"\n    assert pformat(value) in colorized_strings\n\n\n@mock.patch(\"sys.platform\", \"win32\")\n@mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n@mock.patch(\"platform.version\")\n@mock.patch(\"sys.stdout.isatty\")\ndef test_pformat_windows_no_terminal_processing(isatty, version, terminal_processing):\n    isatty.return_value = True\n    version.return_value = \"10.0.14393\"\n    terminal_processing.return_value = False\n    assert pformat(value) == plain_string\n\n\n@mock.patch(\"sys.platform\", \"win32\")\n@mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n@mock.patch(\"platform.version\")\n@mock.patch(\"sys.stdout.isatty\")\ndef test_pformat_windows(isatty, version, terminal_processing):\n    isatty.return_value = True\n    version.return_value = \"10.0.14393\"\n    terminal_processing.return_value = True\n    assert pformat(value) in colorized_strings\n\n\n@mock.patch(\"sys.platform\", \"linux\")\n@mock.patch(\"sys.stdout.isatty\")\ndef test_pformat_no_pygments(isatty):\n    isatty.return_value = True\n\n    real_import = builtins.__import__\n\n    def mock_import(name, globals_, locals_, fromlist, level):\n        if \"pygments\" in name:\n            raise ImportError\n        return real_import(name, globals_, locals_, fromlist, level)\n\n    builtins.__import__ = mock_import\n    assert pformat(value) == plain_string\n    builtins.__import__ = real_import\n\n\ndef test_pprint():\n    with mock.patch(\"sys.stdout\", new=StringIO()) as mock_out:\n        pprint(value)\n        assert mock_out.getvalue() == \"{'a': 1}\\n\"\n", "n_tokens": 728, "byte_len": 2690, "file_sha1": "373b3c421cb542c17850c4dcc53542923c9b92eb", "start_line": 1, "end_line": 94}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 1, "symbols": ["get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "__init__", "update_settings", "test_get_downloader_middleware", "TestBaseCrawler", "TestCrawler", "CustomSettingsSpider", "ParentAddon", "TrackingAddon", "ChildAddon", "ParentDownloaderMiddleware", "TrackingDownloaderMiddleware", "method", "spidercls", "async", "call", "later", "test", "populate", "get", "raw", "append", "from_crawler", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "test_get_item_pipeline_not_crawling", "test_get_item_pipeline_no_engine", "test_get_spider_middleware", "test_get_spider_middleware_not_crawling", "test_get_spider_middleware_no_engine", "test_spider_custom_settings", "test_no_root_handler_installed", "test_spider_custom_settings_log_level", "test_spider_custom_settings_log_append", "unneeded_method", "test_spider_manager_verify_interface", "test_crawler_runner_accepts_dict", "test_crawler_runner_accepts_None"], "ast_kind": "class_or_type", "text": "import asyncio\nimport logging\nimport platform\nimport re\nimport signal\nimport subprocess\nimport sys\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Generator\nfrom pathlib import Path\nfrom typing import Any\n\nimport pytest\nfrom packaging.version import parse as parse_version\nfrom pexpect.popen_spawn import PopenSpawn\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom w3lib import __version__ as w3lib_version\nfrom zope.interface.exceptions import MultipleInvalid\n\nimport scrapy\nfrom scrapy import Spider\nfrom scrapy.crawler import (\n    AsyncCrawlerProcess,\n    AsyncCrawlerRunner,\n    Crawler,\n    CrawlerProcess,\n    CrawlerRunner,\n)\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.extensions.throttle import AutoThrottle\nfrom scrapy.settings import Settings, default_settings\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.log import configure_logging, get_scrapy_root_handler\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler, get_reactor_settings\nfrom tests.mockserver.http import MockServer\nfrom tests.utils import get_script_run_env\n\nBASE_SETTINGS: dict[str, Any] = {}\n\n\ndef get_raw_crawler(spidercls=None, settings_dict=None):\n    \"\"\"get_crawler alternative that only calls the __init__ method of the\n    crawler.\"\"\"\n    settings = Settings()\n    settings.setdict(get_reactor_settings())\n    settings.setdict(settings_dict or {})\n    return Crawler(spidercls or DefaultSpider, settings)\n\n\nclass TestBaseCrawler:\n    def assertOptionIsDefault(self, settings, key):\n        assert isinstance(settings, Settings)\n        assert settings[key] == getattr(default_settings, key)\n\n\nclass TestCrawler(TestBaseCrawler):\n    def test_populate_spidercls_settings(self):\n        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n        project_settings = {\n            **BASE_SETTINGS,\n            \"TEST1\": \"project\",\n            \"TEST3\": \"project\",\n            **get_reactor_settings(),\n        }\n\n        class CustomSettingsSpider(DefaultSpider):\n            custom_settings = spider_settings\n\n        settings = Settings()\n        settings.setdict(project_settings, priority=\"project\")\n        crawler = Crawler(CustomSettingsSpider, settings)\n        crawler._apply_settings()\n\n        assert crawler.settings.get(\"TEST1\") == \"spider\"\n        assert crawler.settings.get(\"TEST2\") == \"spider\"\n        assert crawler.settings.get(\"TEST3\") == \"project\"\n\n        assert not settings.frozen\n        assert crawler.settings.frozen\n\n    def test_crawler_accepts_dict(self):\n        crawler = get_crawler(DefaultSpider, {\"foo\": \"bar\"})\n        assert crawler.settings[\"foo\"] == \"bar\"\n        self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_accepts_None(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            crawler = Crawler(DefaultSpider)\n        self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_rejects_spider_objects(self):\n        with pytest.raises(ValueError, match=\"spidercls argument must be a class\"):\n            Crawler(DefaultSpider())\n\n    @inlineCallbacks\n    def test_crawler_crawl_twice_seq_unsupported(self):\n        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n        yield crawler.crawl()\n        with pytest.raises(RuntimeError, match=\"more than once on the same instance\"):\n            yield crawler.crawl()\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_crawler_crawl_async_twice_seq_unsupported(self):\n        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n        await crawler.crawl_async()\n        with pytest.raises(RuntimeError, match=\"more than once on the same instance\"):\n            await crawler.crawl_async()\n\n    @inlineCallbacks\n    def test_crawler_crawl_twice_parallel_unsupported(self):\n        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n        d1 = crawler.crawl()\n        d2 = crawler.crawl()\n        yield d1\n        with pytest.raises(RuntimeError, match=\"Crawling already taking place\"):\n            yield d2\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_crawler_crawl_async_twice_parallel_unsupported(self):\n        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n        t1 = asyncio.create_task(crawler.crawl_async())\n        t2 = asyncio.create_task(crawler.crawl_async())\n        await t1\n        with pytest.raises(RuntimeError, match=\"Crawling already taking place\"):\n            await t2\n\n    def test_get_addon(self):\n        class ParentAddon:\n            pass\n\n        class TrackingAddon(ParentAddon):\n            instances = []\n\n            def __init__(self):\n                TrackingAddon.instances.append(self)\n\n            def update_settings(self, settings):\n                pass\n\n        settings = {\n            **BASE_SETTINGS,\n            \"ADDONS\": {\n                TrackingAddon: 0,\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        assert len(TrackingAddon.instances) == 1\n        expected = TrackingAddon.instances[-1]\n\n        addon = crawler.get_addon(TrackingAddon)\n        assert addon == expected\n\n        addon = crawler.get_addon(DefaultSpider)\n        assert addon is None\n\n        addon = crawler.get_addon(ParentAddon)\n        assert addon == expected\n\n        class ChildAddon(TrackingAddon):\n            pass\n\n        addon = crawler.get_addon(ChildAddon)\n        assert addon is None\n\n    @inlineCallbacks\n    def test_get_downloader_middleware(self):\n        class ParentDownloaderMiddleware:\n            pass\n\n        class TrackingDownloaderMiddleware(ParentDownloaderMiddleware):\n            instances = []\n", "n_tokens": 1251, "byte_len": 5932, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 1, "end_line": 182}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "from_crawler", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "MySpider", "ChildDownloaderMiddleware", "ParentExtension", "TrackingExtension", "ChildExtension", "ParentItemPipeline", "TrackingItemPipeline", "default", "spider", "test", "get", "settings", "dict", "async", "result", "pass", "except", "extensions", "append", "raw", "return", "parent", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "update_settings", "test_get_downloader_middleware", "test_get_item_pipeline_not_crawling", "test_get_item_pipeline_no_engine", "test_get_spider_middleware", "test_get_spider_middleware_not_crawling", "test_get_spider_middleware_no_engine", "test_spider_custom_settings", "test_no_root_handler_installed", "test_spider_custom_settings_log_level", "test_spider_custom_settings_log_append"], "ast_kind": "class_or_type", "text": "            def __init__(self):\n                TrackingDownloaderMiddleware.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler, **kwargs: Any):\n                super().__init__(**kwargs)\n                self.crawler = crawler\n\n            async def start(self):\n                MySpider.result = crawler.get_downloader_middleware(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"DOWNLOADER_MIDDLEWARES\": {\n                TrackingDownloaderMiddleware: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingDownloaderMiddleware\n        yield crawler.crawl()\n        assert len(TrackingDownloaderMiddleware.instances) == 1\n        assert MySpider.result == TrackingDownloaderMiddleware.instances[-1]\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentDownloaderMiddleware\n        yield crawler.crawl()\n        assert MySpider.result == TrackingDownloaderMiddleware.instances[-1]\n\n        class ChildDownloaderMiddleware(TrackingDownloaderMiddleware):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildDownloaderMiddleware\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n    def test_get_downloader_middleware_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            crawler.get_downloader_middleware(DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_downloader_middleware_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_downloader_middleware(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            yield crawler.crawl()\n\n    @inlineCallbacks\n    def test_get_extension(self):\n        class ParentExtension:\n            pass\n\n        class TrackingExtension(ParentExtension):\n            instances = []\n\n            def __init__(self):\n                TrackingExtension.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler, **kwargs: Any):\n                super().__init__(**kwargs)\n                self.crawler = crawler\n\n            async def start(self):\n                MySpider.result = crawler.get_extension(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"EXTENSIONS\": {\n                TrackingExtension: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingExtension\n        yield crawler.crawl()\n        assert len(TrackingExtension.instances) == 1\n        assert MySpider.result == TrackingExtension.instances[-1]\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentExtension\n        yield crawler.crawl()\n        assert MySpider.result == TrackingExtension.instances[-1]\n\n        class ChildExtension(TrackingExtension):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildExtension\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n    def test_get_extension_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            crawler.get_extension(DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_extension_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_extension(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            yield crawler.crawl()\n\n    @inlineCallbacks\n    def test_get_item_pipeline(self):\n        class ParentItemPipeline:\n            pass\n\n        class TrackingItemPipeline(ParentItemPipeline):\n            instances = []\n\n            def __init__(self):\n                TrackingItemPipeline.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler, **kwargs: Any):\n                super().__init__(**kwargs)\n                self.crawler = crawler\n\n            async def start(self):\n                MySpider.result = crawler.get_item_pipeline(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"ITEM_PIPELINES\": {\n                TrackingItemPipeline: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingItemPipeline\n        yield crawler.crawl()\n        assert len(TrackingItemPipeline.instances) == 1\n        assert MySpider.result == TrackingItemPipeline.instances[-1]\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentItemPipeline\n        yield crawler.crawl()\n        assert MySpider.result == TrackingItemPipeline.instances[-1]\n", "n_tokens": 1219, "byte_len": 6368, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 183, "end_line": 384}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 3, "symbols": ["test_get_item_pipeline_not_crawling", "test_get_item_pipeline_no_engine", "from_crawler", "test_get_spider_middleware", "__init__", "test_get_spider_middleware_not_crawling", "test_get_spider_middleware_no_engine", "test_spider_custom_settings", "test_no_root_handler_installed", "test_spider_custom_settings_log_level", "test_spider_custom_settings_log_append", "ChildItemPipeline", "MySpider", "ParentSpiderMiddleware", "TrackingSpiderMiddleware", "ChildSpiderMiddleware", "TestSpiderSettings", "TestCrawlerLogging", "encoding", "test", "spider", "log015", "async", "get", "raw", "append", "name", "spide", "middlewares", "path", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "update_settings", "test_get_downloader_middleware", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "unneeded_method", "test_spider_manager_verify_interface", "test_crawler_runner_accepts_dict"], "ast_kind": "class_or_type", "text": "        class ChildItemPipeline(TrackingItemPipeline):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildItemPipeline\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n    def test_get_item_pipeline_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            crawler.get_item_pipeline(DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_item_pipeline_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_item_pipeline(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            yield crawler.crawl()\n\n    @inlineCallbacks\n    def test_get_spider_middleware(self):\n        class ParentSpiderMiddleware:\n            pass\n\n        class TrackingSpiderMiddleware(ParentSpiderMiddleware):\n            instances = []\n\n            def __init__(self):\n                TrackingSpiderMiddleware.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler, **kwargs: Any):\n                super().__init__(**kwargs)\n                self.crawler = crawler\n\n            async def start(self):\n                MySpider.result = crawler.get_spider_middleware(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"SPIDER_MIDDLEWARES\": {\n                TrackingSpiderMiddleware: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingSpiderMiddleware\n        yield crawler.crawl()\n        assert len(TrackingSpiderMiddleware.instances) == 1\n        assert MySpider.result == TrackingSpiderMiddleware.instances[-1]\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentSpiderMiddleware\n        yield crawler.crawl()\n        assert MySpider.result == TrackingSpiderMiddleware.instances[-1]\n\n        class ChildSpiderMiddleware(TrackingSpiderMiddleware):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildSpiderMiddleware\n        yield crawler.crawl()\n        assert MySpider.result is None\n\n    def test_get_spider_middleware_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            crawler.get_spider_middleware(DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_spider_middleware_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_spider_middleware(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with pytest.raises(RuntimeError):\n            yield crawler.crawl()\n\n\nclass TestSpiderSettings:\n    def test_spider_custom_settings(self):\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n            custom_settings = {\"AUTOTHROTTLE_ENABLED\": True}\n\n        crawler = get_crawler(MySpider)\n        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n        assert AutoThrottle in enabled_exts\n\n\nclass TestCrawlerLogging:\n    def test_no_root_handler_installed(self):\n        handler = get_scrapy_root_handler()\n        if handler is not None:\n            logging.root.removeHandler(handler)\n\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n\n        get_crawler(MySpider)\n        assert get_scrapy_root_handler() is None\n\n    def test_spider_custom_settings_log_level(self, tmp_path):\n        log_file = Path(tmp_path, \"log.txt\")\n        log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n            custom_settings = {\n                \"LOG_LEVEL\": \"INFO\",\n                \"LOG_FILE\": str(log_file),\n            }\n\n        configure_logging()\n        assert get_scrapy_root_handler().level == logging.DEBUG\n        crawler = get_crawler(MySpider)\n        assert get_scrapy_root_handler().level == logging.INFO\n        info_count = crawler.stats.get_value(\"log_count/INFO\")\n        logging.debug(\"debug message\")  # noqa: LOG015\n        logging.info(\"info message\")  # noqa: LOG015\n        logging.warning(\"warning message\")  # noqa: LOG015\n        logging.error(\"error message\")  # noqa: LOG015\n\n        logged = log_file.read_text(encoding=\"utf-8\")\n\n        assert \"previous message\" in logged\n        assert \"debug message\" not in logged\n        assert \"info message\" in logged\n        assert \"warning message\" in logged\n        assert \"error message\" in logged\n        assert crawler.stats.get_value(\"log_count/ERROR\") == 1\n        assert crawler.stats.get_value(\"log_count/WARNING\") == 1\n        assert crawler.stats.get_value(\"log_count/INFO\") - info_count == 1\n        assert crawler.stats.get_value(\"log_count/DEBUG\", 0) == 0\n\n    def test_spider_custom_settings_log_append(self, tmp_path):\n        log_file = Path(tmp_path, \"log.txt\")\n        log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n", "n_tokens": 1194, "byte_len": 5819, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 385, "end_line": 555}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 4, "symbols": ["unneeded_method", "test_spider_manager_verify_interface", "test_crawler_runner_accepts_dict", "test_crawler_runner_accepts_None", "test_crawler_process_accepts_dict", "test_crawler_process_accepts_None", "from_crawler", "_runner", "_crawl", "test_crawler_runner_bootstrap_successful", "test_crawler_runner_bootstrap_successful_for_several", "test_crawler_runner_bootstrap_failed", "test_crawler_runner_bootstrap_failed_for_several", "test_crawler_runner_asyncio_enabled_true", "MySpider", "SpiderLoaderWithWrongInterface", "TestCrawlerRunner", "TestAsyncCrawlerRunner", "TestCrawlerProcess", "TestAsyncCrawlerProcess", "ExceptionSpider", "NoRequestsSpider", "TestCrawlerRunnerHasSpider", "TestAsyncCrawlerRunnerHasSpider", "ScriptRunnerMixin", "encoding", "log015", "method", "does", "async", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "__init__", "update_settings", "test_get_downloader_middleware", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "test_get_item_pipeline_not_crawling", "test_get_item_pipeline_no_engine"], "ast_kind": "class_or_type", "text": "        class MySpider(scrapy.Spider):\n            name = \"spider\"\n            custom_settings = {\n                \"LOG_FILE\": str(log_file),\n                \"LOG_FILE_APPEND\": False,\n            }\n\n        configure_logging()\n        get_crawler(MySpider)\n        logging.debug(\"debug message\")  # noqa: LOG015\n\n        logged = log_file.read_text(encoding=\"utf-8\")\n\n        assert \"previous message\" not in logged\n        assert \"debug message\" in logged\n\n\nclass SpiderLoaderWithWrongInterface:\n    def unneeded_method(self):\n        pass\n\n\nclass TestCrawlerRunner(TestBaseCrawler):\n    def test_spider_manager_verify_interface(self):\n        settings = Settings(\n            {\n                \"SPIDER_LOADER_CLASS\": SpiderLoaderWithWrongInterface,\n            }\n        )\n        with pytest.raises(MultipleInvalid):\n            CrawlerRunner(settings)\n\n    def test_crawler_runner_accepts_dict(self):\n        runner = CrawlerRunner({\"foo\": \"bar\"})\n        assert runner.settings[\"foo\"] == \"bar\"\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_runner_accepts_None(self):\n        runner = CrawlerRunner()\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n\nclass TestAsyncCrawlerRunner(TestBaseCrawler):\n    def test_spider_manager_verify_interface(self):\n        settings = Settings(\n            {\n                \"SPIDER_LOADER_CLASS\": SpiderLoaderWithWrongInterface,\n            }\n        )\n        with pytest.raises(MultipleInvalid):\n            AsyncCrawlerRunner(settings)\n\n    def test_crawler_runner_accepts_dict(self):\n        runner = AsyncCrawlerRunner({\"foo\": \"bar\"})\n        assert runner.settings[\"foo\"] == \"bar\"\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_runner_accepts_None(self):\n        runner = AsyncCrawlerRunner()\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n\nclass TestCrawlerProcess(TestBaseCrawler):\n    def test_crawler_process_accepts_dict(self):\n        runner = CrawlerProcess({\"foo\": \"bar\"})\n        assert runner.settings[\"foo\"] == \"bar\"\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_process_accepts_None(self):\n        runner = CrawlerProcess()\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n\n@pytest.mark.only_asyncio\nclass TestAsyncCrawlerProcess(TestBaseCrawler):\n    def test_crawler_process_accepts_dict(self):\n        runner = AsyncCrawlerProcess({\"foo\": \"bar\"})\n        assert runner.settings[\"foo\"] == \"bar\"\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_process_accepts_None(self):\n        runner = AsyncCrawlerProcess()\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n\nclass ExceptionSpider(scrapy.Spider):\n    name = \"exception\"\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        raise ValueError(\"Exception in from_crawler method\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nclass TestCrawlerRunnerHasSpider:\n    @staticmethod\n    def _runner():\n        return CrawlerRunner(get_reactor_settings())\n\n    @staticmethod\n    def _crawl(runner, spider):\n        return runner.crawl(spider)\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_successful(self):\n        runner = self._runner()\n        yield self._crawl(runner, NoRequestsSpider)\n        assert not runner.bootstrap_failed\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_successful_for_several(self):\n        runner = self._runner()\n        yield self._crawl(runner, NoRequestsSpider)\n        yield self._crawl(runner, NoRequestsSpider)\n        assert not runner.bootstrap_failed\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_failed(self):\n        runner = self._runner()\n\n        try:\n            yield self._crawl(runner, ExceptionSpider)\n        except ValueError:\n            pass\n        else:\n            pytest.fail(\"Exception should be raised from spider\")\n\n        assert runner.bootstrap_failed\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_failed_for_several(self):\n        runner = self._runner()\n\n        try:\n            yield self._crawl(runner, ExceptionSpider)\n        except ValueError:\n            pass\n        else:\n            pytest.fail(\"Exception should be raised from spider\")\n\n        yield self._crawl(runner, NoRequestsSpider)\n\n        assert runner.bootstrap_failed\n\n    @inlineCallbacks\n    def test_crawler_runner_asyncio_enabled_true(\n        self, reactor_pytest: str\n    ) -> Generator[Deferred[Any], Any, None]:\n        if reactor_pytest != \"asyncio\":\n            runner = CrawlerRunner(\n                settings={\n                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                }\n            )\n            with pytest.raises(\n                Exception,\n                match=r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\",\n            ):\n                yield self._crawl(runner, NoRequestsSpider)\n        else:\n            CrawlerRunner(\n                settings={\n                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                }\n            )\n\n\n@pytest.mark.only_asyncio\nclass TestAsyncCrawlerRunnerHasSpider(TestCrawlerRunnerHasSpider):\n    @staticmethod\n    def _runner():\n        return AsyncCrawlerRunner(get_reactor_settings())\n\n    @staticmethod\n    def _crawl(runner, spider):\n        return deferred_from_coro(runner.crawl(spider))\n\n    def test_crawler_runner_asyncio_enabled_true(self):\n        pytest.skip(\"This test is only for CrawlerRunner\")\n\n\nclass ScriptRunnerMixin(ABC):", "n_tokens": 1212, "byte_len": 5783, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 556, "end_line": 744}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 5, "symbols": ["script_dir", "get_script_dir", "get_script_args", "run_script", "test_simple", "test_multi", "test_reactor_default", "test_asyncio_enabled_no_reactor", "test_asyncio_enabled_reactor", "test_ipv6_default_name_resolver", "test_caching_hostname_resolver_ipv6", "test_caching_hostname_resolver_finite_execution", "test_twisted_reactor_asyncio", "test_twisted_reactor_asyncio_custom_settings", "test_twisted_reactor_asyncio_custom_settings_same", "test_custom_loop_asyncio", "test_custom_loop_asyncio_deferred_signal", "TestCrawlerProcessSubprocessBase", "does", "async", "asyncio", "enabled", "run", "script", "file", "lib", "w3lib", "spider", "name", "get", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "__init__", "update_settings", "test_get_downloader_middleware", "from_crawler", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "test_get_item_pipeline_not_crawling"], "ast_kind": "class_or_type", "text": "    @property\n    @abstractmethod\n    def script_dir(self) -> Path:\n        raise NotImplementedError\n\n    @staticmethod\n    def get_script_dir(name: str) -> Path:\n        return Path(__file__).parent.resolve() / name\n\n    def get_script_args(self, script_name: str, *script_args: str) -> list[str]:\n        script_path = self.script_dir / script_name\n        return [sys.executable, str(script_path), *script_args]\n\n    def run_script(self, script_name: str, *script_args: str) -> str:\n        args = self.get_script_args(script_name, *script_args)\n        p = subprocess.Popen(\n            args,\n            env=get_script_run_env(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = p.communicate()\n        return stderr.decode(\"utf-8\")\n\n\nclass TestCrawlerProcessSubprocessBase(ScriptRunnerMixin):\n    \"\"\"Common tests between CrawlerProcess and AsyncCrawlerProcess,\n    with the same file names and expectations.\n    \"\"\"\n\n    def test_simple(self):\n        log = self.run_script(\"simple.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    def test_multi(self):\n        log = self.run_script(\"multi.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"ReactorAlreadyInstalledError\" not in log\n\n    def test_reactor_default(self):\n        log = self.run_script(\"reactor_default.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"does not match the requested one \"\n            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n        ) in log\n\n    def test_asyncio_enabled_no_reactor(self):\n        log = self.run_script(\"asyncio_enabled_no_reactor.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"RuntimeError\" not in log\n\n    def test_asyncio_enabled_reactor(self):\n        log = self.run_script(\"asyncio_enabled_reactor.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"RuntimeError\" not in log\n\n    @pytest.mark.skipif(\n        parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n        reason=\"w3lib 2.0.0 and later do not allow invalid domains.\",\n    )\n    def test_ipv6_default_name_resolver(self):\n        log = self.run_script(\"default_name_resolver.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\"\n            in log\n        )\n        assert (\n            \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\"\n            in log\n        )\n\n    def test_caching_hostname_resolver_ipv6(self):\n        log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n        assert \"Spider closed (finished)\" in log\n        assert \"twisted.internet.error.DNSLookupError\" not in log\n\n    def test_caching_hostname_resolver_finite_execution(\n        self, mockserver: MockServer\n    ) -> None:\n        log = self.run_script(\"caching_hostname_resolver.py\", mockserver.url(\"/\"))\n        assert \"Spider closed (finished)\" in log\n        assert \"ERROR: Error downloading\" not in log\n        assert \"TimeoutError\" not in log\n        assert \"twisted.internet.error.DNSLookupError\" not in log\n\n    def test_twisted_reactor_asyncio(self):\n        log = self.run_script(\"twisted_reactor_asyncio.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    def test_twisted_reactor_asyncio_custom_settings(self):\n        log = self.run_script(\"twisted_reactor_custom_settings.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    def test_twisted_reactor_asyncio_custom_settings_same(self):\n        log = self.run_script(\"twisted_reactor_custom_settings_same.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    @pytest.mark.requires_uvloop\n    def test_custom_loop_asyncio(self):\n        log = self.run_script(\"asyncio_custom_loop.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" in log\n\n    @pytest.mark.requires_uvloop\n    def test_custom_loop_asyncio_deferred_signal(self):\n        log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" in log\n        assert \"async pipeline opened!\" in log\n", "n_tokens": 1203, "byte_len": 5468, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 745, "end_line": 892}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 6, "symbols": ["test_asyncio_enabled_reactor_same_loop", "test_asyncio_enabled_reactor_different_loop", "test_default_loop_asyncio_deferred_signal", "test_args_change_settings", "test_shutdown_graceful", "test_shutdown_forced", "script_dir", "test_reactor_default_twisted_reactor_select", "test_reactor_select", "test_reactor_select_twisted_reactor_select", "test_reactor_select_subclass_twisted_reactor_select", "test_twisted_reactor_select", "test_twisted_reactor_poll", "test_twisted_reactor_asyncio_custom_settings_conflict", "TestCrawlerProcessSubprocess", "TestAsyncCrawlerProcessSubprocess", "does", "async", "problems", "call", "later", "asyncio", "enabled", "run", "script", "signal", "case", "future", "becomes", "spider", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "__init__", "update_settings", "test_get_downloader_middleware", "from_crawler", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "test_get_item_pipeline_not_crawling"], "ast_kind": "class_or_type", "text": "    @pytest.mark.requires_uvloop\n    def test_asyncio_enabled_reactor_same_loop(self):\n        log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" in log\n\n    @pytest.mark.requires_uvloop\n    def test_asyncio_enabled_reactor_different_loop(self):\n        log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n            \"setting (uvloop.Loop)\"\n        ) in log\n\n    def test_default_loop_asyncio_deferred_signal(self):\n        log = self.run_script(\"asyncio_deferred_signal.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" not in log\n        assert \"async pipeline opened!\" in log\n\n    def test_args_change_settings(self):\n        log = self.run_script(\"args_settings.py\")\n        assert \"Spider closed (finished)\" in log\n        assert \"The value of FOO is 42\" in log\n\n    def test_shutdown_graceful(self):\n        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n        args = self.get_script_args(\"sleeping.py\", \"3\")\n        p = PopenSpawn(args, timeout=5)\n        p.expect_exact(\"Spider opened\")\n        p.expect_exact(\"Crawled (200)\")\n        p.kill(sig)\n        p.expect_exact(\"shutting down gracefully\")\n        p.expect_exact(\"Spider closed (shutdown)\")\n        p.wait()\n\n    @inlineCallbacks\n    def test_shutdown_forced(self):\n        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n        args = self.get_script_args(\"sleeping.py\", \"10\")\n        p = PopenSpawn(args, timeout=5)\n        p.expect_exact(\"Spider opened\")\n        p.expect_exact(\"Crawled (200)\")\n        p.kill(sig)\n        p.expect_exact(\"shutting down gracefully\")\n        # sending the second signal too fast often causes problems\n        d = Deferred()\n        call_later(0.01, d.callback, None)\n        yield d\n        p.kill(sig)\n        p.expect_exact(\"forcing unclean shutdown\")\n        p.wait()\n\n\nclass TestCrawlerProcessSubprocess(TestCrawlerProcessSubprocessBase):\n    @property\n    def script_dir(self) -> Path:\n        return self.get_script_dir(\"CrawlerProcess\")\n\n    def test_reactor_default_twisted_reactor_select(self):\n        log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n        if platform.system() in [\"Windows\", \"Darwin\"]:\n            # The goal of this test function is to test that, when a reactor is\n            # installed (the default one here) and a different reactor is\n            # configured (select here), an error raises.\n            #\n            # In Windows the default reactor is the select reactor, so that\n            # error does not raise.\n            #\n            # If that ever becomes the case on more platforms (i.e. if Linux\n            # also starts using the select reactor by default in a future\n            # version of Twisted), then we will need to rethink this test.\n            assert \"Spider closed (finished)\" in log\n        else:\n            assert \"Spider closed (finished)\" not in log\n            assert (\n                \"does not match the requested one \"\n                \"(twisted.internet.selectreactor.SelectReactor)\"\n            ) in log\n\n    def test_reactor_select(self):\n        log = self.run_script(\"reactor_select.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"does not match the requested one \"\n            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n        ) in log\n\n    def test_reactor_select_twisted_reactor_select(self):\n        log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n        assert \"Spider closed (finished)\" in log\n        assert \"ReactorAlreadyInstalledError\" not in log\n\n    def test_reactor_select_subclass_twisted_reactor_select(self):\n        log = self.run_script(\"reactor_select_subclass_twisted_reactor_select.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"does not match the requested one \"\n            \"(twisted.internet.selectreactor.SelectReactor)\"\n        ) in log\n\n    def test_twisted_reactor_select(self):\n        log = self.run_script(\"twisted_reactor_select.py\")\n        assert \"Spider closed (finished)\" in log\n        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n    )\n    def test_twisted_reactor_poll(self):\n        log = self.run_script(\"twisted_reactor_poll.py\")\n        assert \"Spider closed (finished)\" in log\n        assert \"Using reactor: twisted.internet.pollreactor.PollReactor\" in log\n\n    def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n        log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n        assert (\n            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\"\n            in log\n        )\n\n\nclass TestAsyncCrawlerProcessSubprocess(TestCrawlerProcessSubprocessBase):", "n_tokens": 1231, "byte_len": 5543, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 893, "end_line": 1025}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 7, "symbols": ["script_dir", "test_twisted_reactor_custom_settings_select", "test_asyncio_enabled_reactor_same_loop", "test_asyncio_enabled_reactor_different_loop", "test_simple", "test_multi_parallel", "test_multi_seq", "test_custom_loop_same", "test_custom_loop_different", "test_explicit_default_reactor", "test_response_ip_address", "test_change_default_reactor", "test_simple_default_reactor", "TestCrawlerRunnerSubprocessBase", "TestCrawlerRunnerSubprocess", "TestAsyncCrawlerRunnerSubprocess", "does", "address", "run", "script", "spider", "domain", "common", "between", "dotall", "dir", "path", "simple", "default", "pytest", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "__init__", "update_settings", "test_get_downloader_middleware", "from_crawler", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "test_get_item_pipeline_not_crawling"], "ast_kind": "class_or_type", "text": "    @property\n    def script_dir(self) -> Path:\n        return self.get_script_dir(\"AsyncCrawlerProcess\")\n\n    def test_twisted_reactor_custom_settings_select(self):\n        log = self.run_script(\"twisted_reactor_custom_settings_select.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor) \"\n            \"does not match the requested one \"\n            \"(twisted.internet.selectreactor.SelectReactor)\"\n        ) in log\n\n    @pytest.mark.requires_uvloop\n    def test_asyncio_enabled_reactor_same_loop(self):\n        log = self.run_script(\"asyncio_custom_loop_custom_settings_same.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" in log\n\n    @pytest.mark.requires_uvloop\n    def test_asyncio_enabled_reactor_different_loop(self):\n        log = self.run_script(\"asyncio_custom_loop_custom_settings_different.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n            \"setting (uvloop.Loop)\"\n        ) in log\n\n\nclass TestCrawlerRunnerSubprocessBase(ScriptRunnerMixin):\n    \"\"\"Common tests between CrawlerRunner and AsyncCrawlerRunner,\n    with the same file names and expectations.\n    \"\"\"\n\n    def test_simple(self):\n        log = self.run_script(\"simple.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n\n    def test_multi_parallel(self):\n        log = self.run_script(\"multi_parallel.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert re.search(\n            r\"Spider opened.+Spider opened.+Closing spider.+Closing spider\",\n            log,\n            re.DOTALL,\n        )\n\n    def test_multi_seq(self):\n        log = self.run_script(\"multi_seq.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert re.search(\n            r\"Spider opened.+Closing spider.+Spider opened.+Closing spider\",\n            log,\n            re.DOTALL,\n        )\n\n    @pytest.mark.requires_uvloop\n    def test_custom_loop_same(self):\n        log = self.run_script(\"custom_loop_same.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"Using asyncio event loop: uvloop.Loop\" in log\n\n    @pytest.mark.requires_uvloop\n    def test_custom_loop_different(self):\n        log = self.run_script(\"custom_loop_different.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert (\n            \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n            \"setting (uvloop.Loop)\"\n        ) in log\n\n\nclass TestCrawlerRunnerSubprocess(TestCrawlerRunnerSubprocessBase):\n    @property\n    def script_dir(self) -> Path:\n        return self.get_script_dir(\"CrawlerRunner\")\n\n    def test_explicit_default_reactor(self):\n        log = self.run_script(\"explicit_default_reactor.py\")\n        assert \"Spider closed (finished)\" in log\n        assert (\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            not in log\n        )\n\n    def test_response_ip_address(self):\n        log = self.run_script(\"ip_address.py\")\n        assert \"INFO: Spider closed (finished)\" in log\n        assert \"INFO: Host: not.a.real.domain\" in log\n        assert \"INFO: Type: <class 'ipaddress.IPv4Address'>\" in log\n        assert \"INFO: IP address: 127.0.0.1\" in log\n\n    def test_change_default_reactor(self):\n        log = self.run_script(\"change_reactor.py\")\n        assert (\n            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            in log\n        )\n        assert \"DEBUG: Using asyncio event loop\" in log\n\n\nclass TestAsyncCrawlerRunnerSubprocess(TestCrawlerRunnerSubprocessBase):\n    @property\n    def script_dir(self) -> Path:\n        return self.get_script_dir(\"AsyncCrawlerRunner\")\n\n    def test_simple_default_reactor(self):\n        log = self.run_script(\"simple_default_reactor.py\")\n        assert \"Spider closed (finished)\" not in log\n        assert \"RuntimeError: AsyncCrawlerRunner requires AsyncioSelectorReactor\" in log\n\n\n@pytest.mark.parametrize(\n    (\"settings\", \"items\"),\n    [\n        ({}, default_settings.LOG_VERSIONS),\n        ({\"LOG_VERSIONS\": [\"itemadapter\"]}, [\"itemadapter\"]),\n        ({\"LOG_VERSIONS\": []}, None),\n    ],\n)", "n_tokens": 1085, "byte_len": 4946, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 1026, "end_line": 1165}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_crawler.py", "rel_path": "tests/test_crawler.py", "module": "tests.test_crawler", "ext": "py", "chunk_number": 8, "symbols": ["test_log_scrapy_info", "default", "spider", "versions", "async", "repr", "await", "scrapy", "deprecation", "warns", "return", "crawler", "process", "item", "deferred", "from", "test", "deprecated", "maybe", "log", "with", "level", "scrapybot", "records", "get", "started", "pytest", "settings", "assert", "message", "get_raw_crawler", "assertOptionIsDefault", "test_populate_spidercls_settings", "test_crawler_accepts_dict", "test_crawler_accepts_None", "test_crawler_rejects_spider_objects", "test_crawler_crawl_twice_seq_unsupported", "test_crawler_crawl_twice_parallel_unsupported", "test_get_addon", "__init__", "update_settings", "test_get_downloader_middleware", "from_crawler", "test_get_downloader_middleware_not_crawling", "test_get_downloader_middleware_no_engine", "test_get_extension", "test_get_extension_not_crawling", "test_get_extension_no_engine", "test_get_item_pipeline", "test_get_item_pipeline_not_crawling"], "ast_kind": "function_or_method", "text": "def test_log_scrapy_info(settings, items, caplog):\n    with caplog.at_level(\"INFO\"):\n        CrawlerProcess(settings)\n    assert (\n        caplog.records[0].getMessage()\n        == f\"Scrapy {scrapy.__version__} started (bot: scrapybot)\"\n    ), repr(caplog.records[0].msg)\n    if not items:\n        assert len(caplog.records) == 1\n        return\n    version_string = caplog.records[1].getMessage()\n    expected_items_pattern = \"',\\n '\".join(\n        f\"{item}': '[^']+('\\n +'[^']+)*\" for item in items\n    )\n    assert re.search(r\"^Versions:\\n{'\" + expected_items_pattern + \"'}$\", version_string)\n\n\n@deferred_f_from_coro_f\nasync def test_deprecated_crawler_stop() -> None:\n    crawler = get_crawler(DefaultSpider)\n    d = crawler.crawl()\n    await maybe_deferred_to_future(d)\n    with pytest.warns(\n        ScrapyDeprecationWarning, match=r\"Crawler.stop\\(\\) is deprecated\"\n    ):\n        await maybe_deferred_to_future(crawler.stop())\n", "n_tokens": 244, "byte_len": 933, "file_sha1": "4c60f4b5288a31aa80a61206e63ce0085edae816", "start_line": 1166, "end_line": 1192}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 1, "symbols": ["path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "TestFileFeedStorage", "TestFTPFeedStorage", "TestSpider", "path", "str", "lib", "w3lib", "spider", "name", "deferred", "from", "quote", "url", "spiders", "more", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file", "test_invalid_folder", "test_parse_credentials", "test_init_without_acl", "test_init_with_acl", "test_init_with_endpoint_url", "test_init_with_region_name", "test_from_crawler_without_acl", "test_without_endpoint_url", "test_without_region_name", "test_from_crawler_with_acl", "test_from_crawler_with_endpoint_url", "test_from_crawler_with_region_name", "test_overwrite_default", "test_overwrite_false", "test_parse_settings"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport bz2\nimport csv\nimport gzip\nimport json\nimport lzma\nimport marshal\nimport os\nimport pickle\nimport random\nimport shutil\nimport string\nimport sys\nimport tempfile\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom io import BytesIO\nfrom logging import getLogger\nfrom pathlib import Path\nfrom string import ascii_letters, digits\nfrom typing import IO, TYPE_CHECKING, Any\nfrom unittest import mock\nfrom urllib.parse import quote, urljoin\nfrom urllib.request import pathname2url\n\nimport lxml.etree\nimport pytest\nfrom packaging.version import Version\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.defer import inlineCallbacks\nfrom w3lib.url import file_uri_to_path, path_to_file_uri\nfrom zope.interface import implementer\nfrom zope.interface.verify import verifyObject\n\nimport scrapy\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.exporters import CsvItemExporter, JsonItemExporter\nfrom scrapy.extensions.feedexport import (\n    BlockingFeedStorage,\n    FeedExporter,\n    FeedSlot,\n    FileFeedStorage,\n    FTPFeedStorage,\n    GCSFeedStorage,\n    IFeedStorage,\n    S3FeedStorage,\n    StdoutFeedStorage,\n)\nfrom scrapy.settings import Settings\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.ftp import MockFTPServer\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import ItemSpider\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n    from os import PathLike\n\n\ndef path_to_url(path):\n    return urljoin(\"file:\", pathname2url(str(path)))\n\n\ndef printf_escape(string):\n    return string.replace(\"%\", \"%%\")\n\n\ndef build_url(path: str | PathLike) -> str:\n    path_str = str(path)\n    if path_str[0] != \"/\":\n        path_str = \"/\" + path_str\n    return urljoin(\"file:\", path_str)\n\n\ndef mock_google_cloud_storage() -> tuple[Any, Any, Any]:\n    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n    classes and set their proper return values.\n    \"\"\"\n    from google.cloud.storage import Blob, Bucket, Client  # noqa: PLC0415\n\n    client_mock = mock.create_autospec(Client)\n\n    bucket_mock = mock.create_autospec(Bucket)\n    client_mock.get_bucket.return_value = bucket_mock\n\n    blob_mock = mock.create_autospec(Blob)\n    bucket_mock.blob.return_value = blob_mock\n\n    return (client_mock, bucket_mock, blob_mock)\n\n\nclass TestFileFeedStorage:\n    def test_store_file_uri(self, tmp_path):\n        path = tmp_path / \"file.txt\"\n        uri = path_to_file_uri(str(path))\n        self._assert_stores(FileFeedStorage(uri), path)\n\n    def test_store_file_uri_makedirs(self, tmp_path):\n        path = tmp_path / \"more\" / \"paths\" / \"file.txt\"\n        uri = path_to_file_uri(str(path))\n        self._assert_stores(FileFeedStorage(uri), path)\n\n    def test_store_direct_path(self, tmp_path):\n        path = tmp_path / \"file.txt\"\n        self._assert_stores(FileFeedStorage(str(path)), path)\n\n    def test_store_direct_path_relative(self, tmp_path):\n        old_cwd = Path.cwd()\n        try:\n            os.chdir(tmp_path)\n            path = Path(\"foo\", \"bar\")\n            self._assert_stores(FileFeedStorage(str(path)), path)\n        finally:\n            os.chdir(old_cwd)\n\n    def test_interface(self, tmp_path):\n        path = tmp_path / \"file.txt\"\n        st = FileFeedStorage(str(path))\n        verifyObject(IFeedStorage, st)\n\n    @staticmethod\n    def _store(path: Path, feed_options: dict[str, Any] | None = None) -> None:\n        storage = FileFeedStorage(str(path), feed_options=feed_options)\n        spider = scrapy.Spider(\"default\")\n        file = storage.open(spider)\n        file.write(b\"content\")\n        storage.store(file)\n\n    def test_append(self, tmp_path):\n        path = tmp_path / \"file.txt\"\n        self._store(path)\n        self._assert_stores(FileFeedStorage(str(path)), path, b\"contentcontent\")\n\n    def test_overwrite(self, tmp_path):\n        path = tmp_path / \"file.txt\"\n        self._store(path, {\"overwrite\": True})\n        self._assert_stores(\n            FileFeedStorage(str(path), feed_options={\"overwrite\": True}), path\n        )\n\n    @staticmethod\n    def _assert_stores(\n        storage: FileFeedStorage, path: Path, expected_content: bytes = b\"content\"\n    ) -> None:\n        spider = scrapy.Spider(\"default\")\n        file = storage.open(spider)\n        file.write(b\"content\")\n        storage.store(file)\n        assert path.exists()\n        try:\n            assert path.read_bytes() == expected_content\n        finally:\n            path.unlink()\n\n    def test_preserves_windows_path_without_file_scheme(self):\n        path = r\"C:\\Users\\user\\Desktop\\test.txt\"\n        storage = FileFeedStorage(path)\n        assert storage.path == path\n\n\nclass TestFTPFeedStorage:\n    def get_test_spider(self, settings=None):\n        class TestSpider(scrapy.Spider):\n            name = \"test_spider\"\n\n        crawler = get_crawler(settings_dict=settings)\n        return TestSpider.from_crawler(crawler)\n", "n_tokens": 1178, "byte_len": 5189, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1, "end_line": 174}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 2, "symbols": ["_assert_stored", "test_uri_auth_quote", "_store_in_thread", "get_test_spider", "test_default_temp_dir", "test_temp_file", "test_invalid_folder", "test_parse_credentials", "MyBlockingFeedStorage", "TestBlockingFeedStorage", "TestSpider", "TestS3FeedStorage", "quoted", "async", "settings", "token", "spider", "name", "storage", "file", "deferred", "from", "test", "overwrite", "quote", "error", "oserror", "ftp", "server", "string", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "test_init_without_acl", "test_init_with_acl", "test_init_with_endpoint_url", "test_init_with_region_name", "test_from_crawler_without_acl", "test_without_endpoint_url"], "ast_kind": "class_or_type", "text": "    async def _store(self, uri, content, feed_options=None, settings=None):\n        crawler = get_crawler(settings_dict=settings or {})\n        storage = FTPFeedStorage.from_crawler(\n            crawler,\n            uri,\n            feed_options=feed_options,\n        )\n        verifyObject(IFeedStorage, storage)\n        spider = self.get_test_spider()\n        file = storage.open(spider)\n        file.write(content)\n        await maybe_deferred_to_future(storage.store(file))\n\n    def _assert_stored(self, path: Path, content):\n        assert path.exists()\n        try:\n            assert path.read_bytes() == content\n        finally:\n            path.unlink()\n\n    @deferred_f_from_coro_f\n    async def test_append(self):\n        with MockFTPServer() as ftp_server:\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            feed_options = {\"overwrite\": False}\n            await self._store(url, b\"foo\", feed_options=feed_options)\n            await self._store(url, b\"bar\", feed_options=feed_options)\n            self._assert_stored(ftp_server.path / filename, b\"foobar\")\n\n    @deferred_f_from_coro_f\n    async def test_overwrite(self):\n        with MockFTPServer() as ftp_server:\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            await self._store(url, b\"foo\")\n            await self._store(url, b\"bar\")\n            self._assert_stored(ftp_server.path / filename, b\"bar\")\n\n    @deferred_f_from_coro_f\n    async def test_append_active_mode(self):\n        with MockFTPServer() as ftp_server:\n            settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            feed_options = {\"overwrite\": False}\n            await self._store(url, b\"foo\", feed_options=feed_options, settings=settings)\n            await self._store(url, b\"bar\", feed_options=feed_options, settings=settings)\n            self._assert_stored(ftp_server.path / filename, b\"foobar\")\n\n    @deferred_f_from_coro_f\n    async def test_overwrite_active_mode(self):\n        with MockFTPServer() as ftp_server:\n            settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            await self._store(url, b\"foo\", settings=settings)\n            await self._store(url, b\"bar\", settings=settings)\n            self._assert_stored(ftp_server.path / filename, b\"bar\")\n\n    def test_uri_auth_quote(self):\n        # RFC3986: 3.2.1. User Information\n        pw_quoted = quote(string.punctuation, safe=\"\")\n        st = FTPFeedStorage(f\"ftp://foo:{pw_quoted}@example.com/some_path\", {})\n        assert st.password == string.punctuation\n\n\nclass MyBlockingFeedStorage(BlockingFeedStorage):\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        return\n\n\nclass TestBlockingFeedStorage:\n    def get_test_spider(self, settings=None):\n        class TestSpider(scrapy.Spider):\n            name = \"test_spider\"\n\n        crawler = get_crawler(settings_dict=settings)\n        return TestSpider.from_crawler(crawler)\n\n    def test_default_temp_dir(self):\n        b = MyBlockingFeedStorage()\n\n        storage_file = b.open(self.get_test_spider())\n        storage_dir = Path(storage_file.name).parent\n        assert str(storage_dir) == tempfile.gettempdir()\n\n    def test_temp_file(self, tmp_path):\n        b = MyBlockingFeedStorage()\n\n        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(tmp_path)})\n        storage_file = b.open(spider)\n        storage_dir = Path(storage_file.name).parent\n        assert storage_dir == tmp_path\n\n    def test_invalid_folder(self, tmp_path):\n        b = MyBlockingFeedStorage()\n\n        invalid_path = tmp_path / \"invalid_path\"\n        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(invalid_path)})\n\n        with pytest.raises(OSError, match=\"Not a Directory:\"):\n            b.open(spider=spider)\n\n\n@pytest.mark.requires_boto3\nclass TestS3FeedStorage:\n    def test_parse_credentials(self):\n        aws_credentials = {\n            \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"settings_secret\",\n            \"AWS_SESSION_TOKEN\": \"settings_token\",\n        }\n        crawler = get_crawler(settings_dict=aws_credentials)\n        # Instantiate with crawler\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        assert storage.access_key == \"settings_key\"\n        assert storage.secret_key == \"settings_secret\"\n        assert storage.session_token == \"settings_token\"\n        # Instantiate directly\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n            aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n            session_token=aws_credentials[\"AWS_SESSION_TOKEN\"],\n        )\n        assert storage.access_key == \"settings_key\"\n        assert storage.secret_key == \"settings_secret\"\n        assert storage.session_token == \"settings_token\"\n        # URI priority > settings priority\n        storage = S3FeedStorage(\n            \"s3://uri_key:uri_secret@mybucket/export.csv\",\n            aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n            aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n        )\n        assert storage.access_key == \"uri_key\"\n        assert storage.secret_key == \"uri_secret\"\n", "n_tokens": 1174, "byte_len": 5347, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 175, "end_line": 315}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 3, "symbols": ["test_init_without_acl", "test_init_with_acl", "test_init_with_endpoint_url", "test_init_with_region_name", "test_from_crawler_without_acl", "test_without_endpoint_url", "test_without_region_name", "test_from_crawler_with_acl", "test_from_crawler_with_endpoint_url", "test_from_crawler_with_region_name", "settings", "dict", "async", "await", "test", "store", "magic", "mock", "storage", "fileobj", "fee", "storag", "east", "endpoint", "url", "deferred", "from", "maybe", "without", "call", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "function_or_method", "text": "    @deferred_f_from_coro_f\n    async def test_store(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        storage = S3FeedStorage.from_crawler(crawler, f\"s3://{bucket}/{key}\")\n        verifyObject(IFeedStorage, storage)\n\n        file = mock.MagicMock()\n\n        storage.s3_client = mock.MagicMock()\n        await maybe_deferred_to_future(storage.store(file))\n        assert storage.s3_client.upload_fileobj.call_args == mock.call(\n            Bucket=bucket, Key=key, Fileobj=file\n        )\n\n    def test_init_without_acl(self):\n        storage = S3FeedStorage(\"s3://mybucket/export.csv\", \"access_key\", \"secret_key\")\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.acl is None\n\n    def test_init_with_acl(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.acl == \"custom-acl\"\n\n    def test_init_with_endpoint_url(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            \"access_key\",\n            \"secret_key\",\n            endpoint_url=\"https://example.com\",\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.endpoint_url == \"https://example.com\"\n\n    def test_init_with_region_name(self):\n        region_name = \"ap-east-1\"\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            \"access_key\",\n            \"secret_key\",\n            region_name=region_name,\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.region_name == region_name\n        assert storage.s3_client._client_config.region_name == region_name\n\n    def test_from_crawler_without_acl(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.acl is None\n\n    def test_without_endpoint_url(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.endpoint_url is None\n\n    def test_without_region_name(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.s3_client._client_config.region_name == \"us-east-1\"\n\n    def test_from_crawler_with_acl(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"FEED_STORAGE_S3_ACL\": \"custom-acl\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.acl == \"custom-acl\"\n\n    def test_from_crawler_with_endpoint_url(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"AWS_ENDPOINT_URL\": \"https://example.com\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(crawler, \"s3://mybucket/export.csv\")\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.endpoint_url == \"https://example.com\"\n\n    def test_from_crawler_with_region_name(self):\n        region_name = \"ap-east-1\"\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"AWS_REGION_NAME\": region_name,\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(crawler, \"s3://mybucket/export.csv\")\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.region_name == region_name\n        assert storage.s3_client._client_config.region_name == region_name\n", "n_tokens": 1149, "byte_len": 5285, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 316, "end_line": 456}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 4, "symbols": ["test_overwrite_default", "test_overwrite_false", "test_parse_settings", "test_parse_empty_acl", "test_store", "TestGCSFeedStorage", "TestStdoutFeedStorage", "does", "async", "test", "store", "mock", "stdout", "overwrite", "spider", "deferred", "from", "blob", "parse", "get", "crawler", "pytest", "settings", "google", "none", "return", "value", "call", "args", "extra", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_store_without_acl(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            \"access_key\",\n            \"secret_key\",\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.acl is None\n\n        storage.s3_client = mock.MagicMock()\n        await maybe_deferred_to_future(storage.store(BytesIO(b\"test file\")))\n        acl = (\n            storage.s3_client.upload_fileobj.call_args[1]\n            .get(\"ExtraArgs\", {})\n            .get(\"ACL\")\n        )\n        assert acl is None\n\n    @deferred_f_from_coro_f\n    async def test_store_with_acl(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n        )\n        assert storage.access_key == \"access_key\"\n        assert storage.secret_key == \"secret_key\"\n        assert storage.acl == \"custom-acl\"\n\n        storage.s3_client = mock.MagicMock()\n        await maybe_deferred_to_future(storage.store(BytesIO(b\"test file\")))\n        acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n        assert acl == \"custom-acl\"\n\n    def test_overwrite_default(self):\n        with LogCapture() as log:\n            S3FeedStorage(\n                \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n            )\n        assert \"S3 does not support appending to files\" not in str(log)\n\n    def test_overwrite_false(self):\n        with LogCapture() as log:\n            S3FeedStorage(\n                \"s3://mybucket/export.csv\",\n                \"access_key\",\n                \"secret_key\",\n                \"custom-acl\",\n                feed_options={\"overwrite\": False},\n            )\n        assert \"S3 does not support appending to files\" in str(log)\n\n\nclass TestGCSFeedStorage:\n    def test_parse_settings(self):\n        try:\n            from google.cloud.storage import Client  # noqa: F401,PLC0415\n        except ImportError:\n            pytest.skip(\"GCSFeedStorage requires google-cloud-storage\")\n\n        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"publicRead\"}\n        crawler = get_crawler(settings_dict=settings)\n        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n        assert storage.project_id == \"123\"\n        assert storage.acl == \"publicRead\"\n        assert storage.bucket_name == \"mybucket\"\n        assert storage.blob_name == \"export.csv\"\n\n    def test_parse_empty_acl(self):\n        try:\n            from google.cloud.storage import Client  # noqa: F401,PLC0415\n        except ImportError:\n            pytest.skip(\"GCSFeedStorage requires google-cloud-storage\")\n\n        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"\"}\n        crawler = get_crawler(settings_dict=settings)\n        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n        assert storage.acl is None\n\n        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": None}\n        crawler = get_crawler(settings_dict=settings)\n        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n        assert storage.acl is None\n\n    @deferred_f_from_coro_f\n    async def test_store(self):\n        try:\n            from google.cloud.storage import Client  # noqa: F401,PLC0415\n        except ImportError:\n            pytest.skip(\"GCSFeedStorage requires google-cloud-storage\")\n\n        uri = \"gs://mybucket/export.csv\"\n        project_id = \"myproject-123\"\n        acl = \"publicRead\"\n        (client_mock, bucket_mock, blob_mock) = mock_google_cloud_storage()\n        with mock.patch(\"google.cloud.storage.Client\") as m:\n            m.return_value = client_mock\n\n            f = mock.Mock()\n            storage = GCSFeedStorage(uri, project_id, acl)\n            await maybe_deferred_to_future(storage.store(f))\n\n            f.seek.assert_called_once_with(0)\n            m.assert_called_once_with(project=project_id)\n            client_mock.get_bucket.assert_called_once_with(\"mybucket\")\n            bucket_mock.blob.assert_called_once_with(\"export.csv\")\n            blob_mock.upload_from_file.assert_called_once_with(f, predefined_acl=acl)\n\n    def test_overwrite_default(self):\n        with LogCapture() as log:\n            GCSFeedStorage(\"gs://mybucket/export.csv\", \"myproject-123\", \"custom-acl\")\n        assert \"GCS does not support appending to files\" not in str(log)\n\n    def test_overwrite_false(self):\n        with LogCapture() as log:\n            GCSFeedStorage(\n                \"gs://mybucket/export.csv\",\n                \"myproject-123\",\n                \"custom-acl\",\n                feed_options={\"overwrite\": False},\n            )\n        assert \"GCS does not support appending to files\" in str(log)\n\n\nclass TestStdoutFeedStorage:\n    def test_store(self):\n        out = BytesIO()\n        storage = StdoutFeedStorage(\"stdout:\", _stdout=out)\n        file = storage.open(scrapy.Spider(\"default\"))\n        file.write(b\"content\")\n        storage.store(file)\n        assert out.getvalue() == b\"content\"\n\n    def test_overwrite_default(self):\n        with LogCapture() as log:\n            StdoutFeedStorage(\"stdout:\")\n        assert (\n            \"Standard output (stdout) storage does not support overwriting\"\n            not in str(log)\n        )\n", "n_tokens": 1195, "byte_len": 5343, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 457, "end_line": 597}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 5, "symbols": ["test_overwrite_true", "from_crawler", "__init__", "_store_in_thread", "open", "store", "_random_temp_filename", "setup_class", "teardown_class", "setup_method", "teardown_method", "parse", "FromCrawlerMixin", "FromCrawlerCsvItemExporter", "FromCrawlerFileFeedStorage", "DummyBlockingFeedStorage", "FailingBlockingFeedStorage", "LogOnStoreFileStorage", "TestFeedExportBase", "MyItem", "MyItem2", "TestSpider", "random", "temp", "does", "method", "async", "logs", "yielding", "ignore", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "test_default_temp_dir", "test_temp_file", "test_invalid_folder"], "ast_kind": "class_or_type", "text": "    def test_overwrite_true(self):\n        with LogCapture() as log:\n            StdoutFeedStorage(\"stdout:\", feed_options={\"overwrite\": True})\n        assert \"Standard output (stdout) storage does not support overwriting\" in str(\n            log\n        )\n\n\nclass FromCrawlerMixin:\n    init_with_crawler = False\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n        cls.init_with_crawler = True\n        return cls(*args, **kwargs)\n\n\nclass FromCrawlerCsvItemExporter(CsvItemExporter, FromCrawlerMixin):\n    pass\n\n\nclass FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n    @classmethod\n    def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n        cls.init_with_crawler = True\n        return cls(*args, feed_options=feed_options, **kwargs)\n\n\nclass DummyBlockingFeedStorage(BlockingFeedStorage):\n    def __init__(self, uri, *args, feed_options=None):\n        self.path = Path(file_uri_to_path(uri))\n\n    def _store_in_thread(self, file):\n        dirname = self.path.parent\n        if dirname and not dirname.exists():\n            dirname.mkdir(parents=True)\n        with self.path.open(\"ab\") as output_file:\n            output_file.write(file.read())\n\n        file.close()\n\n\nclass FailingBlockingFeedStorage(DummyBlockingFeedStorage):\n    def _store_in_thread(self, file):\n        raise OSError(\"Cannot store\")\n\n\n@implementer(IFeedStorage)\nclass LogOnStoreFileStorage:\n    \"\"\"\n    This storage logs inside `store` method.\n    It can be used to make sure `store` method is invoked.\n    \"\"\"\n\n    def __init__(self, uri, feed_options=None):\n        self.path = file_uri_to_path(uri)\n        self.logger = getLogger()\n\n    def open(self, spider):\n        return tempfile.NamedTemporaryFile(prefix=\"feed-\")\n\n    def store(self, file):\n        self.logger.info(\"Storage.store is called\")\n        file.close()\n\n\nclass TestFeedExportBase(ABC):\n    mockserver: MockServer\n\n    class MyItem(scrapy.Item):\n        foo = scrapy.Field()\n        egg = scrapy.Field()\n        baz = scrapy.Field()\n\n    class MyItem2(scrapy.Item):\n        foo = scrapy.Field()\n        hello = scrapy.Field()\n\n    def _random_temp_filename(self, inter_dir=\"\") -> Path:\n        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n        filename = \"\".join(chars)\n        return Path(self.temp_dir, inter_dir, filename)\n\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    def setup_method(self):\n        self.temp_dir = tempfile.mkdtemp()\n\n    def teardown_method(self):\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    async def exported_data(\n        self, items: Iterable[Any], settings: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"\n        Return exported data which a spider yielding ``items`` would return.\n        \"\"\"\n\n        class TestSpider(scrapy.Spider):\n            name = \"testspider\"\n\n            def parse(self, response):\n                yield from items\n\n        return await self.run_and_export(TestSpider, settings)\n\n    async def exported_no_data(self, settings: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"\n        Return exported data which a spider yielding no ``items`` would return.\n        \"\"\"\n\n        class TestSpider(scrapy.Spider):\n            name = \"testspider\"\n\n            def parse(self, response):\n                pass\n\n        return await self.run_and_export(TestSpider, settings)\n\n    async def assertExported(\n        self,\n        items: Iterable[Any],\n        header: Iterable[str],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        await self.assertExportedCsv(items, header, rows, settings)\n        await self.assertExportedJsonLines(items, rows, settings)\n        await self.assertExportedXml(items, rows, settings)\n        await self.assertExportedPickle(items, rows, settings)\n        await self.assertExportedMarshal(items, rows, settings)\n        await self.assertExportedMultiple(items, rows, settings)\n\n    async def assertExportedCsv(  # noqa: B027\n        self,\n        items: Iterable[Any],\n        header: Iterable[str],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        pass\n\n    async def assertExportedJsonLines(  # noqa: B027\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        pass\n\n    async def assertExportedXml(  # noqa: B027\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        pass\n\n    async def assertExportedMultiple(  # noqa: B027\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        pass\n\n    async def assertExportedPickle(  # noqa: B027\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        pass\n", "n_tokens": 1202, "byte_len": 5244, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 598, "end_line": 778}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 6, "symbols": ["_load_until_eof", "start_exporting", "finish_exporting", "subscribe__listener", "__init__", "update", "export_item", "InstrumentedFeedSlot", "IsExportingListener", "ExceptionJsonItemExporter", "TestFeedExport", "random", "temp", "method", "async", "loads", "append", "subclass", "made", "exported", "data", "spider", "start", "without", "path", "url", "elif", "eof", "error", "mockserver", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    async def assertExportedMarshal(  # noqa: B027\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        pass\n\n    @abstractmethod\n    async def run_and_export(\n        self, spider_cls: type[Spider], settings: dict[str, Any]\n    ) -> dict[str, Any]:\n        pass\n\n    def _load_until_eof(self, data, load_func):\n        result = []\n        with tempfile.TemporaryFile() as temp:\n            temp.write(data)\n            temp.seek(0)\n            while True:\n                try:\n                    result.append(load_func(temp))\n                except EOFError:\n                    break\n        return result\n\n\nclass InstrumentedFeedSlot(FeedSlot):\n    \"\"\"Instrumented FeedSlot subclass for keeping track of calls to\n    start_exporting and finish_exporting.\"\"\"\n\n    def start_exporting(self):\n        self.update_listener(\"start\")\n        super().start_exporting()\n\n    def finish_exporting(self):\n        self.update_listener(\"finish\")\n        super().finish_exporting()\n\n    @classmethod\n    def subscribe__listener(cls, listener):\n        cls.update_listener = listener.update\n\n\nclass IsExportingListener:\n    \"\"\"When subscribed to InstrumentedFeedSlot, keeps track of when\n    a call to start_exporting has been made without a closing call to\n    finish_exporting and when a call to finish_exporting has been made\n    before a call to start_exporting.\"\"\"\n\n    def __init__(self):\n        self.start_without_finish = False\n        self.finish_without_start = False\n\n    def update(self, method):\n        if method == \"start\":\n            self.start_without_finish = True\n        elif method == \"finish\":\n            if self.start_without_finish:\n                self.start_without_finish = False\n            else:\n                self.finish_before_start = True\n\n\nclass ExceptionJsonItemExporter(JsonItemExporter):\n    \"\"\"JsonItemExporter that throws an exception every time export_item is called.\"\"\"\n\n    def export_item(self, _):\n        raise RuntimeError(\"foo\")\n\n\nclass TestFeedExport(TestFeedExportBase):\n    async def run_and_export(\n        self, spider_cls: type[Spider], settings: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Run spider with specified settings; return exported data.\"\"\"\n\n        FEEDS = settings.get(\"FEEDS\") or {}\n        settings[\"FEEDS\"] = {\n            printf_escape(path_to_url(file_path)): feed_options\n            for file_path, feed_options in FEEDS.items()\n        }\n\n        content: dict[str, Any] = {}\n        try:\n            spider_cls.start_urls = [self.mockserver.url(\"/\")]\n            crawler = get_crawler(spider_cls, settings)\n            await maybe_deferred_to_future(crawler.crawl())\n\n            for file_path, feed_options in FEEDS.items():\n                content[feed_options[\"format\"]] = (\n                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n                )\n\n        finally:\n            for file_path in FEEDS:\n                if not Path(file_path).exists():\n                    continue\n\n                Path(file_path).unlink()\n\n        return content\n\n    async def assertExportedCsv(\n        self,\n        items: Iterable[Any],\n        header: Iterable[str],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"csv\"},\n                },\n            }\n        )\n        data = await self.exported_data(items, settings)\n        reader = csv.DictReader(to_unicode(data[\"csv\"]).splitlines())\n        assert reader.fieldnames == list(header)\n        assert rows == list(reader)\n\n    async def assertExportedJsonLines(\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"jl\"},\n                },\n            }\n        )\n        data = await self.exported_data(items, settings)\n        parsed = [json.loads(to_unicode(line)) for line in data[\"jl\"].splitlines()]\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        assert rows == parsed\n\n    async def assertExportedXml(\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"xml\"},\n                },\n            }\n        )\n        data = await self.exported_data(items, settings)\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        root = lxml.etree.fromstring(data[\"xml\"])\n        got_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n        assert rows == got_rows\n", "n_tokens": 1101, "byte_len": 5114, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 779, "end_line": 940}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 7, "symbols": ["test_stats_file_success", "test_stats_file_failed", "test_stats_multiple_file", "random", "temp", "async", "loads", "spam", "spam2", "exported", "data", "deferred", "from", "path", "url", "load", "bar", "bar2", "mockserver", "mock", "get", "crawler", "settings", "unicode", "items", "object", "item", "none", "feedexport", "json", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "function_or_method", "text": "    async def assertExportedMultiple(\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"xml\"},\n                    self._random_temp_filename(): {\"format\": \"json\"},\n                },\n            }\n        )\n        data = await self.exported_data(items, settings)\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        # XML\n        root = lxml.etree.fromstring(data[\"xml\"])\n        xml_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n        assert rows == xml_rows\n        # JSON\n        json_rows = json.loads(to_unicode(data[\"json\"]))\n        assert rows == json_rows\n\n    async def assertExportedPickle(\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"pickle\"},\n                },\n            }\n        )\n        data = await self.exported_data(items, settings)\n        expected = [{k: v for k, v in row.items() if v} for row in rows]\n\n        result = self._load_until_eof(data[\"pickle\"], load_func=pickle.load)\n        assert result == expected\n\n    async def assertExportedMarshal(\n        self,\n        items: Iterable[Any],\n        rows: Iterable[dict[str, Any]],\n        settings: dict[str, Any] | None = None,\n    ) -> None:\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"marshal\"},\n                },\n            }\n        )\n        data = await self.exported_data(items, settings)\n        expected = [{k: v for k, v in row.items() if v} for row in rows]\n\n        result = self._load_until_eof(data[\"marshal\"], load_func=marshal.load)\n        assert result == expected\n\n    @inlineCallbacks\n    def test_stats_file_success(self):\n        settings = {\n            \"FEEDS\": {\n                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                    \"format\": \"json\",\n                }\n            },\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert \"feedexport/success_count/FileFeedStorage\" in crawler.stats.get_stats()\n        assert crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\") == 1\n\n    @inlineCallbacks\n    def test_stats_file_failed(self):\n        settings = {\n            \"FEEDS\": {\n                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                    \"format\": \"json\",\n                }\n            },\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        with mock.patch(\n            \"scrapy.extensions.feedexport.FileFeedStorage.store\",\n            side_effect=KeyError(\"foo\"),\n        ):\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert \"feedexport/failed_count/FileFeedStorage\" in crawler.stats.get_stats()\n        assert crawler.stats.get_value(\"feedexport/failed_count/FileFeedStorage\") == 1\n\n    @inlineCallbacks\n    def test_stats_multiple_file(self):\n        settings = {\n            \"FEEDS\": {\n                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                    \"format\": \"json\",\n                },\n                \"stdout:\": {\n                    \"format\": \"xml\",\n                },\n            },\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        with mock.patch.object(S3FeedStorage, \"store\"):\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert \"feedexport/success_count/FileFeedStorage\" in crawler.stats.get_stats()\n        assert \"feedexport/success_count/StdoutFeedStorage\" in crawler.stats.get_stats()\n        assert crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\") == 1\n        assert (\n            crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\") == 1\n        )\n\n    @deferred_f_from_coro_f\n    async def test_export_items(self):\n        # feed exporters use field names from Item\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n        ]\n        rows = [\n            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n            {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n        ]\n        header = self.MyItem.fields.keys()\n        await self.assertExported(items, header, rows)\n\n    @deferred_f_from_coro_f\n    async def test_export_no_items_not_store_empty(self):\n        for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_STORE_EMPTY\": False,\n            }\n            data = await self.exported_no_data(settings)\n            assert data[fmt] is None\n", "n_tokens": 1165, "byte_len": 5257, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 941, "end_line": 1085}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 8, "symbols": ["encoding", "random", "temp", "async", "rows", "csv", "test", "export", "exported", "data", "deferred", "from", "start", "without", "world", "world2", "make", "bar", "bar2", "mock", "exporting", "listener", "hello", "settings", "lines", "items", "item", "none", "feedexport", "exception", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "unknown", "text": "    @deferred_f_from_coro_f\n    async def test_start_finish_exporting_items(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n        ]\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            await self.exported_data(items, settings)\n            assert not listener.start_without_finish\n            assert not listener.finish_without_start\n\n    @deferred_f_from_coro_f\n    async def test_start_finish_exporting_no_items(self):\n        items = []\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            await self.exported_data(items, settings)\n            assert not listener.start_without_finish\n            assert not listener.finish_without_start\n\n    @deferred_f_from_coro_f\n    async def test_start_finish_exporting_items_exception(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n        ]\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORTERS\": {\"json\": ExceptionJsonItemExporter},\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            await self.exported_data(items, settings)\n            assert not listener.start_without_finish\n            assert not listener.finish_without_start\n\n    @deferred_f_from_coro_f\n    async def test_start_finish_exporting_no_items_exception(self):\n        items = []\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORTERS\": {\"json\": ExceptionJsonItemExporter},\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            await self.exported_data(items, settings)\n            assert not listener.start_without_finish\n            assert not listener.finish_without_start\n\n    @deferred_f_from_coro_f\n    async def test_export_no_items_store_empty(self):\n        formats = (\n            (\"json\", b\"[]\"),\n            (\"jsonlines\", b\"\"),\n            (\"xml\", b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n            (\"csv\", b\"\"),\n        )\n\n        for fmt, expctd in formats:\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_EXPORT_INDENT\": None,\n            }\n            data = await self.exported_no_data(settings)\n            assert expctd == data[fmt]\n\n    @deferred_f_from_coro_f\n    async def test_export_no_items_multiple_feeds(self):\n        \"\"\"Make sure that `storage.store` is called for every feed.\"\"\"\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n                self._random_temp_filename(): {\"format\": \"xml\"},\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n            \"FEED_STORAGES\": {\"file\": LogOnStoreFileStorage},\n            \"FEED_STORE_EMPTY\": False,\n        }\n\n        with LogCapture() as log:\n            await self.exported_no_data(settings)\n\n        assert str(log).count(\"Storage.store is called\") == 0\n\n    @deferred_f_from_coro_f\n    async def test_export_multiple_item_classes(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"egg\": \"spam3\", \"baz\": \"quux3\"}),\n            {\"hello\": \"world4\", \"egg\": \"spam4\"},\n        ]\n\n        # by default, Scrapy uses fields of the first Item for CSV and\n        # all fields for JSON Lines\n        header = self.MyItem.fields.keys()\n        rows_csv = [\n            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n            {\"egg\": \"\", \"foo\": \"bar2\", \"baz\": \"\"},\n            {\"egg\": \"spam3\", \"foo\": \"bar3\", \"baz\": \"quux3\"},\n            {\"egg\": \"spam4\", \"foo\": \"\", \"baz\": \"\"},\n        ]\n        rows_jl = [dict(row) for row in items]\n        await self.assertExportedCsv(items, header, rows_csv)\n        await self.assertExportedJsonLines(items, rows_jl)\n", "n_tokens": 1141, "byte_len": 5053, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1086, "end_line": 1223}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#9", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 9, "symbols": ["__init__", "accepts", "CustomFilter1", "CustomFilter2", "CustomFilter3", "encoding", "random", "temp", "dumps", "item", "myitem2", "async", "expected", "feed", "options", "myitem", "await", "pass", "test", "export", "filter", "feeds", "custom", "return", "exported", "data", "assert", "deferred", "from", "class", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_export_items_empty_field_list(self):\n        # FEED_EXPORT_FIELDS==[] means the same as default None\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\"]\n        rows = [{\"foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": []}\n        await self.assertExportedCsv(items, header, rows)\n        await self.assertExportedJsonLines(items, rows, settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_items_field_list(self):\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\", \"baz\"]\n        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": header}\n        await self.assertExported(items, header, rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_items_comma_separated_field_list(self):\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\", \"baz\"]\n        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": \",\".join(header)}\n        await self.assertExported(items, header, rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_items_json_field_list(self):\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\", \"baz\"]\n        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n        await self.assertExported(items, header, rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_items_field_names(self):\n        items = [{\"foo\": \"bar\"}]\n        header = {\"foo\": \"Foo\"}\n        rows = [{\"Foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": header}\n        await self.assertExported(items, list(header.values()), rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_items_dict_field_names(self):\n        items = [{\"foo\": \"bar\"}]\n        header = {\n            \"baz\": \"Baz\",\n            \"foo\": \"Foo\",\n        }\n        rows = [{\"Baz\": \"\", \"Foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": header}\n        await self.assertExported(items, [\"Baz\", \"Foo\"], rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_items_json_field_names(self):\n        items = [{\"foo\": \"bar\"}]\n        header = {\"foo\": \"Foo\"}\n        rows = [{\"Foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n        await self.assertExported(items, list(header.values()), rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_based_on_item_classes(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n            {\"hello\": \"world3\", \"egg\": \"spam3\"},\n        ]\n\n        formats = {\n            \"csv\": b\"baz,egg,foo\\r\\n,spam1,bar1\\r\\n\",\n            \"json\": b'[\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n]',\n            \"jsonlines\": (\n                b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n'\n            ),\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n                b\"<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>\"\n                b\"world2</hello><foo>bar2</foo></item>\\n<item><hello>world3\"\n                b\"</hello><egg>spam3</egg></item>\\n</items>\"\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"csv\",\n                    \"item_classes\": [self.MyItem],\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"json\",\n                    \"item_classes\": [self.MyItem2],\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"jsonlines\",\n                    \"item_classes\": [self.MyItem, self.MyItem2],\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"xml\",\n                },\n            },\n        }\n\n        data = await self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            assert data[fmt] == expected\n\n    @deferred_f_from_coro_f\n    async def test_export_based_on_custom_filters(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n            {\"hello\": \"world3\", \"egg\": \"spam3\"},\n        ]\n\n        MyItem = self.MyItem\n\n        class CustomFilter1:\n            def __init__(self, feed_options):\n                pass\n\n            def accepts(self, item):\n                return isinstance(item, MyItem)\n\n        class CustomFilter2(scrapy.extensions.feedexport.ItemFilter):\n            def accepts(self, item):\n                return \"foo\" in item.fields\n\n        class CustomFilter3(scrapy.extensions.feedexport.ItemFilter):", "n_tokens": 1194, "byte_len": 4815, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1224, "end_line": 1352}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#10", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 10, "symbols": ["accepts", "encoding", "used", "random", "temp", "item", "myitem2", "async", "expected", "myitem", "await", "rows", "csv", "select", "test", "export", "spam", "spam2", "items", "feeds", "custom", "filter", "return", "exported", "data", "dict", "assert", "deferred", "from", "columns", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "function_or_method", "text": "            def accepts(self, item):\n                return (\n                    isinstance(item, tuple(self.item_classes)) and item[\"foo\"] == \"bar1\"\n                )\n\n        formats = {\n            \"json\": b'[\\n{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n]',\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n                b\"<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>\"\n                b\"world2</hello><foo>bar2</foo></item>\\n</items>\"\n            ),\n            \"jsonlines\": b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n',\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"json\",\n                    \"item_filter\": CustomFilter1,\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"xml\",\n                    \"item_filter\": CustomFilter2,\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"jsonlines\",\n                    \"item_classes\": [self.MyItem, self.MyItem2],\n                    \"item_filter\": CustomFilter3,\n                },\n            },\n        }\n\n        data = await self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            assert data[fmt] == expected\n\n    @deferred_f_from_coro_f\n    async def test_export_dicts(self):\n        # When dicts are used, only keys from the first row are used as\n        # a header for CSV, and all fields are used for JSON Lines.\n        items = [\n            {\"foo\": \"bar\", \"egg\": \"spam\"},\n            {\"foo\": \"bar\", \"egg\": \"spam\", \"baz\": \"quux\"},\n        ]\n        rows_csv = [{\"egg\": \"spam\", \"foo\": \"bar\"}, {\"egg\": \"spam\", \"foo\": \"bar\"}]\n        rows_jl = items\n        await self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n        await self.assertExportedJsonLines(items, rows_jl)\n\n    @deferred_f_from_coro_f\n    async def test_export_tuple(self):\n        items = [\n            {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n            {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux\"},\n        ]\n\n        settings = {\"FEED_EXPORT_FIELDS\": (\"foo\", \"baz\")}\n        rows = [{\"foo\": \"bar1\", \"baz\": \"\"}, {\"foo\": \"bar2\", \"baz\": \"quux\"}]\n        await self.assertExported(items, [\"foo\", \"baz\"], rows, settings=settings)\n\n    @deferred_f_from_coro_f\n    async def test_export_feed_export_fields(self):\n        # FEED_EXPORT_FIELDS option allows to order export fields\n        # and to select a subset of fields to export, both for Items and dicts.\n\n        for item_cls in [self.MyItem, dict]:\n            items = [\n                item_cls({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n                item_cls({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            ]\n\n            # export all columns\n            settings = {\"FEED_EXPORT_FIELDS\": \"foo,baz,egg\"}\n            rows = [\n                {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n                {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n            ]\n            await self.assertExported(\n                items, [\"foo\", \"baz\", \"egg\"], rows, settings=settings\n            )\n\n            # export a subset of columns\n            settings = {\"FEED_EXPORT_FIELDS\": \"egg,baz\"}\n            rows = [{\"egg\": \"spam1\", \"baz\": \"\"}, {\"egg\": \"spam2\", \"baz\": \"quux2\"}]\n            await self.assertExported(items, [\"egg\", \"baz\"], rows, settings=settings)\n", "n_tokens": 867, "byte_len": 3403, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1353, "end_line": 1439}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#11", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 11, "symbols": ["encoding", "test", "random", "temp", "fee", "expor", "async", "expected", "await", "feeds", "exported", "data", "export", "item", "deferred", "from", "json", "fields", "u00d6", "jsonlines", "version", "settings", "ntest", "format", "assert", "items", "indent", "latin", "none", "encode", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "unknown", "text": "    @deferred_f_from_coro_f\n    async def test_export_encoding(self):\n        items = [{\"foo\": \"Test\\xd6\"}]\n\n        formats = {\n            \"json\": b'[{\"foo\": \"Test\\\\u00d6\"}]',\n            \"jsonlines\": b'{\"foo\": \"Test\\\\u00d6\"}\\n',\n            \"xml\": (\n                '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n                \"<items><item><foo>Test\\xd6</foo></item></items>\"\n            ).encode(),\n            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(),\n        }\n\n        for fmt, expected in formats.items():\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_EXPORT_INDENT\": None,\n            }\n            data = await self.exported_data(items, settings)\n            assert data[fmt] == expected\n\n        formats = {\n            \"json\": b'[{\"foo\": \"Test\\xd6\"}]',\n            \"jsonlines\": b'{\"foo\": \"Test\\xd6\"}\\n',\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                b\"<items><item><foo>Test\\xd6</foo></item></items>\"\n            ),\n            \"csv\": b\"foo\\r\\nTest\\xd6\\r\\n\",\n        }\n\n        for fmt, expected in formats.items():\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_EXPORT_INDENT\": None,\n                \"FEED_EXPORT_ENCODING\": \"latin-1\",\n            }\n            data = await self.exported_data(items, settings)\n            assert data[fmt] == expected\n\n    @deferred_f_from_coro_f\n    async def test_export_multiple_configs(self):\n        items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n\n        formats = {\n            \"json\": b'[\\n{\"bar\": \"BAR\"}\\n]',\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                b\"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n            ),\n            \"csv\": b\"bar,foo\\r\\nBAR,FOO\\r\\n\",\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"json\",\n                    \"indent\": 0,\n                    \"fields\": [\"bar\"],\n                    \"encoding\": \"utf-8\",\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"xml\",\n                    \"indent\": 2,\n                    \"fields\": [\"foo\"],\n                    \"encoding\": \"latin-1\",\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"csv\",\n                    \"indent\": None,\n                    \"fields\": [\"bar\", \"foo\"],\n                    \"encoding\": \"utf-8\",\n                },\n            },\n        }\n\n        data = await self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            assert data[fmt] == expected\n", "n_tokens": 685, "byte_len": 2843, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1440, "end_line": 1524}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#12", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 12, "symbols": ["encoding", "random", "temp", "async", "expected", "test", "export", "init", "with", "await", "fee", "stor", "feeds", "exported", "data", "item", "deferred", "from", "json", "value", "storages", "true", "version", "settings", "cases", "assert", "format", "items", "indent", "file", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "unknown", "text": "    @deferred_f_from_coro_f\n    async def test_export_indentation(self):\n        items = [\n            {\"foo\": [\"bar\"]},\n            {\"key\": \"value\"},\n        ]\n\n        test_cases = [\n            # JSON\n            {\n                \"format\": \"json\",\n                \"indent\": None,\n                \"expected\": b'[{\"foo\": [\"bar\"]},{\"key\": \"value\"}]',\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": -1,\n                \"expected\": b\"\"\"[\n{\"foo\": [\"bar\"]},\n{\"key\": \"value\"}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 0,\n                \"expected\": b\"\"\"[\n{\"foo\": [\"bar\"]},\n{\"key\": \"value\"}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 2,\n                \"expected\": b\"\"\"[\n{\n  \"foo\": [\n    \"bar\"\n  ]\n},\n{\n  \"key\": \"value\"\n}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 4,\n                \"expected\": b\"\"\"[\n{\n    \"foo\": [\n        \"bar\"\n    ]\n},\n{\n    \"key\": \"value\"\n}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 5,\n                \"expected\": b\"\"\"[\n{\n     \"foo\": [\n          \"bar\"\n     ]\n},\n{\n     \"key\": \"value\"\n}\n]\"\"\",\n            },\n            # XML\n            {\n                \"format\": \"xml\",\n                \"indent\": None,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": -1,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n<item><foo><value>bar</value></foo></item>\n<item><key>value</key></item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 0,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n<item><foo><value>bar</value></foo></item>\n<item><key>value</key></item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 2,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <foo>\n      <value>bar</value>\n    </foo>\n  </item>\n  <item>\n    <key>value</key>\n  </item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 4,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n    <item>\n        <foo>\n            <value>bar</value>\n        </foo>\n    </item>\n    <item>\n        <key>value</key>\n    </item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 5,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n     <item>\n          <foo>\n               <value>bar</value>\n          </foo>\n     </item>\n     <item>\n          <key>value</key>\n     </item>\n</items>\"\"\",\n            },\n        ]\n\n        for row in test_cases:\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\n                        \"format\": row[\"format\"],\n                        \"indent\": row[\"indent\"],\n                    },\n                },\n            }\n            data = await self.exported_data(items, settings)\n            assert data[row[\"format\"]] == row[\"expected\"]\n\n    @deferred_f_from_coro_f\n    async def test_init_exporters_storages_with_crawler(self):\n        settings = {\n            \"FEED_EXPORTERS\": {\"csv\": FromCrawlerCsvItemExporter},\n            \"FEED_STORAGES\": {\"file\": FromCrawlerFileFeedStorage},\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n        }\n        await self.exported_data(items=[], settings=settings)\n        assert FromCrawlerCsvItemExporter.init_with_crawler\n        assert FromCrawlerFileFeedStorage.init_with_crawler\n\n    @deferred_f_from_coro_f\n    async def test_str_uri(self):\n        settings = {\n            \"FEED_STORE_EMPTY\": True,\n            \"FEEDS\": {str(self._random_temp_filename()): {\"format\": \"csv\"}},\n        }\n        data = await self.exported_no_data(settings)\n        assert data[\"csv\"] == b\"\"\n", "n_tokens": 1052, "byte_len": 4249, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1525, "end_line": 1702}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#13", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 13, "symbols": ["__init__", "open", "store", "write", "close", "_named_tempfile", "Storage", "TestFeedPostProcessedExports", "MyPlugin1", "random", "temp", "async", "item", "export", "named", "tempfile", "exported", "data", "name", "deferred", "from", "error", "bar", "bar2", "gzip", "plugin", "path", "test", "extend", "settings", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_multiple_feeds_success_logs_blocking_feed_storage(self):\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n                self._random_temp_filename(): {\"format\": \"xml\"},\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n            \"FEED_STORAGES\": {\"file\": DummyBlockingFeedStorage},\n        }\n        items = [\n            {\"foo\": \"bar1\", \"baz\": \"\"},\n            {\"foo\": \"bar2\", \"baz\": \"quux\"},\n        ]\n        with LogCapture() as log:\n            await self.exported_data(items, settings)\n\n        print(log)\n        for fmt in [\"json\", \"xml\", \"csv\"]:\n            assert f\"Stored {fmt} feed (2 items)\" in str(log)\n\n    @deferred_f_from_coro_f\n    async def test_multiple_feeds_failing_logs_blocking_feed_storage(self):\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n                self._random_temp_filename(): {\"format\": \"xml\"},\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n            \"FEED_STORAGES\": {\"file\": FailingBlockingFeedStorage},\n        }\n        items = [\n            {\"foo\": \"bar1\", \"baz\": \"\"},\n            {\"foo\": \"bar2\", \"baz\": \"quux\"},\n        ]\n        with LogCapture() as log:\n            await self.exported_data(items, settings)\n\n        print(log)\n        for fmt in [\"json\", \"xml\", \"csv\"]:\n            assert f\"Error storing {fmt} feed (2 items)\" in str(log)\n\n    @deferred_f_from_coro_f\n    async def test_extend_kwargs(self):\n        items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n\n        expected_with_title_csv = b\"foo,bar\\r\\nFOO,BAR\\r\\n\"\n        expected_without_title_csv = b\"FOO,BAR\\r\\n\"\n        test_cases = [\n            # with title\n            {\n                \"options\": {\n                    \"format\": \"csv\",\n                    \"item_export_kwargs\": {\"include_headers_line\": True},\n                },\n                \"expected\": expected_with_title_csv,\n            },\n            # without title\n            {\n                \"options\": {\n                    \"format\": \"csv\",\n                    \"item_export_kwargs\": {\"include_headers_line\": False},\n                },\n                \"expected\": expected_without_title_csv,\n            },\n        ]\n\n        for row in test_cases:\n            feed_options = row[\"options\"]\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): feed_options,\n                },\n                \"FEED_EXPORT_INDENT\": None,\n            }\n\n            data = await self.exported_data(items, settings)\n            assert data[feed_options[\"format\"]] == row[\"expected\"]\n\n    @deferred_f_from_coro_f\n    async def test_storage_file_no_postprocessing(self):\n        @implementer(IFeedStorage)\n        class Storage:\n            def __init__(self, uri, *, feed_options=None):\n                pass\n\n            def open(self, spider):\n                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n                return Storage.open_file\n\n            def store(self, file):\n                Storage.store_file = file\n                file.close()\n\n        settings = {\n            \"FEEDS\": {self._random_temp_filename(): {\"format\": \"jsonlines\"}},\n            \"FEED_STORAGES\": {\"file\": Storage},\n        }\n        await self.exported_no_data(settings)\n        assert Storage.open_file is Storage.store_file\n\n    @deferred_f_from_coro_f\n    async def test_storage_file_postprocessing(self):\n        @implementer(IFeedStorage)\n        class Storage:\n            def __init__(self, uri, *, feed_options=None):\n                pass\n\n            def open(self, spider):\n                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n                return Storage.open_file\n\n            def store(self, file):\n                Storage.store_file = file\n                Storage.file_was_closed = file.closed\n                file.close()\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"jsonlines\",\n                    \"postprocessing\": [\n                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n                    ],\n                },\n            },\n            \"FEED_STORAGES\": {\"file\": Storage},\n        }\n        await self.exported_no_data(settings)\n        assert Storage.open_file is Storage.store_file\n        assert not Storage.file_was_closed\n\n\nclass TestFeedPostProcessedExports(TestFeedExportBase):\n    items = [{\"foo\": \"bar\"}]\n    expected = b\"foo\\r\\nbar\\r\\n\"\n\n    class MyPlugin1:\n        def __init__(self, file, feed_options):\n            self.file = file\n            self.feed_options = feed_options\n            self.char = self.feed_options.get(\"plugin1_char\", b\"\")\n\n        def write(self, data):\n            written_count = self.file.write(data)\n            written_count += self.file.write(self.char)\n            return written_count\n\n        def close(self):\n            self.file.close()\n\n    def _named_tempfile(self, name) -> str:\n        return str(Path(self.temp_dir, name))\n", "n_tokens": 1097, "byte_len": 5168, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1703, "end_line": 1856}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#14", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 14, "symbols": ["get_gzip_compressed", "gzip", "compresslevel", "async", "named", "tempfile", "decompress", "exported", "data", "spider", "deferred", "from", "path", "url", "error", "oserror", "plugin", "mockserver", "cls", "get", "crawler", "file", "pytest", "settings", "items", "none", "type", "mtime", "fail", "expected", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "function_or_method", "text": "    async def run_and_export(\n        self, spider_cls: type[Spider], settings: dict[str, Any]\n    ) -> dict[str, bytes | None]:\n        \"\"\"Run spider with specified settings; return exported data with filename.\"\"\"\n\n        FEEDS = settings.get(\"FEEDS\") or {}\n        settings[\"FEEDS\"] = {\n            printf_escape(path_to_url(file_path)): feed_options\n            for file_path, feed_options in FEEDS.items()\n        }\n\n        content: dict[str, bytes | None] = {}\n        try:\n            spider_cls.start_urls = [self.mockserver.url(\"/\")]\n            crawler = get_crawler(spider_cls, settings)\n            await maybe_deferred_to_future(crawler.crawl())\n\n            for file_path in FEEDS:\n                content[str(file_path)] = (\n                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n                )\n\n        finally:\n            for file_path in FEEDS:\n                if not Path(file_path).exists():\n                    continue\n\n                Path(file_path).unlink()\n\n        return content\n\n    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=\"\"):\n        data_stream = BytesIO()\n        gzipf = gzip.GzipFile(\n            fileobj=data_stream,\n            filename=filename,\n            mtime=mtime,\n            compresslevel=compresslevel,\n            mode=\"wb\",\n        )\n        gzipf.write(data)\n        gzipf.close()\n        data_stream.seek(0)\n        return data_stream.read()\n\n    @deferred_f_from_coro_f\n    async def test_gzip_plugin(self):\n        filename = self._named_tempfile(\"gzip_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n        try:\n            gzip.decompress(data[filename])\n        except OSError:\n            pytest.fail(\"Received invalid gzip data.\")\n\n    @deferred_f_from_coro_f\n    async def test_gzip_plugin_compresslevel(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"compresslevel_0\"): self.get_gzip_compressed(\n                self.expected, compresslevel=0\n            ),\n            self._named_tempfile(\"compresslevel_9\"): self.get_gzip_compressed(\n                self.expected, compresslevel=9\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"compresslevel_0\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_compresslevel\": 0,\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"\",\n                },\n                self._named_tempfile(\"compresslevel_9\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_compresslevel\": 9,\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"\",\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = gzip.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_gzip_plugin_mtime(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"mtime_123\"): self.get_gzip_compressed(\n                self.expected, mtime=123\n            ),\n            self._named_tempfile(\"mtime_123456789\"): self.get_gzip_compressed(\n                self.expected, mtime=123456789\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"mtime_123\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 123,\n                    \"gzip_filename\": \"\",\n                },\n                self._named_tempfile(\"mtime_123456789\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 123456789,\n                    \"gzip_filename\": \"\",\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = gzip.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n", "n_tokens": 964, "byte_len": 4723, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1857, "end_line": 1992}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#15", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 15, "symbols": ["test", "gzip", "async", "expected", "lzma", "format", "check", "chec", "await", "result", "except", "preset", "named", "tempfile", "forma", "decompress", "alone", "prese", "feeds", "compress", "exported", "data", "received", "deferred", "from", "scrapy", "invalid", "get", "error", "plugin", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "unknown", "text": "    @deferred_f_from_coro_f\n    async def test_gzip_plugin_filename(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"filename_FILE1\"): self.get_gzip_compressed(\n                self.expected, filename=\"FILE1\"\n            ),\n            self._named_tempfile(\"filename_FILE2\"): self.get_gzip_compressed(\n                self.expected, filename=\"FILE2\"\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"filename_FILE1\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"FILE1\",\n                },\n                self._named_tempfile(\"filename_FILE2\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"FILE2\",\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = gzip.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_lzma_plugin(self):\n        filename = self._named_tempfile(\"lzma_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n        try:\n            lzma.decompress(data[filename])\n        except lzma.LZMAError:\n            pytest.fail(\"Received invalid lzma data.\")\n\n    @deferred_f_from_coro_f\n    async def test_lzma_plugin_format(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"format_FORMAT_XZ\"): lzma.compress(\n                self.expected, format=lzma.FORMAT_XZ\n            ),\n            self._named_tempfile(\"format_FORMAT_ALONE\"): lzma.compress(\n                self.expected, format=lzma.FORMAT_ALONE\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"format_FORMAT_XZ\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_format\": lzma.FORMAT_XZ,\n                },\n                self._named_tempfile(\"format_FORMAT_ALONE\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_format\": lzma.FORMAT_ALONE,\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = lzma.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_lzma_plugin_check(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"check_CHECK_NONE\"): lzma.compress(\n                self.expected, check=lzma.CHECK_NONE\n            ),\n            self._named_tempfile(\"check_CHECK_CRC256\"): lzma.compress(\n                self.expected, check=lzma.CHECK_SHA256\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"check_CHECK_NONE\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_check\": lzma.CHECK_NONE,\n                },\n                self._named_tempfile(\"check_CHECK_CRC256\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_check\": lzma.CHECK_SHA256,\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = lzma.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_lzma_plugin_preset(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"preset_PRESET_0\"): lzma.compress(\n                self.expected, preset=0\n            ),\n            self._named_tempfile(\"preset_PRESET_9\"): lzma.compress(\n                self.expected, preset=9\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"preset_PRESET_0\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_preset\": 0,\n                },\n                self._named_tempfile(\"preset_PRESET_9\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_preset\": 9,\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = lzma.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n", "n_tokens": 1144, "byte_len": 5587, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 1993, "end_line": 2145}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#16", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 16, "symbols": ["test", "custom", "async", "expected", "lzma", "gzip", "await", "foss", "result", "except", "plugin", "bz2plugin", "named", "tempfile", "decompress", "feeds", "compress", "exported", "data", "received", "deferred", "from", "doesn", "filters", "work", "char", "pypy", "scrapy", "error", "oserror", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "unknown", "text": "    @deferred_f_from_coro_f\n    async def test_lzma_plugin_filters(self):\n        if \"PyPy\" in sys.version:\n            # https://foss.heptapod.net/pypy/pypy/-/issues/3527\n            pytest.skip(\"lzma filters doesn't work in PyPy\")\n\n        filters = [{\"id\": lzma.FILTER_LZMA2}]\n        compressed = lzma.compress(self.expected, filters=filters)\n        filename = self._named_tempfile(\"filters\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_filters\": filters,\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n        assert compressed == data[filename]\n        result = lzma.decompress(data[filename])\n        assert result == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_bz2_plugin(self):\n        filename = self._named_tempfile(\"bz2_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n        try:\n            bz2.decompress(data[filename])\n        except OSError:\n            pytest.fail(\"Received invalid bz2 data.\")\n\n    @deferred_f_from_coro_f\n    async def test_bz2_plugin_compresslevel(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"compresslevel_1\"): bz2.compress(\n                self.expected, compresslevel=1\n            ),\n            self._named_tempfile(\"compresslevel_9\"): bz2.compress(\n                self.expected, compresslevel=9\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"compresslevel_1\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                    \"bz2_compresslevel\": 1,\n                },\n                self._named_tempfile(\"compresslevel_9\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                    \"bz2_compresslevel\": 9,\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = bz2.decompress(data[filename])\n            assert compressed == data[filename]\n            assert result == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_custom_plugin(self):\n        filename = self._named_tempfile(\"csv_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n        assert data[filename] == self.expected\n\n    @deferred_f_from_coro_f\n    async def test_custom_plugin_with_parameter(self):\n        expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n        filename = self._named_tempfile(\"newline\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [self.MyPlugin1],\n                    \"plugin1_char\": b\"\\n\",\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n        assert data[filename] == expected\n\n    @deferred_f_from_coro_f\n    async def test_custom_plugin_with_compression(self):\n        expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n\n        filename_to_decompressor = {\n            self._named_tempfile(\"bz2\"): bz2.decompress,\n            self._named_tempfile(\"lzma\"): lzma.decompress,\n            self._named_tempfile(\"gzip\"): gzip.decompress,\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"bz2\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\n                        self.MyPlugin1,\n                        \"scrapy.extensions.postprocessing.Bz2Plugin\",\n                    ],\n                    \"plugin1_char\": b\"\\n\",\n                },\n                self._named_tempfile(\"lzma\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\n                        self.MyPlugin1,\n                        \"scrapy.extensions.postprocessing.LZMAPlugin\",\n                    ],\n                    \"plugin1_char\": b\"\\n\",\n                },\n                self._named_tempfile(\"gzip\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\n                        self.MyPlugin1,\n                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n                    ],\n                    \"plugin1_char\": b\"\\n\",\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, decompressor in filename_to_decompressor.items():\n            result = decompressor(data[filename])\n            assert result == expected\n", "n_tokens": 1079, "byte_len": 5232, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 2146, "end_line": 2301}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#17", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 17, "symbols": ["TestBatchDeliveries", "encoding", "expected", "batch", "random", "temp", "async", "loads", "named", "tempfile", "append", "exported", "data", "spider", "deferred", "from", "iterdir", "got", "dir", "name", "elif", "mockserver", "path", "cls", "get", "crawler", "settings", "unicode", "items", "none", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_exports_compatibility_with_postproc(self):\n        filename_to_expected = {\n            self._named_tempfile(\"csv\"): b\"foo\\r\\nbar\\r\\n\",\n            self._named_tempfile(\"json\"): b'[\\n{\"foo\": \"bar\"}\\n]',\n            self._named_tempfile(\"jsonlines\"): b'{\"foo\": \"bar\"}\\n',\n            self._named_tempfile(\"xml\"): b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n            b\"<items>\\n<item><foo>bar</foo></item>\\n</items>\",\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"csv\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [self.MyPlugin1],\n                    # empty plugin to activate postprocessing.PostProcessingManager\n                },\n                self._named_tempfile(\"json\"): {\n                    \"format\": \"json\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"jsonlines\"): {\n                    \"format\": \"jsonlines\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"xml\"): {\n                    \"format\": \"xml\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"marshal\"): {\n                    \"format\": \"marshal\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"pickle\"): {\n                    \"format\": \"pickle\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n            },\n        }\n\n        data = await self.exported_data(self.items, settings)\n\n        for filename, result in data.items():\n            if \"pickle\" in filename:\n                expected, result = self.items[0], pickle.loads(result)\n            elif \"marshal\" in filename:\n                expected, result = self.items[0], marshal.loads(result)\n            else:\n                expected = filename_to_expected[filename]\n            assert result == expected\n\n\nclass TestBatchDeliveries(TestFeedExportBase):\n    _file_mark = \"_%(batch_time)s_#%(batch_id)02d_\"\n\n    async def run_and_export(\n        self, spider_cls: type[Spider], settings: dict[str, Any]\n    ) -> dict[str, list[bytes]]:\n        \"\"\"Run spider with specified settings; return exported data.\"\"\"\n\n        FEEDS = settings.get(\"FEEDS\") or {}\n        settings[\"FEEDS\"] = {\n            build_url(file_path): feed for file_path, feed in FEEDS.items()\n        }\n        content: defaultdict[str, list[bytes]] = defaultdict(list)\n        spider_cls.start_urls = [self.mockserver.url(\"/\")]\n        crawler = get_crawler(spider_cls, settings)\n        await maybe_deferred_to_future(crawler.crawl())\n\n        for path, feed in FEEDS.items():\n            dir_name = Path(path).parent\n            if not dir_name.exists():\n                content[feed[\"format\"]] = []\n                continue\n            for file in sorted(dir_name.iterdir()):\n                content[feed[\"format\"]].append(file.read_bytes())\n        return content\n\n    async def assertExportedJsonLines(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename() / \"jl\" / self._file_mark: {\n                        \"format\": \"jl\"\n                    },\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = await self.exported_data(items, settings)\n        for batch in data[\"jl\"]:\n            got_batch = [\n                json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()\n            ]\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            assert got_batch == expected_batch\n\n    async def assertExportedCsv(self, items, header, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename() / \"csv\" / self._file_mark: {\n                        \"format\": \"csv\"\n                    },\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        data = await self.exported_data(items, settings)\n        for batch in data[\"csv\"]:\n            got_batch = csv.DictReader(to_unicode(batch).splitlines())\n            assert list(header) == got_batch.fieldnames\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            assert list(got_batch) == expected_batch\n\n    async def assertExportedXml(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename() / \"xml\" / self._file_mark: {\n                        \"format\": \"xml\"\n                    },\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = await self.exported_data(items, settings)\n        for batch in data[\"xml\"]:\n            root = lxml.etree.fromstring(batch)\n            got_batch = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            assert got_batch == expected_batch\n", "n_tokens": 1192, "byte_len": 5551, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 2302, "end_line": 2439}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#18", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 18, "symbols": ["test_wrong_path", "expected", "batch", "random", "temp", "async", "loads", "spam", "spam2", "exported", "data", "deferred", "from", "got", "load", "bar", "bar2", "get", "crawler", "pytest", "settings", "items", "none", "json", "without", "feed", "exporter", "assert", "test", "export", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "function_or_method", "text": "    async def assertExportedMultiple(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename() / \"xml\" / self._file_mark: {\n                        \"format\": \"xml\"\n                    },\n                    self._random_temp_filename() / \"json\" / self._file_mark: {\n                        \"format\": \"json\"\n                    },\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = await self.exported_data(items, settings)\n        # XML\n        xml_rows = rows.copy()\n        for batch in data[\"xml\"]:\n            root = lxml.etree.fromstring(batch)\n            got_batch = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n            expected_batch, xml_rows = xml_rows[:batch_size], xml_rows[batch_size:]\n            assert got_batch == expected_batch\n        # JSON\n        json_rows = rows.copy()\n        for batch in data[\"json\"]:\n            got_batch = json.loads(batch.decode(\"utf-8\"))\n            expected_batch, json_rows = json_rows[:batch_size], json_rows[batch_size:]\n            assert got_batch == expected_batch\n\n    async def assertExportedPickle(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename() / \"pickle\" / self._file_mark: {\n                        \"format\": \"pickle\"\n                    },\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = await self.exported_data(items, settings)\n\n        for batch in data[\"pickle\"]:\n            got_batch = self._load_until_eof(batch, load_func=pickle.load)\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            assert got_batch == expected_batch\n\n    async def assertExportedMarshal(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename() / \"marshal\" / self._file_mark: {\n                        \"format\": \"marshal\"\n                    },\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = await self.exported_data(items, settings)\n\n        for batch in data[\"marshal\"]:\n            got_batch = self._load_until_eof(batch, load_func=marshal.load)\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            assert got_batch == expected_batch\n\n    @deferred_f_from_coro_f\n    async def test_export_items(self):\n        \"\"\"Test partial deliveries in all supported formats\"\"\"\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n        ]\n        rows = [\n            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n            {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n            {\"foo\": \"bar3\", \"baz\": \"quux3\", \"egg\": \"\"},\n        ]\n        settings = {\"FEED_EXPORT_BATCH_ITEM_COUNT\": 2}\n        header = self.MyItem.fields.keys()\n        await self.assertExported(items, header, rows, settings=settings)\n\n    def test_wrong_path(self):\n        \"\"\"If path is without %(batch_time)s and %(batch_id) an exception must be raised\"\"\"\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"xml\"},\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        crawler = get_crawler(settings_dict=settings)\n        with pytest.raises(NotConfigured):\n            FeedExporter(crawler)\n\n    @deferred_f_from_coro_f\n    async def test_export_no_items_not_store_empty(self):\n        for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename() / fmt / self._file_mark: {\n                        \"format\": fmt\n                    },\n                },\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n                \"FEED_STORE_EMPTY\": False,\n            }\n            data = await self.exported_no_data(settings)\n            data = dict(data)\n            assert len(data[fmt]) == 0\n", "n_tokens": 1047, "byte_len": 4664, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 2440, "end_line": 2555}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#19", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 19, "symbols": ["test_stats_batch_file_success", "encoding", "expected", "batch", "random", "temp", "async", "spam", "spam2", "each", "exported", "data", "test", "name", "deferred", "from", "got", "export", "bar", "bar2", "mockserver", "get", "crawler", "stats", "settings", "replaced", "items", "none", "feedexport", "value", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "function_or_method", "text": "    @deferred_f_from_coro_f\n    async def test_export_no_items_store_empty(self):\n        formats = (\n            (\"json\", b\"[]\"),\n            (\"jsonlines\", b\"\"),\n            (\"xml\", b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n            (\"csv\", b\"\"),\n        )\n\n        for fmt, expctd in formats:\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename() / fmt / self._file_mark: {\n                        \"format\": fmt\n                    },\n                },\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_EXPORT_INDENT\": None,\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n            }\n            data = await self.exported_no_data(settings)\n            data = dict(data)\n            assert data[fmt][0] == expctd\n\n    @deferred_f_from_coro_f\n    async def test_export_multiple_configs(self):\n        items = [\n            {\"foo\": \"FOO\", \"bar\": \"BAR\"},\n            {\"foo\": \"FOO1\", \"bar\": \"BAR1\"},\n        ]\n\n        formats = {\n            \"json\": [\n                b'[\\n{\"bar\": \"BAR\"}\\n]',\n                b'[\\n{\"bar\": \"BAR1\"}\\n]',\n            ],\n            \"xml\": [\n                (\n                    b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                    b\"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n                ),\n                (\n                    b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                    b\"<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>\"\n                ),\n            ],\n            \"csv\": [\n                b\"foo,bar\\r\\nFOO,BAR\\r\\n\",\n                b\"foo,bar\\r\\nFOO1,BAR1\\r\\n\",\n            ],\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename() / \"json\" / self._file_mark: {\n                    \"format\": \"json\",\n                    \"indent\": 0,\n                    \"fields\": [\"bar\"],\n                    \"encoding\": \"utf-8\",\n                },\n                self._random_temp_filename() / \"xml\" / self._file_mark: {\n                    \"format\": \"xml\",\n                    \"indent\": 2,\n                    \"fields\": [\"foo\"],\n                    \"encoding\": \"latin-1\",\n                },\n                self._random_temp_filename() / \"csv\" / self._file_mark: {\n                    \"format\": \"csv\",\n                    \"indent\": None,\n                    \"fields\": [\"foo\", \"bar\"],\n                    \"encoding\": \"utf-8\",\n                },\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        data = await self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            for expected_batch, got_batch in zip(expected, data[fmt]):\n                assert got_batch == expected_batch\n\n    @deferred_f_from_coro_f\n    async def test_batch_item_count_feeds_setting(self):\n        items = [{\"foo\": \"FOO\"}, {\"foo\": \"FOO1\"}]\n        formats = {\n            \"json\": [\n                b'[{\"foo\": \"FOO\"}]',\n                b'[{\"foo\": \"FOO1\"}]',\n            ],\n        }\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename() / \"json\" / self._file_mark: {\n                    \"format\": \"json\",\n                    \"indent\": None,\n                    \"encoding\": \"utf-8\",\n                    \"batch_item_count\": 1,\n                },\n            },\n        }\n        data = await self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            for expected_batch, got_batch in zip(expected, data[fmt]):\n                assert got_batch == expected_batch\n\n    @deferred_f_from_coro_f\n    async def test_batch_path_differ(self):\n        \"\"\"\n        Test that the name of all batch files differ from each other.\n        So %(batch_id)d replaced with the current id.\n        \"\"\"\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n        ]\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename() / \"%(batch_id)d\": {\n                    \"format\": \"json\",\n                },\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        data = await self.exported_data(items, settings)\n        assert len(items) == len(data[\"json\"])\n\n    @inlineCallbacks\n    def test_stats_batch_file_success(self):\n        settings = {\n            \"FEEDS\": {\n                build_url(\n                    str(self._random_temp_filename() / \"json\" / self._file_mark)\n                ): {\n                    \"format\": \"json\",\n                }\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        yield crawler.crawl(total=2, mockserver=self.mockserver)\n        assert \"feedexport/success_count/FileFeedStorage\" in crawler.stats.get_stats()\n        assert crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\") == 12\n", "n_tokens": 1183, "byte_len": 5031, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 2556, "end_line": 2698}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#20", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 20, "symbols": ["test_s3_export", "open", "parse", "feed_exporter_closed_signal_handler", "feed_slot_closed_signal_handler", "feed_exporter_closed_signal_handler_deferred", "feed_slot_closed_signal_handler_deferred", "run_signaled_feed_exporter", "test_feed_exporter_signals_sent", "test_feed_exporter_signals_sent_deferred", "test_unsupported_storage", "CustomS3FeedStorage", "TestSpider", "TestFeedExporterSignals", "TestFeedExportInit", "checksum", "algorithm", "stubs", "batch", "append", "signal", "spam", "spam2", "spider", "name", "bar", "bar2", "feed", "exporter", "body", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    @pytest.mark.requires_boto3\n    @inlineCallbacks\n    def test_s3_export(self):\n        bucket = \"mybucket\"\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n        ]\n\n        class CustomS3FeedStorage(S3FeedStorage):\n            stubs = []\n\n            def open(self, *args, **kwargs):\n                from botocore import __version__ as botocore_version  # noqa: PLC0415\n                from botocore.stub import ANY, Stubber  # noqa: PLC0415\n\n                expected_params = {\n                    \"Body\": ANY,\n                    \"Bucket\": bucket,\n                    \"Key\": ANY,\n                }\n                if Version(botocore_version) >= Version(\"1.36.0\"):\n                    expected_params[\"ChecksumAlgorithm\"] = ANY\n\n                stub = Stubber(self.s3_client)\n                stub.activate()\n                CustomS3FeedStorage.stubs.append(stub)\n                stub.add_response(\n                    \"put_object\",\n                    expected_params=expected_params,\n                    service_response={},\n                )\n                return super().open(*args, **kwargs)\n\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}/%(batch_id)d.json\"\n        batch_item_count = 1\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": batch_item_count,\n            \"FEED_STORAGES\": {\n                \"s3\": CustomS3FeedStorage,\n            },\n            \"FEEDS\": {\n                uri: {\n                    \"format\": \"json\",\n                },\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(crawler, uri)\n        verifyObject(IFeedStorage, storage)\n\n        class TestSpider(scrapy.Spider):\n            name = \"testspider\"\n\n            def parse(self, response):\n                yield from items\n\n        TestSpider.start_urls = [self.mockserver.url(\"/\")]\n        crawler = get_crawler(TestSpider, settings)\n        yield crawler.crawl()\n\n        assert len(CustomS3FeedStorage.stubs) == len(items)\n        for stub in CustomS3FeedStorage.stubs[:-1]:\n            stub.assert_no_pending_responses()\n\n\n# Test that the FeedExporer sends the feed_exporter_closed and feed_slot_closed signals\nclass TestFeedExporterSignals:\n    items = [\n        {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n        {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"},\n        {\"foo\": \"bar3\", \"baz\": \"quux3\"},\n    ]\n\n    with tempfile.NamedTemporaryFile(suffix=\"json\") as tmp:\n        settings = {\n            \"FEEDS\": {\n                f\"file:///{tmp.name}\": {\n                    \"format\": \"json\",\n                },\n            },\n        }\n\n    def feed_exporter_closed_signal_handler(self):\n        self.feed_exporter_closed_received = True\n\n    def feed_slot_closed_signal_handler(self, slot):\n        self.feed_slot_closed_received = True\n\n    def feed_exporter_closed_signal_handler_deferred(self):\n        d = defer.Deferred()\n        d.addCallback(lambda _: setattr(self, \"feed_exporter_closed_received\", True))\n        d.callback(None)\n        return d\n\n    def feed_slot_closed_signal_handler_deferred(self, slot):\n        d = defer.Deferred()\n        d.addCallback(lambda _: setattr(self, \"feed_slot_closed_received\", True))\n        d.callback(None)\n        return d\n\n    def run_signaled_feed_exporter(\n        self, feed_exporter_signal_handler, feed_slot_signal_handler\n    ):\n        crawler = get_crawler(settings_dict=self.settings)\n        feed_exporter = FeedExporter.from_crawler(crawler)\n        spider = scrapy.Spider(\"default\")\n        spider.crawler = crawler\n        crawler.signals.connect(\n            feed_exporter_signal_handler,\n            signal=signals.feed_exporter_closed,\n        )\n        crawler.signals.connect(\n            feed_slot_signal_handler, signal=signals.feed_slot_closed\n        )\n        feed_exporter.open_spider(spider)\n        for item in self.items:\n            feed_exporter.item_scraped(item, spider)\n        defer.ensureDeferred(feed_exporter.close_spider(spider))\n\n    def test_feed_exporter_signals_sent(self):\n        self.feed_exporter_closed_received = False\n        self.feed_slot_closed_received = False\n\n        self.run_signaled_feed_exporter(\n            self.feed_exporter_closed_signal_handler,\n            self.feed_slot_closed_signal_handler,\n        )\n        assert self.feed_slot_closed_received\n        assert self.feed_exporter_closed_received\n\n    def test_feed_exporter_signals_sent_deferred(self):\n        self.feed_exporter_closed_received = False\n        self.feed_slot_closed_received = False\n\n        self.run_signaled_feed_exporter(\n            self.feed_exporter_closed_signal_handler_deferred,\n            self.feed_slot_closed_signal_handler_deferred,\n        )\n        assert self.feed_slot_closed_received\n        assert self.feed_exporter_closed_received\n\n\nclass TestFeedExportInit:\n    def test_unsupported_storage(self):\n        settings = {\n            \"FEEDS\": {\n                \"unsupported://uri\": {},\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        with pytest.raises(NotConfigured):\n            FeedExporter.from_crawler(crawler)\n", "n_tokens": 1166, "byte_len": 5414, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 2699, "end_line": 2856}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py#21", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_feedexport.py", "rel_path": "tests/test_feedexport.py", "module": "tests.test_feedexport", "ext": "py", "chunk_number": 21, "symbols": ["test_unsupported_format", "test_absolute_pathlib_as_uri", "test_relative_pathlib_as_uri", "build_settings", "_crawler_feed_exporter", "test_default", "test_none", "uri_params", "test_empty_dict", "test_params_as_is", "test_custom_param", "TestURIParams", "TestURIParamsSetting", "TestURIParamsFeedOption", "crawler", "feed", "uri", "params", "deprecated", "options", "spider", "name", "exporter", "test", "empty", "path", "get", "pytest", "settings", "extension", "path_to_url", "printf_escape", "build_url", "mock_google_cloud_storage", "test_store_file_uri", "test_store_file_uri_makedirs", "test_store_direct_path", "test_store_direct_path_relative", "test_interface", "_store", "test_append", "test_overwrite", "_assert_stores", "test_preserves_windows_path_without_file_scheme", "get_test_spider", "_assert_stored", "test_uri_auth_quote", "_store_in_thread", "test_default_temp_dir", "test_temp_file"], "ast_kind": "class_or_type", "text": "    def test_unsupported_format(self):\n        settings = {\n            \"FEEDS\": {\n                \"file://path\": {\n                    \"format\": \"unsupported_format\",\n                },\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        with pytest.raises(NotConfigured):\n            FeedExporter.from_crawler(crawler)\n\n    def test_absolute_pathlib_as_uri(self):\n        with tempfile.NamedTemporaryFile(suffix=\"json\") as tmp:\n            settings = {\n                \"FEEDS\": {\n                    Path(tmp.name).resolve(): {\n                        \"format\": \"json\",\n                    },\n                },\n            }\n            crawler = get_crawler(settings_dict=settings)\n            exporter = FeedExporter.from_crawler(crawler)\n            assert isinstance(exporter, FeedExporter)\n\n    def test_relative_pathlib_as_uri(self):\n        settings = {\n            \"FEEDS\": {\n                Path(\"./items.json\"): {\n                    \"format\": \"json\",\n                },\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        exporter = FeedExporter.from_crawler(crawler)\n        assert isinstance(exporter, FeedExporter)\n\n\nclass TestURIParams(ABC):\n    spider_name = \"uri_params_spider\"\n    deprecated_options = False\n\n    @abstractmethod\n    def build_settings(self, uri=\"file:///tmp/foobar\", uri_params=None):\n        raise NotImplementedError\n\n    def _crawler_feed_exporter(self, settings):\n        if self.deprecated_options:\n            with pytest.warns(\n                ScrapyDeprecationWarning,\n                match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n            ):\n                crawler = get_crawler(settings_dict=settings)\n        else:\n            crawler = get_crawler(settings_dict=settings)\n        feed_exporter = crawler.get_extension(FeedExporter)\n        return crawler, feed_exporter\n\n    def test_default(self):\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            feed_exporter.open_spider(spider)\n\n        assert feed_exporter.slots[0].uri == f\"file:///tmp/{self.spider_name}\"\n\n    def test_none(self):\n        def uri_params(params, spider):\n            pass\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n\n        feed_exporter.open_spider(spider)\n\n        assert feed_exporter.slots[0].uri == f\"file:///tmp/{self.spider_name}\"\n\n    def test_empty_dict(self):\n        def uri_params(params, spider):\n            return {}\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            with pytest.raises(KeyError):\n                feed_exporter.open_spider(spider)\n\n    def test_params_as_is(self):\n        def uri_params(params, spider):\n            return params\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            feed_exporter.open_spider(spider)\n\n        assert feed_exporter.slots[0].uri == f\"file:///tmp/{self.spider_name}\"\n\n    def test_custom_param(self):\n        def uri_params(params, spider):\n            return {**params, \"foo\": self.spider_name}\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(foo)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            feed_exporter.open_spider(spider)\n\n        assert feed_exporter.slots[0].uri == f\"file:///tmp/{self.spider_name}\"\n\n\nclass TestURIParamsSetting(TestURIParams):\n    deprecated_options = True\n\n    def build_settings(self, uri=\"file:///tmp/foobar\", uri_params=None):\n        extra_settings = {}\n        if uri_params:\n            extra_settings[\"FEED_URI_PARAMS\"] = uri_params\n        return {\n            \"FEED_URI\": uri,\n            **extra_settings,\n        }\n\n\nclass TestURIParamsFeedOption(TestURIParams):\n    deprecated_options = False\n\n    def build_settings(self, uri=\"file:///tmp/foobar\", uri_params=None):\n        options = {\n            \"format\": \"jl\",\n        }\n        if uri_params:\n            options[\"uri_params\"] = uri_params\n        return {\n            \"FEEDS\": {\n                uri: options,\n            },\n        }\n", "n_tokens": 1140, "byte_len": 5465, "file_sha1": "84edb337115739fca61247fc1bca7de64c388178", "start_line": 2857, "end_line": 3024}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_offsite.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_offsite.py", "rel_path": "tests/test_downloadermiddleware_offsite.py", "module": "tests.test_downloadermiddleware_offsite", "ext": "py", "chunk_number": 1, "symbols": ["test_process_request_domain_filtering", "test_process_request_dont_filter", "test_process_request_allow_offsite", "test_process_request_no_allowed_domains", "allowed", "allow", "offsite", "false", "domains", "domain", "ignore", "request", "spider", "test", "process", "name", "mark", "meta", "parametrize", "dont", "filter", "downloadermiddlewares", "notb", "with", "filtered", "scrapy", "https", "example", "create", "value", "test_process_request_invalid_domains", "test_request_scheduled_domain_filtering", "test_request_scheduled_dont_filter", "test_request_scheduled_no_allowed_domains", "test_request_scheduled_invalid_domains", "simplefilter", "nota", "get", "crawler", "warnings", "from", "true", "pytest", "user", "warning", "middleware", "assert", "letter", "kwargs", "raises"], "ast_kind": "function_or_method", "text": "import warnings\n\nimport pytest\n\nfrom scrapy import Request, Spider\nfrom scrapy.downloadermiddlewares.offsite import OffsiteMiddleware\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.utils.test import get_crawler\n\nUNSET = object()\n\n\n@pytest.mark.parametrize(\n    (\"allowed_domain\", \"url\", \"allowed\"),\n    [\n        (\"example.com\", \"http://example.com/1\", True),\n        (\"example.com\", \"http://example.org/1\", False),\n        (\"example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://example.com/1\", False),\n        (\"example.com\", \"http://example.com:8000/1\", True),\n        (\"example.com\", \"http://example.org/example.com\", False),\n        (\"example.com\", \"http://example.org/foo.example.com\", False),\n        (\"example.com\", \"http://example.com.example\", False),\n        (\"a.example\", \"http://nota.example\", False),\n        (\"b.a.example\", \"http://notb.a.example\", False),\n    ],\n)\ndef test_process_request_domain_filtering(allowed_domain, url, allowed):\n    crawler = get_crawler(Spider)\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=[allowed_domain])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    request = Request(url)\n    if allowed:\n        assert mw.process_request(request) is None\n    else:\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request)\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"filtered\"),\n    [\n        (UNSET, True),\n        (None, True),\n        (False, True),\n        (True, False),\n    ],\n)\ndef test_process_request_dont_filter(value, filtered):\n    crawler = get_crawler(Spider)\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"dont_filter\"] = value\n    request = Request(\"https://b.example\", **kwargs)\n    if filtered:\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request)\n    else:\n        assert mw.process_request(request) is None\n\n\n@pytest.mark.parametrize(\n    (\"allow_offsite\", \"dont_filter\", \"filtered\"),\n    [\n        (True, UNSET, False),\n        (True, None, False),\n        (True, False, False),\n        (True, True, False),\n        (False, UNSET, True),\n        (False, None, True),\n        (False, False, True),\n        (False, True, False),\n    ],\n)\ndef test_process_request_allow_offsite(allow_offsite, dont_filter, filtered):\n    crawler = get_crawler(Spider)\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    kwargs = {\"meta\": {}}\n    if allow_offsite is not UNSET:\n        kwargs[\"meta\"][\"allow_offsite\"] = allow_offsite\n    if dont_filter is not UNSET:\n        kwargs[\"dont_filter\"] = dont_filter\n    request = Request(\"https://b.example\", **kwargs)\n    if filtered:\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request)\n    else:\n        assert mw.process_request(request) is None\n\n\n@pytest.mark.parametrize(\n    \"value\",\n    [\n        UNSET,\n        None,\n        [],\n    ],\n)\ndef test_process_request_no_allowed_domains(value):\n    crawler = get_crawler(Spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"allowed_domains\"] = value\n    crawler.spider = crawler._create_spider(name=\"a\", **kwargs)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    request = Request(\"https://example.com\")\n    assert mw.process_request(request) is None\n\n", "n_tokens": 887, "byte_len": 3700, "file_sha1": "c1d0e67d31e382caba1b4ac3f56d7db84c26f6d6", "start_line": 1, "end_line": 117}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_offsite.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_offsite.py", "rel_path": "tests/test_downloadermiddleware_offsite.py", "module": "tests.test_downloadermiddleware_offsite", "ext": "py", "chunk_number": 2, "symbols": ["test_process_request_invalid_domains", "test_request_scheduled_domain_filtering", "test_request_scheduled_dont_filter", "test_request_scheduled_no_allowed_domains", "test_request_scheduled_invalid_domains", "allowed", "false", "domains", "domain", "simplefilter", "ignore", "request", "spider", "name", "mark", "parametrize", "dont", "filter", "test", "process", "with", "notb", "filtered", "https", "example", "create", "value", "nota", "get", "crawler", "test_process_request_domain_filtering", "test_process_request_dont_filter", "test_process_request_allow_offsite", "test_process_request_no_allowed_domains", "allow", "offsite", "meta", "downloadermiddlewares", "scrapy", "warnings", "from", "true", "pytest", "user", "warning", "middleware", "assert", "letter", "kwargs", "raises"], "ast_kind": "function_or_method", "text": "def test_process_request_invalid_domains():\n    crawler = get_crawler(Spider)\n    allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        mw.spider_opened(crawler.spider)\n    request = Request(\"https://a.example\")\n    assert mw.process_request(request) is None\n    for letter in (\"b\", \"c\"):\n        request = Request(f\"https://{letter}.example\")\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request)\n\n\n@pytest.mark.parametrize(\n    (\"allowed_domain\", \"url\", \"allowed\"),\n    [\n        (\"example.com\", \"http://example.com/1\", True),\n        (\"example.com\", \"http://example.org/1\", False),\n        (\"example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://example.com/1\", False),\n        (\"example.com\", \"http://example.com:8000/1\", True),\n        (\"example.com\", \"http://example.org/example.com\", False),\n        (\"example.com\", \"http://example.org/foo.example.com\", False),\n        (\"example.com\", \"http://example.com.example\", False),\n        (\"a.example\", \"http://nota.example\", False),\n        (\"b.a.example\", \"http://notb.a.example\", False),\n    ],\n)\ndef test_request_scheduled_domain_filtering(allowed_domain, url, allowed):\n    crawler = get_crawler(Spider)\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=[allowed_domain])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    request = Request(url)\n    if allowed:\n        assert mw.request_scheduled(request, crawler.spider) is None\n    else:\n        with pytest.raises(IgnoreRequest):\n            mw.request_scheduled(request, crawler.spider)\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"filtered\"),\n    [\n        (UNSET, True),\n        (None, True),\n        (False, True),\n        (True, False),\n    ],\n)\ndef test_request_scheduled_dont_filter(value, filtered):\n    crawler = get_crawler(Spider)\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"dont_filter\"] = value\n    request = Request(\"https://b.example\", **kwargs)\n    if filtered:\n        with pytest.raises(IgnoreRequest):\n            mw.request_scheduled(request, crawler.spider)\n    else:\n        assert mw.request_scheduled(request, crawler.spider) is None\n\n\n@pytest.mark.parametrize(\n    \"value\",\n    [\n        UNSET,\n        None,\n        [],\n    ],\n)\ndef test_request_scheduled_no_allowed_domains(value):\n    crawler = get_crawler(Spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"allowed_domains\"] = value\n    crawler.spider = crawler._create_spider(name=\"a\", **kwargs)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(crawler.spider)\n    request = Request(\"https://example.com\")\n    assert mw.request_scheduled(request, crawler.spider) is None\n\n\ndef test_request_scheduled_invalid_domains():\n    crawler = get_crawler(Spider)\n    allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n    crawler.spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        mw.spider_opened(crawler.spider)\n    request = Request(\"https://a.example\")\n    assert mw.request_scheduled(request, crawler.spider) is None\n    for letter in (\"b\", \"c\"):\n        request = Request(f\"https://{letter}.example\")\n        with pytest.raises(IgnoreRequest):\n            mw.request_scheduled(request, crawler.spider)\n", "n_tokens": 931, "byte_len": 3938, "file_sha1": "c1d0e67d31e382caba1b4ac3f56d7db84c26f6d6", "start_line": 118, "end_line": 222}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_loop.py", "rel_path": "tests/test_engine_loop.py", "module": "tests.test_engine_loop", "ext": "py", "chunk_number": 1, "symbols": ["parse", "track_url", "setup_class", "teardown_class", "request", "get_num", "parse_fn", "TestMain", "TestSpider", "TestRequestSendOrder", "method", "processed", "async", "append", "miss", "spider", "name", "deferred", "from", "popleft", "scheduler", "sleep", "cause", "level", "nums", "future", "typ", "checking", "mockserver", "get", "track_num", "_request", "takes", "does", "immediately", "queues", "about", "concurren", "requests", "spiders", "below", "crawler", "enter", "sent", "delay", "squeues", "pytest", "settings", "order", "falsy"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom collections import deque\nfrom logging import ERROR\nfrom typing import TYPE_CHECKING\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.test_scheduler import MemoryScheduler\n\nif TYPE_CHECKING:\n    import pytest\n\n    from scrapy.http import Response\n\n\nasync def sleep(seconds: float = 0.001) -> None:\n    from twisted.internet import reactor\n\n    deferred: Deferred[None] = Deferred()\n    reactor.callLater(seconds, deferred.callback, None)\n    await maybe_deferred_to_future(deferred)\n\n\nclass TestMain:\n    @deferred_f_from_coro_f\n    async def test_sleep(self):\n        \"\"\"Neither asynchronous sleeps on Spider.start() nor the equivalent on\n        the scheduler (returning no requests while also returning True from\n        the has_pending_requests() method) should cause the spider to miss the\n        processing of any later requests.\"\"\"\n        seconds = 2\n\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                from twisted.internet import reactor\n\n                yield Request(\"data:,a\")\n\n                await sleep(seconds)\n\n                self.crawler.engine._slot.scheduler.pause()\n                self.crawler.engine._slot.scheduler.enqueue_request(Request(\"data:,b\"))\n\n                # During this time, the scheduler reports having requests but\n                # returns None.\n                await sleep(seconds)\n\n                self.crawler.engine._slot.scheduler.unpause()\n\n                # The scheduler request is processed.\n                await sleep(seconds)\n\n                yield Request(\"data:,c\")\n\n                await sleep(seconds)\n\n                self.crawler.engine._slot.scheduler.pause()\n                self.crawler.engine._slot.scheduler.enqueue_request(Request(\"data:,d\"))\n\n                # The last start request is processed during the time until the\n                # delayed call below, proving that the start iteration can\n                # finish before a scheduler “sleep” without causing the\n                # scheduler to finish.\n                reactor.callLater(seconds, self.crawler.engine._slot.scheduler.unpause)\n\n            def parse(self, response):\n                pass\n\n        actual_urls = []\n\n        def track_url(request, spider):\n            actual_urls.append(request.url)\n\n        settings = {\"SCHEDULER\": MemoryScheduler}\n        crawler = get_crawler(TestSpider, settings_dict=settings)\n        crawler.signals.connect(track_url, signals.request_reached_downloader)\n        await maybe_deferred_to_future(crawler.crawl())\n        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n        expected_urls = [\"data:,a\", \"data:,b\", \"data:,c\", \"data:,d\"]\n        assert actual_urls == expected_urls, f\"{actual_urls=} != {expected_urls=}\"\n\n    @deferred_f_from_coro_f\n    async def test_close_during_start_iteration(\n        self, caplog: pytest.LogCaptureFixture\n    ) -> None:\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                assert self.crawler.engine is not None\n                await self.crawler.engine.close_async()\n                yield Request(\"data:,a\")\n\n            def parse(self, response):\n                pass\n\n        actual_urls = []\n\n        def track_url(request, spider):\n            actual_urls.append(request.url)\n\n        settings = {\"SCHEDULER\": MemoryScheduler}\n        crawler = get_crawler(TestSpider, settings_dict=settings)\n        crawler.signals.connect(track_url, signals.request_reached_downloader)\n\n        caplog.clear()\n        with caplog.at_level(ERROR):\n            await maybe_deferred_to_future(crawler.crawl())\n\n        assert not caplog.records\n        assert crawler.stats\n        assert crawler.stats.get_value(\"finish_reason\") == \"shutdown\"\n        assert not actual_urls\n\n\nclass TestRequestSendOrder:\n    seconds = 0.1  # increase if flaky\n\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)  # increase if flaky\n\n    def request(self, num, response_seconds, download_slots, priority=0):\n        url = self.mockserver.url(f\"/delay?n={response_seconds}&{num}\")\n        meta = {\"download_slot\": str(num % download_slots)}\n        return Request(url, meta=meta, priority=priority)\n\n    def get_num(self, request_or_response: Request | Response):\n        return int(request_or_response.url.rsplit(\"&\", maxsplit=1)[1])\n\n    async def _test_request_order(\n        self,\n        start_nums,\n        cb_nums=None,\n        settings=None,\n        response_seconds=None,\n        download_slots=1,\n        start_fn=None,\n        parse_fn=None,\n    ):\n        cb_nums = cb_nums or []\n        settings = settings or {}\n        response_seconds = response_seconds or self.seconds\n\n        cb_requests = deque(\n            [self.request(num, response_seconds, download_slots) for num in cb_nums]\n        )\n\n        if start_fn is None:\n\n            async def start_fn(spider):\n                for num in start_nums:\n                    yield self.request(num, response_seconds, download_slots)\n\n        if parse_fn is None:\n\n            def parse_fn(spider, response):\n                while cb_requests:\n                    yield cb_requests.popleft()\n\n        class TestSpider(Spider):\n            name = \"test\"\n            start = start_fn\n            parse = parse_fn\n\n        actual_nums = []\n", "n_tokens": 1154, "byte_len": 5749, "file_sha1": "767f35a833dfec1f510c51da4cbaa63c494a54c9", "start_line": 1, "end_line": 179}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_loop.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_loop.py", "rel_path": "tests/test_engine_loop.py", "module": "tests.test_engine_loop", "ext": "py", "chunk_number": 2, "symbols": ["track_num", "_request", "parse", "does", "takes", "async", "immediately", "queues", "append", "nums", "deferred", "from", "about", "scheduler", "concurren", "requests", "cause", "spiders", "below", "get", "crawler", "sent", "squeues", "settings", "order", "falsy", "finish", "reason", "none", "priorized", "track_url", "setup_class", "teardown_class", "request", "get_num", "parse_fn", "TestMain", "TestSpider", "TestRequestSendOrder", "method", "processed", "miss", "spider", "name", "popleft", "sleep", "level", "future", "typ", "checking"], "ast_kind": "function_or_method", "text": "        def track_num(request, spider):\n            actual_nums.append(self.get_num(request))\n\n        crawler = get_crawler(TestSpider, settings_dict=settings)\n        crawler.signals.connect(track_num, signals.request_reached_downloader)\n        await maybe_deferred_to_future(crawler.crawl())\n        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n        expected_nums = sorted(start_nums + cb_nums)\n        assert actual_nums == expected_nums, f\"{actual_nums=} != {expected_nums=}\"\n\n    @deferred_f_from_coro_f\n    async def test_default(self):\n        \"\"\"By default, callback requests take priority over start requests and\n        are sent in order. Priority matters, but given the same priority, a\n        callback request takes precedence.\"\"\"\n        nums = [1, 2, 3, 4, 5, 6]\n        response_seconds = 0\n        download_slots = 1\n\n        def _request(num, priority=0):\n            return self.request(\n                num, response_seconds, download_slots, priority=priority\n            )\n\n        async def start(spider):\n            # The first CONCURRENT_REQUESTS start requests are sent\n            # immediately.\n            yield _request(1)\n\n            for request in (\n                _request(2, priority=1),\n                _request(5),\n            ):\n                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n            yield _request(6)\n            yield _request(3, priority=1)\n            yield _request(4, priority=1)\n\n        def parse(spider, response):\n            return\n            yield\n\n        await self._test_request_order(\n            start_nums=nums,\n            settings={\"CONCURRENT_REQUESTS\": 1},\n            response_seconds=response_seconds,\n            start_fn=start,\n            parse_fn=parse,\n        )\n\n    @deferred_f_from_coro_f\n    async def test_lifo_start(self):\n        \"\"\"Changing the queues of start requests to LIFO, matching the queues\n        of non-start requests, does not cause all requests to be stored in the\n        same queue objects, it only affects the order of start requests.\"\"\"\n        nums = [1, 2, 3, 4, 5, 6]\n        response_seconds = 0\n        download_slots = 1\n\n        def _request(num, priority=0):\n            return self.request(\n                num, response_seconds, download_slots, priority=priority\n            )\n\n        async def start(spider):\n            # The first CONCURRENT_REQUESTS start requests are sent\n            # immediately.\n            yield _request(1)\n\n            for request in (\n                _request(2, priority=1),\n                _request(5),\n            ):\n                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n            yield _request(6)\n            yield _request(4, priority=1)\n            yield _request(3, priority=1)\n\n        def parse(spider, response):\n            return\n            yield\n\n        await self._test_request_order(\n            start_nums=nums,\n            settings={\n                \"CONCURRENT_REQUESTS\": 1,\n                \"SCHEDULER_START_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n            },\n            response_seconds=response_seconds,\n            start_fn=start,\n            parse_fn=parse,\n        )\n\n    @deferred_f_from_coro_f\n    async def test_shared_queues(self):\n        \"\"\"If SCHEDULER_START_*_QUEUE is falsy, start requests and other\n        requests share the same queue, i.e. start requests are not priorized\n        over other requests if their priority matches.\"\"\"\n        nums = list(range(1, 14))\n        response_seconds = 0\n        download_slots = 1\n\n        def _request(num, priority=0):\n            return self.request(\n                num, response_seconds, download_slots, priority=priority\n            )\n\n        async def start(spider):\n            # The first CONCURRENT_REQUESTS start requests are sent\n            # immediately.\n            yield _request(1)\n\n            # Below, priority 1 requests are sent first, and requests are sent\n            # in LIFO order.\n\n            for request in (\n                _request(7, priority=1),\n                _request(6, priority=1),\n                _request(13),\n                _request(12),\n            ):\n                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n\n            yield _request(11)\n            yield _request(10)\n            yield _request(5, priority=1)\n            yield _request(4, priority=1)\n\n            for request in (\n                _request(3, priority=1),\n                _request(2, priority=1),\n                _request(9),\n                _request(8),\n            ):\n                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n\n        def parse(spider, response):\n            return\n            yield\n\n        await self._test_request_order(\n            start_nums=nums,\n            settings={\n                \"CONCURRENT_REQUESTS\": 1,\n                \"SCHEDULER_START_MEMORY_QUEUE\": None,\n            },\n            response_seconds=response_seconds,\n            start_fn=start,\n            parse_fn=parse,\n        )\n\n    # Examples from the “Start requests” section of the documentation about\n    # spiders.\n\n    @deferred_f_from_coro_f\n    async def test_lazy(self):\n        start_nums = [1, 2, 4]\n        cb_nums = [3]\n        response_seconds = self.seconds * 2**1  # increase if flaky\n        download_slots = 1\n", "n_tokens": 1145, "byte_len": 5394, "file_sha1": "767f35a833dfec1f510c51da4cbaa63c494a54c9", "start_line": 180, "end_line": 340}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_loop.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_loop.py", "rel_path": "tests/test_engine_loop.py", "module": "tests.test_engine_loop", "ext": "py", "chunk_number": 3, "symbols": ["async", "await", "spider", "response", "seconds", "test", "request", "concurren", "requests", "nums", "download", "slots", "yield", "settings", "engine", "signals", "wait", "for", "needs", "backout", "start", "crawler", "scheduler", "empty", "self", "parse", "track_url", "setup_class", "teardown_class", "get_num", "parse_fn", "track_num", "_request", "TestMain", "TestSpider", "TestRequestSendOrder", "method", "processed", "takes", "does", "immediately", "queues", "append", "miss", "name", "deferred", "from", "popleft", "about", "sleep"], "ast_kind": "unknown", "text": "        async def start(spider):\n            for num in start_nums:\n                if spider.crawler.engine.needs_backout():\n                    await spider.crawler.signals.wait_for(signals.scheduler_empty)\n                request = self.request(num, response_seconds, download_slots)\n                yield request\n\n        await self._test_request_order(\n            start_nums=start_nums,\n            cb_nums=cb_nums,\n            settings={\n                \"CONCURRENT_REQUESTS\": 1,\n            },\n            response_seconds=response_seconds,\n            start_fn=start,\n        )\n", "n_tokens": 104, "byte_len": 587, "file_sha1": "767f35a833dfec1f510c51da4cbaa63c494a54c9", "start_line": 341, "end_line": 357}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_left.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_request_left.py", "rel_path": "tests/test_request_left.py", "module": "tests.test_request_left", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "on_request_left", "setup_class", "teardown_class", "test_success", "test_timeout", "test_disconnect", "test_noconnect", "SignalCatcherSpider", "TestCatching", "internet", "teardown", "class", "signal", "request", "left", "spider", "exit", "twisted", "return", "drop", "name", "thereisdefinetelynosuchdomain", "spiders", "scrapy", "test", "noconnect", "defer", "classmethod", "mockserver", "get", "crawler", "init", "connect", "catching", "enter", "timeout", "from", "yield", "downloa", "delay", "start", "urls", "assert", "signals", "kwargs", "disconnect", "mock", "server"], "ast_kind": "class_or_type", "text": "from twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.signals import request_left_downloader\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\n\n\nclass SignalCatcherSpider(Spider):\n    name = \"signal_catcher\"\n\n    def __init__(self, crawler, url, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        crawler.signals.connect(self.on_request_left, signal=request_left_downloader)\n        self.caught_times = 0\n        self.start_urls = [url]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        return cls(crawler, *args, **kwargs)\n\n    def on_request_left(self, request, spider):\n        self.caught_times += 1\n\n\nclass TestCatching:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @inlineCallbacks\n    def test_success(self):\n        crawler = get_crawler(SignalCatcherSpider)\n        yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n        assert crawler.spider.caught_times == 1\n\n    @inlineCallbacks\n    def test_timeout(self):\n        crawler = get_crawler(SignalCatcherSpider, {\"DOWNLOAD_TIMEOUT\": 0.1})\n        yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n        assert crawler.spider.caught_times == 1\n\n    @inlineCallbacks\n    def test_disconnect(self):\n        crawler = get_crawler(SignalCatcherSpider)\n        yield crawler.crawl(self.mockserver.url(\"/drop\"))\n        assert crawler.spider.caught_times == 1\n\n    @inlineCallbacks\n    def test_noconnect(self):\n        crawler = get_crawler(SignalCatcherSpider)\n        yield crawler.crawl(\"http://thereisdefinetelynosuchdomain.com\")\n        assert crawler.spider.caught_times == 1\n", "n_tokens": 436, "byte_len": 1855, "file_sha1": "ac516285feb4b19c6770f287bb6b1f0aa2056b64", "start_line": 1, "end_line": 59}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_check.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_check.py", "rel_path": "tests/test_command_check.py", "module": "tests.test_command_check", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "_write_contract", "parse", "_test_contract", "test_check_returns_requests_contract", "test_check_returns_items_contract", "test_check_cb_kwargs_contract", "test_check_scrapes_contract", "test_check_all_default_contracts", "test_SCRAPY_CHECK_set", "test_printSummary_with_unsuccessful_test_result_without_errors_and_without_failures", "test_printSummary_with_unsuccessful_test_result_with_only_failures", "test_printSummary_with_unsuccessful_test_result_with_only_errors", "test_printSummary_with_unsuccessful_test_result_with_both_failures_and_errors", "TestCheckCommand", "CheckSpider", "encoding", "arg", "arg2", "mock", "def", "test", "check", "spider", "name", "stream", "key", "key1", "command", "spiders", "test_run_with_opts_list_prints_spider", "test_run_without_opts_list_does_not_crawl_spider_with_no_tested_methods", "loader", "load", "stdout", "val", "val2", "scrap", "stop", "time", "arg1", "print", "summary", "settings", "tested", "methods", "items", "object", "return", "value"], "ast_kind": "class_or_type", "text": "import sys\nfrom io import StringIO\nfrom unittest.mock import Mock, PropertyMock, call, patch\n\nfrom scrapy.commands.check import Command, TextTestResult\nfrom tests.test_commands import TestCommandBase\n\n\nclass TestCheckCommand(TestCommandBase):\n    def setup_method(self):\n        super().setup_method()\n        self.spider_name = \"check_spider\"\n        self.spider = (self.proj_mod_path / \"spiders\" / \"checkspider.py\").resolve()\n\n    def _write_contract(self, contracts, parse_def):\n        self.spider.write_text(\n            f\"\"\"\nimport scrapy\n\nclass CheckSpider(scrapy.Spider):\n    name = '{self.spider_name}'\n    start_urls = ['data:,']\n\n    def parse(self, response, **cb_kwargs):\n        \\\"\\\"\\\"\n        @url data:,\n        {contracts}\n        \\\"\\\"\\\"\n        {parse_def}\n        \"\"\",\n            encoding=\"utf-8\",\n        )\n\n    def _test_contract(self, contracts=\"\", parse_def=\"pass\"):\n        self._write_contract(contracts, parse_def)\n        p, out, err = self.proc(\"check\")\n        assert \"F\" not in out\n        assert \"OK\" in err\n        assert p.returncode == 0\n\n    def test_check_returns_requests_contract(self):\n        contracts = \"\"\"\n        @returns requests 1\n        \"\"\"\n        parse_def = \"\"\"\n        yield scrapy.Request(url='http://next-url.com')\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_returns_items_contract(self):\n        contracts = \"\"\"\n        @returns items 1\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_cb_kwargs_contract(self):\n        contracts = \"\"\"\n        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n        \"\"\"\n        parse_def = \"\"\"\n        if len(cb_kwargs.items()) == 0:\n            raise Exception(\"Callback args not set\")\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_scrapes_contract(self):\n        contracts = \"\"\"\n        @scrapes key1 key2\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_all_default_contracts(self):\n        contracts = \"\"\"\n        @returns items 1\n        @returns requests 1\n        @scrapes key1 key2\n        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        yield scrapy.Request(url='http://next-url.com')\n        if len(cb_kwargs.items()) == 0:\n            raise Exception(\"Callback args not set\")\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_SCRAPY_CHECK_set(self):\n        parse_def = \"\"\"\n        import os\n        if not os.environ.get('SCRAPY_CHECK'):\n            raise Exception('SCRAPY_CHECK not set')\n        \"\"\"\n        self._test_contract(parse_def=parse_def)\n\n    def test_printSummary_with_unsuccessful_test_result_without_errors_and_without_failures(\n        self,\n    ):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = []\n        result.errors = []\n        result.unexpectedSuccesses = [\"a\", \"b\"]\n        with patch.object(result.stream, \"write\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_has_calls([call(\"FAILED\"), call(\"\\n\")])\n\n    def test_printSummary_with_unsuccessful_test_result_with_only_failures(self):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = [(self, \"failure\")]\n        result.errors = []\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (failures=1)\")\n\n    def test_printSummary_with_unsuccessful_test_result_with_only_errors(self):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = []\n        result.errors = [(self, \"error\")]\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (errors=1)\")\n\n    def test_printSummary_with_unsuccessful_test_result_with_both_failures_and_errors(\n        self,\n    ):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = [(self, \"failure\")]\n        result.errors = [(self, \"error\")]\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (failures=1, errors=1)\")\n", "n_tokens": 1153, "byte_len": 4933, "file_sha1": "b5cacc6f8bd210407c3e29e378ece4fb232df254", "start_line": 1, "end_line": 149}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_check.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_check.py", "rel_path": "tests/test_command_check.py", "module": "tests.test_command_check", "ext": "py", "chunk_number": 2, "symbols": ["test_run_with_opts_list_prints_spider", "test_run_without_opts_list_does_not_crawl_spider_with_no_tested_methods", "spider", "loader", "getwithbase", "false", "patch", "mock", "stdout", "getvalue", "name", "check", "lambda", "commands", "fake", "cls", "method", "output", "property", "load", "command", "scrapy", "assert", "not", "true", "string", "stringio", "settings", "list", "test", "setup_method", "_write_contract", "parse", "_test_contract", "test_check_returns_requests_contract", "test_check_returns_items_contract", "test_check_cb_kwargs_contract", "test_check_scrapes_contract", "test_check_all_default_contracts", "test_SCRAPY_CHECK_set", "test_printSummary_with_unsuccessful_test_result_without_errors_and_without_failures", "test_printSummary_with_unsuccessful_test_result_with_only_failures", "test_printSummary_with_unsuccessful_test_result_with_only_errors", "test_printSummary_with_unsuccessful_test_result_with_both_failures_and_errors", "TestCheckCommand", "CheckSpider", "encoding", "arg", "arg2", "def"], "ast_kind": "function_or_method", "text": "    @patch(\"scrapy.commands.check.ContractsManager\")\n    def test_run_with_opts_list_prints_spider(self, cm_cls_mock):\n        output = StringIO()\n        sys.stdout = output\n        cmd = Command()\n        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n        cm_cls_mock.return_value = cm_mock = Mock()\n        spider_loader_mock = Mock()\n        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n        spider_name = \"FakeSpider\"\n        spider_cls_mock = Mock()\n        type(spider_cls_mock).name = PropertyMock(return_value=spider_name)\n        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n            x\n        ]\n        tested_methods = [\"fakeMethod1\", \"fakeMethod2\"]\n        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n            spider_cls_mock: tested_methods\n        }[x]\n\n        cmd.run([spider_name], Mock(list=True))\n\n        assert output.getvalue() == \"FakeSpider\\n  * fakeMethod1\\n  * fakeMethod2\\n\"\n        sys.stdout = sys.__stdout__\n\n    @patch(\"scrapy.commands.check.ContractsManager\")\n    def test_run_without_opts_list_does_not_crawl_spider_with_no_tested_methods(\n        self, cm_cls_mock\n    ):\n        cmd = Command()\n        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n        cm_cls_mock.return_value = cm_mock = Mock()\n        spider_loader_mock = Mock()\n        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n        spider_name = \"FakeSpider\"\n        spider_cls_mock = Mock()\n        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n            x\n        ]\n        tested_methods = []\n        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n            spider_cls_mock: tested_methods\n        }[x]\n\n        cmd.run([spider_name], Mock(list=False))\n\n        cmd.crawler_process.crawl.assert_not_called()\n", "n_tokens": 431, "byte_len": 1885, "file_sha1": "b5cacc6f8bd210407c3e29e378ece4fb232df254", "start_line": 150, "end_line": 197}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_defaultheaders.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloadermiddleware_defaultheaders.py", "rel_path": "tests/test_downloadermiddleware_defaultheaders.py", "module": "tests.test_downloadermiddleware_defaultheaders", "ext": "py", "chunk_number": 1, "symbols": ["get_defaults_mw", "test_process_request", "test_update_headers", "TestDefaultHeadersMiddleware", "test", "python", "update", "return", "spider", "class", "downloadermiddlewares", "spiders", "scrapy", "headers", "defaults", "get", "crawler", "from", "settings", "process", "scrapytest", "assert", "request", "items", "language", "defaul", "reques", "defaultheaders", "utils", "accept", "import", "default", "http", "bytes", "self", "header"], "ast_kind": "class_or_type", "text": "from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestDefaultHeadersMiddleware:\n    def get_defaults_mw(self):\n        crawler = get_crawler(Spider)\n        defaults = {\n            to_bytes(k): [to_bytes(v)]\n            for k, v in crawler.settings.get(\"DEFAULT_REQUEST_HEADERS\").items()\n        }\n        return defaults, DefaultHeadersMiddleware.from_crawler(crawler)\n\n    def test_process_request(self):\n        defaults, mw = self.get_defaults_mw()\n        req = Request(\"http://www.scrapytest.org\")\n        mw.process_request(req)\n        assert req.headers == defaults\n\n    def test_update_headers(self):\n        defaults, mw = self.get_defaults_mw()\n        headers = {\"Accept-Language\": [\"es\"], \"Test-Header\": [\"test\"]}\n        bytes_headers = {b\"Accept-Language\": [b\"es\"], b\"Test-Header\": [b\"test\"]}\n        req = Request(\"http://www.scrapytest.org\", headers=headers)\n        assert req.headers == bytes_headers\n\n        mw.process_request(req)\n        defaults.update(bytes_headers)\n        assert req.headers == defaults\n", "n_tokens": 262, "byte_len": 1221, "file_sha1": "e420e14eca3db3d07c77d6afebcd4f3e7bced7e3", "start_line": 1, "end_line": 33}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/pipelines.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/pipelines.py", "rel_path": "tests/pipelines.py", "module": "tests.pipelines", "ext": "py", "chunk_number": 1, "symbols": ["open_spider", "process_item", "ZeroDivisionErrorPipeline", "ProcessWithZeroDivisionErrorPipeline", "used", "item", "some", "class", "pipelines", "self", "open", "spider", "process", "with", "return", "testing", "zero", "division"], "ast_kind": "class_or_type", "text": "\"\"\"\nSome pipelines used for testing\n\"\"\"\n\n\nclass ZeroDivisionErrorPipeline:\n    def open_spider(self):\n        1 / 0\n\n    def process_item(self, item):\n        return item\n\n\nclass ProcessWithZeroDivisionErrorPipeline:\n    def process_item(self, item):\n        1 / 0\n", "n_tokens": 63, "byte_len": 265, "file_sha1": "d7fee7ea67e78f87b06a4789757d758eeea8e50f", "start_line": 1, "end_line": 17}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_httperror.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_httperror.py", "rel_path": "tests/test_spidermiddleware_httperror.py", "module": "tests.test_spidermiddleware_httperror", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "parse", "on_error", "_response", "res200", "res402", "res404", "mw", "test_process_spider_input", "test_process_spider_exception", "test_handle_httpstatus_list", "test_meta_overrides_settings", "test_spider_override_settings", "_HttpErrorSpider", "TestHttpErrorMiddleware", "TestHttpErrorMiddlewareSettings", "TestHttpErrorMiddlewareHandleAll", "async", "http", "error", "name", "spiders", "future", "mockserver", "get", "crawler", "mock", "server", "failed", "pytest", "test_httperror_allow_all_false", "setup_class", "teardown_class", "test_middleware_works", "test_logging", "test_logging_level", "TestHttpErrorMiddlewareIntegrational", "logs", "test", "logging", "responses", "handle", "httpstatus", "capture", "enter", "settings", "res", "isinstance", "anything", "none"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom scrapy.http import Request, Response\nfrom scrapy.spidermiddlewares.httperror import HttpError, HttpErrorMiddleware\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver.http import MockServer\nfrom tests.spiders import MockServerSpider\n\n\nclass _HttpErrorSpider(MockServerSpider):\n    name = \"httperror\"\n    bypass_status_codes: set[int] = set()\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = [\n            self.mockserver.url(\"/status?n=200\"),\n            self.mockserver.url(\"/status?n=404\"),\n            self.mockserver.url(\"/status?n=402\"),\n            self.mockserver.url(\"/status?n=500\"),\n        ]\n        self.failed = set()\n        self.skipped = set()\n        self.parsed = set()\n\n    async def start(self):\n        for url in self.start_urls:\n            yield Request(url, self.parse, errback=self.on_error)\n\n    def parse(self, response):\n        self.parsed.add(response.url[-3:])\n\n    def on_error(self, failure):\n        if isinstance(failure.value, HttpError):\n            response = failure.value.response\n            if response.status in self.bypass_status_codes:\n                self.skipped.add(response.url[-3:])\n                return self.parse(response)\n\n        # it assumes there is a response attached to failure\n        self.failed.add(failure.value.response.url[-3:])\n        return failure\n\n\nreq = Request(\"http://scrapytest.org\")\n\n\ndef _response(request: Request, status_code: int) -> Response:\n    return Response(request.url, status=status_code, request=request)\n\n\n@pytest.fixture\ndef res200() -> Response:\n    return _response(req, 200)\n\n\n@pytest.fixture\ndef res402() -> Response:\n    return _response(req, 402)\n\n\n@pytest.fixture\ndef res404() -> Response:\n    return _response(req, 404)\n\n\nclass TestHttpErrorMiddleware:\n    @pytest.fixture\n    def mw(self) -> HttpErrorMiddleware:\n        crawler = get_crawler(DefaultSpider)\n        crawler.spider = crawler._create_spider()\n        return HttpErrorMiddleware.from_crawler(crawler)\n\n    def test_process_spider_input(\n        self, mw: HttpErrorMiddleware, res200: Response, res404: Response\n    ) -> None:\n        mw.process_spider_input(res200)\n        with pytest.raises(HttpError):\n            mw.process_spider_input(res404)\n\n    def test_process_spider_exception(\n        self, mw: HttpErrorMiddleware, res404: Response\n    ) -> None:\n        assert mw.process_spider_exception(res404, HttpError(res404)) == []\n        assert mw.process_spider_exception(res404, Exception()) is None\n\n    def test_handle_httpstatus_list(\n        self, mw: HttpErrorMiddleware, res404: Response\n    ) -> None:\n        request = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n        )\n        res = _response(request, 404)\n        mw.process_spider_input(res)\n\n        assert mw.crawler.spider\n        mw.crawler.spider.handle_httpstatus_list = [404]  # type: ignore[attr-defined]\n        mw.process_spider_input(res404)\n\n\nclass TestHttpErrorMiddlewareSettings:\n    \"\"\"Similar test, but with settings\"\"\"\n\n    @pytest.fixture\n    def mw(self) -> HttpErrorMiddleware:\n        crawler = get_crawler(DefaultSpider, {\"HTTPERROR_ALLOWED_CODES\": (402,)})\n        crawler.spider = crawler._create_spider()\n        return HttpErrorMiddleware.from_crawler(crawler)\n\n    def test_process_spider_input(\n        self,\n        mw: HttpErrorMiddleware,\n        res200: Response,\n        res402: Response,\n        res404: Response,\n    ) -> None:\n        mw.process_spider_input(res200)\n        with pytest.raises(HttpError):\n            mw.process_spider_input(res404)\n        mw.process_spider_input(res402)\n\n    def test_meta_overrides_settings(self, mw: HttpErrorMiddleware) -> None:\n        request = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n        )\n        res404 = _response(request, 404)\n        res402 = _response(request, 402)\n\n        mw.process_spider_input(res404)\n        with pytest.raises(HttpError):\n            mw.process_spider_input(res402)\n\n    def test_spider_override_settings(\n        self, mw: HttpErrorMiddleware, res402: Response, res404: Response\n    ) -> None:\n        assert mw.crawler.spider\n        mw.crawler.spider.handle_httpstatus_list = [404]  # type: ignore[attr-defined]\n        mw.process_spider_input(res404)\n        with pytest.raises(HttpError):\n            mw.process_spider_input(res402)\n\n\nclass TestHttpErrorMiddlewareHandleAll:\n    @pytest.fixture\n    def mw(self) -> HttpErrorMiddleware:\n        crawler = get_crawler(DefaultSpider, {\"HTTPERROR_ALLOW_ALL\": True})\n        crawler.spider = crawler._create_spider()\n        return HttpErrorMiddleware.from_crawler(crawler)\n\n    def test_process_spider_input(\n        self,\n        mw: HttpErrorMiddleware,\n        res200: Response,\n        res404: Response,\n    ) -> None:\n        mw.process_spider_input(res200)\n        mw.process_spider_input(res404)\n", "n_tokens": 1153, "byte_len": 5168, "file_sha1": "8cfb7c5aa78b633ccf2bfa9fee66e9f664f13d0f", "start_line": 1, "end_line": 165}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_httperror.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spidermiddleware_httperror.py", "rel_path": "tests/test_spidermiddleware_httperror.py", "module": "tests.test_spidermiddleware_httperror", "ext": "py", "chunk_number": 2, "symbols": ["test_meta_overrides_settings", "test_httperror_allow_all_false", "setup_class", "teardown_class", "test_middleware_works", "test_logging", "test_logging_level", "TestHttpErrorMiddlewareIntegrational", "test", "http", "warning", "false", "ignoring", "teardown", "class", "error", "logs", "spider", "exit", "res", "res402", "logging", "middleware", "meta", "responses", "ignored", "with", "log", "capture", "response", "__init__", "parse", "on_error", "_response", "res200", "res404", "mw", "test_process_spider_input", "test_process_spider_exception", "test_handle_httpstatus_list", "test_spider_override_settings", "_HttpErrorSpider", "TestHttpErrorMiddleware", "TestHttpErrorMiddlewareSettings", "TestHttpErrorMiddlewareHandleAll", "async", "name", "spiders", "future", "handle"], "ast_kind": "class_or_type", "text": "    def test_meta_overrides_settings(self, mw: HttpErrorMiddleware) -> None:\n        request = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n        )\n        res404 = _response(request, 404)\n        res402 = _response(request, 402)\n\n        mw.process_spider_input(res404)\n        with pytest.raises(HttpError):\n            mw.process_spider_input(res402)\n\n    def test_httperror_allow_all_false(self) -> None:\n        crawler = get_crawler(_HttpErrorSpider)\n        mw = HttpErrorMiddleware.from_crawler(crawler)\n        request_httpstatus_false = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_all\": False}\n        )\n        request_httpstatus_true = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_all\": True}\n        )\n        res404 = _response(request_httpstatus_false, 404)\n        res402 = _response(request_httpstatus_true, 402)\n\n        with pytest.raises(HttpError):\n            mw.process_spider_input(res404)\n        mw.process_spider_input(res402)\n\n\nclass TestHttpErrorMiddlewareIntegrational:\n    @classmethod\n    def setup_class(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def teardown_class(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @inlineCallbacks\n    def test_middleware_works(self):\n        crawler = get_crawler(_HttpErrorSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert not crawler.spider.skipped\n        assert crawler.spider.parsed == {\"200\"}\n        assert crawler.spider.failed == {\"404\", \"402\", \"500\"}\n\n        get_value = crawler.stats.get_value\n        assert get_value(\"httperror/response_ignored_count\") == 3\n        assert get_value(\"httperror/response_ignored_status_count/404\") == 1\n        assert get_value(\"httperror/response_ignored_status_count/402\") == 1\n        assert get_value(\"httperror/response_ignored_status_count/500\") == 1\n\n    @inlineCallbacks\n    def test_logging(self):\n        crawler = get_crawler(_HttpErrorSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver, bypass_status_codes={402})\n        assert crawler.spider.parsed == {\"200\", \"402\"}\n        assert crawler.spider.skipped == {\"402\"}\n        assert crawler.spider.failed == {\"404\", \"500\"}\n\n        assert \"Ignoring response <404\" in str(log)\n        assert \"Ignoring response <500\" in str(log)\n        assert \"Ignoring response <200\" not in str(log)\n        assert \"Ignoring response <402\" not in str(log)\n\n    @inlineCallbacks\n    def test_logging_level(self):\n        # HttpError logs ignored responses with level INFO\n        crawler = get_crawler(_HttpErrorSpider)\n        with LogCapture(level=logging.INFO) as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert crawler.spider.parsed == {\"200\"}\n        assert crawler.spider.failed == {\"404\", \"402\", \"500\"}\n\n        assert \"Ignoring response <402\" in str(log)\n        assert \"Ignoring response <404\" in str(log)\n        assert \"Ignoring response <500\" in str(log)\n        assert \"Ignoring response <200\" not in str(log)\n\n        # with level WARNING, we shouldn't capture anything from HttpError\n        crawler = get_crawler(_HttpErrorSpider)\n        with LogCapture(level=logging.WARNING) as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        assert crawler.spider.parsed == {\"200\"}\n        assert crawler.spider.failed == {\"404\", \"402\", \"500\"}\n\n        assert \"Ignoring response <402\" not in str(log)\n        assert \"Ignoring response <404\" not in str(log)\n        assert \"Ignoring response <500\" not in str(log)\n        assert \"Ignoring response <200\" not in str(log)\n", "n_tokens": 841, "byte_len": 3732, "file_sha1": "8cfb7c5aa78b633ccf2bfa9fee66e9f664f13d0f", "start_line": 166, "end_line": 257}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_ftp.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_ftp.py", "rel_path": "tests/test_downloader_handler_twisted_ftp.py", "module": "tests.test_downloader_handler_twisted_ftp", "ext": "py", "chunk_number": 1, "symbols": ["_create_files", "_get_factory", "dh", "TestFTPBase", "test", "ftp", "async", "protocol", "local", "fname", "credentials", "deferred", "from", "bytes", "future", "typ", "checking", "spaces", "port", "path", "https", "interface", "get", "crawler", "username", "stop", "listening", "pytest", "userdir", "none", "TestFTP", "TestAnonymousFTP", "unidiomatic", "anonymous", "ianonymous", "handlers", "password", "fixture", "html", "without", "skipped", "type", "misc", "http", "memory", "size", "response", "await", "internet", "server"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom tempfile import mkstemp\nfrom typing import TYPE_CHECKING, Any\n\nimport pytest\nfrom pytest_twisted import async_yield_fixture\nfrom twisted.cred import checkers, credentials, portal\n\nfrom scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Generator\n\n\nclass TestFTPBase:\n    username = \"scrapy\"\n    password = \"passwd\"\n    req_meta: dict[str, Any] = {\"ftp_user\": username, \"ftp_password\": password}\n\n    test_files = (\n        (\"file.txt\", b\"I have the power!\"),\n        (\"file with spaces.txt\", b\"Moooooooooo power!\"),\n        (\"html-file-without-extension\", b\"<!DOCTYPE html>\\n<title>.</title>\"),\n    )\n\n    def _create_files(self, root: Path) -> None:\n        userdir = root / self.username\n        userdir.mkdir()\n        for filename, content in self.test_files:\n            (userdir / filename).write_bytes(content)\n\n    def _get_factory(self, root):\n        from twisted.protocols.ftp import FTPFactory, FTPRealm\n\n        realm = FTPRealm(anonymousRoot=str(root), userHome=str(root))\n        p = portal.Portal(realm)\n        users_checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()\n        users_checker.addUser(self.username, self.password)\n        p.registerChecker(users_checker, credentials.IUsernamePassword)\n        return FTPFactory(portal=p)\n\n    @async_yield_fixture\n    async def server_url(self, tmp_path: Path) -> AsyncGenerator[str]:\n        from twisted.internet import reactor\n\n        self._create_files(tmp_path)\n        factory = self._get_factory(tmp_path)\n        port = reactor.listenTCP(0, factory, interface=\"127.0.0.1\")\n        portno = port.getHost().port\n\n        yield f\"https://127.0.0.1:{portno}/\"\n\n        await port.stopListening()\n\n    @staticmethod\n    @pytest.fixture\n    def dh() -> Generator[FTPDownloadHandler]:\n        crawler = get_crawler()\n        dh = build_from_crawler(FTPDownloadHandler, crawler)\n\n        yield dh\n\n        # if the test was skipped, there will be no client attribute\n        if hasattr(dh, \"client\"):\n            assert dh.client.transport\n            dh.client.transport.loseConnection()\n\n    @staticmethod\n    async def download_request(dh: FTPDownloadHandler, request: Request) -> Response:\n        return await maybe_deferred_to_future(\n            dh.download_request(request, DefaultSpider())\n        )\n\n    @deferred_f_from_coro_f\n    async def test_ftp_download_success(\n        self, server_url: str, dh: FTPDownloadHandler\n    ) -> None:\n        request = Request(url=server_url + \"file.txt\", meta=self.req_meta)\n        r = await self.download_request(dh, request)\n        assert r.status == 200\n        assert r.body == b\"I have the power!\"\n        assert r.headers == {b\"Local Filename\": [b\"\"], b\"Size\": [b\"17\"]}\n        assert r.protocol is None\n\n    @deferred_f_from_coro_f\n    async def test_ftp_download_path_with_spaces(\n        self, server_url: str, dh: FTPDownloadHandler\n    ) -> None:\n        request = Request(\n            url=server_url + \"file with spaces.txt\",\n            meta=self.req_meta,\n        )\n        r = await self.download_request(dh, request)\n        assert r.status == 200\n        assert r.body == b\"Moooooooooo power!\"\n        assert r.headers == {b\"Local Filename\": [b\"\"], b\"Size\": [b\"18\"]}\n\n    @deferred_f_from_coro_f\n    async def test_ftp_download_nonexistent(\n        self, server_url: str, dh: FTPDownloadHandler\n    ) -> None:\n        request = Request(url=server_url + \"nonexistent.txt\", meta=self.req_meta)\n        r = await self.download_request(dh, request)\n        assert r.status == 404\n\n    @deferred_f_from_coro_f\n    async def test_ftp_local_filename(\n        self, server_url: str, dh: FTPDownloadHandler\n    ) -> None:\n        f, local_fname = mkstemp()\n        fname_bytes = to_bytes(local_fname)\n        local_path = Path(local_fname)\n        os.close(f)\n        meta = {\"ftp_local_filename\": fname_bytes}\n        meta.update(self.req_meta)\n        request = Request(url=server_url + \"file.txt\", meta=meta)\n        r = await self.download_request(dh, request)\n        assert r.body == fname_bytes\n        assert r.headers == {b\"Local Filename\": [fname_bytes], b\"Size\": [b\"17\"]}\n        assert local_path.exists()\n        assert local_path.read_bytes() == b\"I have the power!\"\n        local_path.unlink()\n\n    @pytest.mark.parametrize(\n        (\"filename\", \"response_class\"),\n        [\n            (\"file.txt\", TextResponse),\n            (\"html-file-without-extension\", HtmlResponse),\n        ],\n    )", "n_tokens": 1156, "byte_len": 4969, "file_sha1": "c0c7351fafa2e8ea56eae1e8afa41a37749eb3cf", "start_line": 1, "end_line": 141}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_ftp.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_downloader_handler_twisted_ftp.py", "rel_path": "tests/test_downloader_handler_twisted_ftp.py", "module": "tests.test_downloader_handler_twisted_ftp", "ext": "py", "chunk_number": 2, "symbols": ["_create_files", "_get_factory", "TestFTP", "TestAnonymousFTP", "unidiomatic", "async", "test", "ftp", "anonymous", "ianonymous", "local", "fname", "credentials", "deferred", "from", "path", "username", "pytest", "none", "type", "response", "await", "server", "url", "realm", "return", "update", "meta", "class", "invalid", "dh", "TestFTPBase", "protocol", "bytes", "future", "typ", "checking", "spaces", "port", "https", "interface", "get", "crawler", "stop", "listening", "userdir", "handlers", "password", "fixture", "html"], "ast_kind": "class_or_type", "text": "    @deferred_f_from_coro_f\n    async def test_response_class(\n        self,\n        filename: str,\n        response_class: type[Response],\n        server_url: str,\n        dh: FTPDownloadHandler,\n    ) -> None:\n        f, local_fname = mkstemp()\n        local_fname_path = Path(local_fname)\n        os.close(f)\n        meta = {}\n        meta.update(self.req_meta)\n        request = Request(url=server_url + filename, meta=meta)\n        r = await self.download_request(dh, request)\n        assert type(r) is response_class  # pylint: disable=unidiomatic-typecheck\n        local_fname_path.unlink()\n\n\nclass TestFTP(TestFTPBase):\n    @deferred_f_from_coro_f\n    async def test_invalid_credentials(\n        self, server_url: str, dh: FTPDownloadHandler, reactor_pytest: str\n    ) -> None:\n        if reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n            pytest.skip(\n                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n            )\n\n        from twisted.protocols.ftp import ConnectionLost\n\n        meta = dict(self.req_meta)\n        meta.update({\"ftp_password\": \"invalid\"})\n        request = Request(url=server_url + \"file.txt\", meta=meta)\n        with pytest.raises(ConnectionLost):\n            await self.download_request(dh, request)\n\n\nclass TestAnonymousFTP(TestFTPBase):\n    username = \"anonymous\"\n    req_meta = {}\n\n    def _create_files(self, root: Path) -> None:\n        for filename, content in self.test_files:\n            (root / filename).write_bytes(content)\n\n    def _get_factory(self, tmp_path):\n        from twisted.protocols.ftp import FTPFactory, FTPRealm\n\n        realm = FTPRealm(anonymousRoot=str(tmp_path))\n        p = portal.Portal(realm)\n        p.registerChecker(checkers.AllowAnonymousAccess(), credentials.IAnonymous)\n        return FTPFactory(portal=p, userAnonymous=self.username)\n", "n_tokens": 415, "byte_len": 1860, "file_sha1": "c0c7351fafa2e8ea56eae1e8afa41a37749eb3cf", "start_line": 142, "end_line": 195}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_asyncio.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_asyncio.py", "rel_path": "tests/test_utils_asyncio.py", "module": "tests.test_utils_asyncio", "ext": "py", "chunk_number": 1, "symbols": ["test_is_asyncio_available", "get_async_iterable", "test_looping_call", "test_looping_call_now", "test_looping_call_already_running", "test_looping_call_interval", "test_looping_call_bad_function", "TestAsyncio", "TestParallelAsyncio", "TestAsyncioLoopingCall", "async", "callable", "wrapped", "append", "looping", "call", "test", "asyncio", "deferred", "from", "sleep", "between", "future", "typ", "checking", "length", "mock", "get", "pytest", "generator", "trivial", "than", "utils", "results", "none", "stop", "without", "sorted", "runtime", "error", "match", "parallel", "based", "await", "internet", "argument", "greater", "typing", "return", "annotations"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport random\nfrom typing import TYPE_CHECKING\nfrom unittest import mock\n\nimport pytest\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils.asyncgen import as_async_generator\nfrom scrapy.utils.asyncio import (\n    AsyncioLoopingCall,\n    _parallel_asyncio,\n    is_asyncio_available,\n)\nfrom scrapy.utils.defer import deferred_f_from_coro_f\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator\n\n\nclass TestAsyncio:\n    def test_is_asyncio_available(self, reactor_pytest: str) -> None:\n        # the result should depend only on the pytest --reactor argument\n        assert is_asyncio_available() == (reactor_pytest == \"asyncio\")\n\n\n@pytest.mark.only_asyncio\nclass TestParallelAsyncio:\n    \"\"\"Test for scrapy.utils.asyncio.parallel_asyncio(), based on tests.test_utils_defer.TestParallelAsync.\"\"\"\n\n    CONCURRENT_ITEMS = 50\n\n    @staticmethod\n    async def callable(o: int, results: list[int]) -> None:\n        if random.random() < 0.4:\n            # simulate async processing\n            await asyncio.sleep(random.random() / 8)\n        # simulate trivial sync processing\n        results.append(o)\n\n    async def callable_wrapped(\n        self,\n        o: int,\n        results: list[int],\n        parallel_count: list[int],\n        max_parallel_count: list[int],\n    ) -> None:\n        parallel_count[0] += 1\n        max_parallel_count[0] = max(max_parallel_count[0], parallel_count[0])\n        await self.callable(o, results)\n        assert parallel_count[0] > 0, parallel_count[0]\n        parallel_count[0] -= 1\n\n    @staticmethod\n    def get_async_iterable(length: int) -> AsyncGenerator[int, None]:\n        # simulate a simple callback without delays between results\n        return as_async_generator(range(length))\n\n    @staticmethod\n    async def get_async_iterable_with_delays(length: int) -> AsyncGenerator[int, None]:\n        # simulate a callback with delays between some of the results\n        for i in range(length):\n            if random.random() < 0.1:\n                await asyncio.sleep(random.random() / 20)\n            yield i\n\n    @deferred_f_from_coro_f\n    async def test_simple(self):\n        for length in [20, 50, 100]:\n            parallel_count = [0]\n            max_parallel_count = [0]\n            results = []\n            ait = self.get_async_iterable(length)\n            await _parallel_asyncio(\n                ait,\n                self.CONCURRENT_ITEMS,\n                self.callable_wrapped,\n                results,\n                parallel_count,\n                max_parallel_count,\n            )\n            assert list(range(length)) == sorted(results)\n            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS\n\n    @deferred_f_from_coro_f\n    async def test_delays(self):\n        for length in [20, 50, 100]:\n            parallel_count = [0]\n            max_parallel_count = [0]\n            results = []\n            ait = self.get_async_iterable_with_delays(length)\n            await _parallel_asyncio(\n                ait,\n                self.CONCURRENT_ITEMS,\n                self.callable_wrapped,\n                results,\n                parallel_count,\n                max_parallel_count,\n            )\n            assert list(range(length)) == sorted(results)\n            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS\n\n\n@pytest.mark.only_asyncio\nclass TestAsyncioLoopingCall:\n    def test_looping_call(self):\n        func = mock.MagicMock()\n        looping_call = AsyncioLoopingCall(func)\n        looping_call.start(1, now=False)\n        assert looping_call.running\n        looping_call.stop()\n        assert not looping_call.running\n        assert not func.called\n\n    def test_looping_call_now(self):\n        func = mock.MagicMock()\n        looping_call = AsyncioLoopingCall(func)\n        looping_call.start(1)\n        looping_call.stop()\n        assert func.called\n\n    def test_looping_call_already_running(self):\n        looping_call = AsyncioLoopingCall(lambda: None)\n        looping_call.start(1)\n        with pytest.raises(RuntimeError):\n            looping_call.start(1)\n        looping_call.stop()\n\n    def test_looping_call_interval(self):\n        looping_call = AsyncioLoopingCall(lambda: None)\n        with pytest.raises(ValueError, match=\"Interval must be greater than 0\"):\n            looping_call.start(0)\n        with pytest.raises(ValueError, match=\"Interval must be greater than 0\"):\n            looping_call.start(-1)\n        assert not looping_call.running\n\n    def test_looping_call_bad_function(self):\n        looping_call = AsyncioLoopingCall(Deferred)\n        with pytest.raises(TypeError):\n            looping_call.start(0.1)\n        assert not looping_call.running\n", "n_tokens": 1008, "byte_len": 4725, "file_sha1": "daa8a2e8d4b50e449aa8a38e10b3ca0598f8030a", "start_line": 1, "end_line": 144}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider_start.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spider_start.py", "rel_path": "tests/test_spider_start.py", "module": "tests.test_spider_start", "ext": "py", "chunk_number": 1, "symbols": ["track_item", "start_requests", "TestMain", "TestSpider", "BaseSpider", "method", "async", "append", "subclass", "spider", "name", "deferred", "from", "about", "sleep", "deprecated", "future", "test", "get", "crawler", "start", "defines", "requests", "pytest", "none", "finish", "reason", "type", "parse", "runtime", "error", "actual", "items", "value", "match", "ite", "item", "await", "twisted", "typing", "annotations", "class", "base", "warnings", "universal", "list", "signals", "exceptions", "main", "super"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\nfrom asyncio import sleep\nfrom typing import Any\n\nimport pytest\nfrom testfixtures import LogCapture\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\n\nfrom .utils import twisted_sleep\n\nSLEEP_SECONDS = 0.1\n\nITEM_A = {\"id\": \"a\"}\nITEM_B = {\"id\": \"b\"}\n\n\nclass TestMain:\n    async def _test_spider(\n        self, spider: type[Spider], expected_items: list[Any] | None = None\n    ) -> None:\n        actual_items = []\n        expected_items = [] if expected_items is None else expected_items\n\n        def track_item(item, response, spider):\n            actual_items.append(item)\n\n        crawler = get_crawler(spider)\n        crawler.signals.connect(track_item, signals.item_scraped)\n        await maybe_deferred_to_future(crawler.crawl())\n        assert crawler.stats\n        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n        assert actual_items == expected_items\n\n    @deferred_f_from_coro_f\n    async def test_start_urls(self):\n        class TestSpider(Spider):\n            name = \"test\"\n            start_urls = [\"data:,\"]\n\n            async def parse(self, response):\n                yield ITEM_A\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_start(self):\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                yield ITEM_A\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_start_subclass(self):\n        class BaseSpider(Spider):\n            async def start(self):\n                yield ITEM_A\n\n        class TestSpider(BaseSpider):\n            name = \"test\"\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_deprecated(self):\n        class TestSpider(Spider):\n            name = \"test\"\n\n            def start_requests(self):\n                yield ITEM_A\n\n        with pytest.warns(ScrapyDeprecationWarning):\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_deprecated_subclass(self):\n        class BaseSpider(Spider):\n            def start_requests(self):\n                yield ITEM_A\n\n        class TestSpider(BaseSpider):\n            name = \"test\"\n\n        # The warning must be about the base class and not the subclass.\n        with pytest.warns(ScrapyDeprecationWarning, match=\"BaseSpider\"):\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_universal(self):\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                yield ITEM_A\n\n            def start_requests(self):\n                yield ITEM_B\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_universal_subclass(self):\n        class BaseSpider(Spider):\n            async def start(self):\n                yield ITEM_A\n\n            def start_requests(self):\n                yield ITEM_B\n\n        class TestSpider(BaseSpider):\n            name = \"test\"\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            await self._test_spider(TestSpider, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_start_deprecated_super(self):\n        class TestSpider(Spider):\n            name = \"test\"\n\n            async def start(self):\n                for item_or_request in super().start_requests():\n                    yield item_or_request\n\n        with pytest.warns(\n            ScrapyDeprecationWarning, match=r\"use Spider\\.start\\(\\) instead\"\n        ) as messages:\n            await self._test_spider(TestSpider, [])\n        assert messages[0].filename.endswith(\"test_spider_start.py\")\n\n    async def _test_start(self, start_, expected_items=None):\n        class TestSpider(Spider):\n            name = \"test\"\n            start = start_\n\n        await self._test_spider(TestSpider, expected_items)\n\n    @pytest.mark.only_asyncio\n    @deferred_f_from_coro_f\n    async def test_asyncio_delayed(self):\n        async def start(spider):\n            await sleep(SLEEP_SECONDS)\n            yield ITEM_A\n\n        await self._test_start(start, [ITEM_A])\n\n    @deferred_f_from_coro_f\n    async def test_twisted_delayed(self):\n        async def start(spider):\n            await maybe_deferred_to_future(twisted_sleep(SLEEP_SECONDS))\n            yield ITEM_A\n\n        await self._test_start(start, [ITEM_A])\n\n    # Exceptions\n\n    @deferred_f_from_coro_f\n    async def test_deprecated_non_generator_exception(self):\n        class TestSpider(Spider):\n            name = \"test\"\n\n            def start_requests(self):\n                raise RuntimeError\n\n        with (\n            LogCapture() as log,\n            pytest.warns(\n                ScrapyDeprecationWarning,\n                match=r\"defines the deprecated start_requests\\(\\) method\",\n            ),\n        ):\n            await self._test_spider(TestSpider, [])\n\n        assert \"in start_requests\\n    raise RuntimeError\" in str(log)\n", "n_tokens": 1207, "byte_len": 5597, "file_sha1": "ffc4b759b3759c031ae0405fa089a3ee42c26b70", "start_line": 1, "end_line": 192}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_sitemap.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_sitemap.py", "rel_path": "tests/test_utils_sitemap.py", "module": "tests.test_utils_sitemap", "ext": "py", "chunk_number": 1, "symbols": ["test_sitemap", "test_sitemap_index", "test_sitemap_strip", "test_sitemap_wrong_ns", "encoding", "sitemap", "sitemap1", "those", "works", "test", "changefreq", "seen", "still", "priority", "these", "deal", "wrongs", "google", "special", "with", "scrapy", "example", "spaces", "sitemap2", "xmlns", "lastmod", "daily", "sitemapindex", "urls", "presumably", "test_sitemap_wrong_ns2", "test_sitemap_urls_from_robots", "test_sitemap_blanklines", "test_comment", "test_alternate", "test_xml_entity_expansion", "starting", "element", "entity", "robots", "name", "alternate", "href", "base", "url", "html", "type", "comment", "without", "jsp"], "ast_kind": "function_or_method", "text": "from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n\n\ndef test_sitemap():\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n<url>\n<loc>http://www.example.com/</loc>\n<lastmod>2009-08-16</lastmod>\n<changefreq>daily</changefreq>\n<priority>1</priority>\n</url>\n<url>\n<loc>http://www.example.com/Special-Offers.html</loc>\n<lastmod>2009-08-16</lastmod>\n<changefreq>weekly</changefreq>\n<priority>0.8</priority>\n</url>\n</urlset>\"\"\"\n    )\n    assert s.type == \"urlset\"\n    assert list(s) == [\n        {\n            \"priority\": \"1\",\n            \"loc\": \"http://www.example.com/\",\n            \"lastmod\": \"2009-08-16\",\n            \"changefreq\": \"daily\",\n        },\n        {\n            \"priority\": \"0.8\",\n            \"loc\": \"http://www.example.com/Special-Offers.html\",\n            \"lastmod\": \"2009-08-16\",\n            \"changefreq\": \"weekly\",\n        },\n    ]\n\n\ndef test_sitemap_index():\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<sitemap>\n  <loc>http://www.example.com/sitemap1.xml.gz</loc>\n  <lastmod>2004-10-01T18:23:17+00:00</lastmod>\n</sitemap>\n<sitemap>\n  <loc>http://www.example.com/sitemap2.xml.gz</loc>\n  <lastmod>2005-01-01</lastmod>\n</sitemap>\n</sitemapindex>\"\"\"\n    )\n    assert s.type == \"sitemapindex\"\n    assert list(s) == [\n        {\n            \"loc\": \"http://www.example.com/sitemap1.xml.gz\",\n            \"lastmod\": \"2004-10-01T18:23:17+00:00\",\n        },\n        {\n            \"loc\": \"http://www.example.com/sitemap2.xml.gz\",\n            \"lastmod\": \"2005-01-01\",\n        },\n    ]\n\n\ndef test_sitemap_strip():\n    \"\"\"Assert we can deal with trailing spaces inside <loc> tags - we've\n    seen those\n    \"\"\"\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n<url>\n<loc> http://www.example.com/</loc>\n<lastmod>2009-08-16</lastmod>\n<changefreq>daily</changefreq>\n<priority>1</priority>\n</url>\n<url>\n<loc> http://www.example.com/2</loc>\n<lastmod />\n</url>\n</urlset>\n\"\"\"\n    )\n    assert list(s) == [\n        {\n            \"priority\": \"1\",\n            \"loc\": \"http://www.example.com/\",\n            \"lastmod\": \"2009-08-16\",\n            \"changefreq\": \"daily\",\n        },\n        {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n    ]\n\n\ndef test_sitemap_wrong_ns():\n    \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n    with these, though is not 100% confirmed\"\"\"\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n<url xmlns=\"\">\n<loc> http://www.example.com/</loc>\n<lastmod>2009-08-16</lastmod>\n<changefreq>daily</changefreq>\n<priority>1</priority>\n</url>\n<url xmlns=\"\">\n<loc> http://www.example.com/2</loc>\n<lastmod />\n</url>\n</urlset>\n\"\"\"\n    )\n    assert list(s) == [\n        {\n            \"priority\": \"1\",\n            \"loc\": \"http://www.example.com/\",\n            \"lastmod\": \"2009-08-16\",\n            \"changefreq\": \"daily\",\n        },\n        {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n    ]\n\n", "n_tokens": 983, "byte_len": 3172, "file_sha1": "8d7bf7cf55c79a8868dbb389a8180dfd3d383ed8", "start_line": 1, "end_line": 126}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_sitemap.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_sitemap.py", "rel_path": "tests/test_utils_sitemap.py", "module": "tests.test_utils_sitemap", "ext": "py", "chunk_number": 2, "symbols": ["test_sitemap_wrong_ns2", "test_sitemap_urls_from_robots", "test_sitemap_blanklines", "test_comment", "test_alternate", "encoding", "test", "sitemap", "sitemap1", "starting", "changefreq", "seen", "robots", "name", "alternate", "daily", "href", "base", "url", "type", "comment", "without", "jsp", "http", "deutsch", "tags", "cache", "implemented", "google", "search", "test_sitemap", "test_sitemap_index", "test_sitemap_strip", "test_sitemap_wrong_ns", "test_xml_entity_expansion", "those", "element", "entity", "spaces", "html", "files", "shopping", "example", "xmlns", "presumably", "though", "facet", "urls", "refining", "list"], "ast_kind": "function_or_method", "text": "def test_sitemap_wrong_ns2():\n    \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n    with these, though is not 100% confirmed\"\"\"\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset>\n<url xmlns=\"\">\n<loc> http://www.example.com/</loc>\n<lastmod>2009-08-16</lastmod>\n<changefreq>daily</changefreq>\n<priority>1</priority>\n</url>\n<url xmlns=\"\">\n<loc> http://www.example.com/2</loc>\n<lastmod />\n</url>\n</urlset>\n\"\"\"\n    )\n    assert s.type == \"urlset\"\n    assert list(s) == [\n        {\n            \"priority\": \"1\",\n            \"loc\": \"http://www.example.com/\",\n            \"lastmod\": \"2009-08-16\",\n            \"changefreq\": \"daily\",\n        },\n        {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n    ]\n\n\ndef test_sitemap_urls_from_robots():\n    robots = \"\"\"User-agent: *\nDisallow: /aff/\nDisallow: /wl/\n\n# Search and shopping refining\nDisallow: /s*/*facet\nDisallow: /s*/*tags\n\n# Sitemap files\nSitemap: http://example.com/sitemap.xml\nSitemap: http://example.com/sitemap-product-index.xml\nSitemap: HTTP://example.com/sitemap-uppercase.xml\nSitemap: /sitemap-relative-url.xml\n\n# Forums\nDisallow: /forum/search/\nDisallow: /forum/active/\n\"\"\"\n    assert list(sitemap_urls_from_robots(robots, base_url=\"http://example.com\")) == [\n        \"http://example.com/sitemap.xml\",\n        \"http://example.com/sitemap-product-index.xml\",\n        \"http://example.com/sitemap-uppercase.xml\",\n        \"http://example.com/sitemap-relative-url.xml\",\n    ]\n\n\ndef test_sitemap_blanklines():\n    \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n    s = Sitemap(\n        b\"\"\"\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n\n<!-- cache: cached = yes name = sitemap_jspCache key = sitemap -->\n<sitemap>\n<loc>http://www.example.com/sitemap1.xml</loc>\n<lastmod>2013-07-15</lastmod>\n</sitemap>\n\n<sitemap>\n<loc>http://www.example.com/sitemap2.xml</loc>\n<lastmod>2013-07-15</lastmod>\n</sitemap>\n\n<sitemap>\n<loc>http://www.example.com/sitemap3.xml</loc>\n<lastmod>2013-07-15</lastmod>\n</sitemap>\n\n<!-- end cache -->\n</sitemapindex>\n\"\"\"\n    )\n    assert list(s) == [\n        {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap1.xml\"},\n        {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap2.xml\"},\n        {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap3.xml\"},\n    ]\n\n\ndef test_comment():\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n    xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n    <url>\n        <loc>http://www.example.com/</loc>\n        <!-- this is a comment on which the parser might raise an exception if implemented incorrectly -->\n    </url>\n</urlset>\"\"\"\n    )\n    assert list(s) == [{\"loc\": \"http://www.example.com/\"}]\n\n\ndef test_alternate():\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n    xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n    <url>\n        <loc>http://www.example.com/english/</loc>\n        <xhtml:link rel=\"alternate\" hreflang=\"de\"\n            href=\"http://www.example.com/deutsch/\"/>\n        <xhtml:link rel=\"alternate\" hreflang=\"de-ch\"\n            href=\"http://www.example.com/schweiz-deutsch/\"/>\n        <xhtml:link rel=\"alternate\" hreflang=\"en\"\n            href=\"http://www.example.com/english/\"/>\n        <xhtml:link rel=\"alternate\" hreflang=\"en\"/><!-- wrong tag without href -->\n    </url>\n</urlset>\"\"\"\n    )\n    assert list(s) == [\n        {\n            \"loc\": \"http://www.example.com/english/\",\n            \"alternate\": [\n                \"http://www.example.com/deutsch/\",\n                \"http://www.example.com/schweiz-deutsch/\",\n                \"http://www.example.com/english/\",\n            ],\n        }\n    ]\n\n", "n_tokens": 1098, "byte_len": 3855, "file_sha1": "8d7bf7cf55c79a8868dbb389a8180dfd3d383ed8", "start_line": 127, "end_line": 261}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_sitemap.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_sitemap.py", "rel_path": "tests/test_utils_sitemap.py", "module": "tests.test_utils_sitemap", "ext": "py", "chunk_number": 3, "symbols": ["test_xml_entity_expansion", "encoding", "version", "element", "list", "system", "entity", "http", "sitemap", "assert", "urlset", "sitemaps", "doctype", "schemas", "passwd", "xmlns", "file", "test", "xml", "test_sitemap", "test_sitemap_index", "test_sitemap_strip", "test_sitemap_wrong_ns", "test_sitemap_wrong_ns2", "test_sitemap_urls_from_robots", "test_sitemap_blanklines", "test_comment", "test_alternate", "sitemap1", "those", "starting", "changefreq", "seen", "robots", "name", "spaces", "alternate", "daily", "google", "href", "base", "url", "html", "type", "comment", "without", "jsp", "deutsch", "tags", "cache"], "ast_kind": "function_or_method", "text": "def test_xml_entity_expansion():\n    s = Sitemap(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n      <!DOCTYPE foo [\n      <!ELEMENT foo ANY >\n      <!ENTITY xxe SYSTEM \"file:///etc/passwd\" >\n      ]>\n      <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n        <url>\n          <loc>http://127.0.0.1:8000/&xxe;</loc>\n        </url>\n      </urlset>\n    \"\"\"\n    )\n    assert list(s) == [{\"loc\": \"http://127.0.0.1:8000/\"}]\n", "n_tokens": 144, "byte_len": 440, "file_sha1": "8d7bf7cf55c79a8868dbb389a8180dfd3d383ed8", "start_line": 262, "end_line": 277}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_stop_download_headers.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_engine_stop_download_headers.py", "rel_path": "tests/test_engine_stop_download_headers.py", "module": "tests.test_engine_stop_download_headers", "ext": "py", "chunk_number": 1, "symbols": ["headers_received", "_assert_bytes_received", "_assert_visited_urls", "HeadersReceivedCrawlerRun", "TestHeadersReceivedEngine", "respplug", "downloader", "async", "core", "headers", "received", "false", "crawler", "run", "await", "http", "http11", "visited", "spider", "signal", "typing", "test", "engine", "annotations", "deferred", "from", "class", "with", "urls", "expected", "scrapy", "assert", "signals", "future", "typ", "checking", "testfixtures", "defer", "myspider", "mockserver", "bytes", "staticmethod", "numbers", "redirect", "geturl", "must", "list", "attrs", "items", "body"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom testfixtures import LogCapture\n\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom tests.test_engine import (\n    AttrsItemsSpider,\n    CrawlerRun,\n    DataClassItemsSpider,\n    DictItemsSpider,\n    MySpider,\n    TestEngineBase,\n)\n\nif TYPE_CHECKING:\n    from tests.mockserver.http import MockServer\n\n\nclass HeadersReceivedCrawlerRun(CrawlerRun):\n    def headers_received(self, headers, body_length, request, spider):\n        super().headers_received(headers, body_length, request, spider)\n        raise StopDownload(fail=False)\n\n\nclass TestHeadersReceivedEngine(TestEngineBase):\n    @deferred_f_from_coro_f\n    async def test_crawler(self, mockserver: MockServer) -> None:\n        for spider in (\n            MySpider,\n            DictItemsSpider,\n            AttrsItemsSpider,\n            DataClassItemsSpider,\n        ):\n            run = HeadersReceivedCrawlerRun(spider)\n            with LogCapture() as log:\n                await run.run(mockserver)\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET {mockserver.url('/redirected')}> from\"\n                        \" signal handler HeadersReceivedCrawlerRun.headers_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET {mockserver.url('/static/')}> from signal\"\n                        \" handler HeadersReceivedCrawlerRun.headers_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET {mockserver.url('/numbers')}> from\"\n                        \" signal handler HeadersReceivedCrawlerRun.headers_received\",\n                    )\n                )\n            self._assert_visited_urls(run)\n            self._assert_downloaded_responses(run, count=6)\n            self._assert_signals_caught(run)\n            self._assert_bytes_received(run)\n            self._assert_headers_received(run)\n\n    @staticmethod\n    def _assert_bytes_received(run: CrawlerRun) -> None:\n        assert len(run.bytes) == 0\n\n    @staticmethod\n    def _assert_visited_urls(run: CrawlerRun) -> None:\n        must_be_visited = [\"/static/\", \"/redirect\", \"/redirected\"]\n        urls_visited = {rp[0].url for rp in run.respplug}\n        urls_expected = {run.geturl(p) for p in must_be_visited}\n        assert urls_expected <= urls_visited, (\n            f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n        )\n", "n_tokens": 570, "byte_len": 2937, "file_sha1": "ddfb88c470013472fad6fe518b5e2393370ec44e", "start_line": 1, "end_line": 82}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_startproject.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_startproject.py", "rel_path": "tests/test_command_startproject.py", "module": "tests.test_command_startproject", "ext": "py", "chunk_number": 1, "symbols": ["test_startproject", "test_startproject_with_project_dir", "test_existing_project_dir", "get_permissions_dict", "get_permissions", "setup_method", "test_startproject_template_override", "TestStartprojectCommand", "TestStartprojectTemplates", "stat", "name", "tmpl", "proj", "spiders", "future", "path", "obj", "contextlib", "settings", "items", "replacement", "none", "like", "template", "dir", "test", "startproject", "contextmanager", "mkdtemp", "return", "test_startproject_permissions_from_writable", "test_startproject_permissions_from_read_only", "_make_read_only", "test_startproject_permissions_unchanged_in_destination", "test_startproject_permissions_umask_022", "umask", "those", "made", "https", "pull", "system", "process", "than", "actual", "permissions", "umaskproject", "check", "make", "read", "existing"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom contextlib import contextmanager\nfrom itertools import chain\nfrom pathlib import Path\nfrom shutil import copytree\nfrom stat import S_IWRITE as ANYONE_WRITE_PERMISSION\nfrom tempfile import mkdtemp\n\nimport scrapy\nfrom scrapy.commands.startproject import IGNORE\nfrom tests.test_commands import TestProjectBase\n\n\nclass TestStartprojectCommand(TestProjectBase):\n    def test_startproject(self):\n        p, out, err = self.proc(\"startproject\", self.project_name)\n        print(out)\n        print(err, file=sys.stderr)\n        assert p.returncode == 0\n\n        assert Path(self.proj_path, \"scrapy.cfg\").exists()\n        assert Path(self.proj_path, \"testproject\").exists()\n        assert Path(self.proj_mod_path, \"__init__.py\").exists()\n        assert Path(self.proj_mod_path, \"items.py\").exists()\n        assert Path(self.proj_mod_path, \"pipelines.py\").exists()\n        assert Path(self.proj_mod_path, \"settings.py\").exists()\n        assert Path(self.proj_mod_path, \"spiders\", \"__init__.py\").exists()\n\n        assert self.call(\"startproject\", self.project_name) == 1\n        assert self.call(\"startproject\", \"wrong---project---name\") == 1\n        assert self.call(\"startproject\", \"sys\") == 1\n\n    def test_startproject_with_project_dir(self):\n        project_dir = mkdtemp()\n        assert self.call(\"startproject\", self.project_name, project_dir) == 0\n\n        assert Path(project_dir, \"scrapy.cfg\").exists()\n        assert Path(project_dir, \"testproject\").exists()\n        assert Path(project_dir, self.project_name, \"__init__.py\").exists()\n        assert Path(project_dir, self.project_name, \"items.py\").exists()\n        assert Path(project_dir, self.project_name, \"pipelines.py\").exists()\n        assert Path(project_dir, self.project_name, \"settings.py\").exists()\n        assert Path(project_dir, self.project_name, \"spiders\", \"__init__.py\").exists()\n\n        assert self.call(\"startproject\", self.project_name, project_dir + \"2\") == 0\n\n        assert self.call(\"startproject\", self.project_name, project_dir) == 1\n        assert self.call(\"startproject\", self.project_name + \"2\", project_dir) == 1\n        assert self.call(\"startproject\", \"wrong---project---name\") == 1\n        assert self.call(\"startproject\", \"sys\") == 1\n        assert self.call(\"startproject\") == 2\n        assert (\n            self.call(\"startproject\", self.project_name, project_dir, \"another_params\")\n            == 2\n        )\n\n    def test_existing_project_dir(self):\n        project_dir = mkdtemp()\n        project_name = self.project_name + \"_existing\"\n        project_path = Path(project_dir, project_name)\n        project_path.mkdir()\n\n        p, out, err = self.proc(\"startproject\", project_name, cwd=project_dir)\n        print(out)\n        print(err, file=sys.stderr)\n        assert p.returncode == 0\n\n        assert Path(project_path, \"scrapy.cfg\").exists()\n        assert Path(project_path, project_name).exists()\n        assert Path(project_path, project_name, \"__init__.py\").exists()\n        assert Path(project_path, project_name, \"items.py\").exists()\n        assert Path(project_path, project_name, \"pipelines.py\").exists()\n        assert Path(project_path, project_name, \"settings.py\").exists()\n        assert Path(project_path, project_name, \"spiders\", \"__init__.py\").exists()\n\n\ndef get_permissions_dict(\n    path: str | os.PathLike, renamings=None, ignore=None\n) -> dict[str, str]:\n    def get_permissions(path: Path) -> str:\n        return oct(path.stat().st_mode)\n\n    path_obj = Path(path)\n\n    renamings = renamings or ()\n    permissions_dict = {\n        \".\": get_permissions(path_obj),\n    }\n    for root, dirs, files in os.walk(path_obj):\n        nodes = list(chain(dirs, files))\n        if ignore:\n            ignored_names = ignore(root, nodes)\n            nodes = [node for node in nodes if node not in ignored_names]\n        for node in nodes:\n            absolute_path = Path(root, node)\n            relative_path = str(absolute_path.relative_to(path))\n            for search_string, replacement in renamings:\n                relative_path = relative_path.replace(search_string, replacement)\n            permissions = get_permissions(absolute_path)\n            permissions_dict[relative_path] = permissions\n    return permissions_dict\n\n\nclass TestStartprojectTemplates(TestProjectBase):\n    def setup_method(self):\n        super().setup_method()\n        self.tmpl = str(Path(self.temp_path, \"templates\"))\n        self.tmpl_proj = str(Path(self.tmpl, \"project\"))\n\n    def test_startproject_template_override(self):\n        copytree(Path(scrapy.__path__[0], \"templates\"), self.tmpl)\n        Path(self.tmpl_proj, \"root_template\").write_bytes(b\"\")\n        assert Path(self.tmpl_proj, \"root_template\").exists()\n\n        args = [\"--set\", f\"TEMPLATES_DIR={self.tmpl}\"]\n        p, out, err = self.proc(\"startproject\", self.project_name, *args)\n        assert (\n            f\"New Scrapy project '{self.project_name}', using template directory\" in out\n        )\n        assert self.tmpl_proj in out\n        assert Path(self.proj_path, \"root_template\").exists()\n", "n_tokens": 1161, "byte_len": 5131, "file_sha1": "5ec38d065d9474bc34a2e2a43c30fcc762933c3e", "start_line": 1, "end_line": 126}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_startproject.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_command_startproject.py", "rel_path": "tests/test_command_startproject.py", "module": "tests.test_command_startproject", "ext": "py", "chunk_number": 2, "symbols": ["test_startproject_permissions_from_writable", "test_startproject_permissions_from_read_only", "_make_read_only", "test_startproject_permissions_unchanged_in_destination", "test_startproject_permissions_umask_022", "umask", "those", "stat", "made", "test", "startproject", "https", "path", "pull", "system", "tmpl", "items", "process", "than", "actual", "permissions", "umaskproject", "template", "dir", "make", "read", "check", "existing", "nodes", "scrapy", "test_startproject", "test_startproject_with_project_dir", "test_existing_project_dir", "get_permissions_dict", "get_permissions", "setup_method", "test_startproject_template_override", "TestStartprojectCommand", "TestStartprojectTemplates", "name", "proj", "spiders", "future", "obj", "contextlib", "settings", "replacement", "none", "like", "chmod"], "ast_kind": "function_or_method", "text": "    def test_startproject_permissions_from_writable(self):\n        \"\"\"Check that generated files have the right permissions when the\n        template folder has the same permissions as in the project, i.e.\n        everything is writable.\"\"\"\n        scrapy_path = scrapy.__path__[0]\n        project_template = Path(scrapy_path, \"templates\", \"project\")\n        project_name = \"startproject1\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        destination = mkdtemp()\n        process = subprocess.Popen(\n            (\n                sys.executable,\n                \"-m\",\n                \"scrapy.cmdline\",\n                \"startproject\",\n                project_name,\n            ),\n            cwd=destination,\n            env=self.env,\n        )\n        process.wait()\n\n        project_dir = Path(destination, project_name)\n        actual_permissions = get_permissions_dict(project_dir)\n\n        assert actual_permissions == expected_permissions\n\n    def test_startproject_permissions_from_read_only(self):\n        \"\"\"Check that generated files have the right permissions when the\n        template folder has been made read-only, which is something that some\n        systems do.\n\n        See https://github.com/scrapy/scrapy/pull/4604\n        \"\"\"\n        scrapy_path = scrapy.__path__[0]\n        templates_dir = Path(scrapy_path, \"templates\")\n        project_template = Path(templates_dir, \"project\")\n        project_name = \"startproject2\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        def _make_read_only(path: Path):\n            current_permissions = path.stat().st_mode\n            path.chmod(current_permissions & ~ANYONE_WRITE_PERMISSION)\n\n        read_only_templates_dir = str(Path(mkdtemp()) / \"templates\")\n        copytree(templates_dir, read_only_templates_dir)\n\n        for root, dirs, files in os.walk(read_only_templates_dir):\n            for node in chain(dirs, files):\n                _make_read_only(Path(root, node))\n\n        destination = mkdtemp()\n        process = subprocess.Popen(\n            (\n                sys.executable,\n                \"-m\",\n                \"scrapy.cmdline\",\n                \"startproject\",\n                project_name,\n                \"--set\",\n                f\"TEMPLATES_DIR={read_only_templates_dir}\",\n            ),\n            cwd=destination,\n            env=self.env,\n        )\n        process.wait()\n\n        project_dir = Path(destination, project_name)\n        actual_permissions = get_permissions_dict(project_dir)\n\n        assert actual_permissions == expected_permissions\n\n    def test_startproject_permissions_unchanged_in_destination(self):\n        \"\"\"Check that preexisting folders and files in the destination folder\n        do not see their permissions modified.\"\"\"\n        scrapy_path = scrapy.__path__[0]\n        project_template = Path(scrapy_path, \"templates\", \"project\")\n        project_name = \"startproject3\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        destination = mkdtemp()\n        project_dir = Path(destination, project_name)\n\n        existing_nodes = {\n            oct(permissions)[2:] + extension: permissions\n            for extension in (\"\", \".d\")\n            for permissions in (\n                0o444,\n                0o555,\n                0o644,\n                0o666,\n                0o755,\n                0o777,\n            )\n        }\n        project_dir.mkdir()\n        for node, permissions in existing_nodes.items():\n            path = project_dir / node\n            if node.endswith(\".d\"):\n                path.mkdir(mode=permissions)\n            else:\n                path.touch(mode=permissions)\n            expected_permissions[node] = oct(path.stat().st_mode)\n\n        process = subprocess.Popen(\n            (\n                sys.executable,\n                \"-m\",\n                \"scrapy.cmdline\",\n                \"startproject\",\n                project_name,\n                \".\",\n            ),\n            cwd=project_dir,\n            env=self.env,\n        )\n        process.wait()\n\n        actual_permissions = get_permissions_dict(project_dir)\n\n        assert actual_permissions == expected_permissions\n\n    def test_startproject_permissions_umask_022(self):\n        \"\"\"Check that generated files have the right permissions when the\n        system uses a umask value that causes new files to have different\n        permissions than those from the template folder.\"\"\"\n\n        @contextmanager\n        def umask(new_mask):\n            cur_mask = os.umask(new_mask)\n            yield\n            os.umask(cur_mask)\n\n        scrapy_path = scrapy.__path__[0]\n        project_template = Path(scrapy_path, \"templates\", \"project\")\n        project_name = \"umaskproject\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        with umask(0o002):\n            destination = mkdtemp()\n            process = subprocess.Popen(\n                (\n                    sys.executable,\n                    \"-m\",\n                    \"scrapy.cmdline\",\n                    \"startproject\",\n                    project_name,\n                ),\n                cwd=destination,\n                env=self.env,\n            )\n            process.wait()\n\n            project_dir = Path(destination, project_name)\n            actual_permissions = get_permissions_dict(project_dir)\n\n            assert actual_permissions == expected_permissions\n", "n_tokens": 1138, "byte_len": 6082, "file_sha1": "5ec38d065d9474bc34a2e2a43c30fcc762933c3e", "start_line": 127, "end_line": 317}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mocks/dummydbm.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mocks/dummydbm.py", "rel_path": "tests/mocks/dummydbm.py", "module": "tests.mocks.dummydbm", "ext": "py", "chunk_number": 1, "symbols": ["close", "open", "DummyDB", "pass", "argument", "module", "instance", "typing", "databases", "dummy", "dict", "return", "class", "ignored", "mode", "like", "collections", "dummydb", "interface", "flag", "compatible", "noqa", "key", "error", "from", "arguments", "provide", "file", "same", "create", "a001", "import", "self", "defaultdict", "database"], "ast_kind": "class_or_type", "text": "\"\"\"DBM-like dummy module\"\"\"\n\nfrom collections import defaultdict\nfrom typing import Any\n\n\nclass DummyDB(dict):\n    \"\"\"Provide dummy DBM-like interface.\"\"\"\n\n    def close(self):\n        pass\n\n\nerror = KeyError\n\n\n_DATABASES: defaultdict[Any, DummyDB] = defaultdict(DummyDB)\n\n\ndef open(file, flag=\"r\", mode=0o666):  # noqa: A001\n    \"\"\"Open or create a dummy database compatible.\n\n    Arguments ``flag`` and ``mode`` are ignored.\n    \"\"\"\n    # return same instance for same file argument\n    return _DATABASES[file]\n", "n_tokens": 120, "byte_len": 513, "file_sha1": "05c1b0b2de06d54e632d2b724e08c57f1e6dbde8", "start_line": 1, "end_line": 27}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/custom_loop_same.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/custom_loop_same.py", "rel_path": "tests/AsyncCrawlerRunner/custom_loop_same.py", "module": "tests.AsyncCrawlerRunner.custom_loop_same", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "task", "react", "runner", "async", "await", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "deferred", "from", "class", "configure", "logging", "scrapy", "defer", "main", "asyncio", "selector", "loop", "yield", "asyncioreactor", "utils", "twiste", "import", "crawler", "start", "custom", "settings", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerRunner\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\n@deferred_f_from_coro_f\nasync def main(reactor):\n    configure_logging()\n    runner = AsyncCrawlerRunner()\n    await runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\", \"uvloop.Loop\")\nreact(main)\n", "n_tokens": 185, "byte_len": 792, "file_sha1": "3fc397f9c6724c0b3341a908628a43a996f14f55", "start_line": 1, "end_line": 32}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/custom_loop_different.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/custom_loop_different.py", "rel_path": "tests/AsyncCrawlerRunner/custom_loop_different.py", "module": "tests.AsyncCrawlerRunner.custom_loop_different", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "task", "react", "runner", "async", "await", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "deferred", "from", "class", "configure", "logging", "scrapy", "defer", "main", "asyncio", "selector", "loop", "yield", "asyncioreactor", "utils", "twiste", "import", "crawler", "start", "custom", "settings", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerRunner\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\n@deferred_f_from_coro_f\nasync def main(reactor):\n    configure_logging()\n    runner = AsyncCrawlerRunner()\n    await runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 179, "byte_len": 777, "file_sha1": "e2a5bbf7067987e4b74808b55a5da23318b69765", "start_line": 1, "end_line": 32}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/multi_parallel.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/multi_parallel.py", "rel_path": "tests/AsyncCrawlerRunner/multi_parallel.py", "module": "tests.AsyncCrawlerRunner.multi_parallel", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "task", "react", "runner", "async", "await", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "deferred", "from", "class", "configure", "logging", "scrapy", "defer", "main", "asyncio", "selector", "yield", "join", "asyncioreactor", "utils", "import", "crawler", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerRunner\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\n@deferred_f_from_coro_f\nasync def main(reactor):\n    configure_logging()\n    runner = AsyncCrawlerRunner()\n    runner.crawl(NoRequestsSpider)\n    runner.crawl(NoRequestsSpider)\n    await runner.join()\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 149, "byte_len": 669, "file_sha1": "29400d17e0a825951c60f0da27f4a4437c25c824", "start_line": 1, "end_line": 29}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/simple_default_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/simple_default_reactor.py", "rel_path": "tests/AsyncCrawlerRunner/simple_default_reactor.py", "module": "tests.AsyncCrawlerRunner.simple_default_reactor", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "task", "react", "runner", "async", "await", "internet", "requests", "spider", "twisted", "return", "name", "deferred", "from", "class", "configure", "logging", "scrapy", "defer", "main", "yield", "reactor", "utils", "import", "crawler", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerRunner\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.log import configure_logging\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\n@deferred_f_from_coro_f\nasync def main(reactor):\n    configure_logging()\n    runner = AsyncCrawlerRunner()\n    await runner.crawl(NoRequestsSpider)\n\n\nreact(main)\n", "n_tokens": 110, "byte_len": 493, "file_sha1": "2a1472138ee6e267f835e8f354fe3cfddd13a84f", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/simple.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/simple.py", "rel_path": "tests/AsyncCrawlerRunner/simple.py", "module": "tests.AsyncCrawlerRunner.simple", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "task", "react", "runner", "async", "await", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "deferred", "from", "class", "configure", "logging", "scrapy", "defer", "main", "asyncio", "selector", "yield", "asyncioreactor", "utils", "import", "crawler", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerRunner\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\n@deferred_f_from_coro_f\nasync def main(reactor):\n    configure_logging()\n    runner = AsyncCrawlerRunner()\n    await runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 136, "byte_len": 616, "file_sha1": "3e119e4c2b891ea082c44f4e308f1f0ea3cc0584", "start_line": 1, "end_line": 27}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/multi_seq.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerRunner/multi_seq.py", "rel_path": "tests/AsyncCrawlerRunner/multi_seq.py", "module": "tests.AsyncCrawlerRunner.multi_seq", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "task", "react", "runner", "async", "await", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "deferred", "from", "class", "configure", "logging", "scrapy", "defer", "main", "asyncio", "selector", "yield", "asyncioreactor", "utils", "import", "crawler", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerRunner\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\n@deferred_f_from_coro_f\nasync def main(reactor):\n    configure_logging()\n    runner = AsyncCrawlerRunner()\n    await runner.crawl(NoRequestsSpider)\n    await runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 146, "byte_len": 657, "file_sha1": "bf0395e6822d10ddbb98ea032067786268f07fe5", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_poll.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_poll.py", "rel_path": "tests/CrawlerProcess/twisted_reactor_poll.py", "module": "tests.CrawlerProcess.twisted_reactor_poll", "ext": "py", "chunk_number": 1, "symbols": ["PollReactorSpider", "internet", "twisted", "crawler", "process", "spider", "name", "pollreactor", "class", "scrapy", "poll", "reactor", "from", "settings", "twiste", "import", "start", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass PollReactorSpider(scrapy.Spider):\n    name = \"poll_reactor\"\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.pollreactor.PollReactor\",\n    }\n)\nprocess.crawl(PollReactorSpider)\nprocess.start()\n", "n_tokens": 75, "byte_len": 295, "file_sha1": "b23e293d98f34c501f22bf53408b9ce36345b1a5", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/sleeping.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/sleeping.py", "rel_path": "tests/CrawlerProcess/sleeping.py", "module": "tests.CrawlerProcess.sleeping", "ext": "py", "chunk_number": 1, "symbols": ["SleepingSpider", "async", "await", "internet", "twisted", "crawler", "process", "spider", "sleeping", "name", "maybe", "deferred", "class", "scrapy", "defer", "call", "later", "argv", "from", "settings", "start", "urls", "none", "reactor", "data", "utils", "import", "parse", "self", "callback", "crawl", "response"], "ast_kind": "class_or_type", "text": "import sys\n\nfrom twisted.internet.defer import Deferred\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass SleepingSpider(scrapy.Spider):\n    name = \"sleeping\"\n\n    start_urls = [\"data:,;\"]\n\n    async def parse(self, response):\n        from twisted.internet import reactor\n\n        d = Deferred()\n        reactor.callLater(int(sys.argv[1]), d.callback, None)\n        await maybe_deferred_to_future(d)\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(SleepingSpider)\nprocess.start()\n", "n_tokens": 122, "byte_len": 558, "file_sha1": "f45143fc1cb29b3d92140e16129bcd77837c44d2", "start_line": 1, "end_line": 27}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py", "rel_path": "tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py", "module": "tests.CrawlerProcess.twisted_reactor_custom_settings_conflict", "ext": "py", "chunk_number": 1, "symbols": ["SelectReactorSpider", "AsyncioReactorSpider", "add", "errback", "internet", "python", "select", "reactor", "asyncio", "twisted", "crawler", "process", "spider", "name", "class", "scrapy", "selector", "from", "asyncioreactor", "twiste", "import", "start", "custom", "settings", "crawl", "selectreactor"], "ast_kind": "class_or_type", "text": "from twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass SelectReactorSpider(scrapy.Spider):\n    name = \"select_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = CrawlerProcess()\nd1 = process.crawl(SelectReactorSpider)\nd1.addErrback(log.err)\nd2 = process.crawl(AsyncioReactorSpider)\nd2.addErrback(log.err)\nprocess.start()\n", "n_tokens": 162, "byte_len": 626, "file_sha1": "367e09caeeab1cecdaf20b061838c13016490426", "start_line": 1, "end_line": 27}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_no_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_no_reactor.py", "rel_path": "tests/CrawlerProcess/asyncio_enabled_no_reactor.py", "module": "tests.CrawlerProcess.asyncio_enabled_no_reactor", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "ReactorCheckExtension", "NoRequestsSpider", "async", "requires", "internet", "extensions", "requests", "spider", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "asyncio", "selector", "reactor", "init", "yield", "from", "settings", "asyncioreactor", "utils", "twiste", "import", "start", "self", "runtime", "error", "raise", "request", "crawl", "check"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed\n\n\nclass ReactorCheckExtension:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 149, "byte_len": 662, "file_sha1": "3e6b86372b1f66c8ebae9eb1d052cca5d4794f92", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_deferred_signal.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_deferred_signal.py", "rel_path": "tests/CrawlerProcess/asyncio_deferred_signal.py", "module": "tests.CrawlerProcess.asyncio_deferred_signal", "ext": "py", "chunk_number": 1, "symbols": ["open_spider", "process_item", "parse", "UppercasePipeline", "UrlSpider", "async", "pipeline", "await", "asynci", "even", "upper", "except", "internet", "open", "spider", "twisted", "return", "crawler", "process", "item", "annotations", "name", "class", "sleep", "main", "url", "scrapy", "logger", "future", "defer", "deferred", "from", "asyncio", "selector", "info", "argv", "yield", "settings", "start", "urls", "index", "error", "none", "asyncioreactor", "opened", "data", "utils", "ite", "pipelines", "twiste"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport sys\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.defer import deferred_from_coro\n\n\nclass UppercasePipeline:\n    async def _open_spider(self, spider):\n        spider.logger.info(\"async pipeline opened!\")\n        await asyncio.sleep(0.1)\n\n    def open_spider(self, spider):\n        return deferred_from_coro(self._open_spider(spider))\n\n    def process_item(self, item):\n        return {\"url\": item[\"url\"].upper()}\n\n\nclass UrlSpider(Spider):\n    name = \"url_spider\"\n    start_urls = [\"data:,\"]\n    custom_settings = {\n        \"ITEM_PIPELINES\": {UppercasePipeline: 100},\n    }\n\n    def parse(self, response):\n        yield {\"url\": response.url}\n\n\nif __name__ == \"__main__\":\n    ASYNCIO_EVENT_LOOP: str | None\n    try:\n        ASYNCIO_EVENT_LOOP = sys.argv[1]\n    except IndexError:\n        ASYNCIO_EVENT_LOOP = None\n\n    process = CrawlerProcess(\n        settings={\n            \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n        }\n    )\n    process.crawl(UrlSpider)\n    process.start()\n", "n_tokens": 288, "byte_len": 1170, "file_sha1": "e4b9450efea1c8e3055af41c4e1e0318da74d274", "start_line": 1, "end_line": 49}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/args_settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/args_settings.py", "rel_path": "tests/CrawlerProcess/args_settings.py", "module": "tests.CrawlerProcess.args_settings", "ext": "py", "chunk_number": 1, "symbols": ["from_crawler", "NoRequestsSpider", "async", "requests", "spider", "typing", "return", "crawler", "process", "name", "getint", "class", "scrapy", "logger", "classmethod", "value", "info", "from", "yield", "settings", "kwargs", "super", "import", "start", "self", "request", "crawl", "args"], "ast_kind": "class_or_type", "text": "from typing import Any\n\nimport scrapy\nfrom scrapy.crawler import Crawler, CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n        return spider\n\n    async def start(self):\n        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n        return\n        yield\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider, foo=42)\nprocess.start()\n", "n_tokens": 152, "byte_len": 617, "file_sha1": "9fd7035d51c47f9ef3c8c4c10a61ee4f27554303", "start_line": 1, "end_line": 26}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_custom_settings_same.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_custom_settings_same.py", "rel_path": "tests/CrawlerProcess/twisted_reactor_custom_settings_same.py", "module": "tests.CrawlerProcess.twisted_reactor_custom_settings_same", "ext": "py", "chunk_number": 1, "symbols": ["AsyncioReactorSpider1", "AsyncioReactorSpider2", "asyncio", "reactor", "internet", "twisted", "crawler", "process", "spider", "name", "class", "scrapy", "selector", "from", "asyncioreactor", "twiste", "import", "start", "custom", "settings", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass AsyncioReactorSpider1(scrapy.Spider):\n    name = \"asyncio_reactor1\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nclass AsyncioReactorSpider2(scrapy.Spider):\n    name = \"asyncio_reactor2\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = CrawlerProcess()\nprocess.crawl(AsyncioReactorSpider1)\nprocess.crawl(AsyncioReactorSpider2)\nprocess.start()\n", "n_tokens": 145, "byte_len": 557, "file_sha1": "de8fbcfd76ad98a968ed3c8a024fe640df7b7c4d", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_select.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_select.py", "rel_path": "tests/CrawlerProcess/reactor_select.py", "module": "tests.CrawlerProcess.reactor_select", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "add", "errback", "async", "internet", "python", "requests", "spider", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "yield", "from", "settings", "install", "import", "start", "self", "request", "crawl", "selectreactor"], "ast_kind": "class_or_type", "text": "from twisted.internet import selectreactor\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nselectreactor.install()\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(settings={})\n\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "n_tokens": 87, "byte_len": 393, "file_sha1": "67f2fa7c412ccbb05eed70648f64bbebf32cd42e", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/multi.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/multi.py", "rel_path": "tests/CrawlerProcess/multi.py", "module": "tests.CrawlerProcess.multi", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "crawler", "process", "spider", "name", "yield", "class", "async", "from", "import", "settings", "start", "self", "scrapy", "request", "requests", "crawl", "return"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 67, "byte_len": 299, "file_sha1": "9d1e495c47c47974c135a53b76dd277a77748a00", "start_line": 1, "end_line": 18}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_reactor.py", "rel_path": "tests/CrawlerProcess/asyncio_enabled_reactor.py", "module": "tests.CrawlerProcess.asyncio_enabled_reactor", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "ReactorCheckExtension", "NoRequestsSpider", "async", "except", "pass", "internet", "requires", "extensions", "requests", "spider", "twisted", "return", "after", "install", "reactor", "crawler", "process", "installed", "name", "class", "support", "before", "scrapy", "asyncio", "selector", "init", "yield", "from", "settings", "wrong", "already", "asyncioreactor", "available", "utils", "twiste", "import", "start", "runtime", "error", "self", "raise", "request", "crawl", "else", "check"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.asyncio import is_asyncio_available\nfrom scrapy.utils.reactor import (\n    install_reactor,\n    is_asyncio_reactor_installed,\n    is_reactor_installed,\n)\n\nif is_reactor_installed():\n    raise RuntimeError(\n        \"Reactor already installed before is_asyncio_reactor_installed().\"\n    )\n\ntry:\n    is_asyncio_reactor_installed()\nexcept RuntimeError:\n    pass\nelse:\n    raise RuntimeError(\"is_asyncio_reactor_installed() did not raise RuntimeError.\")\n\ntry:\n    is_asyncio_available()\nexcept RuntimeError:\n    pass\nelse:\n    raise RuntimeError(\"is_asyncio_available() did not raise RuntimeError.\")\n\nif is_reactor_installed():\n    raise RuntimeError(\n        \"Reactor already installed after is_asyncio_reactor_installed().\"\n    )\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\nif not is_asyncio_reactor_installed():\n    raise RuntimeError(\"Wrong reactor installed after install_reactor().\")\n\n\nclass ReactorCheckExtension:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n        if not is_asyncio_available():\n            raise RuntimeError(\"ReactorCheckExtension requires asyncio support.\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 350, "byte_len": 1656, "file_sha1": "6a718d5f81ce4bf86d953572f6303496b3cd8604", "start_line": 1, "end_line": 64}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py", "rel_path": "tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py", "module": "tests.CrawlerProcess.asyncio_enabled_reactor_same_loop", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "set", "event", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "platform", "install", "asyncioreactor", "twiste", "reactor", "import", "start", "windows", "self", "request", "crawl", "uvloop", "win", "win32"], "ast_kind": "class_or_type", "text": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\nfrom uvloop import Loop\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nif sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.set_event_loop(Loop())\nasyncioreactor.install()\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 154, "byte_len": 664, "file_sha1": "d5ec019126e8cc01ee5fb599d2dfbcd874c415f2", "start_line": 1, "end_line": 32}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_default_twisted_reactor_select.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_default_twisted_reactor_select.py", "rel_path": "tests/CrawlerProcess/reactor_default_twisted_reactor_select.py", "module": "tests.CrawlerProcess.reactor_default_twisted_reactor_select", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "add", "errback", "async", "internet", "python", "f401", "requests", "spider", "select", "reactor", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "yield", "noqa", "from", "settings", "twiste", "import", "tid253", "start", "self", "request", "crawl", "selectreactor"], "ast_kind": "class_or_type", "text": "from twisted.internet import reactor  # noqa: F401,TID253\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\n\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "n_tokens": 113, "byte_len": 469, "file_sha1": "c3b0dd0ae4d2056f6ca08d16d2f9b376de741857", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_asyncio.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_asyncio.py", "rel_path": "tests/CrawlerProcess/twisted_reactor_asyncio.py", "module": "tests.CrawlerProcess.twisted_reactor_asyncio", "ext": "py", "chunk_number": 1, "symbols": ["AsyncioReactorSpider", "internet", "asyncio", "reactor", "twisted", "crawler", "process", "spider", "name", "class", "scrapy", "selector", "from", "settings", "asyncioreactor", "twiste", "import", "start", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n)\nprocess.crawl(AsyncioReactorSpider)\nprocess.start()\n", "n_tokens": 79, "byte_len": 318, "file_sha1": "ef9eb50f969d80256d772f0069fe4714ec4c02e4", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/caching_hostname_resolver.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/caching_hostname_resolver.py", "rel_path": "tests/CrawlerProcess/caching_hostname_resolver.py", "module": "tests.CrawlerProcess.caching_hostname_resolver", "ext": "py", "chunk_number": 1, "symbols": ["parse", "ignore_response", "CachingHostnameResolverSpider", "does", "caching", "hostname", "async", "repr", "false", "crawler", "process", "spider", "name", "class", "resolution", "address", "dont", "filter", "main", "time", "scrapy", "logger", "finite", "amount", "info", "argv", "finishes", "ignore", "response", "resolver", "dns", "yield", "true", "from", "settings", "indefinitely", "retr", "enabled", "request", "range", "import", "hang", "start", "self", "callback", "crawl"], "ast_kind": "class_or_type", "text": "import sys\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass CachingHostnameResolverSpider(scrapy.Spider):\n    \"\"\"\n    Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)\n    \"\"\"\n\n    name = \"caching_hostname_resolver_spider\"\n\n    async def start(self):\n        yield scrapy.Request(self.url)\n\n    def parse(self, response):\n        for _ in range(10):\n            yield scrapy.Request(\n                response.url, dont_filter=True, callback=self.ignore_response\n            )\n\n    def ignore_response(self, response):\n        self.logger.info(repr(response.ip_address))\n\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess(\n        settings={\n            \"RETRY_ENABLED\": False,\n            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n        }\n    )\n    process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])\n    process.start()\n", "n_tokens": 193, "byte_len": 910, "file_sha1": "168d2c5f396314a33dbd1b06ea7b298243fdedae", "start_line": 1, "end_line": 36}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py", "rel_path": "tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py", "module": "tests.CrawlerProcess.asyncio_enabled_reactor_different_loop", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "add", "errback", "set", "event", "async", "asynci", "even", "internet", "python", "requests", "spider", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "platform", "install", "asyncioreactor", "twiste", "reactor", "import", "start", "windows", "self", "request", "crawl", "uvloop", "win", "win32"], "ast_kind": "class_or_type", "text": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nif sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncioreactor.install()\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "n_tokens": 155, "byte_len": 666, "file_sha1": "64fc973365dce8c16a13f57c7c451182a1382842", "start_line": 1, "end_line": 32}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py", "rel_path": "tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py", "module": "tests.CrawlerProcess.reactor_select_subclass_twisted_reactor_select", "ext": "py", "chunk_number": 1, "symbols": ["SelectReactorSubclass", "NoRequestsSpider", "add", "errback", "async", "pass", "internet", "python", "requests", "spider", "select", "reactor", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "main", "yield", "from", "settings", "twiste", "import", "start", "install", "self", "request", "crawl", "selectreactor"], "ast_kind": "class_or_type", "text": "from twisted.internet.main import installReactor\nfrom twisted.internet.selectreactor import SelectReactor\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass SelectReactorSubclass(SelectReactor):\n    pass\n\n\nreactor = SelectReactorSubclass()\ninstallReactor(reactor)\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\n\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "n_tokens": 148, "byte_len": 632, "file_sha1": "8b3eccdd235d9b38f98789bdba6c331a229af2cd", "start_line": 1, "end_line": 34}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_custom_settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_custom_settings.py", "rel_path": "tests/CrawlerProcess/twisted_reactor_custom_settings.py", "module": "tests.CrawlerProcess.twisted_reactor_custom_settings", "ext": "py", "chunk_number": 1, "symbols": ["AsyncioReactorSpider", "internet", "asyncio", "reactor", "twisted", "crawler", "process", "spider", "name", "class", "scrapy", "selector", "from", "asyncioreactor", "twiste", "import", "start", "custom", "settings", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = CrawlerProcess()\nprocess.crawl(AsyncioReactorSpider)\nprocess.start()\n", "n_tokens": 80, "byte_len": 326, "file_sha1": "c877b633c1ecec45f8552cd4df465f74dab218cf", "start_line": 1, "end_line": 15}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_default.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_default.py", "rel_path": "tests/CrawlerProcess/reactor_default.py", "module": "tests.CrawlerProcess.reactor_default", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "add", "errback", "async", "internet", "python", "f401", "requests", "spider", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "yield", "noqa", "from", "settings", "reactor", "import", "tid253", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet import reactor  # noqa: F401,TID253\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(settings={})\n\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "n_tokens": 89, "byte_len": 383, "file_sha1": "3d569308585e14b1590c871bf49743b3a3568d3c", "start_line": 1, "end_line": 21}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/simple.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/simple.py", "rel_path": "tests/CrawlerProcess/simple.py", "module": "tests.CrawlerProcess.simple", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "crawler", "process", "spider", "name", "yield", "class", "async", "from", "import", "settings", "start", "self", "scrapy", "request", "requests", "crawl", "return"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 59, "byte_len": 267, "file_sha1": "bd44a9287454de0501a4b0f626421d94e029af6c", "start_line": 1, "end_line": 17}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/caching_hostname_resolver_ipv6.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/caching_hostname_resolver_ipv6.py", "rel_path": "tests/CrawlerProcess/caching_hostname_resolver_ipv6.py", "module": "tests.CrawlerProcess.caching_hostname_resolver_ipv6", "ext": "py", "chunk_number": 1, "symbols": ["CachingHostnameResolverSpider", "caching", "hostname", "false", "internet", "exception", "twisted", "crawler", "process", "spider", "name", "class", "main", "scrapy", "finishes", "resolver", "dns", "from", "settings", "start", "urls", "retr", "enabled", "without", "import", "http", "lookup", "crawl", "error"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass CachingHostnameResolverSpider(scrapy.Spider):\n    \"\"\"\n    Finishes without a twisted.internet.error.DNSLookupError exception\n    \"\"\"\n\n    name = \"caching_hostname_resolver_spider\"\n    start_urls = [\"http://[::1]\"]\n\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess(\n        settings={\n            \"RETRY_ENABLED\": False,\n            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n        }\n    )\n    process.crawl(CachingHostnameResolverSpider)\n    process.start()\n", "n_tokens": 126, "byte_len": 548, "file_sha1": "d78f746bd58710d56a62e45dd4ae76f01c63998c", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_select_twisted_reactor_select.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/reactor_select_twisted_reactor_select.py", "rel_path": "tests/CrawlerProcess/reactor_select_twisted_reactor_select.py", "module": "tests.CrawlerProcess.reactor_select_twisted_reactor_select", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "async", "internet", "requests", "spider", "select", "reactor", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "yield", "from", "settings", "install", "twiste", "import", "start", "self", "request", "crawl", "selectreactor"], "ast_kind": "class_or_type", "text": "from twisted.internet import selectreactor\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nselectreactor.install()\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 96, "byte_len": 422, "file_sha1": "a4ef1b7adb100b50259c803f0bc50aa8764bc9b2", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_select.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/twisted_reactor_select.py", "rel_path": "tests/CrawlerProcess/twisted_reactor_select.py", "module": "tests.CrawlerProcess.twisted_reactor_select", "ext": "py", "chunk_number": 1, "symbols": ["SelectReactorSpider", "internet", "select", "reactor", "twisted", "crawler", "process", "spider", "name", "class", "scrapy", "from", "settings", "twiste", "import", "start", "epoll", "crawl", "selectreactor"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass SelectReactorSpider(scrapy.Spider):\n    name = \"epoll_reactor\"\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\nprocess.crawl(SelectReactorSpider)\nprocess.start()\n", "n_tokens": 75, "byte_len": 304, "file_sha1": "22a985815b91ef59b1764f5db94204d2ddcc8923", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_custom_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/asyncio_custom_loop.py", "rel_path": "tests/CrawlerProcess/asyncio_custom_loop.py", "module": "tests.CrawlerProcess.asyncio_custom_loop", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "crawler", "process", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "asyncioreactor", "twiste", "reactor", "import", "start", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 99, "byte_len": 407, "file_sha1": "3d7aedcbc0c9b763961a987e7c1e37add9cd0961", "start_line": 1, "end_line": 21}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/default_name_resolver.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerProcess/default_name_resolver.py", "rel_path": "tests/CrawlerProcess/default_name_resolver.py", "module": "tests.CrawlerProcess.default_name_resolver", "ext": "py", "chunk_number": 1, "symbols": ["IPv6Spider", "does", "false", "internet", "raises", "twisted", "crawler", "process", "spider", "name", "handle", "class", "main", "scrapy", "from", "settings", "start", "urls", "retr", "enabled", "import", "ipv6spider", "resolver", "http", "default", "dns", "lookup", "addresses", "crawl", "error", "ipv", "ipv6"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass IPv6Spider(scrapy.Spider):\n    \"\"\"\n    Raises a twisted.internet.error.DNSLookupError:\n    the default name resolver does not handle IPv6 addresses.\n    \"\"\"\n\n    name = \"ipv6_spider\"\n    start_urls = [\"http://[::1]\"]\n\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess(settings={\"RETRY_ENABLED\": False})\n    process.crawl(IPv6Spider)\n    process.start()\n", "n_tokens": 105, "byte_len": 424, "file_sha1": "849fcf346b36704eb3e07f2fa8bb8ee1932c13cb", "start_line": 1, "end_line": 19}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/__init__.py", "rel_path": "tests/test_utils_misc/__init__.py", "module": "tests.test_utils_misc.__init__", "ext": "py", "chunk_number": 1, "symbols": ["test_load_object_class", "test_load_object_function", "test_load_object_exceptions", "test_walk_modules", "test_walk_modules_egg", "test_arg_to_iter", "test_create_instance", "_test_with_settings", "TestUtilsMisc", "TestItem", "append", "file", "test", "with", "name", "item", "spiders", "lala", "path", "utils", "mock", "load", "spec", "set", "pytest", "settings", "walk", "create", "instance", "john", "_test_with_crawler", "test_build_from_crawler", "test_set_environ", "test_rel_has_nofollow", "qualname", "elif", "four", "three", "correct", "alternative", "none", "return", "value", "abcd", "nofollow", "mod", "mod1", "misc", "match", "iter"], "ast_kind": "class_or_type", "text": "import os\nimport sys\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.misc import (\n    arg_to_iter,\n    build_from_crawler,\n    create_instance,\n    load_object,\n    rel_has_nofollow,\n    set_environ,\n    walk_modules,\n)\n\n\nclass TestUtilsMisc:\n    def test_load_object_class(self):\n        obj = load_object(Field)\n        assert obj is Field\n        obj = load_object(\"scrapy.item.Field\")\n        assert obj is Field\n\n    def test_load_object_function(self):\n        obj = load_object(load_object)\n        assert obj is load_object\n        obj = load_object(\"scrapy.utils.misc.load_object\")\n        assert obj is load_object\n\n    def test_load_object_exceptions(self):\n        with pytest.raises(ImportError):\n            load_object(\"nomodule999.mod.function\")\n        with pytest.raises(NameError):\n            load_object(\"scrapy.utils.misc.load_object999\")\n        with pytest.raises(TypeError):\n            load_object({})\n\n    def test_walk_modules(self):\n        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules\")\n        expected = [\n            \"tests.test_utils_misc.test_walk_modules\",\n            \"tests.test_utils_misc.test_walk_modules.mod\",\n            \"tests.test_utils_misc.test_walk_modules.mod.mod0\",\n            \"tests.test_utils_misc.test_walk_modules.mod1\",\n        ]\n        assert {m.__name__ for m in mods} == set(expected)\n\n        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules.mod\")\n        expected = [\n            \"tests.test_utils_misc.test_walk_modules.mod\",\n            \"tests.test_utils_misc.test_walk_modules.mod.mod0\",\n        ]\n        assert {m.__name__ for m in mods} == set(expected)\n\n        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules.mod1\")\n        expected = [\n            \"tests.test_utils_misc.test_walk_modules.mod1\",\n        ]\n        assert {m.__name__ for m in mods} == set(expected)\n\n        with pytest.raises(ImportError):\n            walk_modules(\"nomodule999\")\n\n    def test_walk_modules_egg(self):\n        egg = str(Path(__file__).parent / \"test.egg\")\n        sys.path.append(egg)\n        try:\n            mods = walk_modules(\"testegg\")\n            expected = [\n                \"testegg.spiders\",\n                \"testegg.spiders.a\",\n                \"testegg.spiders.b\",\n                \"testegg\",\n            ]\n            assert {m.__name__ for m in mods} == set(expected)\n        finally:\n            sys.path.remove(egg)\n\n    def test_arg_to_iter(self):\n        class TestItem(Item):\n            name = Field()\n\n        assert hasattr(arg_to_iter(None), \"__iter__\")\n        assert hasattr(arg_to_iter(100), \"__iter__\")\n        assert hasattr(arg_to_iter(\"lala\"), \"__iter__\")\n        assert hasattr(arg_to_iter([1, 2, 3]), \"__iter__\")\n        assert hasattr(arg_to_iter(c for c in \"abcd\"), \"__iter__\")\n\n        assert not list(arg_to_iter(None))\n        assert list(arg_to_iter(\"lala\")) == [\"lala\"]\n        assert list(arg_to_iter(100)) == [100]\n        assert list(arg_to_iter(c for c in \"abc\")) == [\"a\", \"b\", \"c\"]\n        assert list(arg_to_iter([1, 2, 3])) == [1, 2, 3]\n        assert list(arg_to_iter({\"a\": 1})) == [{\"a\": 1}]\n        assert list(arg_to_iter(TestItem(name=\"john\"))) == [TestItem(name=\"john\")]\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_create_instance(self):\n        settings = mock.MagicMock()\n        crawler = mock.MagicMock(spec_set=[\"settings\"])\n        args = (True, 100.0)\n        kwargs = {\"key\": \"val\"}\n\n        def _test_with_settings(mock, settings):\n            create_instance(mock, settings, None, *args, **kwargs)\n            if hasattr(mock, \"from_crawler\"):\n                assert mock.from_crawler.call_count == 0\n            if hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                assert mock.call_count == 0\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n", "n_tokens": 907, "byte_len": 4042, "file_sha1": "450eb84f1759b29dfe3e22ff4638c6b7a60718f7", "start_line": 1, "end_line": 116}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/__init__.py", "rel_path": "tests/test_utils_misc/__init__.py", "module": "tests.test_utils_misc.__init__", "ext": "py", "chunk_number": 2, "symbols": ["_test_with_crawler", "test_build_from_crawler", "test_set_environ", "test_rel_has_nofollow", "hasattr", "false", "qualname", "magic", "mock", "test", "value", "some", "assert", "called", "with", "error", "least", "build", "environ", "elif", "usage", "reset", "adoption", "four", "type", "correct", "call", "count", "three", "nofollowfoo", "test_load_object_class", "test_load_object_function", "test_load_object_exceptions", "test_walk_modules", "test_walk_modules_egg", "test_arg_to_iter", "test_create_instance", "_test_with_settings", "TestUtilsMisc", "TestItem", "append", "file", "name", "item", "spiders", "lala", "path", "utils", "load", "spec"], "ast_kind": "function_or_method", "text": "        def _test_with_crawler(mock, settings, crawler):\n            create_instance(mock, settings, crawler, *args, **kwargs)\n            if hasattr(mock, \"from_crawler\"):\n                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)\n                if hasattr(mock, \"from_settings\"):\n                    assert mock.from_settings.call_count == 0\n                assert mock.call_count == 0\n            elif hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                assert mock.call_count == 0\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n\n        # Check usage of correct constructor using four mocks:\n        #   1. with no alternative constructors\n        #   2. with from_settings() constructor\n        #   3. with from_crawler() constructor\n        #   4. with from_settings() and from_crawler() constructor\n        spec_sets = (\n            [\"__qualname__\"],\n            [\"__qualname__\", \"from_settings\"],\n            [\"__qualname__\", \"from_crawler\"],\n            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n        )\n        for specs in spec_sets:\n            m = mock.MagicMock(spec_set=specs)\n            _test_with_settings(m, settings)\n            m.reset_mock()\n            _test_with_crawler(m, settings, crawler)\n\n        # Check adoption of crawler settings\n        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n        create_instance(m, None, crawler, *args, **kwargs)\n        m.from_settings.assert_called_once_with(crawler.settings, *args, **kwargs)\n\n        with pytest.raises(\n            ValueError, match=\"Specify at least one of settings and crawler\"\n        ):\n            create_instance(m, None, None)\n\n        m.from_settings.return_value = None\n        with pytest.raises(TypeError):\n            create_instance(m, settings, None)\n\n    def test_build_from_crawler(self):\n        settings = mock.MagicMock()\n        crawler = mock.MagicMock(spec_set=[\"settings\"])\n        args = (True, 100.0)\n        kwargs = {\"key\": \"val\"}\n\n        def _test_with_crawler(mock, settings, crawler):\n            build_from_crawler(mock, crawler, *args, **kwargs)\n            if hasattr(mock, \"from_crawler\"):\n                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)\n                if hasattr(mock, \"from_settings\"):\n                    assert mock.from_settings.call_count == 0\n                assert mock.call_count == 0\n            elif hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                assert mock.call_count == 0\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n\n        # Check usage of correct constructor using three mocks:\n        #   1. with no alternative constructors\n        #   2. with from_crawler() constructor\n        #   3. with from_settings() and from_crawler() constructor\n        spec_sets = (\n            [\"__qualname__\"],\n            [\"__qualname__\", \"from_crawler\"],\n            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n        )\n        for specs in spec_sets:\n            m = mock.MagicMock(spec_set=specs)\n            _test_with_crawler(m, settings, crawler)\n            m.reset_mock()\n\n        # Check adoption of crawler\n        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_crawler\"])\n        m.from_crawler.return_value = None\n        with pytest.raises(TypeError):\n            build_from_crawler(m, crawler, *args, **kwargs)\n\n    def test_set_environ(self):\n        assert os.environ.get(\"some_test_environ\") is None\n        with set_environ(some_test_environ=\"test_value\"):\n            assert os.environ.get(\"some_test_environ\") == \"test_value\"\n        assert os.environ.get(\"some_test_environ\") is None\n\n        os.environ[\"some_test_environ\"] = \"test\"\n        assert os.environ.get(\"some_test_environ\") == \"test\"\n        with set_environ(some_test_environ=\"test_value\"):\n            assert os.environ.get(\"some_test_environ\") == \"test_value\"\n        assert os.environ.get(\"some_test_environ\") == \"test\"\n\n    def test_rel_has_nofollow(self):\n        assert rel_has_nofollow(\"ugc nofollow\") is True\n        assert rel_has_nofollow(\"ugc,nofollow\") is True\n        assert rel_has_nofollow(\"ugc\") is False\n        assert rel_has_nofollow(\"nofollow\") is True\n        assert rel_has_nofollow(\"nofollowfoo\") is False\n        assert rel_has_nofollow(\"foonofollow\") is False\n        assert rel_has_nofollow(\"ugc,  ,  nofollow\") is True\n", "n_tokens": 1033, "byte_len": 4589, "file_sha1": "450eb84f1759b29dfe3e22ff4638c6b7a60718f7", "start_line": 117, "end_line": 220}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/test_return_with_argument_inside_generator.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/test_return_with_argument_inside_generator.py", "rel_path": "tests/test_utils_misc/test_return_with_argument_inside_generator.py", "module": "tests.test_utils_misc.test_return_with_argument_inside_generator", "ext": "py", "chunk_number": 1, "symbols": ["_indentation_error", "top_level_return_something", "top_level_return_none", "generator_that_returns_stuff", "mock_spider", "__init__", "getbool", "test_generators_return_something", "f1", "g1", "h1", "helper", "i1", "test_generators_return_none", "f2", "g2", "h2", "i2", "j2", "k2", "l2", "test_generators_return_none_with_decorator", "decorator", "inner_func", "f3", "g3", "TestUtilsMisc", "MockSettings", "MockSpider", "settings", "h3", "i3", "j3", "k3", "l3", "test_indentation_error", "test_partial", "cb", "test_warn_on_generator_with_return_value_settings_disabled", "gen_with_return", "dict", "method", "unable", "war", "generato", "false", "arg", "arg2", "determine", "func"], "ast_kind": "class_or_type", "text": "import warnings\nfrom functools import partial\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.utils.misc import (\n    is_generator_with_return_value,\n    warn_on_generator_with_return_value,\n)\n\n\ndef _indentation_error(*args, **kwargs):\n    raise IndentationError\n\n\ndef top_level_return_something():\n    \"\"\"\n    docstring\n    \"\"\"\n    url = \"\"\"\nhttps://example.org\n\"\"\"\n    yield url\n    return 1\n\n\ndef top_level_return_none():\n    \"\"\"\n    docstring\n    \"\"\"\n    url = \"\"\"\nhttps://example.org\n\"\"\"\n    yield url\n\n\ndef generator_that_returns_stuff():\n    yield 1\n    yield 2\n    return 3\n\n\nclass TestUtilsMisc:\n    @pytest.fixture\n    def mock_spider(self):\n        class MockSettings:\n            def __init__(self, settings_dict=None):\n                self.settings_dict = settings_dict or {\n                    \"WARN_ON_GENERATOR_RETURN_VALUE\": True\n                }\n\n            def getbool(self, name, default=False):\n                return self.settings_dict.get(name, default)\n\n        class MockSpider:\n            def __init__(self):\n                self.settings = MockSettings()\n\n        return MockSpider()\n\n    def test_generators_return_something(self, mock_spider):\n        def f1():\n            yield 1\n            return 2\n\n        def g1():\n            yield 1\n            return \"asdf\"\n\n        def h1():\n            yield 1\n\n            def helper():\n                return 0\n\n            yield helper()\n            return 2\n\n        def i1():\n            \"\"\"\n            docstring\n            \"\"\"\n            url = \"\"\"\nhttps://example.org\n        \"\"\"\n            yield url\n            return 1\n\n        assert is_generator_with_return_value(top_level_return_something)\n        assert is_generator_with_return_value(f1)\n        assert is_generator_with_return_value(g1)\n        assert is_generator_with_return_value(h1)\n        assert is_generator_with_return_value(i1)\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, top_level_return_something)\n            assert len(w) == 1\n            assert (\n                'The \"MockSpider.top_level_return_something\" method is a generator'\n                in str(w[0].message)\n            )\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, f1)\n            assert len(w) == 1\n            assert 'The \"MockSpider.f1\" method is a generator' in str(w[0].message)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, g1)\n            assert len(w) == 1\n            assert 'The \"MockSpider.g1\" method is a generator' in str(w[0].message)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, h1)\n            assert len(w) == 1\n            assert 'The \"MockSpider.h1\" method is a generator' in str(w[0].message)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, i1)\n            assert len(w) == 1\n            assert 'The \"MockSpider.i1\" method is a generator' in str(w[0].message)\n\n    def test_generators_return_none(self, mock_spider):\n        def f2():\n            yield 1\n\n        def g2():\n            yield 1\n\n        def h2():\n            yield 1\n\n        def i2():\n            yield 1\n            yield from generator_that_returns_stuff()\n\n        def j2():\n            yield 1\n\n            def helper():\n                return 0\n\n            yield helper()\n\n        def k2():\n            \"\"\"\n            docstring\n            \"\"\"\n            url = \"\"\"\nhttps://example.org\n        \"\"\"\n            yield url\n\n        def l2():\n            return\n\n        assert not is_generator_with_return_value(top_level_return_none)\n        assert not is_generator_with_return_value(f2)\n        assert not is_generator_with_return_value(g2)\n        assert not is_generator_with_return_value(h2)\n        assert not is_generator_with_return_value(i2)\n        assert not is_generator_with_return_value(j2)  # not recursive\n        assert not is_generator_with_return_value(k2)  # not recursive\n        assert not is_generator_with_return_value(l2)\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, top_level_return_none)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, f2)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, g2)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, h2)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, i2)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, j2)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, k2)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, l2)\n            assert len(w) == 0\n\n    def test_generators_return_none_with_decorator(self, mock_spider):\n        def decorator(func):\n            def inner_func():\n                func()\n\n            return inner_func\n\n        @decorator\n        def f3():\n            yield 1\n\n        @decorator\n        def g3():\n            yield 1\n", "n_tokens": 1249, "byte_len": 5763, "file_sha1": "5c9b456cbc1721609a2e1e80434a38e865f1efb3", "start_line": 1, "end_line": 202}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/test_return_with_argument_inside_generator.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_utils_misc/test_return_with_argument_inside_generator.py", "rel_path": "tests/test_utils_misc/test_return_with_argument_inside_generator.py", "module": "tests.test_utils_misc.test_return_with_argument_inside_generator", "ext": "py", "chunk_number": 2, "symbols": ["h3", "i3", "j3", "helper", "k3", "l3", "test_indentation_error", "test_partial", "cb", "test_warn_on_generator_with_return_value_settings_disabled", "__init__", "getbool", "gen_with_return", "MockSettings", "MockSpider", "unable", "settings", "dict", "war", "generato", "false", "arg", "arg2", "determine", "patch", "partial", "spider", "generator", "with", "top", "_indentation_error", "top_level_return_something", "top_level_return_none", "generator_that_returns_stuff", "mock_spider", "test_generators_return_something", "f1", "g1", "h1", "i1", "test_generators_return_none", "f2", "g2", "h2", "i2", "j2", "k2", "l2", "test_generators_return_none_with_decorator", "decorator"], "ast_kind": "class_or_type", "text": "        @decorator\n        def h3():\n            yield 1\n\n        @decorator\n        def i3():\n            yield 1\n            yield from generator_that_returns_stuff()\n\n        @decorator\n        def j3():\n            yield 1\n\n            def helper():\n                return 0\n\n            yield helper()\n\n        @decorator\n        def k3():\n            \"\"\"\n            docstring\n            \"\"\"\n            url = \"\"\"\nhttps://example.org\n        \"\"\"\n            yield url\n\n        @decorator\n        def l3():\n            return\n\n        assert not is_generator_with_return_value(top_level_return_none)\n        assert not is_generator_with_return_value(f3)\n        assert not is_generator_with_return_value(g3)\n        assert not is_generator_with_return_value(h3)\n        assert not is_generator_with_return_value(i3)\n        assert not is_generator_with_return_value(j3)  # not recursive\n        assert not is_generator_with_return_value(k3)  # not recursive\n        assert not is_generator_with_return_value(l3)\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, top_level_return_none)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, f3)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, g3)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, h3)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, i3)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, j3)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, k3)\n            assert len(w) == 0\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, l3)\n            assert len(w) == 0\n\n    @mock.patch(\n        \"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error\n    )\n    def test_indentation_error(self, mock_spider):\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(mock_spider, top_level_return_none)\n            assert len(w) == 1\n            assert \"Unable to determine\" in str(w[0].message)\n\n    def test_partial(self):\n        def cb(arg1, arg2):\n            yield {}\n\n        partial_cb = partial(cb, arg1=42)\n        assert not is_generator_with_return_value(partial_cb)\n\n    def test_warn_on_generator_with_return_value_settings_disabled(self):\n        class MockSettings:\n            def __init__(self, settings_dict=None):\n                self.settings_dict = settings_dict or {}\n\n            def getbool(self, name, default=False):\n                return self.settings_dict.get(name, default)\n\n        class MockSpider:\n            def __init__(self):\n                self.settings = MockSettings({\"WARN_ON_GENERATOR_RETURN_VALUE\": False})\n\n        spider = MockSpider()\n\n        def gen_with_return():\n            yield 1\n            return \"value\"\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(spider, gen_with_return)\n            assert len(w) == 0\n\n        spider.settings.settings_dict[\"WARN_ON_GENERATOR_RETURN_VALUE\"] = True\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(spider, gen_with_return)\n            assert len(w) == 1\n            assert \"is a generator\" in str(w[0].message)\n", "n_tokens": 824, "byte_len": 3870, "file_sha1": "5c9b456cbc1721609a2e1e80434a38e865f1efb3", "start_line": 203, "end_line": 313}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline/__init__.py", "rel_path": "tests/test_cmdline/__init__.py", "module": "tests.test_cmdline.__init__", "ext": "py", "chunk_number": 1, "symbols": ["setup_method", "_execute", "test_default_settings", "test_override_settings_using_set_arg", "test_profiling", "test_override_dict_settings", "test_pathlib_path_as_feeds_key", "TestCmdline", "encoding", "dumps", "price", "loads", "file", "stream", "name", "print", "stats", "smarter", "path", "prof", "scrap", "setting", "settings", "test", "profiling", "execute", "items", "default", "pathsep", "there", "override", "pstats", "seek", "rmtree", "settingsdict", "tes", "test1", "mkdtemp", "get", "testenv", "return", "stdout", "commands", "class", "ext", "fields", "finally", "kwargs", "exists", "filename"], "ast_kind": "class_or_type", "text": "import json\nimport os\nimport pstats\nimport shutil\nimport sys\nimport tempfile\nfrom io import StringIO\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\n\nfrom scrapy.utils.test import get_testenv\n\n\nclass TestCmdline:\n    def setup_method(self):\n        self.env = get_testenv()\n        tests_path = Path(__file__).parent.parent\n        self.env[\"PYTHONPATH\"] += os.pathsep + str(tests_path.parent)\n        self.env[\"SCRAPY_SETTINGS_MODULE\"] = \"tests.test_cmdline.settings\"\n\n    def _execute(self, *new_args, **kwargs):\n        encoding = sys.stdout.encoding or \"utf-8\"\n        args = (sys.executable, \"-m\", \"scrapy.cmdline\", *new_args)\n        proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n\n    def test_default_settings(self):\n        assert self._execute(\"settings\", \"--get\", \"TEST1\") == \"default\"\n\n    def test_override_settings_using_set_arg(self):\n        assert (\n            self._execute(\"settings\", \"--get\", \"TEST1\", \"-s\", \"TEST1=override\")\n            == \"override\"\n        )\n\n    def test_profiling(self):\n        path = Path(tempfile.mkdtemp())\n        filename = path / \"res.prof\"\n        try:\n            self._execute(\"version\", \"--profile\", str(filename))\n            assert filename.exists()\n            out = StringIO()\n            stats = pstats.Stats(str(filename), stream=out)\n            stats.print_stats()\n            out.seek(0)\n            stats = out.read()\n            assert str(Path(\"scrapy\", \"commands\", \"version.py\")) in stats\n            assert \"tottime\" in stats\n        finally:\n            shutil.rmtree(path)\n\n    def test_override_dict_settings(self):\n        EXT_PATH = \"tests.test_cmdline.extensions.DummyExtension\"\n        EXTENSIONS = {EXT_PATH: 200}\n        settingsstr = self._execute(\n            \"settings\",\n            \"--get\",\n            \"EXTENSIONS\",\n            \"-s\",\n            \"EXTENSIONS=\" + json.dumps(EXTENSIONS),\n        )\n        # XXX: There's gotta be a smarter way to do this...\n        assert \"...\" not in settingsstr\n        for char in (\"'\", \"<\", \">\"):\n            settingsstr = settingsstr.replace(char, '\"')\n        settingsdict = json.loads(settingsstr)\n        assert set(settingsdict.keys()) == set(EXTENSIONS.keys())\n        assert settingsdict[EXT_PATH] == 200\n\n    def test_pathlib_path_as_feeds_key(self):\n        assert self._execute(\"settings\", \"--get\", \"FEEDS\") == json.dumps(\n            {\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}\n        )\n", "n_tokens": 590, "byte_len": 2551, "file_sha1": "c2d9a6ffa4cc5637e747277034c5d4b9d2bed89c", "start_line": 1, "end_line": 75}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline/extensions.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline/extensions.py", "rel_path": "tests/test_cmdline/extensions.py", "module": "tests.test_cmdline.extensions", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "TestExtension", "DummyExtension", "used", "pass", "tes", "test1", "return", "test", "extension", "check", "class", "classmethod", "init", "started", "from", "crawler", "settings", "order", "dummy", "loading", "self"], "ast_kind": "class_or_type", "text": "\"\"\"A test extension used to check the settings loading order\"\"\"\n\n\nclass TestExtension:\n    def __init__(self, settings):\n        settings.set(\"TEST1\", f\"{settings['TEST1']} + started\")\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.settings)\n\n\nclass DummyExtension:\n    pass\n", "n_tokens": 69, "byte_len": 309, "file_sha1": "6a3a23dad70a29024884e50d1858f71b9a7d1fe6", "start_line": 1, "end_line": 15}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline/settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline/settings.py", "rel_path": "tests/test_cmdline/settings.py", "module": "tests.test_cmdline.settings", "ext": "py", "chunk_number": 1, "symbols": ["test", "extension", "name", "cmdline", "price", "from", "import", "default", "tests", "extensions", "items", "format", "tes", "test1", "path", "pathlib", "feeds", "fields"], "ast_kind": "imports", "text": "from pathlib import Path\n\nEXTENSIONS = {\n    \"tests.test_cmdline.extensions.TestExtension\": 0,\n}\n\nTEST1 = \"default\"\n\nFEEDS = {\n    Path(\"items.csv\"): {\n        \"format\": \"csv\",\n        \"fields\": [\"price\", \"name\"],\n    },\n}\n", "n_tokens": 61, "byte_len": 223, "file_sha1": "3544b4c55db7bd51df54a8e0460edde0e29e35da", "start_line": 1, "end_line": 15}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/proxy_echo.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/proxy_echo.py", "rel_path": "tests/mockserver/proxy_echo.py", "module": "tests.mockserver.proxy_echo", "ext": "py", "chunk_number": 1, "symbols": ["ProxyEchoMockServer", "used", "uri", "resource", "name", "annotations", "class", "main", "future", "mockserver", "proxy", "echo", "test", "downloader", "from", "factory", "http", "base", "module", "resources", "import", "this", "tests", "only", "mock"], "ast_kind": "class_or_type", "text": "# This is only used by tests.test_downloader_handlers_http_base.TestHttpProxyBase\n\nfrom __future__ import annotations\n\nfrom .http_base import BaseMockServer, main_factory\nfrom .http_resources import UriResource\n\n\nclass ProxyEchoMockServer(BaseMockServer):\n    module_name = \"tests.mockserver.proxy_echo\"\n\n\nmain = main_factory(UriResource)\n\n\nif __name__ == \"__main__\":\n    main()\n", "n_tokens": 83, "byte_len": 379, "file_sha1": "5a12a777e303358108a1f3377efc7ee0378c8bb6", "start_line": 1, "end_line": 18}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http_resources.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http_resources.py", "rel_path": "tests/mockserver/http_resources.py", "module": "tests.mockserver.http_resources", "ext": "py", "chunk_number": 1, "symbols": ["getarg", "close_connection", "__init__", "render", "deferRequest", "_cancelrequest", "renderRequest", "render_GET", "_delayedRender", "ForeverTakingResource", "HostHeaderResource", "PayloadResource", "LeafResource", "Follow", "Delay", "Status", "Raw", "Echo", "RedirectTo", "dumps", "started", "writing", "defer", "later", "get", "all", "cancelrequest", "connection", "name", "forever", "response", "getChild", "Partial", "Drop", "ArbitraryLengthPayloadResource", "NoMetaRefreshRedirect", "ContentLengthHeaderResource", "ChunkedResource", "BrokenChunkedResource", "BrokenDownloadResource", "EmptyContentTypeHeaderResource", "LargeChunkedFileResource", "DuplicateHeaderResource", "UriResource", "method", "broken", "download", "redirecting", "large", "chunked"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport json\nimport random\nfrom urllib.parse import urlencode\n\nfrom twisted.internet.task import deferLater\nfrom twisted.web import resource, server\nfrom twisted.web.server import NOT_DONE_YET\nfrom twisted.web.util import Redirect, redirectTo\n\nfrom scrapy.utils.python import to_bytes, to_unicode\n\n\ndef getarg(request, name, default=None, type_=None):\n    if name in request.args:\n        value = request.args[name][0]\n        if type_ is not None:\n            value = type_(value)\n        return value\n    return default\n\n\ndef close_connection(request):\n    # We have to force a disconnection for HTTP/1.1 clients. Otherwise\n    # client keeps the connection open waiting for more data.\n    request.channel.loseConnection()\n    request.finish()\n\n\n# most of the following resources are copied from twisted.web.test.test_webclient\nclass ForeverTakingResource(resource.Resource):\n    \"\"\"\n    L{ForeverTakingResource} is a resource which never finishes responding\n    to requests.\n    \"\"\"\n\n    def __init__(self, write=False):\n        resource.Resource.__init__(self)\n        self._write = write\n\n    def render(self, request):\n        if self._write:\n            request.write(b\"some bytes\")\n        return server.NOT_DONE_YET\n\n\nclass HostHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of the host header\n    from the request.\n    \"\"\"\n\n    def render(self, request):\n        return request.requestHeaders.getRawHeaders(b\"host\")[0]\n\n\nclass PayloadResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the contents of the request body\n    as long as the request body is 100 bytes long, otherwise which renders\n    itself as C{\"ERROR\"}.\n    \"\"\"\n\n    def render(self, request):\n        data = request.content.read()\n        contentLength = request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n        if len(data) != 100 or int(contentLength) != 100:\n            return b\"ERROR\"\n        return data\n\n\nclass LeafResource(resource.Resource):\n    isLeaf = True\n\n    def deferRequest(self, request, delay, f, *a, **kw):\n        from twisted.internet import reactor\n\n        def _cancelrequest(_):\n            # silence CancelledError\n            d.addErrback(lambda _: None)\n            d.cancel()\n\n        d = deferLater(reactor, delay, f, *a, **kw)\n        request.notifyFinish().addErrback(_cancelrequest)\n        return d\n\n\nclass Follow(LeafResource):\n    def render(self, request):\n        total = getarg(request, b\"total\", 100, type_=int)\n        show = getarg(request, b\"show\", 1, type_=int)\n        order = getarg(request, b\"order\", b\"desc\")\n        maxlatency = getarg(request, b\"maxlatency\", 0, type_=float)\n        n = getarg(request, b\"n\", total, type_=int)\n        if order == b\"rand\":\n            nlist = [random.randint(1, total) for _ in range(show)]\n        else:  # order == \"desc\"\n            nlist = range(n, max(n - show, 0), -1)\n\n        lag = random.random() * maxlatency\n        self.deferRequest(request, lag, self.renderRequest, request, nlist)\n        return NOT_DONE_YET\n\n    def renderRequest(self, request, nlist):\n        s = \"\"\"<html> <head></head> <body>\"\"\"\n        args = request.args.copy()\n        for nl in nlist:\n            args[b\"n\"] = [to_bytes(str(nl))]\n            argstr = urlencode(args, doseq=True)\n            s += f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n        s += \"\"\"</body>\"\"\"\n        request.write(to_bytes(s))\n        request.finish()\n\n\nclass Delay(LeafResource):\n    def render_GET(self, request):\n        n = getarg(request, b\"n\", 1, type_=float)\n        b = getarg(request, b\"b\", 1, type_=int)\n        if b:\n            # send headers now and delay body\n            request.write(\"\")\n        self.deferRequest(request, n, self._delayedRender, request, n)\n        return NOT_DONE_YET\n\n    def _delayedRender(self, request, n):\n        request.write(to_bytes(f\"Response delayed for {n:.3f} seconds\\n\"))\n        request.finish()\n\n\nclass Status(LeafResource):\n    def render_GET(self, request):\n        n = getarg(request, b\"n\", 200, type_=int)\n        request.setResponseCode(n)\n        return b\"\"\n\n\nclass Raw(LeafResource):\n    def render_GET(self, request):\n        request.startedWriting = 1\n        self.deferRequest(request, 0, self._delayedRender, request)\n        return NOT_DONE_YET\n\n    render_POST = render_GET\n\n    def _delayedRender(self, request):\n        raw = getarg(request, b\"raw\", b\"HTTP 1.1 200 OK\\n\")\n        request.startedWriting = 1\n        request.write(raw)\n        request.channel.transport.loseConnection()\n        request.finish()\n\n\nclass Echo(LeafResource):\n    def render_GET(self, request):\n        output = {\n            \"headers\": {\n                to_unicode(k): [to_unicode(v) for v in vs]\n                for k, vs in request.requestHeaders.getAllRawHeaders()\n            },\n            \"body\": to_unicode(request.content.read()),\n        }\n        return to_bytes(json.dumps(output))\n\n    render_POST = render_GET\n\n\nclass RedirectTo(LeafResource):", "n_tokens": 1190, "byte_len": 5062, "file_sha1": "331ef29600145f5e1c542c814aacc2b6d8352491", "start_line": 1, "end_line": 169}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http_resources.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http_resources.py", "rel_path": "tests/mockserver/http_resources.py", "module": "tests.mockserver.http_resources", "ext": "py", "chunk_number": 2, "symbols": ["render", "render_GET", "_delayedRender", "response", "getChild", "Partial", "Drop", "ArbitraryLengthPayloadResource", "NoMetaRefreshRedirect", "ContentLengthHeaderResource", "ChunkedResource", "BrokenChunkedResource", "BrokenDownloadResource", "EmptyContentTypeHeaderResource", "LargeChunkedFileResource", "DuplicateHeaderResource", "UriResource", "method", "broken", "download", "connection", "redirecting", "large", "chunked", "https", "length", "equiv", "chunk", "defer", "request", "getarg", "close_connection", "__init__", "deferRequest", "_cancelrequest", "renderRequest", "ForeverTakingResource", "HostHeaderResource", "PayloadResource", "LeafResource", "Follow", "Delay", "Status", "Raw", "Echo", "RedirectTo", "dumps", "started", "writing", "later"], "ast_kind": "class_or_type", "text": "    def render(self, request):\n        goto = getarg(request, b\"goto\", b\"/\")\n        # we force the body content, otherwise Twisted redirectTo()\n        # returns HTML with <meta http-equiv=\"refresh\"\n        redirectTo(goto, request)\n        return b\"redirecting...\"\n\n\nclass Partial(LeafResource):\n    def render_GET(self, request):\n        request.setHeader(b\"Content-Length\", b\"1024\")\n        self.deferRequest(request, 0, self._delayedRender, request)\n        return NOT_DONE_YET\n\n    def _delayedRender(self, request):\n        request.write(b\"partial content\\n\")\n        request.finish()\n\n\nclass Drop(Partial):\n    def _delayedRender(self, request):\n        abort = getarg(request, b\"abort\", 0, type_=int)\n        request.write(b\"this connection will be dropped\\n\")\n        tr = request.channel.transport\n        try:\n            if abort and hasattr(tr, \"abortConnection\"):\n                tr.abortConnection()\n            else:\n                tr.loseConnection()\n        finally:\n            request.finish()\n\n\nclass ArbitraryLengthPayloadResource(LeafResource):\n    def render(self, request):\n        return request.content.read()\n\n\nclass NoMetaRefreshRedirect(Redirect):\n    def render(self, request: server.Request) -> bytes:\n        content = Redirect.render(self, request)\n        return content.replace(\n            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n        )\n\n\nclass ContentLengthHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of the Content-Length\n    header from the request.\n    \"\"\"\n\n    def render(self, request):\n        return request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n\n\nclass ChunkedResource(resource.Resource):\n    def render(self, request):\n        from twisted.internet import reactor\n\n        def response():\n            request.write(b\"chunked \")\n            request.write(b\"content\\n\")\n            request.finish()\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass BrokenChunkedResource(resource.Resource):\n    def render(self, request):\n        from twisted.internet import reactor\n\n        def response():\n            request.write(b\"chunked \")\n            request.write(b\"content\\n\")\n            # Disable terminating chunk on finish.\n            request.chunked = False\n            close_connection(request)\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass BrokenDownloadResource(resource.Resource):\n    def render(self, request):\n        from twisted.internet import reactor\n\n        def response():\n            request.setHeader(b\"Content-Length\", b\"20\")\n            request.write(b\"partial\")\n            close_connection(request)\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass EmptyContentTypeHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of request body\n    without content-type header in response.\n    \"\"\"\n\n    def render(self, request):\n        request.setHeader(\"content-type\", \"\")\n        return request.content.read()\n\n\nclass LargeChunkedFileResource(resource.Resource):\n    def render(self, request):\n        from twisted.internet import reactor\n\n        def response():\n            for i in range(1024):\n                request.write(b\"x\" * 1024)\n            request.finish()\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass DuplicateHeaderResource(resource.Resource):\n    def render(self, request):\n        request.responseHeaders.setRawHeaders(b\"Set-Cookie\", [b\"a=b\", b\"c=d\"])\n        return b\"\"\n\n\nclass UriResource(resource.Resource):\n    \"\"\"Return the full uri that was requested\"\"\"\n\n    def getChild(self, path, request):\n        return self\n\n    def render(self, request):\n        # Note: this is an ugly hack for CONNECT request timeout test.\n        #       Returning some data here fail SSL/TLS handshake\n        # ToDo: implement proper HTTPS proxy tests, not faking them.\n        if request.method != b\"CONNECT\":\n            return request.uri\n        return b\"\"\n", "n_tokens": 821, "byte_len": 4105, "file_sha1": "331ef29600145f5e1c542c814aacc2b6d8352491", "start_line": 170, "end_line": 310}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/dns.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/dns.py", "rel_path": "tests/mockserver/dns.py", "module": "tests.mockserver.dns", "ext": "py", "chunk_number": 1, "symbols": ["_resolve", "query", "lookupAllRecords", "__enter__", "__exit__", "main", "print_listening", "MockDNSResolver", "MockDNSServer", "address", "protocol", "implements", "factory", "subprocess", "internet", "get", "script", "mock", "dns", "exit", "twisted", "payload", "return", "stdout", "traceback", "name", "annotations", "listen", "udp", "class", "header", "rrheader", "kill", "names", "interfaces", "domain", "error", "record", "future", "defer", "port", "exc", "type", "mockserver", "listener", "succeed", "print", "call", "when", "host"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport sys\nfrom subprocess import PIPE, Popen\n\nfrom twisted.internet import defer\nfrom twisted.names import dns, error\nfrom twisted.names.server import DNSServerFactory\n\nfrom tests.utils import get_script_run_env\n\n\nclass MockDNSResolver:\n    \"\"\"\n    Implements twisted.internet.interfaces.IResolver partially\n    \"\"\"\n\n    def _resolve(self, name):\n        record = dns.Record_A(address=b\"127.0.0.1\")\n        answer = dns.RRHeader(name=name, payload=record)\n        return [answer], [], []\n\n    def query(self, query, timeout=None):\n        if query.type == dns.A:\n            return defer.succeed(self._resolve(query.name.name))\n        return defer.fail(error.DomainError())\n\n    def lookupAllRecords(self, name, timeout=None):\n        return defer.succeed(self._resolve(name))\n\n\nclass MockDNSServer:\n    def __enter__(self):\n        self.proc = Popen(\n            [sys.executable, \"-u\", \"-m\", \"tests.mockserver.dns\"],\n            stdout=PIPE,\n            env=get_script_run_env(),\n        )\n        self.host = \"127.0.0.1\"\n        self.port = int(\n            self.proc.stdout.readline().strip().decode(\"ascii\").split(\":\")[1]\n        )\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.communicate()\n\n\ndef main() -> None:\n    from twisted.internet import reactor\n\n    clients = [MockDNSResolver()]\n    factory = DNSServerFactory(clients=clients)\n    protocol = dns.DNSDatagramProtocol(controller=factory)\n    listener = reactor.listenUDP(0, protocol)\n\n    def print_listening():\n        host = listener.getHost()\n        print(f\"{host.host}:{host.port}\")\n\n    reactor.callWhenRunning(print_listening)\n    reactor.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "n_tokens": 399, "byte_len": 1767, "file_sha1": "2d41edb566097c38b9e218b6e0a373c8abd2c4ce", "start_line": 1, "end_line": 68}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http_base.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http_base.py", "rel_path": "tests/mockserver/http_base.py", "module": "tests.mockserver.http_base", "ext": "py", "chunk_number": 1, "symbols": ["module_name", "__init__", "__enter__", "__exit__", "get_additional_args", "port", "url", "main_factory", "main", "print_listening", "BaseMockServer", "bool", "get", "additional", "traceback", "doesn", "https", "address", "future", "typ", "checking", "string", "enter", "factory", "ssl", "context", "none", "ascii", "server", "type", "parse", "http", "default", "callable", "keyfile", "secure", "internet", "script", "functions", "typing", "resource", "cipher", "return", "least", "stdout", "listen", "tcp", "annotations", "class", "provide"], "ast_kind": "class_or_type", "text": "\"\"\"Base classes and functions for HTTP mockservers.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport sys\nfrom abc import ABC, abstractmethod\nfrom subprocess import PIPE, Popen\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urlparse\n\nfrom twisted.web.server import Site\n\nfrom tests.utils import get_script_run_env\n\nfrom .utils import ssl_context_factory\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n    from twisted.web import resource\n\n\nclass BaseMockServer(ABC):\n    listen_http: bool = True\n    listen_https: bool = True\n\n    @property\n    @abstractmethod\n    def module_name(self) -> str:\n        raise NotImplementedError\n\n    def __init__(self) -> None:\n        if not self.listen_http and not self.listen_https:\n            raise ValueError(\"At least one of listen_http and listen_https must be set\")\n\n        self.proc: Popen | None = None\n        self.host: str = \"127.0.0.1\"\n        self.http_port: int | None = None\n        self.https_port: int | None = None\n\n    def __enter__(self):\n        self.proc = Popen(\n            [sys.executable, \"-u\", \"-m\", self.module_name, *self.get_additional_args()],\n            stdout=PIPE,\n            env=get_script_run_env(),\n        )\n        if self.listen_http:\n            http_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n            http_parsed = urlparse(http_address)\n            self.http_port = http_parsed.port\n        if self.listen_https:\n            https_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n            https_parsed = urlparse(https_address)\n            self.https_port = https_parsed.port\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.proc:\n            self.proc.kill()\n            self.proc.communicate()\n\n    def get_additional_args(self) -> list[str]:\n        return []\n\n    def port(self, is_secure: bool = False) -> int:\n        if not is_secure and not self.listen_http:\n            raise ValueError(\"This server doesn't provide HTTP\")\n        if is_secure and not self.listen_https:\n            raise ValueError(\"This server doesn't provide HTTPS\")\n        port = self.https_port if is_secure else self.http_port\n        assert port is not None\n        return port\n\n    def url(self, path: str, is_secure: bool = False) -> str:\n        port = self.port(is_secure)\n        scheme = \"https\" if is_secure else \"http\"\n        return f\"{scheme}://{self.host}:{port}{path}\"\n\n\ndef main_factory(\n    resource_class: type[resource.Resource],\n    *,\n    listen_http: bool = True,\n    listen_https: bool = True,\n) -> Callable[[], None]:\n    if not listen_http and not listen_https:\n        raise ValueError(\"At least one of listen_http and listen_https must be set\")\n\n    def main() -> None:\n        from twisted.internet import reactor\n\n        root = resource_class()\n        factory = Site(root)\n\n        if listen_http:\n            http_port = reactor.listenTCP(0, factory)\n\n        if listen_https:\n            parser = argparse.ArgumentParser()\n            parser.add_argument(\"--keyfile\", help=\"SSL key file\")\n            parser.add_argument(\"--certfile\", help=\"SSL certificate file\")\n            parser.add_argument(\n                \"--cipher-string\",\n                default=None,\n                help=\"SSL cipher string (optional)\",\n            )\n            args = parser.parse_args()\n            context_factory_kw = {}\n            if args.keyfile:\n                context_factory_kw[\"keyfile\"] = args.keyfile\n            if args.certfile:\n                context_factory_kw[\"certfile\"] = args.certfile\n            if args.cipher_string:\n                context_factory_kw[\"cipher_string\"] = args.cipher_string\n            context_factory = ssl_context_factory(**context_factory_kw)\n            https_port = reactor.listenSSL(0, factory, context_factory)\n\n        def print_listening():\n            if listen_http:\n                http_host = http_port.getHost()\n                http_address = f\"http://{http_host.host}:{http_host.port}\"\n                print(http_address)\n            if listen_https:\n                https_host = https_port.getHost()\n                https_address = f\"https://{https_host.host}:{https_host.port}\"\n                print(https_address)\n\n        reactor.callWhenRunning(print_listening)\n        reactor.run()\n\n    return main\n", "n_tokens": 905, "byte_len": 4361, "file_sha1": "71b7b9514afe2019377d2ea5500c14d489282312", "start_line": 1, "end_line": 133}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/utils.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/utils.py", "rel_path": "tests/mockserver/utils.py", "module": "tests.mockserver.utils", "ext": "py", "chunk_number": 1, "symbols": ["ssl_context_factory", "factory", "internet", "python", "file", "get", "context", "twisted", "cipher", "string", "tlsv1", "return", "annotations", "scrapy", "future", "open", "ssl", "pathlib", "path", "unconditionally", "some", "disabling", "enables", "certfile", "set", "from", "ciphers", "parent", "keys", "none", "localhost", "tls1", "default", "utils", "import", "because", "bytes", "ciphe", "serve", "strong", "options", "keyfile"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nfrom pathlib import Path\n\nfrom OpenSSL import SSL\nfrom twisted.internet import ssl\n\nfrom scrapy.utils.python import to_bytes\n\n\ndef ssl_context_factory(\n    keyfile=\"keys/localhost.key\", certfile=\"keys/localhost.crt\", cipher_string=None\n):\n    factory = ssl.DefaultOpenSSLContextFactory(\n        str(Path(__file__).parent.parent / keyfile),\n        str(Path(__file__).parent.parent / certfile),\n    )\n    if cipher_string:\n        ctx = factory.getContext()\n        # disabling TLS1.3 because it unconditionally enables some strong ciphers\n        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_3)\n        ctx.set_cipher_list(to_bytes(cipher_string))\n    return factory\n", "n_tokens": 158, "byte_len": 724, "file_sha1": "23a7e3afff23d486f5c6f0c9642c8f7be096fec4", "start_line": 1, "end_line": 24}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/simple_https.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/simple_https.py", "rel_path": "tests/mockserver/simple_https.py", "module": "tests.mockserver.simple_https", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "getChild", "get_additional_args", "Root", "SimpleMockServer", "used", "simple", "https", "text", "false", "root", "get", "additional", "plain", "resource", "twisted", "cipher", "string", "return", "name", "args", "annotations", "class", "put", "child", "main", "future", "mockserver", "test", "downloader", "init", "certfile", "data", "from", "factory", "list", "http", "base", "module", "file", "none", "super", "import", "mock", "static", "self", "this", "tests", "only", "extend"], "ast_kind": "class_or_type", "text": "# This is only used by tests.test_downloader_handlers_http_base.TestSimpleHttpsBase\n\nfrom __future__ import annotations\n\nfrom twisted.web import resource\nfrom twisted.web.static import Data\n\nfrom .http_base import BaseMockServer, main_factory\n\n\nclass Root(resource.Resource):\n    def __init__(self):\n        resource.Resource.__init__(self)\n        self.putChild(b\"file\", Data(b\"0123456789\", \"text/plain\"))\n\n    def getChild(self, name, request):\n        return self\n\n\nclass SimpleMockServer(BaseMockServer):\n    listen_http = False\n    module_name = \"tests.mockserver.simple_https\"\n\n    def __init__(self, keyfile: str, certfile: str, cipher_string: str | None):\n        super().__init__()\n        self.keyfile = keyfile\n        self.certfile = certfile\n        self.cipher_string = cipher_string or \"\"\n\n    def get_additional_args(self) -> list[str]:\n        args = [\n            \"--keyfile\",\n            self.keyfile,\n            \"--certfile\",\n            self.certfile,\n        ]\n        if self.cipher_string is not None:\n            args.extend([\"--cipher-string\", self.cipher_string])\n        return args\n\n\nmain = main_factory(Root, listen_http=False)\n\n\nif __name__ == \"__main__\":\n    main()\n", "n_tokens": 269, "byte_len": 1199, "file_sha1": "328f88cb111f0e11f5aa98a2bcce17a7799c2f96", "start_line": 1, "end_line": 47}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/http.py", "rel_path": "tests/mockserver/http.py", "module": "tests.mockserver.http", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "getChild", "render", "Root", "MockServer", "encoding", "gb18030", "broken", "download", "works", "payload", "after", "large", "chunked", "name", "forever", "taking", "redirect", "redirectto", "future", "main", "path", "mockserver", "mock", "numbers", "delay", "factory", "http", "base", "echo", "here", "partial", "encode", "join", "server", "html", "follow", "world", "get", "child", "utf", "utf8", "largechunkedfile", "resource", "drop", "return", "annotations", "class", "meta", "duplicate"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom pathlib import Path\n\nfrom twisted.web import resource\nfrom twisted.web.static import Data, File\nfrom twisted.web.util import Redirect\n\nfrom tests import tests_datadir\n\nfrom .http_base import BaseMockServer, main_factory\nfrom .http_resources import (\n    ArbitraryLengthPayloadResource,\n    BrokenChunkedResource,\n    BrokenDownloadResource,\n    ChunkedResource,\n    ContentLengthHeaderResource,\n    Delay,\n    Drop,\n    DuplicateHeaderResource,\n    Echo,\n    EmptyContentTypeHeaderResource,\n    Follow,\n    ForeverTakingResource,\n    HostHeaderResource,\n    LargeChunkedFileResource,\n    NoMetaRefreshRedirect,\n    Partial,\n    PayloadResource,\n    Raw,\n    RedirectTo,\n    Status,\n)\n\n\nclass Root(resource.Resource):\n    def __init__(self):\n        super().__init__()\n        self.putChild(b\"status\", Status())\n        self.putChild(b\"follow\", Follow())\n        self.putChild(b\"delay\", Delay())\n        self.putChild(b\"partial\", Partial())\n        self.putChild(b\"drop\", Drop())\n        self.putChild(b\"raw\", Raw())\n        self.putChild(b\"echo\", Echo())\n        self.putChild(b\"payload\", PayloadResource())\n        self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n        self.putChild(b\"static\", File(str(Path(tests_datadir, \"test_site/\"))))\n        self.putChild(b\"redirect-to\", RedirectTo())\n        self.putChild(b\"text\", Data(b\"Works\", \"text/plain\"))\n        self.putChild(\n            b\"html\",\n            Data(\n                b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n                \"text/html\",\n            ),\n        )\n        self.putChild(\n            b\"enc-gb18030\",\n            Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n        )\n        self.putChild(b\"redirect\", Redirect(b\"/redirected\"))\n        self.putChild(\n            b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\")\n        )\n        self.putChild(b\"redirected\", Data(b\"Redirected here\", \"text/plain\"))\n        numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n        self.putChild(b\"numbers\", Data(b\"\".join(numbers), \"text/plain\"))\n        self.putChild(b\"wait\", ForeverTakingResource())\n        self.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n        self.putChild(b\"host\", HostHeaderResource())\n        self.putChild(b\"broken\", BrokenDownloadResource())\n        self.putChild(b\"chunked\", ChunkedResource())\n        self.putChild(b\"broken-chunked\", BrokenChunkedResource())\n        self.putChild(b\"contentlength\", ContentLengthHeaderResource())\n        self.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n        self.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n        self.putChild(b\"duplicate-header\", DuplicateHeaderResource())\n\n    def getChild(self, name, request):\n        return self\n\n    def render(self, request):\n        return b\"Scrapy mock HTTP server\\n\"\n\n\nclass MockServer(BaseMockServer):\n    module_name = \"tests.mockserver.http\"\n\n\nmain = main_factory(Root)\n\n\nif __name__ == \"__main__\":\n    main()\n", "n_tokens": 700, "byte_len": 3068, "file_sha1": "186ff2ca12b7093f14ca664cdabd1d77c934596a", "start_line": 1, "end_line": 96}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/ftp.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/mockserver/ftp.py", "rel_path": "tests/mockserver/ftp.py", "module": "tests.mockserver.ftp", "ext": "py", "chunk_number": 1, "symbols": ["__enter__", "__exit__", "url", "main", "MockFTPServer", "address", "argparse", "user", "starting", "perm", "shutil", "rmtree", "subprocess", "get", "script", "mkdtemp", "exit", "add", "anonymous", "authorizer", "return", "break", "traceback", "elradfmw", "elradfmwmt", "args", "annotations", "name", "class", "full", "permissions", "with", "tempfile", "temporary", "path", "servers", "attr", "future", "port", "exc", "type", "pathlib", "mockserver", "dummy", "argument", "stderr", "enter", "from", "pipe", "mock"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport sys\nfrom argparse import ArgumentParser\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom subprocess import PIPE, Popen\nfrom tempfile import mkdtemp\n\nfrom pyftpdlib.authorizers import DummyAuthorizer\nfrom pyftpdlib.handlers import FTPHandler\nfrom pyftpdlib.servers import FTPServer\n\nfrom tests.utils import get_script_run_env\n\n\nclass MockFTPServer:\n    \"\"\"Creates an FTP server on port 2121 with a default passwordless user\n    (anonymous) and a temporary root path that you can read from the\n    :attr:`path` attribute.\"\"\"\n\n    def __enter__(self):\n        self.path = Path(mkdtemp())\n        self.proc = Popen(\n            [sys.executable, \"-u\", \"-m\", \"tests.mockserver.ftp\", \"-d\", str(self.path)],\n            stderr=PIPE,\n            env=get_script_run_env(),\n        )\n        for line in self.proc.stderr:\n            if b\"starting FTP server\" in line:\n                break\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        rmtree(str(self.path))\n        self.proc.kill()\n        self.proc.communicate()\n\n    def url(self, path):\n        return \"ftp://127.0.0.1:2121/\" + path\n\n\ndef main() -> None:\n    parser = ArgumentParser()\n    parser.add_argument(\"-d\", \"--directory\")\n    args = parser.parse_args()\n\n    authorizer = DummyAuthorizer()\n    full_permissions = \"elradfmwMT\"\n    authorizer.add_anonymous(args.directory, perm=full_permissions)\n    handler = FTPHandler\n    handler.authorizer = authorizer\n    address = (\"127.0.0.1\", 2121)\n    server = FTPServer(address, handler)\n    server.serve_forever()\n\n\nif __name__ == \"__main__\":\n    main()\n", "n_tokens": 398, "byte_len": 1638, "file_sha1": "85ee889d5e07ad4215e8864e175dc442a4ce028f", "start_line": 1, "end_line": 60}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/utils/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/utils/__init__.py", "rel_path": "tests/utils/__init__.py", "module": "tests.utils.__init__", "ext": "py", "chunk_number": 1, "symbols": ["twisted_sleep", "get_script_run_env", "internet", "get", "script", "twisted", "sleep", "file", "pythonpath", "return", "dict", "tests", "path", "with", "environ", "defer", "pathlib", "call", "later", "from", "scripts", "parent", "copy", "suitable", "none", "seconds", "environment", "reactor", "shipped", "import", "callback", "deferred", "pathsep"], "ast_kind": "function_or_method", "text": "import os\nfrom pathlib import Path\n\nfrom twisted.internet.defer import Deferred\n\n\ndef twisted_sleep(seconds):\n    from twisted.internet import reactor\n\n    d = Deferred()\n    reactor.callLater(seconds, d.callback, None)\n    return d\n\n\ndef get_script_run_env() -> dict[str, str]:\n    \"\"\"Return a OS environment dict suitable to run scripts shipped with tests.\"\"\"\n\n    tests_path = Path(__file__).parent.parent\n    pythonpath = str(tests_path) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = pythonpath\n    return env\n", "n_tokens": 128, "byte_len": 568, "file_sha1": "24422185166a9ffd22e53063bd6bfd8c32467a89", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/__init__.py", "rel_path": "tests/test_spiderloader/__init__.py", "module": "tests.test_spiderloader.__init__", "ext": "py", "chunk_number": 1, "symbols": ["_copytree", "spider_loader_env", "spider_loader", "test_interface", "test_list", "test_load", "test_find_by_request", "test_load_spider_module", "test_load_spider_module_multiple", "test_load_base_spider", "test_load_spider_module_from_addons", "update_pre_crawler_settings", "test_crawler_runner_loading", "test_bad_spider_modules_exception", "TestSpiderLoader", "SpiderModuleAddon", "spidercls", "append", "file", "spider", "loader", "name", "spiders", "from", "target", "orig", "error", "load", "interfaces", "contextlib", "test_bad_spider_modules_warning", "test_syntax_error_exception", "test_syntax_error_warning", "test_dupename_warning", "test_multiple_dupename_warning", "test_custom_spider_loader", "test_dummy_spider_loader", "TestDuplicateSpiderNameLoader", "CustomSpiderLoader", "dupe", "test", "syntax", "dummy", "each", "https", "path", "mock", "get", "addons", "interface"], "ast_kind": "class_or_type", "text": "import contextlib\nimport shutil\nimport sys\nimport warnings\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\nfrom zope.interface.verify import verifyObject\n\n# ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n# alone\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.http import Request\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.settings import Settings\nfrom scrapy.spiderloader import DummySpiderLoader, SpiderLoader, get_spider_loader\n\nmodule_dir = Path(__file__).resolve().parent\n\n\ndef _copytree(source: Path, target: Path):\n    with contextlib.suppress(shutil.Error):\n        shutil.copytree(source, target)\n\n\n@pytest.fixture\ndef spider_loader_env(tmp_path):\n    orig_spiders_dir = module_dir / \"test_spiders\"\n    spiders_dir = tmp_path / \"test_spiders_xxx\"\n    _copytree(orig_spiders_dir, spiders_dir)\n    sys.path.append(str(tmp_path))\n    settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n\n    yield settings, spiders_dir\n\n    sys.modules.pop(\"test_spiders_xxx\", None)\n    sys.path.remove(str(tmp_path))\n\n\n@pytest.fixture\ndef spider_loader(spider_loader_env):\n    settings, _ = spider_loader_env\n    return SpiderLoader.from_settings(settings)\n\n\nclass TestSpiderLoader:\n    def test_interface(self, spider_loader):\n        verifyObject(ISpiderLoader, spider_loader)\n\n    def test_list(self, spider_loader):\n        assert set(spider_loader.list()) == {\n            \"spider1\",\n            \"spider2\",\n            \"spider3\",\n            \"spider4\",\n        }\n\n    def test_load(self, spider_loader):\n        spider1 = spider_loader.load(\"spider1\")\n        assert spider1.__name__ == \"Spider1\"\n\n    def test_find_by_request(self, spider_loader):\n        assert spider_loader.find_by_request(Request(\"http://scrapy1.org/test\")) == [\n            \"spider1\"\n        ]\n        assert spider_loader.find_by_request(Request(\"http://scrapy2.org/test\")) == [\n            \"spider2\"\n        ]\n        assert set(\n            spider_loader.find_by_request(Request(\"http://scrapy3.org/test\"))\n        ) == {\"spider1\", \"spider2\"}\n        assert spider_loader.find_by_request(Request(\"http://scrapy999.org/test\")) == []\n        assert spider_loader.find_by_request(Request(\"http://spider3.com\")) == []\n        assert spider_loader.find_by_request(\n            Request(\"http://spider3.com/onlythis\")\n        ) == [\"spider3\"]\n\n    def test_load_spider_module(self):\n        module = \"tests.test_spiderloader.test_spiders.spider1\"\n        settings = Settings({\"SPIDER_MODULES\": [module]})\n        spider_loader = SpiderLoader.from_settings(settings)\n        assert len(spider_loader._spiders) == 1\n\n    def test_load_spider_module_multiple(self):\n        prefix = \"tests.test_spiderloader.test_spiders.\"\n        module = \",\".join(prefix + s for s in (\"spider1\", \"spider2\"))\n        settings = Settings({\"SPIDER_MODULES\": module})\n        spider_loader = SpiderLoader.from_settings(settings)\n        assert len(spider_loader._spiders) == 2\n\n    def test_load_base_spider(self):\n        module = \"tests.test_spiderloader.test_spiders.spider0\"\n        settings = Settings({\"SPIDER_MODULES\": [module]})\n        spider_loader = SpiderLoader.from_settings(settings)\n        assert len(spider_loader._spiders) == 0\n\n    def test_load_spider_module_from_addons(self):\n        module = \"tests.test_spiderloader.spiders_from_addons.spider0\"\n\n        class SpiderModuleAddon:\n            @classmethod\n            def update_pre_crawler_settings(cls, settings):\n                settings.set(\n                    \"SPIDER_MODULES\",\n                    [module],\n                    \"project\",\n                )\n\n        runner = CrawlerRunner({\"ADDONS\": {SpiderModuleAddon: 1}})\n\n        crawler = runner.create_crawler(\"spider_from_addon\")\n        assert issubclass(crawler.spidercls, scrapy.Spider)\n        assert crawler.spidercls.name == \"spider_from_addon\"\n        assert len(crawler.settings[\"SPIDER_MODULES\"]) == 1\n\n    def test_crawler_runner_loading(self):\n        module = \"tests.test_spiderloader.test_spiders.spider1\"\n        runner = CrawlerRunner(\n            {\n                \"SPIDER_MODULES\": [module],\n            }\n        )\n\n        with pytest.raises(KeyError, match=\"Spider not found\"):\n            runner.create_crawler(\"spider2\")\n\n        crawler = runner.create_crawler(\"spider1\")\n        assert issubclass(crawler.spidercls, scrapy.Spider)\n        assert crawler.spidercls.name == \"spider1\"\n\n    def test_bad_spider_modules_exception(self):\n        module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n        settings = Settings({\"SPIDER_MODULES\": [module]})\n        with pytest.raises(ImportError):\n            SpiderLoader.from_settings(settings)\n", "n_tokens": 1057, "byte_len": 4738, "file_sha1": "61806ea7149330530b22f88c2a63665ad70bf8f3", "start_line": 1, "end_line": 138}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/__init__.py", "rel_path": "tests/test_spiderloader/__init__.py", "module": "tests.test_spiderloader.__init__", "ext": "py", "chunk_number": 2, "symbols": ["test_bad_spider_modules_warning", "test_syntax_error_exception", "test_syntax_error_warning", "test_dupename_warning", "test_multiple_dupename_warning", "test_custom_spider_loader", "test_dummy_spider_loader", "TestDuplicateSpiderNameLoader", "CustomSpiderLoader", "spider", "dupe", "test", "syntax", "dummy", "each", "loader", "name", "load", "spiders", "https", "mock", "get", "pytest", "doesnotexist", "settings", "isinstance", "object", "issues", "vendored", "spider3", "_copytree", "spider_loader_env", "spider_loader", "test_interface", "test_list", "test_load", "test_find_by_request", "test_load_spider_module", "test_load_spider_module_multiple", "test_load_base_spider", "test_load_spider_module_from_addons", "update_pre_crawler_settings", "test_crawler_runner_loading", "test_bad_spider_modules_exception", "TestSpiderLoader", "SpiderModuleAddon", "spidercls", "append", "file", "from"], "ast_kind": "class_or_type", "text": "    def test_bad_spider_modules_warning(self):\n        with warnings.catch_warnings(record=True) as w:\n            module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n            settings = Settings(\n                {\"SPIDER_MODULES\": [module], \"SPIDER_LOADER_WARN_ONLY\": True}\n            )\n            spider_loader = SpiderLoader.from_settings(settings)\n            if str(w[0].message).startswith(\"_SixMetaPathImporter\"):\n                # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,\n                # at least until all six versions we can import (including botocore.vendored.six)\n                # are updated to 1.16.0+\n                w.pop(0)\n            assert \"Could not load spiders from module\" in str(w[0].message)\n\n            spiders = spider_loader.list()\n            assert not spiders\n\n    def test_syntax_error_exception(self):\n        module = \"tests.test_spiderloader.test_spiders.spider1\"\n        with mock.patch.object(SpiderLoader, \"_load_spiders\") as m:\n            m.side_effect = SyntaxError\n            settings = Settings({\"SPIDER_MODULES\": [module]})\n            with pytest.raises(SyntaxError):\n                SpiderLoader.from_settings(settings)\n\n    def test_syntax_error_warning(self):\n        with (\n            warnings.catch_warnings(record=True) as w,\n            mock.patch.object(SpiderLoader, \"_load_spiders\") as m,\n        ):\n            m.side_effect = SyntaxError\n            module = \"tests.test_spiderloader.test_spiders.spider1\"\n            settings = Settings(\n                {\"SPIDER_MODULES\": [module], \"SPIDER_LOADER_WARN_ONLY\": True}\n            )\n            spider_loader = SpiderLoader.from_settings(settings)\n            if str(w[0].message).startswith(\"_SixMetaPathImporter\"):\n                # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,\n                # at least until all six versions we can import (including botocore.vendored.six)\n                # are updated to 1.16.0+\n                w.pop(0)\n            assert \"Could not load spiders from module\" in str(w[0].message)\n\n            spiders = spider_loader.list()\n            assert not spiders\n\n\nclass TestDuplicateSpiderNameLoader:\n    def test_dupename_warning(self, spider_loader_env):\n        settings, spiders_dir = spider_loader_env\n\n        # copy 1 spider module so as to have duplicate spider name\n        shutil.copyfile(spiders_dir / \"spider3.py\", spiders_dir / \"spider3dupe.py\")\n\n        with warnings.catch_warnings(record=True) as w:\n            spider_loader = SpiderLoader.from_settings(settings)\n\n            assert len(w) == 1\n            msg = str(w[0].message)\n            assert \"several spiders with the same name\" in msg\n            assert \"'spider3'\" in msg\n            assert msg.count(\"'spider3'\") == 2\n\n            assert \"'spider1'\" not in msg\n            assert \"'spider2'\" not in msg\n            assert \"'spider4'\" not in msg\n\n            spiders = set(spider_loader.list())\n            assert spiders == {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n\n    def test_multiple_dupename_warning(self, spider_loader_env):\n        settings, spiders_dir = spider_loader_env\n        # copy 2 spider modules so as to have duplicate spider name\n        # This should issue 2 warning, 1 for each duplicate spider name\n        shutil.copyfile(spiders_dir / \"spider1.py\", spiders_dir / \"spider1dupe.py\")\n        shutil.copyfile(spiders_dir / \"spider2.py\", spiders_dir / \"spider2dupe.py\")\n\n        with warnings.catch_warnings(record=True) as w:\n            spider_loader = SpiderLoader.from_settings(settings)\n\n            assert len(w) == 1\n            msg = str(w[0].message)\n            assert \"several spiders with the same name\" in msg\n            assert \"'spider1'\" in msg\n            assert msg.count(\"'spider1'\") == 2\n\n            assert \"'spider2'\" in msg\n            assert msg.count(\"'spider2'\") == 2\n\n            assert \"'spider3'\" not in msg\n            assert \"'spider4'\" not in msg\n\n            spiders = set(spider_loader.list())\n            assert spiders == {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n\n\nclass CustomSpiderLoader(SpiderLoader):\n    pass\n\n\ndef test_custom_spider_loader():\n    settings = Settings(\n        {\n            \"SPIDER_LOADER_CLASS\": CustomSpiderLoader,\n        }\n    )\n    spider_loader = get_spider_loader(settings)\n    assert isinstance(spider_loader, CustomSpiderLoader)\n\n\ndef test_dummy_spider_loader(spider_loader_env):\n    settings, _ = spider_loader_env\n    spider_loader = DummySpiderLoader.from_settings(settings)\n    assert not spider_loader.list()\n    with pytest.raises(KeyError):\n        spider_loader.load(\"spider1\")\n", "n_tokens": 1066, "byte_len": 4675, "file_sha1": "61806ea7149330530b22f88c2a63665ad70bf8f3", "start_line": 139, "end_line": 255}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider2.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider2.py", "rel_path": "tests/test_spiderloader/test_spiders/spider2.py", "module": "tests.test_spiderloader.test_spiders.spider2", "ext": "py", "chunk_number": 1, "symbols": ["Spider2", "spider", "name", "class", "spider2", "from", "import", "scrapy", "scrapy3", "allowed", "domains", "spiders", "scrapy2"], "ast_kind": "class_or_type", "text": "from scrapy.spiders import Spider\n\n\nclass Spider2(Spider):\n    name = \"spider2\"\n    allowed_domains = [\"scrapy2.org\", \"scrapy3.org\"]\n", "n_tokens": 38, "byte_len": 133, "file_sha1": "c6ad7d2f754a38eb4d8f7cb73ff265d669838d11", "start_line": 1, "end_line": 7}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider3.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider3.py", "rel_path": "tests/test_spiderloader/test_spiders/spider3.py", "module": "tests.test_spiderloader.test_spiders.spider3", "ext": "py", "chunk_number": 1, "symbols": ["handles_request", "Spider3", "spider", "name", "class", "from", "import", "allowed", "domains", "handles", "request", "spiders", "scrapy", "spider3", "http", "classmethod", "onlythis", "return"], "ast_kind": "class_or_type", "text": "from scrapy.spiders import Spider\n\n\nclass Spider3(Spider):\n    name = \"spider3\"\n    allowed_domains = [\"spider3.com\"]\n\n    @classmethod\n    def handles_request(cls, request):\n        return request.url == \"http://spider3.com/onlythis\"\n", "n_tokens": 60, "byte_len": 235, "file_sha1": "46bac2c7b40b596eaf73292424ecef325bb974d5", "start_line": 1, "end_line": 11}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider0.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider0.py", "rel_path": "tests/test_spiderloader/test_spiders/spider0.py", "module": "tests.test_spiderloader.test_spiders.spider0", "ext": "py", "chunk_number": 1, "symbols": ["Spider0", "spider", "class", "scrapy", "scrapy3", "from", "import", "scrapy1", "allowed", "domains", "spider0", "spiders"], "ast_kind": "class_or_type", "text": "from scrapy.spiders import Spider\n\n\nclass Spider0(Spider):\n    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n", "n_tokens": 30, "byte_len": 112, "file_sha1": "ad3857b16b8155c629b97a706950ca8a54abd276", "start_line": 1, "end_line": 6}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider1.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/spider1.py", "rel_path": "tests/test_spiderloader/test_spiders/spider1.py", "module": "tests.test_spiderloader.test_spiders.spider1", "ext": "py", "chunk_number": 1, "symbols": ["Spider1", "spider", "spider1", "name", "class", "scrapy", "scrapy3", "from", "import", "scrapy1", "allowed", "domains", "spiders"], "ast_kind": "class_or_type", "text": "from scrapy.spiders import Spider\n\n\nclass Spider1(Spider):\n    name = \"spider1\"\n    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n", "n_tokens": 38, "byte_len": 133, "file_sha1": "afd63b7283c20771b43eab2f709b572abf3c55c9", "start_line": 1, "end_line": 7}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/nested/spider4.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/test_spiders/nested/spider4.py", "rel_path": "tests/test_spiderloader/test_spiders/nested/spider4.py", "module": "tests.test_spiderloader.test_spiders.nested.spider4", "ext": "py", "chunk_number": 1, "symbols": ["handles_request", "Spider4", "spider", "name", "class", "from", "import", "spider4", "allowed", "domains", "handles", "request", "spiders", "scrapy", "http", "classmethod", "onlythis", "return"], "ast_kind": "class_or_type", "text": "from scrapy.spiders import Spider\n\n\nclass Spider4(Spider):\n    name = \"spider4\"\n    allowed_domains = [\"spider4.com\"]\n\n    @classmethod\n    def handles_request(cls, request):\n        return request.url == \"http://spider4.com/onlythis\"\n", "n_tokens": 60, "byte_len": 235, "file_sha1": "17021e34f1ac3540d5eda5f31652b5faa9f04a55", "start_line": 1, "end_line": 11}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/spiders_from_addons/spider0.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_spiderloader/spiders_from_addons/spider0.py", "rel_path": "tests/test_spiderloader/spiders_from_addons/spider0.py", "module": "tests.test_spiderloader.spiders_from_addons.spider0", "ext": "py", "chunk_number": 1, "symbols": ["SpiderFromAddon", "spider", "name", "class", "scrapy", "scrapy3", "from", "import", "scrapy1", "allowed", "domains", "spiders"], "ast_kind": "class_or_type", "text": "from scrapy.spiders import Spider\n\n\nclass SpiderFromAddon(Spider):\n    name = \"spider_from_addon\"\n    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n", "n_tokens": 41, "byte_len": 151, "file_sha1": "1b60b1290008ce69252a76c3a2a0516ce7b98509", "start_line": 1, "end_line": 7}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 1, "symbols": ["test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "TestSettingsGlobalFuncs", "TestSettingsAttribute", "TestBaseSettings", "new", "dict", "tes", "attr", "getpriority", "test", "setdefault", "false", "mock", "setattr", "four", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs", "test_update_iterable", "test_update_jsonstring", "test_delete", "test_get", "test_getpriority", "test_getwithbase", "test_maxpriority", "test_copy", "test_copy_to_dict", "test_freeze", "test_frozencopy", "test_initial_defaults", "test_initial_values", "test_autopromote_dicts", "test_getdict_autodegrade_basesettings", "test_passing_objects_as_values"], "ast_kind": "class_or_type", "text": "# pylint: disable=unsubscriptable-object,unsupported-membership-test,use-implicit-booleaness-not-comparison\n# (too many false positives)\n\nimport warnings\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.core.downloader.handlers.file import FileDownloadHandler\nfrom scrapy.settings import (\n    SETTINGS_PRIORITIES,\n    BaseSettings,\n    Settings,\n    SettingsAttribute,\n    get_settings_priority,\n)\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.test import get_crawler\n\nfrom . import default_settings\n\n\nclass TestSettingsGlobalFuncs:\n    def test_get_settings_priority(self):\n        for prio_str, prio_num in SETTINGS_PRIORITIES.items():\n            assert get_settings_priority(prio_str) == prio_num\n        assert get_settings_priority(99) == 99\n\n\nclass TestSettingsAttribute:\n    def setup_method(self):\n        self.attribute = SettingsAttribute(\"value\", 10)\n\n    def test_set_greater_priority(self):\n        self.attribute.set(\"value2\", 20)\n        assert self.attribute.value == \"value2\"\n        assert self.attribute.priority == 20\n\n    def test_set_equal_priority(self):\n        self.attribute.set(\"value2\", 10)\n        assert self.attribute.value == \"value2\"\n        assert self.attribute.priority == 10\n\n    def test_set_less_priority(self):\n        self.attribute.set(\"value2\", 0)\n        assert self.attribute.value == \"value\"\n        assert self.attribute.priority == 10\n\n    def test_overwrite_basesettings(self):\n        original_dict = {\"one\": 10, \"two\": 20}\n        original_settings = BaseSettings(original_dict, 0)\n        attribute = SettingsAttribute(original_settings, 0)\n\n        new_dict = {\"three\": 11, \"four\": 21}\n        attribute.set(new_dict, 10)\n        assert isinstance(attribute.value, BaseSettings)\n        assert set(attribute.value) == set(new_dict)\n        assert set(original_settings) == set(original_dict)\n\n        new_settings = BaseSettings({\"five\": 12}, 0)\n        attribute.set(new_settings, 0)  # Insufficient priority\n        assert set(attribute.value) == set(new_dict)\n        attribute.set(new_settings, 10)\n        assert set(attribute.value) == set(new_settings)\n\n    def test_repr(self):\n        assert repr(self.attribute) == \"<SettingsAttribute value='value' priority=10>\"\n\n\nclass TestBaseSettings:\n    def setup_method(self):\n        self.settings = BaseSettings()\n\n    def test_setdefault_not_existing_value(self):\n        settings = BaseSettings()\n        value = settings.setdefault(\"TEST_OPTION\", \"value\")\n        assert settings[\"TEST_OPTION\"] == \"value\"\n        assert value == \"value\"\n        assert value is not None\n\n    def test_setdefault_existing_value(self):\n        settings = BaseSettings({\"TEST_OPTION\": \"value\"})\n        value = settings.setdefault(\"TEST_OPTION\", None)\n        assert settings[\"TEST_OPTION\"] == \"value\"\n        assert value == \"value\"\n\n    def test_set_new_attribute(self):\n        self.settings.set(\"TEST_OPTION\", \"value\", 0)\n        assert \"TEST_OPTION\" in self.settings.attributes\n\n        attr = self.settings.attributes[\"TEST_OPTION\"]\n        assert isinstance(attr, SettingsAttribute)\n        assert attr.value == \"value\"\n        assert attr.priority == 0\n\n    def test_set_settingsattribute(self):\n        myattr = SettingsAttribute(0, 30)  # Note priority 30\n        self.settings.set(\"TEST_ATTR\", myattr, 10)\n        assert self.settings.get(\"TEST_ATTR\") == 0\n        assert self.settings.getpriority(\"TEST_ATTR\") == 30\n\n    def test_set_instance_identity_on_update(self):\n        attr = SettingsAttribute(\"value\", 0)\n        self.settings.attributes = {\"TEST_OPTION\": attr}\n        self.settings.set(\"TEST_OPTION\", \"othervalue\", 10)\n\n        assert \"TEST_OPTION\" in self.settings.attributes\n        assert attr is self.settings.attributes[\"TEST_OPTION\"]\n\n    def test_set_calls_settings_attributes_methods_on_update(self):\n        attr = SettingsAttribute(\"value\", 10)\n        with (\n            mock.patch.object(attr, \"__setattr__\") as mock_setattr,\n            mock.patch.object(attr, \"set\") as mock_set,\n        ):\n            self.settings.attributes = {\"TEST_OPTION\": attr}\n\n            for priority in (0, 10, 20):\n                self.settings.set(\"TEST_OPTION\", \"othervalue\", priority)\n                mock_set.assert_called_once_with(\"othervalue\", priority)\n                assert not mock_setattr.called\n                mock_set.reset_mock()\n                mock_setattr.reset_mock()\n\n    def test_setitem(self):\n        settings = BaseSettings()\n        settings.set(\"key\", \"a\", \"default\")\n        settings[\"key\"] = \"b\"\n        assert settings[\"key\"] == \"b\"\n        assert settings.getpriority(\"key\") == 20\n        settings[\"key\"] = \"c\"\n        assert settings[\"key\"] == \"c\"\n        settings[\"key2\"] = \"x\"\n        assert \"key2\" in settings\n        assert settings[\"key2\"] == \"x\"\n        assert settings.getpriority(\"key2\") == 20\n\n    def test_setdict_alias(self):\n        with mock.patch.object(self.settings, \"set\") as mock_set:\n            self.settings.setdict({\"TEST_1\": \"value1\", \"TEST_2\": \"value2\"}, 10)\n            assert mock_set.call_count == 2\n            calls = [\n                mock.call(\"TEST_1\", \"value1\", 10),\n                mock.call(\"TEST_2\", \"value2\", 10),\n            ]\n            mock_set.assert_has_calls(calls, any_order=True)\n\n    def test_setmodule_only_load_uppercase_vars(self):", "n_tokens": 1183, "byte_len": 5346, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 1, "end_line": 148}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 2, "symbols": ["test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs", "test_update_iterable", "test_update_jsonstring", "test_delete", "ModuleMock", "test", "update", "tes", "default", "newval", "newval2", "setmodule", "number", "custom", "dict", "delete", "anothervalue", "patch", "priority", "input", "notkey", "getpriority", "newkey", "one", "mark", "class", "doesn", "test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_get", "test_getpriority", "test_getwithbase", "test_maxpriority"], "ast_kind": "class_or_type", "text": "        class ModuleMock:\n            UPPERCASE_VAR = \"value\"\n            MIXEDcase_VAR = \"othervalue\"\n            lowercase_var = \"anothervalue\"\n\n        self.settings.attributes = {}\n        self.settings.setmodule(ModuleMock(), 10)\n        assert \"UPPERCASE_VAR\" in self.settings.attributes\n        assert \"MIXEDcase_VAR\" not in self.settings.attributes\n        assert \"lowercase_var\" not in self.settings.attributes\n        assert len(self.settings.attributes) == 1\n\n    def test_setmodule_alias(self):\n        with mock.patch.object(self.settings, \"set\") as mock_set:\n            self.settings.setmodule(default_settings, 10)\n            mock_set.assert_any_call(\"TEST_DEFAULT\", \"defvalue\", 10)\n            mock_set.assert_any_call(\"TEST_DICT\", {\"key\": \"val\"}, 10)\n\n    def test_setmodule_by_path(self):\n        self.settings.attributes = {}\n        self.settings.setmodule(default_settings, 10)\n        ctrl_attributes = self.settings.attributes.copy()\n\n        self.settings.attributes = {}\n        self.settings.setmodule(\"tests.test_settings.default_settings\", 10)\n\n        assert set(self.settings.attributes) == set(ctrl_attributes)\n\n        for key in ctrl_attributes:\n            attr = self.settings.attributes[key]\n            ctrl_attr = ctrl_attributes[key]\n            assert attr.value == ctrl_attr.value\n            assert attr.priority == ctrl_attr.priority\n\n    def test_update(self):\n        settings = BaseSettings({\"key_lowprio\": 0}, priority=0)\n        settings.set(\"key_highprio\", 10, priority=50)\n        custom_settings = BaseSettings(\n            {\"key_lowprio\": 1, \"key_highprio\": 11}, priority=30\n        )\n        custom_settings.set(\"newkey_one\", None, priority=50)\n        custom_dict = {\"key_lowprio\": 2, \"key_highprio\": 12, \"newkey_two\": None}\n\n        settings.update(custom_dict, priority=20)\n        assert settings[\"key_lowprio\"] == 2\n        assert settings.getpriority(\"key_lowprio\") == 20\n        assert settings[\"key_highprio\"] == 10\n        assert \"newkey_two\" in settings\n        assert settings.getpriority(\"newkey_two\") == 20\n\n        settings.update(custom_settings)\n        assert settings[\"key_lowprio\"] == 1\n        assert settings.getpriority(\"key_lowprio\") == 30\n        assert settings[\"key_highprio\"] == 10\n        assert \"newkey_one\" in settings\n        assert settings.getpriority(\"newkey_one\") == 50\n\n        settings.update({\"key_lowprio\": 3}, priority=20)\n        assert settings[\"key_lowprio\"] == 1\n\n    @pytest.mark.xfail(\n        raises=TypeError, reason=\"BaseSettings.update doesn't support kwargs input\"\n    )\n    def test_update_kwargs(self):\n        settings = BaseSettings({\"key\": 0})\n        settings.update(key=1)  # pylint: disable=unexpected-keyword-arg\n\n    @pytest.mark.xfail(\n        raises=AttributeError,\n        reason=\"BaseSettings.update doesn't support iterable input\",\n    )\n    def test_update_iterable(self):\n        settings = BaseSettings({\"key\": 0})\n        settings.update([(\"key\", 1)])\n\n    def test_update_jsonstring(self):\n        settings = BaseSettings({\"number\": 0, \"dict\": BaseSettings({\"key\": \"val\"})})\n        settings.update('{\"number\": 1, \"newnumber\": 2}')\n        assert settings[\"number\"] == 1\n        assert settings[\"newnumber\"] == 2\n        settings.set(\"dict\", '{\"key\": \"newval\", \"newkey\": \"newval2\"}')\n        assert settings[\"dict\"][\"key\"] == \"newval\"\n        assert settings[\"dict\"][\"newkey\"] == \"newval2\"\n\n    def test_delete(self):\n        settings = BaseSettings({\"key\": None})\n        settings.set(\"key_highprio\", None, priority=50)\n        settings.delete(\"key\")\n        settings.delete(\"key_highprio\")\n        assert \"key\" not in settings\n        assert \"key_highprio\" in settings\n        del settings[\"key_highprio\"]\n        assert \"key_highprio\" not in settings\n        with pytest.raises(KeyError):\n            settings.delete(\"notkey\")\n        with pytest.raises(KeyError):\n            del settings[\"notkey\"]\n", "n_tokens": 928, "byte_len": 3929, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 149, "end_line": 246}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 3, "symbols": ["test_get", "test_getpriority", "test_getwithbase", "test_maxpriority", "test", "tes", "dic", "hasnobase", "getpriority", "disable", "key", "key1", "false", "length", "enable", "pytest", "settings", "lis", "items", "none", "nonexistent", "default", "match", "values", "expected", "getwithbase", "empty", "maxpriority", "getdict", "configuration", "test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs"], "ast_kind": "function_or_method", "text": "    def test_get(self):\n        test_configuration = {\n            \"TEST_ENABLED1\": \"1\",\n            \"TEST_ENABLED2\": True,\n            \"TEST_ENABLED3\": 1,\n            \"TEST_ENABLED4\": \"True\",\n            \"TEST_ENABLED5\": \"true\",\n            \"TEST_ENABLED_WRONG\": \"on\",\n            \"TEST_DISABLED1\": \"0\",\n            \"TEST_DISABLED2\": False,\n            \"TEST_DISABLED3\": 0,\n            \"TEST_DISABLED4\": \"False\",\n            \"TEST_DISABLED5\": \"false\",\n            \"TEST_DISABLED_WRONG\": \"off\",\n            \"TEST_INT1\": 123,\n            \"TEST_INT2\": \"123\",\n            \"TEST_FLOAT1\": 123.45,\n            \"TEST_FLOAT2\": \"123.45\",\n            \"TEST_LIST1\": [\"one\", \"two\"],\n            \"TEST_LIST2\": \"one,two\",\n            \"TEST_LIST3\": \"\",\n            \"TEST_STR\": \"value\",\n            \"TEST_DICT1\": {\"key1\": \"val1\", \"ke2\": 3},\n            \"TEST_DICT2\": '{\"key1\": \"val1\", \"ke2\": 3}',\n        }\n        settings = self.settings\n        settings.attributes = {\n            key: SettingsAttribute(value, 0)\n            for key, value in test_configuration.items()\n        }\n\n        assert settings.getbool(\"TEST_ENABLED1\")\n        assert settings.getbool(\"TEST_ENABLED2\")\n        assert settings.getbool(\"TEST_ENABLED3\")\n        assert settings.getbool(\"TEST_ENABLED4\")\n        assert settings.getbool(\"TEST_ENABLED5\")\n        assert not settings.getbool(\"TEST_ENABLEDx\")\n        assert settings.getbool(\"TEST_ENABLEDx\", True)\n        assert not settings.getbool(\"TEST_DISABLED1\")\n        assert not settings.getbool(\"TEST_DISABLED2\")\n        assert not settings.getbool(\"TEST_DISABLED3\")\n        assert not settings.getbool(\"TEST_DISABLED4\")\n        assert not settings.getbool(\"TEST_DISABLED5\")\n        assert settings.getint(\"TEST_INT1\") == 123\n        assert settings.getint(\"TEST_INT2\") == 123\n        assert settings.getint(\"TEST_INTx\") == 0\n        assert settings.getint(\"TEST_INTx\", 45) == 45\n        assert settings.getfloat(\"TEST_FLOAT1\") == 123.45\n        assert settings.getfloat(\"TEST_FLOAT2\") == 123.45\n        assert settings.getfloat(\"TEST_FLOATx\") == 0.0\n        assert settings.getfloat(\"TEST_FLOATx\", 55.0) == 55.0\n        assert settings.getlist(\"TEST_LIST1\") == [\"one\", \"two\"]\n        assert settings.getlist(\"TEST_LIST2\") == [\"one\", \"two\"]\n        assert settings.getlist(\"TEST_LIST3\") == []\n        assert settings.getlist(\"TEST_LISTx\") == []\n        assert settings.getlist(\"TEST_LISTx\", [\"default\"]) == [\"default\"]\n        assert settings[\"TEST_STR\"] == \"value\"\n        assert settings.get(\"TEST_STR\") == \"value\"\n        assert settings[\"TEST_STRx\"] is None\n        assert settings.get(\"TEST_STRx\") is None\n        assert settings.get(\"TEST_STRx\", \"default\") == \"default\"\n        assert settings.getdict(\"TEST_DICT1\") == {\"key1\": \"val1\", \"ke2\": 3}\n        assert settings.getdict(\"TEST_DICT2\") == {\"key1\": \"val1\", \"ke2\": 3}\n        assert settings.getdict(\"TEST_DICT3\") == {}\n        assert settings.getdict(\"TEST_DICT3\", {\"key1\": 5}) == {\"key1\": 5}\n        with pytest.raises(\n            ValueError,\n            match=\"dictionary update sequence element #0 has length 3; 2 is required|sequence of pairs expected\",\n        ):\n            settings.getdict(\"TEST_LIST1\")\n        with pytest.raises(\n            ValueError, match=\"Supported values for boolean settings are\"\n        ):\n            settings.getbool(\"TEST_ENABLED_WRONG\")\n        with pytest.raises(\n            ValueError, match=\"Supported values for boolean settings are\"\n        ):\n            settings.getbool(\"TEST_DISABLED_WRONG\")\n\n    def test_getpriority(self):\n        settings = BaseSettings({\"key\": \"value\"}, priority=99)\n        assert settings.getpriority(\"key\") == 99\n        assert settings.getpriority(\"nonexistentkey\") is None\n\n    def test_getwithbase(self):\n        s = BaseSettings(\n            {\n                \"TEST_BASE\": BaseSettings({1: 1, 2: 2}, \"project\"),\n                \"TEST\": BaseSettings({1: 10, 3: 30}, \"default\"),\n                \"HASNOBASE\": BaseSettings({3: 3000}, \"default\"),\n            }\n        )\n        s[\"TEST\"].set(2, 200, \"cmdline\")\n        assert set(s.getwithbase(\"TEST\")) == {1, 2, 3}\n        assert set(s.getwithbase(\"HASNOBASE\")) == set(s[\"HASNOBASE\"])\n        assert s.getwithbase(\"NONEXISTENT\") == {}\n\n    def test_maxpriority(self):\n        # Empty settings should return 'default'\n        assert self.settings.maxpriority() == 0\n        self.settings.set(\"A\", 0, 10)\n        self.settings.set(\"B\", 0, 30)\n        assert self.settings.maxpriority() == 30\n", "n_tokens": 1114, "byte_len": 4493, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 247, "end_line": 350}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 4, "symbols": ["test_copy", "test_copy_to_dict", "test_freeze", "test_frozencopy", "setup_method", "test_initial_defaults", "test_initial_values", "test_autopromote_dicts", "test_getdict_autodegrade_basesettings", "test_passing_objects_as_values", "process_item", "test_pop_item_with_default_value", "TestSettings", "TestPipeline", "test", "hasnobase", "initial", "append", "getpriority", "tes", "bool", "string", "mock", "three", "lis", "boolean", "get", "crawler", "pytest", "default", "test_get_settings_priority", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs", "test_update_iterable"], "ast_kind": "class_or_type", "text": "    def test_copy(self):\n        values = {\n            \"TEST_BOOL\": True,\n            \"TEST_LIST\": [\"one\", \"two\"],\n            \"TEST_LIST_OF_LISTS\": [\n                [\"first_one\", \"first_two\"],\n                [\"second_one\", \"second_two\"],\n            ],\n        }\n        self.settings.setdict(values)\n        copy = self.settings.copy()\n        self.settings.set(\"TEST_BOOL\", False)\n        assert copy.get(\"TEST_BOOL\")\n\n        test_list = self.settings.get(\"TEST_LIST\")\n        test_list.append(\"three\")\n        assert copy.get(\"TEST_LIST\") == [\"one\", \"two\"]\n\n        test_list_of_lists = self.settings.get(\"TEST_LIST_OF_LISTS\")\n        test_list_of_lists[0].append(\"first_three\")\n        assert copy.get(\"TEST_LIST_OF_LISTS\")[0] == [\"first_one\", \"first_two\"]\n\n    def test_copy_to_dict(self):\n        s = BaseSettings(\n            {\n                \"TEST_STRING\": \"a string\",\n                \"TEST_LIST\": [1, 2],\n                \"TEST_BOOLEAN\": False,\n                \"TEST_BASE\": BaseSettings({1: 1, 2: 2}, \"project\"),\n                \"TEST\": BaseSettings({1: 10, 3: 30}, \"default\"),\n                \"HASNOBASE\": BaseSettings({3: 3000}, \"default\"),\n            }\n        )\n        assert s.copy_to_dict() == {\n            \"HASNOBASE\": {3: 3000},\n            \"TEST\": {1: 10, 3: 30},\n            \"TEST_BASE\": {1: 1, 2: 2},\n            \"TEST_LIST\": [1, 2],\n            \"TEST_BOOLEAN\": False,\n            \"TEST_STRING\": \"a string\",\n        }\n\n    def test_freeze(self):\n        self.settings.freeze()\n        with pytest.raises(\n            TypeError, match=\"Trying to modify an immutable Settings object\"\n        ):\n            self.settings.set(\"TEST_BOOL\", False)\n\n    def test_frozencopy(self):\n        frozencopy = self.settings.frozencopy()\n        assert frozencopy.frozen\n        assert frozencopy is not self.settings\n\n\nclass TestSettings:\n    def setup_method(self):\n        self.settings = Settings()\n\n    @mock.patch.dict(\"scrapy.settings.SETTINGS_PRIORITIES\", {\"default\": 10})\n    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n    def test_initial_defaults(self):\n        settings = Settings()\n        assert len(settings.attributes) == 2\n        assert \"TEST_DEFAULT\" in settings.attributes\n\n        attr = settings.attributes[\"TEST_DEFAULT\"]\n        assert isinstance(attr, SettingsAttribute)\n        assert attr.value == \"defvalue\"\n        assert attr.priority == 10\n\n    @mock.patch.dict(\"scrapy.settings.SETTINGS_PRIORITIES\", {})\n    @mock.patch(\"scrapy.settings.default_settings\", {})\n    def test_initial_values(self):\n        settings = Settings({\"TEST_OPTION\": \"value\"}, 10)\n        assert len(settings.attributes) == 1\n        assert \"TEST_OPTION\" in settings.attributes\n\n        attr = settings.attributes[\"TEST_OPTION\"]\n        assert isinstance(attr, SettingsAttribute)\n        assert attr.value == \"value\"\n        assert attr.priority == 10\n\n    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n    def test_autopromote_dicts(self):\n        settings = Settings()\n        mydict = settings.get(\"TEST_DICT\")\n        assert isinstance(mydict, BaseSettings)\n        assert \"key\" in mydict\n        assert mydict[\"key\"] == \"val\"\n        assert mydict.getpriority(\"key\") == 0\n\n    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n    def test_getdict_autodegrade_basesettings(self):\n        settings = Settings()\n        mydict = settings.getdict(\"TEST_DICT\")\n        assert isinstance(mydict, dict)\n        assert len(mydict) == 1\n        assert \"key\" in mydict\n        assert mydict[\"key\"] == \"val\"\n\n    def test_passing_objects_as_values(self):\n        class TestPipeline:\n            def process_item(self, i):\n                return i\n\n        settings = Settings(\n            {\n                \"ITEM_PIPELINES\": {\n                    TestPipeline: 800,\n                },\n                \"DOWNLOAD_HANDLERS\": {\n                    \"ftp\": FileDownloadHandler,\n                },\n            }\n        )\n\n        assert \"ITEM_PIPELINES\" in settings.attributes\n\n        mypipeline, priority = settings.getdict(\"ITEM_PIPELINES\").popitem()\n        assert priority == 800\n        assert mypipeline == TestPipeline\n        assert isinstance(mypipeline(), TestPipeline)\n        assert mypipeline().process_item(\"item\") == \"item\"\n\n        myhandler = settings.getdict(\"DOWNLOAD_HANDLERS\").pop(\"ftp\")\n        assert myhandler == FileDownloadHandler\n        myhandler_instance = build_from_crawler(myhandler, get_crawler())\n        assert isinstance(myhandler_instance, FileDownloadHandler)\n        assert hasattr(myhandler_instance, \"download_request\")\n\n    def test_pop_item_with_default_value(self):\n        settings = Settings()\n\n        with pytest.raises(KeyError):\n            settings.pop(\"DUMMY_CONFIG\")\n\n        dummy_config_value = settings.pop(\"DUMMY_CONFIG\", \"dummy_value\")\n        assert dummy_config_value == \"dummy_value\"\n", "n_tokens": 1093, "byte_len": 4911, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 351, "end_line": 490}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 5, "symbols": ["test_pop_item_with_immutable_settings", "test_add_to_list", "test_remove_from_list", "test_deprecated_concurrent_requests_per_ip_setting", "Component1", "Component1Subclass", "Component2", "Component3", "component", "component2", "other", "dummy", "pass", "issubclass", "value", "priority", "warns", "test", "pop", "getpriority", "after", "error", "return", "name", "item", "mark", "expected", "concurren", "request", "parametrize", "test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs"], "ast_kind": "class_or_type", "text": "    def test_pop_item_with_immutable_settings(self):\n        settings = Settings(\n            {\"DUMMY_CONFIG\": \"dummy_value\", \"OTHER_DUMMY_CONFIG\": \"other_dummy_value\"}\n        )\n\n        assert settings.pop(\"DUMMY_CONFIG\") == \"dummy_value\"\n\n        settings.freeze()\n\n        with pytest.raises(\n            TypeError, match=\"Trying to modify an immutable Settings object\"\n        ):\n            settings.pop(\"OTHER_DUMMY_CONFIG\")\n\n\n@pytest.mark.parametrize(\n    (\"before\", \"name\", \"item\", \"after\"),\n    [\n        ({}, \"FOO\", \"BAR\", {\"FOO\": [\"BAR\"]}),\n        ({\"FOO\": []}, \"FOO\", \"BAR\", {\"FOO\": [\"BAR\"]}),\n        ({\"FOO\": [\"BAR\"]}, \"FOO\", \"BAZ\", {\"FOO\": [\"BAR\", \"BAZ\"]}),\n        ({\"FOO\": [\"BAR\"]}, \"FOO\", \"BAR\", {\"FOO\": [\"BAR\"]}),\n        ({\"FOO\": \"\"}, \"FOO\", \"BAR\", {\"FOO\": [\"BAR\"]}),\n        ({\"FOO\": \"BAR\"}, \"FOO\", \"BAR\", {\"FOO\": \"BAR\"}),\n        ({\"FOO\": \"BAR\"}, \"FOO\", \"BAZ\", {\"FOO\": [\"BAR\", \"BAZ\"]}),\n        ({\"FOO\": \"BAR,BAZ\"}, \"FOO\", \"BAZ\", {\"FOO\": \"BAR,BAZ\"}),\n        ({\"FOO\": \"BAR,BAZ\"}, \"FOO\", \"QUX\", {\"FOO\": [\"BAR\", \"BAZ\", \"QUX\"]}),\n    ],\n)\ndef test_add_to_list(before, name, item, after):\n    settings = BaseSettings(before, priority=0)\n    settings.add_to_list(name, item)\n    expected_priority = settings.getpriority(name) or 0\n    expected_settings = BaseSettings(after, priority=expected_priority)\n    assert settings == expected_settings, (\n        f\"{settings[name]=} != {expected_settings[name]=}\"\n    )\n    assert settings.getpriority(name) == expected_settings.getpriority(name)\n\n\n@pytest.mark.parametrize(\n    (\"before\", \"name\", \"item\", \"after\"),\n    [\n        ({}, \"FOO\", \"BAR\", ValueError),\n        ({\"FOO\": [\"BAR\"]}, \"FOO\", \"BAR\", {\"FOO\": []}),\n        ({\"FOO\": [\"BAR\"]}, \"FOO\", \"BAZ\", ValueError),\n        ({\"FOO\": [\"BAR\", \"BAZ\"]}, \"FOO\", \"BAR\", {\"FOO\": [\"BAZ\"]}),\n        ({\"FOO\": \"\"}, \"FOO\", \"BAR\", ValueError),\n        ({\"FOO\": \"[]\"}, \"FOO\", \"BAR\", ValueError),\n        ({\"FOO\": \"BAR\"}, \"FOO\", \"BAR\", {\"FOO\": []}),\n        ({\"FOO\": \"BAR\"}, \"FOO\", \"BAZ\", ValueError),\n        ({\"FOO\": \"BAR,BAZ\"}, \"FOO\", \"BAR\", {\"FOO\": [\"BAZ\"]}),\n    ],\n)\ndef test_remove_from_list(before, name, item, after):\n    settings = BaseSettings(before, priority=0)\n\n    if isinstance(after, type) and issubclass(after, Exception):\n        with pytest.raises(after):\n            settings.remove_from_list(name, item)\n        return\n\n    settings.remove_from_list(name, item)\n    expected_priority = settings.getpriority(name) or 0\n    expected_settings = BaseSettings(after, priority=expected_priority)\n    assert settings == expected_settings, (\n        f\"{settings[name]=} != {expected_settings[name]=}\"\n    )\n    assert settings.getpriority(name) == expected_settings.getpriority(name)\n\n\ndef test_deprecated_concurrent_requests_per_ip_setting():\n    with warnings.catch_warnings(record=True) as warns:\n        settings = Settings({\"CONCURRENT_REQUESTS_PER_IP\": 1})\n        settings.get(\"CONCURRENT_REQUESTS_PER_IP\")\n\n    assert (\n        str(warns[0].message)\n        == \"The CONCURRENT_REQUESTS_PER_IP setting is deprecated, use CONCURRENT_REQUESTS_PER_DOMAIN instead.\"\n    )\n\n\nclass Component1:\n    pass\n\n\nComponent1Alias = Component1\n\n\nclass Component1Subclass(Component1):\n    pass\n\n\nComponent1SubclassAlias = Component1Subclass\n\n\nclass Component2:\n    pass\n\n\nclass Component3:\n    pass\n\n", "n_tokens": 918, "byte_len": 3306, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 491, "end_line": 594}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 6, "symbols": ["Component4", "unrelated", "component", "component2", "expected", "pass", "priority", "after", "name", "mark", "new", "cls", "class", "parametrize", "before", "null", "component4", "component1", "value", "alias", "pytest", "key", "error", "test", "settings", "component3", "old", "none", "kept", "tests", "test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs"], "ast_kind": "class_or_type", "text": "class Component4:\n    pass\n\n\n@pytest.mark.parametrize(\n    (\"before\", \"name\", \"old_cls\", \"new_cls\", \"priority\", \"after\"),\n    [\n        ({}, \"FOO\", Component1, Component2, None, KeyError),\n        (\n            {\"FOO\": {Component1: 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 1}},\n        ),\n        (\n            {\"FOO\": {Component1: 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            2,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\"FOO\": {\"tests.test_settings.Component1\": 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 1}},\n        ),\n        (\n            {\"FOO\": {Component1Alias: 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 1}},\n        ),\n        (\n            {\"FOO\": {Component1Alias: 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            2,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\"FOO\": {\"tests.test_settings.Component1Alias\": 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 1}},\n        ),\n        (\n            {\"FOO\": {\"tests.test_settings.Component1Alias\": 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            2,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\n                \"FOO\": {\n                    \"tests.test_settings.Component1\": 1,\n                    \"tests.test_settings.Component1Alias\": 2,\n                }\n            },\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\n                \"FOO\": {\n                    \"tests.test_settings.Component1\": 1,\n                    \"tests.test_settings.Component1Alias\": 2,\n                }\n            },\n            \"FOO\",\n            Component1,\n            Component2,\n            3,\n            {\"FOO\": {Component2: 3}},\n        ),\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1\": 1}'},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 1}},\n        ),\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1\": 1}'},\n            \"FOO\",\n            Component1,\n            Component2,\n            2,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1Alias\": 1}'},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 1}},\n        ),\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1Alias\": 1}'},\n            \"FOO\",\n            Component1,\n            Component2,\n            2,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\n                \"FOO\": '{\"tests.test_settings.Component1\": 1, \"tests.test_settings.Component1Alias\": 2}'\n            },\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            {\"FOO\": {Component2: 2}},\n        ),\n        (\n            {\n                \"FOO\": '{\"tests.test_settings.Component1\": 1, \"tests.test_settings.Component1Alias\": 2}'\n            },\n            \"FOO\",\n            Component1,\n            Component2,\n            3,\n            {\"FOO\": {Component2: 3}},\n        ),\n        # If old_cls has None as value, raise KeyError.\n        (\n            {\"FOO\": {Component1: None}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            KeyError,\n        ),\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1\": null}'},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            KeyError,\n        ),\n        (\n            {\"FOO\": {Component1: None, \"tests.test_settings.Component1\": None}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            KeyError,\n        ),\n        (\n            {\"FOO\": {Component1: 1, \"tests.test_settings.Component1\": None}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            KeyError,\n        ),\n        (\n            {\"FOO\": {Component1: None, \"tests.test_settings.Component1\": 1}},\n            \"FOO\",\n            Component1,\n            Component2,\n            None,\n            KeyError,\n        ),\n        # Unrelated components are kept as is, as expected.\n        (\n            {\n                \"FOO\": {\n                    Component1: 1,\n                    \"tests.test_settings.Component2\": 2,\n                    Component3: 3,\n                }\n            },\n            \"FOO\",\n            Component3,\n            Component4,\n            None,\n            {\n                \"FOO\": {\n                    Component1: 1,\n                    \"tests.test_settings.Component2\": 2,\n                    Component4: 3,\n                }\n            },\n        ),\n    ],\n)", "n_tokens": 1157, "byte_len": 5199, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 595, "end_line": 800}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#7", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 7, "symbols": ["test_replace_in_component_priority_dict", "component", "component2", "issubclass", "priority", "return", "after", "getpriority", "name", "new", "cls", "replace", "expected", "mark", "parametrize", "subclass", "with", "before", "component1", "exception", "string", "values", "alias", "pytest", "settings", "test", "base", "old", "assert", "isinstance", "test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs"], "ast_kind": "function_or_method", "text": "def test_replace_in_component_priority_dict(\n    before, name, old_cls, new_cls, priority, after\n):\n    settings = BaseSettings(before, priority=0)\n\n    if isinstance(after, type) and issubclass(after, Exception):\n        with pytest.raises(after):\n            settings.replace_in_component_priority_dict(\n                name, old_cls, new_cls, priority\n            )\n        return\n\n    expected_priority = settings.getpriority(name) or 0\n    settings.replace_in_component_priority_dict(name, old_cls, new_cls, priority)\n    expected_settings = BaseSettings(after, priority=expected_priority)\n    assert settings == expected_settings\n    assert settings.getpriority(name) == expected_settings.getpriority(name)\n\n\n@pytest.mark.parametrize(\n    (\"before\", \"name\", \"cls\", \"priority\", \"after\"),\n    [\n        # Set\n        ({}, \"FOO\", Component1, None, {\"FOO\": {Component1: None}}),\n        ({}, \"FOO\", Component1, 0, {\"FOO\": {Component1: 0}}),\n        ({}, \"FOO\", Component1, 1, {\"FOO\": {Component1: 1}}),\n        # Add\n        (\n            {\"FOO\": {Component1: 0}},\n            \"FOO\",\n            Component2,\n            None,\n            {\"FOO\": {Component1: 0, Component2: None}},\n        ),\n        (\n            {\"FOO\": {Component1: 0}},\n            \"FOO\",\n            Component2,\n            0,\n            {\"FOO\": {Component1: 0, Component2: 0}},\n        ),\n        (\n            {\"FOO\": {Component1: 0}},\n            \"FOO\",\n            Component2,\n            1,\n            {\"FOO\": {Component1: 0, Component2: 1}},\n        ),\n        # Replace\n        (\n            {\n                \"FOO\": {\n                    Component1: None,\n                    \"tests.test_settings.Component1\": 0,\n                    \"tests.test_settings.Component1Alias\": 1,\n                    Component1Subclass: None,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1,\n                }\n            },\n            \"FOO\",\n            Component1,\n            None,\n            {\n                \"FOO\": {\n                    Component1: None,\n                    Component1Subclass: None,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1,\n                }\n            },\n        ),\n        (\n            {\n                \"FOO\": {\n                    Component1: 0,\n                    \"tests.test_settings.Component1\": 1,\n                    \"tests.test_settings.Component1Alias\": None,\n                    Component1Subclass: 0,\n                    \"tests.test_settings.Component1Subclass\": 1,\n                    \"tests.test_settings.Component1SubclassAlias\": None,\n                }\n            },\n            \"FOO\",\n            Component1,\n            0,\n            {\n                \"FOO\": {\n                    Component1: 0,\n                    Component1Subclass: 0,\n                    \"tests.test_settings.Component1Subclass\": 1,\n                    \"tests.test_settings.Component1SubclassAlias\": None,\n                }\n            },\n        ),\n        (\n            {\n                \"FOO\": {\n                    Component1: 1,\n                    \"tests.test_settings.Component1\": None,\n                    \"tests.test_settings.Component1Alias\": 0,\n                    Component1Subclass: 1,\n                    \"tests.test_settings.Component1Subclass\": None,\n                    \"tests.test_settings.Component1SubclassAlias\": 0,\n                }\n            },\n            \"FOO\",\n            Component1,\n            1,\n            {\n                \"FOO\": {\n                    Component1: 1,\n                    Component1Subclass: 1,\n                    \"tests.test_settings.Component1Subclass\": None,\n                    \"tests.test_settings.Component1SubclassAlias\": 0,\n                }\n            },\n        ),\n        # String-based setting values\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1\": 0}'},\n            \"FOO\",\n            Component2,\n            None,\n            {\"FOO\": {\"tests.test_settings.Component1\": 0, Component2: None}},\n        ),\n        (\n            {\n                \"FOO\": \"\"\"{\n                    \"tests.test_settings.Component1\": 0,\n                    \"tests.test_settings.Component1Alias\": 1,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1\n                }\"\"\"\n            },\n            \"FOO\",\n            Component1,\n            None,\n            {\n                \"FOO\": {\n                    Component1: None,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1,\n                }\n            },\n        ),\n    ],\n)", "n_tokens": 1015, "byte_len": 4816, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 801, "end_line": 948}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py#8", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/__init__.py", "rel_path": "tests/test_settings/__init__.py", "module": "tests.test_settings.__init__", "ext": "py", "chunk_number": 8, "symbols": ["test_set_in_component_priority_dict", "test_setdefault_in_component_priority_dict", "component", "component2", "test", "setdefault", "priority", "set", "getpriority", "after", "name", "expected", "mark", "parametrize", "subclass", "before", "keep", "component1", "string", "values", "alias", "pytest", "settings", "base", "assert", "none", "tests", "setting", "based", "test_get_settings_priority", "setup_method", "test_set_greater_priority", "test_set_equal_priority", "test_set_less_priority", "test_overwrite_basesettings", "test_repr", "test_setdefault_not_existing_value", "test_setdefault_existing_value", "test_set_new_attribute", "test_set_settingsattribute", "test_set_instance_identity_on_update", "test_set_calls_settings_attributes_methods_on_update", "test_setitem", "test_setdict_alias", "test_setmodule_only_load_uppercase_vars", "test_setmodule_alias", "test_setmodule_by_path", "test_update", "test_update_kwargs", "test_update_iterable"], "ast_kind": "function_or_method", "text": "def test_set_in_component_priority_dict(before, name, cls, priority, after):\n    settings = BaseSettings(before, priority=0)\n    expected_priority = settings.getpriority(name) or 0\n    settings.set_in_component_priority_dict(name, cls, priority)\n    expected_settings = BaseSettings(after, priority=expected_priority)\n    assert settings == expected_settings\n    assert settings.getpriority(name) == expected_settings.getpriority(name), (\n        f\"{settings.getpriority(name)=} != {expected_settings.getpriority(name)=}\"\n    )\n\n\n@pytest.mark.parametrize(\n    (\"before\", \"name\", \"cls\", \"priority\", \"after\"),\n    [\n        # Set\n        ({}, \"FOO\", Component1, None, {\"FOO\": {Component1: None}}),\n        ({}, \"FOO\", Component1, 0, {\"FOO\": {Component1: 0}}),\n        ({}, \"FOO\", Component1, 1, {\"FOO\": {Component1: 1}}),\n        # Add\n        (\n            {\"FOO\": {Component1: 0}},\n            \"FOO\",\n            Component2,\n            None,\n            {\"FOO\": {Component1: 0, Component2: None}},\n        ),\n        (\n            {\"FOO\": {Component1: 0}},\n            \"FOO\",\n            Component2,\n            0,\n            {\"FOO\": {Component1: 0, Component2: 0}},\n        ),\n        (\n            {\"FOO\": {Component1: 0}},\n            \"FOO\",\n            Component2,\n            1,\n            {\"FOO\": {Component1: 0, Component2: 1}},\n        ),\n        # Keep\n        (\n            {\n                \"FOO\": {\n                    Component1: None,\n                    \"tests.test_settings.Component1\": 0,\n                    \"tests.test_settings.Component1Alias\": 1,\n                    Component1Subclass: None,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1,\n                }\n            },\n            \"FOO\",\n            Component1,\n            None,\n            {\n                \"FOO\": {\n                    Component1: None,\n                    \"tests.test_settings.Component1\": 0,\n                    \"tests.test_settings.Component1Alias\": 1,\n                    Component1Subclass: None,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1,\n                }\n            },\n        ),\n        (\n            {\n                \"FOO\": {\n                    Component1: 0,\n                    \"tests.test_settings.Component1\": 1,\n                    \"tests.test_settings.Component1Alias\": None,\n                    Component1Subclass: 0,\n                    \"tests.test_settings.Component1Subclass\": 1,\n                    \"tests.test_settings.Component1SubclassAlias\": None,\n                }\n            },\n            \"FOO\",\n            Component1,\n            0,\n            {\n                \"FOO\": {\n                    Component1: 0,\n                    \"tests.test_settings.Component1\": 1,\n                    \"tests.test_settings.Component1Alias\": None,\n                    Component1Subclass: 0,\n                    \"tests.test_settings.Component1Subclass\": 1,\n                    \"tests.test_settings.Component1SubclassAlias\": None,\n                }\n            },\n        ),\n        (\n            {\n                \"FOO\": {\n                    Component1: 1,\n                    \"tests.test_settings.Component1\": None,\n                    \"tests.test_settings.Component1Alias\": 0,\n                    Component1Subclass: 1,\n                    \"tests.test_settings.Component1Subclass\": None,\n                    \"tests.test_settings.Component1SubclassAlias\": 0,\n                }\n            },\n            \"FOO\",\n            Component1,\n            1,\n            {\n                \"FOO\": {\n                    Component1: 1,\n                    \"tests.test_settings.Component1\": None,\n                    \"tests.test_settings.Component1Alias\": 0,\n                    Component1Subclass: 1,\n                    \"tests.test_settings.Component1Subclass\": None,\n                    \"tests.test_settings.Component1SubclassAlias\": 0,\n                }\n            },\n        ),\n        # String-based setting values\n        (\n            {\"FOO\": '{\"tests.test_settings.Component1\": 0}'},\n            \"FOO\",\n            Component2,\n            None,\n            {\"FOO\": {\"tests.test_settings.Component1\": 0, Component2: None}},\n        ),\n        (\n            {\n                \"FOO\": \"\"\"{\n                    \"tests.test_settings.Component1\": 0,\n                    \"tests.test_settings.Component1Alias\": 1,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1\n                }\"\"\"\n            },\n            \"FOO\",\n            Component1,\n            None,\n            {\n                \"FOO\": \"\"\"{\n                    \"tests.test_settings.Component1\": 0,\n                    \"tests.test_settings.Component1Alias\": 1,\n                    \"tests.test_settings.Component1Subclass\": 0,\n                    \"tests.test_settings.Component1SubclassAlias\": 1\n                }\"\"\"\n            },\n        ),\n    ],\n)\ndef test_setdefault_in_component_priority_dict(before, name, cls, priority, after):\n    settings = BaseSettings(before, priority=0)\n    expected_priority = settings.getpriority(name) or 0\n    settings.setdefault_in_component_priority_dict(name, cls, priority)\n    expected_settings = BaseSettings(after, priority=expected_priority)\n    assert settings == expected_settings\n    assert settings.getpriority(name) == expected_settings.getpriority(name)\n", "n_tokens": 1156, "byte_len": 5528, "file_sha1": "3762741d6829008e45a57ffb941c33dd1d81f557", "start_line": 949, "end_line": 1103}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/default_settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_settings/default_settings.py", "rel_path": "tests/test_settings/default_settings.py", "module": "tests.test_settings.default_settings", "ext": "py", "chunk_number": 1, "symbols": ["defvalue", "tes", "default", "dict"], "ast_kind": "unknown", "text": "TEST_DEFAULT = \"defvalue\"\n\nTEST_DICT = {\"key\": \"val\"}\n", "n_tokens": 16, "byte_len": 54, "file_sha1": "8d64279401765876623c3d0df68e91972167e103", "start_line": 1, "end_line": 4}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/__init__.py", "rel_path": "tests/test_cmdline_crawl_with_pipeline/__init__.py", "module": "tests.test_cmdline_crawl_with_pipeline.__init__", "ext": "py", "chunk_number": 1, "symbols": ["_execute", "test_open_spider_normally_in_pipeline", "test_exception_at_open_spider_in_pipeline", "TestCmdlineCrawlPipeline", "pipeline", "subprocess", "twiste", "keep", "file", "exception", "return", "stdout", "class", "else", "normal", "scrapy", "test", "open", "returncode", "path", "pathlib", "cmdline", "stderr", "unhandled", "spname", "from", "pipe", "execute", "parent", "assert", "popen", "stop", "executable", "proc", "import", "should", "self", "runtime", "error", "tests", "crawl", "resolve", "args", "communicate"], "ast_kind": "class_or_type", "text": "import sys\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\n\nfrom tests import TWISTED_KEEPS_TRACEBACKS\n\n\nclass TestCmdlineCrawlPipeline:\n    def _execute(self, spname):\n        args = (sys.executable, \"-m\", \"scrapy.cmdline\", \"crawl\", spname)\n        cwd = Path(__file__).resolve().parent\n        proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)\n        _, stderr = proc.communicate()\n        return proc.returncode, stderr\n\n    def test_open_spider_normally_in_pipeline(self):\n        returncode, stderr = self._execute(\"normal\")\n        assert returncode == 0\n\n    def test_exception_at_open_spider_in_pipeline(self):\n        returncode, stderr = self._execute(\"exception\")\n        # An unhandled exception in a pipeline should not stop the crawl\n        assert returncode == 0\n        if TWISTED_KEEPS_TRACEBACKS:\n            assert b'RuntimeError(\"exception\")' in stderr\n        else:\n            assert b\"RuntimeError: exception\" in stderr\n", "n_tokens": 234, "byte_len": 961, "file_sha1": "ee5a45ae7b7cae12ad336d3600750aae7215658f", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/settings.py", "rel_path": "tests/test_cmdline_crawl_with_pipeline/test_spider/settings.py", "module": "tests.test_cmdline_crawl_with_pipeline.test_spider.settings", "ext": "py", "chunk_number": 1, "symbols": ["spiders", "test", "spider", "spide", "modules", "name", "bot"], "ast_kind": "unknown", "text": "BOT_NAME = \"test_spider\"\nSPIDER_MODULES = [\"test_spider.spiders\"]\n", "n_tokens": 19, "byte_len": 66, "file_sha1": "c3e18fc52921bbc7c9c2257f81a42bee15831a26", "start_line": 1, "end_line": 3}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/pipelines.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/pipelines.py", "rel_path": "tests/test_cmdline_crawl_with_pipeline/test_spider/pipelines.py", "module": "tests.test_cmdline_crawl_with_pipeline.test_spider.pipelines", "ext": "py", "chunk_number": 1, "symbols": ["open_spider", "process_item", "TestSpiderPipeline", "TestSpiderExceptionPipeline", "item", "test", "spider", "class", "self", "pass", "runtime", "error", "open", "raise", "return", "exception", "process"], "ast_kind": "class_or_type", "text": "class TestSpiderPipeline:\n    def open_spider(self, spider):\n        pass\n\n    def process_item(self, item):\n        return item\n\n\nclass TestSpiderExceptionPipeline:\n    def open_spider(self, spider):\n        raise RuntimeError(\"exception\")\n\n    def process_item(self, item):\n        return item\n", "n_tokens": 62, "byte_len": 296, "file_sha1": "5b7cf2dc26aae52c907c87e06fbd9e674ca89235", "start_line": 1, "end_line": 15}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/exception.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/exception.py", "rel_path": "tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/exception.py", "module": "tests.test_cmdline_crawl_with_pipeline.test_spider.spiders.exception", "ext": "py", "chunk_number": 1, "symbols": ["parse", "ExceptionSpider", "spider", "name", "ite", "pipelines", "class", "import", "test", "custom", "settings", "exception", "scrapy", "self", "pass", "response"], "ast_kind": "class_or_type", "text": "import scrapy\n\n\nclass ExceptionSpider(scrapy.Spider):\n    name = \"exception\"\n\n    custom_settings = {\n        \"ITEM_PIPELINES\": {\"test_spider.pipelines.TestSpiderExceptionPipeline\": 300}\n    }\n\n    def parse(self, response):\n        pass\n", "n_tokens": 55, "byte_len": 238, "file_sha1": "8ea31ef32ee6c5a4e486d68f02a09552031332aa", "start_line": 1, "end_line": 13}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/normal.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/normal.py", "rel_path": "tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/normal.py", "module": "tests.test_cmdline_crawl_with_pipeline.test_spider.spiders.normal", "ext": "py", "chunk_number": 1, "symbols": ["parse", "NormalSpider", "spider", "name", "ite", "pipelines", "class", "test", "normal", "import", "custom", "settings", "scrapy", "self", "pass", "response"], "ast_kind": "class_or_type", "text": "import scrapy\n\n\nclass NormalSpider(scrapy.Spider):\n    name = \"normal\"\n\n    custom_settings = {\n        \"ITEM_PIPELINES\": {\"test_spider.pipelines.TestSpiderPipeline\": 300}\n    }\n\n    def parse(self, response):\n        pass\n", "n_tokens": 54, "byte_len": 223, "file_sha1": "a861a62d80d9a00b79359f53f15c2feba8c212b0", "start_line": 1, "end_line": 13}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/explicit_default_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/explicit_default_reactor.py", "rel_path": "tests/CrawlerRunner/explicit_default_reactor.py", "module": "tests.CrawlerRunner.explicit_default_reactor", "ext": "py", "chunk_number": 1, "symbols": ["main", "NoRequestsSpider", "task", "react", "runner", "async", "levelname", "internet", "requests", "spider", "twisted", "return", "format", "log", "name", "crawler", "class", "configure", "logging", "scrapy", "level", "yield", "from", "debug", "none", "reactor", "utils", "twiste", "import", "start", "custom", "settings", "self", "message", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": None,\n    }\n\n    async def start(self):\n        return\n        yield\n\n\ndef main(reactor):\n    configure_logging(\n        {\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"}\n    )\n    runner = CrawlerRunner()\n    return runner.crawl(NoRequestsSpider)\n\n\nreact(main)\n", "n_tokens": 127, "byte_len": 544, "file_sha1": "417b88f8bce7dccd2acbea6f11bfbf48f6c10a14", "start_line": 1, "end_line": 29}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/custom_loop_same.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/custom_loop_same.py", "rel_path": "tests/CrawlerRunner/custom_loop_same.py", "module": "tests.CrawlerRunner.custom_loop_same", "ext": "py", "chunk_number": 1, "symbols": ["main", "NoRequestsSpider", "task", "react", "runner", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "crawler", "class", "configure", "logging", "scrapy", "asyncio", "selector", "loop", "yield", "from", "asyncioreactor", "utils", "twiste", "import", "start", "custom", "settings", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\ndef main(reactor):\n    configure_logging()\n    runner = CrawlerRunner()\n    return runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\", \"uvloop.Loop\")\nreact(main)\n", "n_tokens": 161, "byte_len": 699, "file_sha1": "fdeba2e60b4ecae751f0292249e7a08e091c290b", "start_line": 1, "end_line": 30}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/change_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/change_reactor.py", "rel_path": "tests/CrawlerRunner/change_reactor.py", "module": "tests.CrawlerRunner.change_reactor", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "runner", "async", "levelname", "internet", "requests", "spider", "add", "both", "twisted", "return", "format", "log", "install", "reactor", "name", "lambda", "crawler", "class", "configure", "logging", "scrapy", "level", "asyncio", "selector", "yield", "noqa", "from", "e402", "debug", "stop", "asyncioreactor", "utils", "twiste", "import", "tid253", "start", "custom", "settings", "self", "callback", "message", "request", "crawl"], "ast_kind": "class_or_type", "text": "from scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\nconfigure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n\n\nfrom scrapy.utils.reactor import install_reactor  # noqa: E402\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\nrunner = CrawlerRunner()\n\nd = runner.crawl(NoRequestsSpider)\n\nfrom twisted.internet import reactor  # noqa: E402,TID253\n\nd.addBoth(callback=lambda _: reactor.stop())\nreactor.run()\n", "n_tokens": 177, "byte_len": 754, "file_sha1": "38ca8f819ad0de6a30eeed5fd48d5a42adf3b989", "start_line": 1, "end_line": 33}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/custom_loop_different.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/custom_loop_different.py", "rel_path": "tests/CrawlerRunner/custom_loop_different.py", "module": "tests.CrawlerRunner.custom_loop_different", "ext": "py", "chunk_number": 1, "symbols": ["main", "NoRequestsSpider", "task", "react", "runner", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "crawler", "class", "configure", "logging", "scrapy", "asyncio", "selector", "loop", "yield", "from", "asyncioreactor", "utils", "twiste", "import", "start", "custom", "settings", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\ndef main(reactor):\n    configure_logging()\n    runner = CrawlerRunner()\n    return runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 155, "byte_len": 684, "file_sha1": "c999c329acbcbf1ac8cfc9bb8f68668ecdc2ccad", "start_line": 1, "end_line": 30}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/multi_parallel.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/multi_parallel.py", "rel_path": "tests/CrawlerRunner/multi_parallel.py", "module": "tests.CrawlerRunner.multi_parallel", "ext": "py", "chunk_number": 1, "symbols": ["main", "NoRequestsSpider", "task", "react", "runner", "async", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "crawler", "class", "configure", "logging", "scrapy", "asyncio", "selector", "yield", "from", "join", "asyncioreactor", "utils", "import", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\ndef main(reactor):\n    configure_logging()\n    runner = CrawlerRunner()\n    runner.crawl(NoRequestsSpider)\n    runner.crawl(NoRequestsSpider)\n    return runner.join()\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 125, "byte_len": 576, "file_sha1": "b8177f5ea75cef61baba9b41ed68e58502de1d49", "start_line": 1, "end_line": 27}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/ip_address.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/ip_address.py", "rel_path": "tests/CrawlerRunner/ip_address.py", "module": "tests.CrawlerRunner.ip_address", "ext": "py", "chunk_number": 1, "symbols": ["createResolver", "parse", "LocalhostSpider", "address", "async", "posix", "netloc", "spider", "name", "domain", "resolver", "chain", "https", "port", "mockserver", "urlparse", "cached", "hosts", "module", "install", "windows", "none", "stop", "type", "http", "mock", "echo", "runner", "cache", "internet", "add", "both", "return", "real", "class", "configure", "logging", "servers", "stackoverflow", "asyncio", "selector", "noqa", "e402", "split", "asyncioreactor", "self", "tests", "httpobj", "response", "else"], "ast_kind": "class_or_type", "text": "# ruff: noqa: E402\n\nfrom scrapy.utils.reactor import install_reactor\nfrom tests.mockserver.dns import MockDNSServer\nfrom tests.mockserver.http import MockServer\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n\nfrom twisted.names import cache, resolve\nfrom twisted.names import hosts as hostsModule\nfrom twisted.names.client import Resolver\nfrom twisted.python.runtime import platform\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.log import configure_logging\n\n\n# https://stackoverflow.com/a/32784190\ndef createResolver(servers=None, resolvconf=None, hosts=None):\n    if hosts is None:\n        hosts = b\"/etc/hosts\" if platform.getType() == \"posix\" else r\"c:\\windows\\hosts\"\n    theResolver = Resolver(resolvconf, servers)\n    hostResolver = hostsModule.Resolver(hosts)\n    chain = [hostResolver, cache.CacheResolver(), theResolver]\n    return resolve.ResolverChain(chain)\n\n\nclass LocalhostSpider(Spider):\n    name = \"localhost_spider\"\n\n    async def start(self):\n        yield Request(self.url)\n\n    def parse(self, response):\n        netloc = urlparse_cached(response).netloc\n        host = netloc.split(\":\")[0]\n        self.logger.info(f\"Host: {host}\")\n        self.logger.info(f\"Type: {type(response.ip_address)}\")\n        self.logger.info(f\"IP address: {response.ip_address}\")\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor\n\n    with MockServer() as mock_http_server, MockDNSServer() as mock_dns_server:\n        port = mock_http_server.http_port\n        url = f\"http://not.a.real.domain:{port}/echo\"\n\n        servers = [(mock_dns_server.host, mock_dns_server.port)]\n        reactor.installResolver(createResolver(servers=servers))\n\n        configure_logging()\n        runner = CrawlerRunner()\n        d = runner.crawl(LocalhostSpider, url=url)\n        d.addBoth(lambda _: reactor.stop())\n        reactor.run()\n", "n_tokens": 439, "byte_len": 1962, "file_sha1": "cbb251adc729961f83070edff5a30fad244d7ee7", "start_line": 1, "end_line": 60}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/simple.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/simple.py", "rel_path": "tests/CrawlerRunner/simple.py", "module": "tests.CrawlerRunner.simple", "ext": "py", "chunk_number": 1, "symbols": ["main", "NoRequestsSpider", "task", "react", "runner", "async", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "crawler", "class", "configure", "logging", "scrapy", "asyncio", "selector", "yield", "from", "asyncioreactor", "utils", "import", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\ndef main(reactor):\n    configure_logging()\n    runner = CrawlerRunner()\n    return runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 112, "byte_len": 523, "file_sha1": "fb7944f8912ea32d3ad3d1739e3d1f1dd4322077", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/multi_seq.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/CrawlerRunner/multi_seq.py", "rel_path": "tests/CrawlerRunner/multi_seq.py", "module": "tests.CrawlerRunner.multi_seq", "ext": "py", "chunk_number": 1, "symbols": ["main", "NoRequestsSpider", "task", "react", "runner", "async", "internet", "requests", "spider", "twisted", "return", "install", "reactor", "name", "crawler", "class", "configure", "logging", "scrapy", "defer", "asyncio", "selector", "yield", "from", "asyncioreactor", "utils", "import", "start", "inline", "callbacks", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet.defer import inlineCallbacks\nfrom twisted.internet.task import react\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.reactor import install_reactor\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\n@inlineCallbacks\ndef main(reactor):\n    configure_logging()\n    runner = CrawlerRunner()\n    yield runner.crawl(NoRequestsSpider)\n    yield runner.crawl(NoRequestsSpider)\n\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nreact(main)\n", "n_tokens": 134, "byte_len": 631, "file_sha1": "d0e425896f49f306b306e95af16f311c9862de4e", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/keys/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/keys/__init__.py", "rel_path": "tests/keys/__init__.py", "module": "tests.keys.__init__", "ext": "py", "chunk_number": 1, "symbols": ["generate_keys", "encoding", "dns", "name", "primitives", "public", "key", "subject", "alternative", "false", "encryption", "algorithm", "file", "asymmetric", "hazmat", "cryptography", "certificate", "generate", "keys", "https", "scrapy", "pathlib", "path", "cert", "default", "backend", "not", "valid", "sha256", "private", "format", "creating", "countr", "from", "write", "bytes", "noencryption", "size", "backends", "timedelta", "serialization", "folder", "tutorial", "parent", "traditional", "open", "attribute", "x509", "random", "serial"], "ast_kind": "function_or_method", "text": "from datetime import datetime, timedelta, timezone\nfrom pathlib import Path\n\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives.hashes import SHA256\nfrom cryptography.hazmat.primitives.serialization import (\n    Encoding,\n    NoEncryption,\n    PrivateFormat,\n)\nfrom cryptography.x509 import (\n    CertificateBuilder,\n    DNSName,\n    Name,\n    NameAttribute,\n    SubjectAlternativeName,\n    random_serial_number,\n)\nfrom cryptography.x509.oid import NameOID\n\n\n# https://cryptography.io/en/latest/x509/tutorial/#creating-a-self-signed-certificate\ndef generate_keys():\n    folder = Path(__file__).parent\n\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n        backend=default_backend(),\n    )\n    (folder / \"localhost.key\").write_bytes(\n        key.private_bytes(\n            encoding=Encoding.PEM,\n            format=PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=NoEncryption(),\n        ),\n    )\n\n    subject = issuer = Name(\n        [\n            NameAttribute(NameOID.COUNTRY_NAME, \"IE\"),\n            NameAttribute(NameOID.ORGANIZATION_NAME, \"Scrapy\"),\n            NameAttribute(NameOID.COMMON_NAME, \"localhost\"),\n        ]\n    )\n    cert = (\n        CertificateBuilder()\n        .subject_name(subject)\n        .issuer_name(issuer)\n        .public_key(key.public_key())\n        .serial_number(random_serial_number())\n        .not_valid_before(datetime.now(tz=timezone.utc))\n        .not_valid_after(datetime.now(tz=timezone.utc) + timedelta(days=10))\n        .add_extension(\n            SubjectAlternativeName([DNSName(\"localhost\")]),\n            critical=False,\n        )\n        .sign(key, SHA256(), default_backend())\n    )\n    (folder / \"localhost.crt\").write_bytes(cert.public_bytes(Encoding.PEM))\n", "n_tokens": 393, "byte_len": 1867, "file_sha1": "54ad200d908153ad10d55fcdd51c1acab9089603", "start_line": 1, "end_line": 62}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/sleeping.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/sleeping.py", "rel_path": "tests/AsyncCrawlerProcess/sleeping.py", "module": "tests.AsyncCrawlerProcess.sleeping", "ext": "py", "chunk_number": 1, "symbols": ["SleepingSpider", "async", "await", "spider", "name", "sleeping", "class", "sleep", "scrapy", "argv", "from", "settings", "start", "urls", "process", "crawler", "data", "import", "parse", "self", "crawl", "asyncio", "response"], "ast_kind": "class_or_type", "text": "import asyncio\nimport sys\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass SleepingSpider(scrapy.Spider):\n    name = \"sleeping\"\n\n    start_urls = [\"data:,;\"]\n\n    async def parse(self, response):\n        await asyncio.sleep(int(sys.argv[1]))\n\n\nprocess = AsyncCrawlerProcess(settings={})\n\nprocess.crawl(SleepingSpider)\nprocess.start()\n", "n_tokens": 82, "byte_len": 355, "file_sha1": "4e529ea50d5c21fd17d18e7afe2ad2fc3222d53b", "start_line": 1, "end_line": 21}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_no_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_no_reactor.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_enabled_no_reactor.py", "module": "tests.AsyncCrawlerProcess.asyncio_enabled_no_reactor", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "ReactorCheckExtension", "NoRequestsSpider", "async", "requires", "internet", "extensions", "requests", "spider", "twisted", "return", "name", "class", "scrapy", "asyncio", "selector", "reactor", "init", "yield", "from", "settings", "process", "crawler", "asyncioreactor", "utils", "twiste", "import", "start", "self", "runtime", "error", "raise", "request", "crawl", "check"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed\n\n\nclass ReactorCheckExtension:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 151, "byte_len": 672, "file_sha1": "93b87e7c3ea363a4e9b08ec92d1a46313581f4e6", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_custom_settings_select.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_custom_settings_select.py", "rel_path": "tests/AsyncCrawlerProcess/twisted_reactor_custom_settings_select.py", "module": "tests.AsyncCrawlerProcess.twisted_reactor_custom_settings_select", "ext": "py", "chunk_number": 1, "symbols": ["log_task_exception", "AsyncioReactorSpider", "task", "add", "done", "log015", "result", "except", "internet", "select", "reactor", "asyncio", "exception", "typing", "twisted", "spider", "name", "annotations", "class", "log", "scrapy", "future", "typ", "checking", "logging", "failed", "noqa", "from", "crawl", "process", "async", "crawler", "none", "twiste", "import", "start", "custom", "settings", "selectreactor"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\nif TYPE_CHECKING:\n    from asyncio import Task\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n\n\ndef log_task_exception(task: Task) -> None:\n    try:\n        task.result()\n    except Exception:\n        logging.exception(\"Crawl task failed\")  # noqa: LOG015\n\n\nprocess = AsyncCrawlerProcess()\ntask = process.crawl(AsyncioReactorSpider)\ntask.add_done_callback(log_task_exception)\nprocess.start()\n", "n_tokens": 156, "byte_len": 671, "file_sha1": "307533dcde9a886035a2b396173e419dfa69081c", "start_line": 1, "end_line": 31}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_deferred_signal.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_deferred_signal.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_deferred_signal.py", "module": "tests.AsyncCrawlerProcess.asyncio_deferred_signal", "ext": "py", "chunk_number": 1, "symbols": ["open_spider", "process_item", "parse", "UppercasePipeline", "UrlSpider", "async", "pipeline", "await", "asynci", "even", "upper", "except", "internet", "open", "spider", "twisted", "return", "name", "item", "annotations", "class", "sleep", "main", "url", "scrapy", "logger", "future", "defer", "deferred", "from", "asyncio", "selector", "info", "argv", "yield", "settings", "start", "urls", "process", "index", "error", "crawler", "none", "asyncioreactor", "opened", "data", "utils", "ite", "pipelines", "twiste"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport sys\n\nfrom scrapy import Spider\nfrom scrapy.crawler import AsyncCrawlerProcess\nfrom scrapy.utils.defer import deferred_from_coro\n\n\nclass UppercasePipeline:\n    async def _open_spider(self, spider):\n        spider.logger.info(\"async pipeline opened!\")\n        await asyncio.sleep(0.1)\n\n    def open_spider(self, spider):\n        return deferred_from_coro(self._open_spider(spider))\n\n    def process_item(self, item):\n        return {\"url\": item[\"url\"].upper()}\n\n\nclass UrlSpider(Spider):\n    name = \"url_spider\"\n    start_urls = [\"data:,\"]\n    custom_settings = {\n        \"ITEM_PIPELINES\": {UppercasePipeline: 100},\n    }\n\n    def parse(self, response):\n        yield {\"url\": response.url}\n\n\nif __name__ == \"__main__\":\n    ASYNCIO_EVENT_LOOP: str | None\n    try:\n        ASYNCIO_EVENT_LOOP = sys.argv[1]\n    except IndexError:\n        ASYNCIO_EVENT_LOOP = None\n\n    process = AsyncCrawlerProcess(\n        settings={\n            \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n        }\n    )\n    process.crawl(UrlSpider)\n    process.start()\n", "n_tokens": 290, "byte_len": 1180, "file_sha1": "d69008eacfdf740f78e465ac42c6bbf7b5d55b50", "start_line": 1, "end_line": 49}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/args_settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/args_settings.py", "rel_path": "tests/AsyncCrawlerProcess/args_settings.py", "module": "tests.AsyncCrawlerProcess.args_settings", "ext": "py", "chunk_number": 1, "symbols": ["from_crawler", "NoRequestsSpider", "async", "requests", "spider", "typing", "return", "name", "getint", "class", "scrapy", "logger", "classmethod", "value", "info", "from", "crawler", "yield", "settings", "kwargs", "process", "super", "import", "start", "self", "request", "crawl", "args"], "ast_kind": "class_or_type", "text": "from typing import Any\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess, Crawler\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n        return spider\n\n    async def start(self):\n        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider, foo=42)\nprocess.start()\n", "n_tokens": 154, "byte_len": 627, "file_sha1": "523e52851f98ae089f0e63d5a2ec45aa6d694e34", "start_line": 1, "end_line": 26}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_custom_settings_same.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_custom_settings_same.py", "rel_path": "tests/AsyncCrawlerProcess/twisted_reactor_custom_settings_same.py", "module": "tests.AsyncCrawlerProcess.twisted_reactor_custom_settings_same", "ext": "py", "chunk_number": 1, "symbols": ["AsyncioReactorSpider1", "AsyncioReactorSpider2", "asyncio", "reactor", "internet", "twisted", "spider", "name", "class", "scrapy", "selector", "from", "process", "async", "crawler", "asyncioreactor", "twiste", "import", "start", "custom", "settings", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass AsyncioReactorSpider1(scrapy.Spider):\n    name = \"asyncio_reactor1\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nclass AsyncioReactorSpider2(scrapy.Spider):\n    name = \"asyncio_reactor2\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = AsyncCrawlerProcess()\nprocess.crawl(AsyncioReactorSpider1)\nprocess.crawl(AsyncioReactorSpider2)\nprocess.start()\n", "n_tokens": 147, "byte_len": 567, "file_sha1": "6ebe0f4955370a51aebd18b7038cf03dfa0ccb3d", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/multi.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/multi.py", "rel_path": "tests/AsyncCrawlerProcess/multi.py", "module": "tests.AsyncCrawlerProcess.multi", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "spider", "name", "yield", "class", "async", "from", "import", "settings", "start", "crawler", "self", "scrapy", "request", "requests", "process", "crawl", "return"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 69, "byte_len": 309, "file_sha1": "dabea0847b473aafe6997e5265ca4cf73bcc0983", "start_line": 1, "end_line": 18}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_reactor.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_enabled_reactor.py", "module": "tests.AsyncCrawlerProcess.asyncio_enabled_reactor", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "ReactorCheckExtension", "NoRequestsSpider", "async", "except", "pass", "internet", "requires", "extensions", "requests", "spider", "twisted", "return", "after", "install", "reactor", "installed", "name", "class", "before", "scrapy", "asyncio", "selector", "init", "yield", "from", "settings", "wrong", "already", "process", "crawler", "asyncioreactor", "utils", "twiste", "import", "start", "runtime", "error", "self", "raise", "request", "crawl", "else", "check"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\nfrom scrapy.utils.reactor import (\n    install_reactor,\n    is_asyncio_reactor_installed,\n    is_reactor_installed,\n)\n\nif is_reactor_installed():\n    raise RuntimeError(\n        \"Reactor already installed before is_asyncio_reactor_installed().\"\n    )\n\ntry:\n    is_asyncio_reactor_installed()\nexcept RuntimeError:\n    pass\nelse:\n    raise RuntimeError(\"is_asyncio_reactor_installed() did not raise RuntimeError.\")\n\nif is_reactor_installed():\n    raise RuntimeError(\n        \"Reactor already installed after is_asyncio_reactor_installed().\"\n    )\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\nif not is_asyncio_reactor_installed():\n    raise RuntimeError(\"Wrong reactor installed after install_reactor().\")\n\n\nclass ReactorCheckExtension:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 291, "byte_len": 1345, "file_sha1": "32764445140fda8cd3f4ad49f4d19f9bf408137c", "start_line": 1, "end_line": 54}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_reactor_same_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_reactor_same_loop.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_enabled_reactor_same_loop.py", "module": "tests.AsyncCrawlerProcess.asyncio_enabled_reactor_same_loop", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "set", "event", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "get", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "platform", "process", "install", "crawler", "asyncioreactor", "twiste", "reactor", "import", "start", "windows", "self", "request", "crawl", "uvloop", "win", "win32"], "ast_kind": "class_or_type", "text": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\nfrom uvloop import Loop\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\nif sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.set_event_loop(Loop())\nasyncioreactor.install(asyncio.get_event_loop())\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 161, "byte_len": 698, "file_sha1": "e84a9b3154e5ff2eac6fee810daf6f92d8b48e15", "start_line": 1, "end_line": 32}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_asyncio.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_asyncio.py", "rel_path": "tests/AsyncCrawlerProcess/twisted_reactor_asyncio.py", "module": "tests.AsyncCrawlerProcess.twisted_reactor_asyncio", "ext": "py", "chunk_number": 1, "symbols": ["AsyncioReactorSpider", "internet", "asyncio", "reactor", "twisted", "spider", "name", "class", "scrapy", "selector", "from", "settings", "process", "async", "crawler", "asyncioreactor", "twiste", "import", "start", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n)\nprocess.crawl(AsyncioReactorSpider)\nprocess.start()\n", "n_tokens": 81, "byte_len": 328, "file_sha1": "3f58e86ea72a484a9f891ccd7449f808c969e966", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_custom_loop_custom_settings_different.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_custom_loop_custom_settings_different.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_custom_loop_custom_settings_different.py", "module": "tests.AsyncCrawlerProcess.asyncio_custom_loop_custom_settings_different", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "process", "crawler", "none", "asyncioreactor", "twiste", "reactor", "import", "start", "custom", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n    custom_settings = {\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": None,\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 118, "byte_len": 483, "file_sha1": "db8bad41ae7af21953110d35fea95f9247d93763", "start_line": 1, "end_line": 24}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_custom_loop_custom_settings_same.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_custom_loop_custom_settings_same.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_custom_loop_custom_settings_same.py", "module": "tests.AsyncCrawlerProcess.asyncio_custom_loop_custom_settings_same", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "process", "crawler", "asyncioreactor", "twiste", "reactor", "import", "start", "custom", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n    custom_settings = {\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 122, "byte_len": 492, "file_sha1": "4b57b10e75fafd8b6da3737671ff27fed2c8096b", "start_line": 1, "end_line": 24}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/caching_hostname_resolver.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/caching_hostname_resolver.py", "rel_path": "tests/AsyncCrawlerProcess/caching_hostname_resolver.py", "module": "tests.AsyncCrawlerProcess.caching_hostname_resolver", "ext": "py", "chunk_number": 1, "symbols": ["parse", "ignore_response", "CachingHostnameResolverSpider", "does", "caching", "hostname", "async", "repr", "false", "spider", "name", "class", "resolution", "address", "dont", "filter", "main", "time", "scrapy", "logger", "finite", "amount", "info", "argv", "finishes", "ignore", "response", "resolver", "dns", "yield", "true", "from", "settings", "indefinitely", "retr", "enabled", "request", "process", "range", "crawler", "import", "hang", "start", "self", "callback", "crawl"], "ast_kind": "class_or_type", "text": "import sys\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass CachingHostnameResolverSpider(scrapy.Spider):\n    \"\"\"\n    Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)\n    \"\"\"\n\n    name = \"caching_hostname_resolver_spider\"\n\n    async def start(self):\n        yield scrapy.Request(self.url)\n\n    def parse(self, response):\n        for _ in range(10):\n            yield scrapy.Request(\n                response.url, dont_filter=True, callback=self.ignore_response\n            )\n\n    def ignore_response(self, response):\n        self.logger.info(repr(response.ip_address))\n\n\nif __name__ == \"__main__\":\n    process = AsyncCrawlerProcess(\n        settings={\n            \"RETRY_ENABLED\": False,\n            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n        }\n    )\n    process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])\n    process.start()\n", "n_tokens": 195, "byte_len": 920, "file_sha1": "071a234d945016d3fb0954848b03ddbb717030da", "start_line": 1, "end_line": 36}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_reactor_different_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_enabled_reactor_different_loop.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_enabled_reactor_different_loop.py", "module": "tests.AsyncCrawlerProcess.asyncio_enabled_reactor_different_loop", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "set", "event", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "get", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "platform", "process", "install", "crawler", "asyncioreactor", "twiste", "reactor", "import", "start", "windows", "self", "request", "crawl", "uvloop", "win", "win32"], "ast_kind": "class_or_type", "text": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\nif sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncioreactor.install(asyncio.get_event_loop())\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 147, "byte_len": 643, "file_sha1": "0d588cb37bd9239fe67ae3e3e3f4cd1b2fbba3f5", "start_line": 1, "end_line": 30}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_custom_settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/twisted_reactor_custom_settings.py", "rel_path": "tests/AsyncCrawlerProcess/twisted_reactor_custom_settings.py", "module": "tests.AsyncCrawlerProcess.twisted_reactor_custom_settings", "ext": "py", "chunk_number": 1, "symbols": ["AsyncioReactorSpider", "internet", "asyncio", "reactor", "twisted", "spider", "name", "class", "scrapy", "selector", "from", "process", "async", "crawler", "asyncioreactor", "twiste", "import", "start", "custom", "settings", "crawl"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = AsyncCrawlerProcess()\nprocess.crawl(AsyncioReactorSpider)\nprocess.start()\n", "n_tokens": 82, "byte_len": 336, "file_sha1": "821b8fcce596b3e22ac589ace34dabb24a6f7d9b", "start_line": 1, "end_line": 15}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/reactor_default.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/reactor_default.py", "rel_path": "tests/AsyncCrawlerProcess/reactor_default.py", "module": "tests.AsyncCrawlerProcess.reactor_default", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "async", "internet", "f401", "requests", "spider", "twisted", "return", "name", "class", "scrapy", "yield", "noqa", "from", "settings", "process", "crawler", "reactor", "import", "tid253", "start", "self", "request", "crawl"], "ast_kind": "class_or_type", "text": "from twisted.internet import reactor  # noqa: F401,TID253\n\nimport scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(settings={})\n\nd = process.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 78, "byte_len": 340, "file_sha1": "e9f7d52680b0e80a09f398940283303569a72203", "start_line": 1, "end_line": 19}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/simple.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/simple.py", "rel_path": "tests/AsyncCrawlerProcess/simple.py", "module": "tests.AsyncCrawlerProcess.simple", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "spider", "name", "yield", "class", "async", "from", "import", "settings", "start", "crawler", "self", "scrapy", "request", "requests", "process", "crawl", "return"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 61, "byte_len": 277, "file_sha1": "2d9c915e04cafa6574bf9243ff1daf087d84eebc", "start_line": 1, "end_line": 17}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/caching_hostname_resolver_ipv6.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/caching_hostname_resolver_ipv6.py", "rel_path": "tests/AsyncCrawlerProcess/caching_hostname_resolver_ipv6.py", "module": "tests.AsyncCrawlerProcess.caching_hostname_resolver_ipv6", "ext": "py", "chunk_number": 1, "symbols": ["CachingHostnameResolverSpider", "caching", "hostname", "false", "internet", "exception", "twisted", "spider", "name", "class", "main", "scrapy", "finishes", "resolver", "dns", "from", "settings", "start", "urls", "retr", "enabled", "process", "async", "crawler", "without", "import", "http", "lookup", "crawl", "error"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass CachingHostnameResolverSpider(scrapy.Spider):\n    \"\"\"\n    Finishes without a twisted.internet.error.DNSLookupError exception\n    \"\"\"\n\n    name = \"caching_hostname_resolver_spider\"\n    start_urls = [\"http://[::1]\"]\n\n\nif __name__ == \"__main__\":\n    process = AsyncCrawlerProcess(\n        settings={\n            \"RETRY_ENABLED\": False,\n            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n        }\n    )\n    process.crawl(CachingHostnameResolverSpider)\n    process.start()\n", "n_tokens": 128, "byte_len": 558, "file_sha1": "a833fa0db0b66e1a7c53c1c7d2966bfb79007e1f", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_custom_loop.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/asyncio_custom_loop.py", "rel_path": "tests/AsyncCrawlerProcess/asyncio_custom_loop.py", "module": "tests.AsyncCrawlerProcess.asyncio_custom_loop", "ext": "py", "chunk_number": 1, "symbols": ["NoRequestsSpider", "async", "asynci", "even", "internet", "requests", "spider", "twisted", "return", "name", "class", "scrapy", "asyncio", "selector", "loop", "yield", "from", "settings", "process", "crawler", "asyncioreactor", "twiste", "reactor", "import", "start", "self", "request", "crawl", "uvloop"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    async def start(self):\n        return\n        yield\n\n\nprocess = AsyncCrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "n_tokens": 101, "byte_len": 417, "file_sha1": "c35008deba68957fcdc7ad4f6ac8e00df7236047", "start_line": 1, "end_line": 21}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/default_name_resolver.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/tests/AsyncCrawlerProcess/default_name_resolver.py", "rel_path": "tests/AsyncCrawlerProcess/default_name_resolver.py", "module": "tests.AsyncCrawlerProcess.default_name_resolver", "ext": "py", "chunk_number": 1, "symbols": ["IPv6Spider", "does", "false", "internet", "raises", "twisted", "spider", "name", "handle", "class", "main", "scrapy", "from", "settings", "start", "urls", "retr", "enabled", "process", "async", "crawler", "import", "ipv6spider", "resolver", "http", "default", "dns", "lookup", "addresses", "crawl", "error", "ipv", "ipv6"], "ast_kind": "class_or_type", "text": "import scrapy\nfrom scrapy.crawler import AsyncCrawlerProcess\n\n\nclass IPv6Spider(scrapy.Spider):\n    \"\"\"\n    Raises a twisted.internet.error.DNSLookupError:\n    the default name resolver does not handle IPv6 addresses.\n    \"\"\"\n\n    name = \"ipv6_spider\"\n    start_urls = [\"http://[::1]\"]\n\n\nif __name__ == \"__main__\":\n    process = AsyncCrawlerProcess(settings={\"RETRY_ENABLED\": False})\n    process.crawl(IPv6Spider)\n    process.start()\n", "n_tokens": 107, "byte_len": 434, "file_sha1": "898f58fa6c7f0624d9dd12f0531334fc7625efe9", "start_line": 1, "end_line": 19}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/conftest.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/conftest.py", "rel_path": "docs/conftest.py", "module": "docs.conftest", "ext": "py", "chunk_number": 1, "symbols": ["load_response", "setup", "except", "namespace", "pattern", "codeblock", "tests", "file", "python", "code", "return", "pytest", "collect", "sybil", "load", "response", "print", "function", "import", "error", "scrapy", "doctest", "pathlib", "path", "body", "doc", "test", "ellipsis", "from", "parent", "normaliz", "whitespace", "parsers", "html", "input", "optionflags", "block", "filename", "skip", "http", "read", "bytes", "future", "imports"], "ast_kind": "function_or_method", "text": "from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\nfrom pathlib import Path\n\nfrom sybil import Sybil\nfrom sybil.parsers.doctest import DocTestParser\nfrom sybil.parsers.skip import skip\n\ntry:\n    # >2.0.1\n    from sybil.parsers.codeblock import PythonCodeBlockParser\nexcept ImportError:\n    from sybil.parsers.codeblock import CodeBlockParser as PythonCodeBlockParser\n\nfrom scrapy.http.response.html import HtmlResponse\n\n\ndef load_response(url: str, filename: str) -> HtmlResponse:\n    input_path = Path(__file__).parent / \"_tests\" / filename\n    return HtmlResponse(url, body=input_path.read_bytes())\n\n\ndef setup(namespace):\n    namespace[\"load_response\"] = load_response\n\n\npytest_collect_file = Sybil(\n    parsers=[\n        DocTestParser(optionflags=ELLIPSIS | NORMALIZE_WHITESPACE),\n        PythonCodeBlockParser(future_imports=[\"print_function\"]),\n        skip,\n    ],\n    pattern=\"*.rst\",\n    setup=setup,\n).pytest()\n", "n_tokens": 215, "byte_len": 921, "file_sha1": "8a72a96e658398e95583fd96963fa44aab2ce155", "start_line": 1, "end_line": 35}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/conf.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/conf.py", "rel_path": "docs/conf.py", "module": "docs.conf", "ext": "py", "chunk_number": 1, "symbols": ["used", "full", "sphinx", "project", "copyright", "general", "configuration", "various", "built", "your", "rtd", "notfound", "append", "scrapyfixautodoc", "file", "another", "after", "absolute", "must", "coverage", "documenting", "directory", "builder", "path", "exclude", "patterns", "documents", "scrapydocs", "collections", "https", "read", "method", "those", "whom", "intersphinx", "disabled", "static", "usage", "error", "subclasses", "reimplement", "filtering", "middleware", "manual", "lib", "w3lib", "extension", "name", "lxml", "link"], "ast_kind": "imports", "text": "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\nimport os\nimport sys\nfrom collections.abc import Sequence\nfrom pathlib import Path\n\n# If your extensions are in another directory, add it here. If the directory\n# is relative to the documentation root, use Path.absolute to make it absolute.\nsys.path.append(str(Path(__file__).parent / \"_ext\"))\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = \"Scrapy\"\nproject_copyright = \"Scrapy developers\"\nauthor = \"Scrapy developers\"\n\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    \"notfound.extension\",\n    \"scrapydocs\",\n    \"sphinx.ext.autodoc\",\n    \"scrapyfixautodoc\",  # Must be after \"sphinx.ext.autodoc\"\n    \"sphinx.ext.coverage\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx_rtd_dark_mode\",\n]\n\ntemplates_path = [\"_templates\"]\nexclude_patterns = [\"build\", \"Thumbs.db\", \".DS_Store\"]\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\ntry:", "n_tokens": 340, "byte_len": 1528, "file_sha1": "44beb4459616066e5ffe1c2235c11a3e6bf3129c", "start_line": 1, "end_line": 47}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/conf.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/conf.py", "rel_path": "docs/conf.py", "module": "docs.conf", "ext": "py", "chunk_number": 2, "symbols": ["read", "method", "those", "whom", "intersphinx", "disabled", "static", "usage", "error", "subclasses", "reimplement", "filtering", "middleware", "manual", "lib", "w3lib", "extension", "name", "lxml", "link", "target", "docs", "controlled", "private", "deprecated", "https", "interface", "domain", "add", "post", "your", "append", "file", "after", "documents", "path", "make", "thumbs", "acts", "lxmlhtml", "request", "seen", "pytest", "settings", "never", "document", "linkcheck", "anchors", "suppress", "warnings"], "ast_kind": "class_or_type", "text": "    import scrapy\n\n    version = \".\".join(map(str, scrapy.version_info[:2]))\n    release = scrapy.__version__\nexcept ImportError:\n    version = \"\"\n    release = \"\"\n\nsuppress_warnings = [\"epub.unknown_project_files\"]\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"sphinx_rtd_theme\"\nhtml_static_path = [\"_static\"]\n\nhtml_last_updated_fmt = \"%b %d, %Y\"\n\nhtml_css_files = [\n    \"custom.css\",\n]\n\n# Set canonical URL from the Read the Docs Domain\nhtml_baseurl = os.environ.get(\"READTHEDOCS_CANONICAL_URL\", \"\")\n\n# -- Options for LaTeX output ------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-latex-output\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, document class [howto/manual]).\nlatex_documents = [\n    (\"index\", \"Scrapy.tex\", \"Scrapy Documentation\", \"Scrapy developers\", \"manual\"),\n]\n\n\n# -- Options for the linkcheck builder ---------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-the-linkcheck-builder\n\nlinkcheck_ignore = [\n    r\"http://localhost:\\d+\",\n    \"http://hg.scrapy.org\",\n    r\"https://github.com/scrapy/scrapy/commit/\\w+\",\n    r\"https://github.com/scrapy/scrapy/issues/\\d+\",\n]\n\nlinkcheck_anchors_ignore_for_url = [\"https://github.com/pyca/cryptography/issues/2692\"]\n\n# -- Options for the Coverage extension --------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/extensions/coverage.html#configuration\n\ncoverage_ignore_pyobjects = [\n    # Contract’s add_pre_hook and add_post_hook are not documented because\n    # they should be transparent to contract developers, for whom pre_hook and\n    # post_hook should be the actual concern.\n    r\"\\bContract\\.add_(pre|post)_hook$\",\n    # ContractsManager is an internal class, developers are not expected to\n    # interact with it directly in any way.\n    r\"\\bContractsManager\\b$\",\n    # For default contracts we only want to document their general purpose in\n    # their __init__ method, the methods they reimplement to achieve that purpose\n    # should be irrelevant to developers using those contracts.\n    r\"\\w+Contract\\.(adjust_request_args|(pre|post)_process)$\",\n    # Methods of downloader middlewares are not documented, only the classes\n    # themselves, since downloader middlewares are controlled through Scrapy\n    # settings.\n    r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.(\\w*?Middleware|DownloaderStats)\\.\",\n    # Base classes of downloader middlewares are implementation details that\n    # are not meant for users.\n    r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware\",\n    # The interface methods of duplicate request filtering classes are already\n    # covered in the interface documentation part of the DUPEFILTER_CLASS\n    # setting documentation.\n    r\"^scrapy\\.dupefilters\\.[A-Z]\\w*?\\.(from_settings|request_seen|open|close|log)$\",\n    # Private exception used by the command-line interface implementation.\n    r\"^scrapy\\.exceptions\\.UsageError\",\n    # Methods of BaseItemExporter subclasses are only documented in\n    # BaseItemExporter.\n    r\"^scrapy\\.exporters\\.(?!BaseItemExporter\\b)\\w*?\\.\",\n    # Extension behavior is only modified through settings. Methods of\n    # extension classes, as well as helper functions, are implementation\n    # details that are not documented.\n    r\"^scrapy\\.extensions\\.[a-z]\\w*?\\.[A-Z]\\w*?\\.\",  # methods\n    r\"^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]\",  # helper functions\n    # Never documented before, and deprecated now.\n    r\"^scrapy\\.linkextractors\\.FilteringLinkExtractor$\",\n    # Implementation detail of LxmlLinkExtractor\n    r\"^scrapy\\.linkextractors\\.lxmlhtml\\.LxmlParserLinkExtractor\",\n]\n\n\n# -- Options for the InterSphinx extension -----------------------------------\n# https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#configuration\n\nintersphinx_mapping = {\n    \"attrs\": (\"https://www.attrs.org/en/stable/\", None),\n    \"coverage\": (\"https://coverage.readthedocs.io/en/latest\", None),\n    \"cryptography\": (\"https://cryptography.io/en/latest/\", None),\n    \"cssselect\": (\"https://cssselect.readthedocs.io/en/latest\", None),\n    \"itemloaders\": (\"https://itemloaders.readthedocs.io/en/latest/\", None),\n    \"parsel\": (\"https://parsel.readthedocs.io/en/latest/\", None),\n    \"pytest\": (\"https://docs.pytest.org/en/latest\", None),\n    \"python\": (\"https://docs.python.org/3\", None),\n    \"sphinx\": (\"https://www.sphinx-doc.org/en/master\", None),\n    \"tox\": (\"https://tox.wiki/en/latest/\", None),\n    \"twisted\": (\"https://docs.twisted.org/en/stable/\", None),\n    \"twistedapi\": (\"https://docs.twisted.org/en/stable/api/\", None),\n    \"w3lib\": (\"https://w3lib.readthedocs.io/en/latest\", None),\n}\nintersphinx_disabled_reftypes: Sequence[str] = []\n\n# -- Other options ------------------------------------------------------------\ndefault_dark_mode = False\n", "n_tokens": 1199, "byte_len": 5055, "file_sha1": "44beb4459616066e5ffe1c2235c11a3e6bf3129c", "start_line": 48, "end_line": 161}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/_static/custom.css#1", "repo_id": "scrapy-master", "language": "css", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/_static/custom.css", "rel_path": "docs/_static/custom.css", "module": "docs._static.custom", "ext": "css", "chunk_number": 1, "symbols": ["Move", "lists", "closer", "their", "introducing", "paragraph", "content", "section", "margin", "bottom", "Compensates", "override", "some", "styles", "sphinx", "dark", "mode", "static", "dark_mode_css", "general", "theme", "switcher", "right", "important", "webkit", "shadow", "rgba", "height", "width", "place", "toggle", "button", "corner", "small", "screens", "media", "auto", "persist", "blue", "color", "left", "used", "default", "html", "data", "side", "search", "background", "below", "present"], "ast_kind": "unknown", "text": "/* Move lists closer to their introducing paragraph */\n.rst-content .section ol p, .rst-content .section ul p {\n    margin-bottom: 0px;\n}\n.rst-content p + ol, .rst-content p + ul {\n    margin-top: -18px; /* Compensates margin-top: 24px of p  */\n}\n.rst-content dl p + ol, .rst-content dl p + ul {\n    margin-top: -6px; /* Compensates margin-top: 12px of p  */\n}\n\n/*override some styles in\nsphinx-rtd-dark-mode/static/dark_mode_css/general.css*/\n.theme-switcher {\n    right: 0.4em !important;\n    top: 0.6em !important;\n    -webkit-box-shadow: 0px 3px 14px 4px rgba(0, 0, 0, 0.30) !important;\n    box-shadow: 0px 3px 14px 4px rgba(0, 0, 0, 0.30) !important;\n    height: 2em !important;\n    width: 2em !important;\n}\n\n/*place the toggle button for dark mode\nat the bottom right corner on small screens*/\n@media (max-width: 768px) {\n    .theme-switcher {\n        right: 0.4em !important;\n        bottom: 2.6em !important;\n        top: auto !important;\n    }\n}\n\n/*persist blue color at the top left used in\ndefault rtd theme*/\nhtml[data-theme=\"dark\"] .wy-side-nav-search,\nhtml[data-theme=\"dark\"] .wy-nav-top {\n    background-color: #1d577d !important;\n}\n\n/*all the styles below used to present\nAPI objects nicely in dark mode*/\nhtml[data-theme=\"dark\"] .sig.sig-object {\n    border-left-color: #3e4446 !important;\n    background-color: #202325 !important\n}\n\nhtml[data-theme=\"dark\"] .sig-name,\nhtml[data-theme=\"dark\"] .sig-prename,\nhtml[data-theme=\"dark\"] .property,\nhtml[data-theme=\"dark\"] .sig-param,\nhtml[data-theme=\"dark\"] .sig-paren,\nhtml[data-theme=\"dark\"] .sig-return-icon,\nhtml[data-theme=\"dark\"] .sig-return-typehint,\nhtml[data-theme=\"dark\"] .optional {\n    color: #e8e6e3 !important\n}\n", "n_tokens": 511, "byte_len": 1687, "file_sha1": "5f133bd312075c652b0a0692afbccce38a122afa", "start_line": 1, "end_line": 57}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/utils/linkfix.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/utils/linkfix.py", "rel_path": "docs/utils/linkfix.py", "module": "docs.utils.linkfix", "ext": "py", "chunk_number": 1, "symbols": ["main", "encoding", "read", "line", "fixed", "error", "oserror", "https", "path", "lower", "newfilename", "linkfix", "issues", "created", "none", "docs", "group", "author", "found", "match", "write", "text", "memory", "linkcheck", "sphinx", "output", "matches", "print", "standard", "understand", "filename", "every", "sure", "readlines", "understood", "build", "else", "have", "please", "contents", "python", "errortype", "uses", "replace", "links", "update", "open", "lines", "first", "previous"], "ast_kind": "function_or_method", "text": "#!/usr/bin/python\n\n\"\"\"\n\nLinkfix - a companion to sphinx's linkcheck builder.\n\nUses the linkcheck's output file to fix links in docs.\n\nOriginally created for this issue:\nhttps://github.com/scrapy/scrapy/issues/606\n\nAuthor: dufferzafar\n\"\"\"\n\nimport re\nimport sys\nfrom pathlib import Path\n\n\ndef main():\n    # Used for remembering the file (and its contents)\n    # so we don't have to open the same file again.\n    _filename = None\n    _contents = None\n\n    # A regex that matches standard linkcheck output lines\n    line_re = re.compile(r\"(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))\")\n\n    # Read lines from the linkcheck output file\n    try:\n        with Path(\"build/linkcheck/output.txt\").open(encoding=\"utf-8\") as out:\n            output_lines = out.readlines()\n    except OSError:\n        print(\"linkcheck output not found; please run linkcheck first.\")\n        sys.exit(1)\n\n    # For every line, fix the respective file\n    for line in output_lines:\n        match = re.match(line_re, line)\n\n        if match:\n            newfilename = match.group(1)\n            errortype = match.group(2)\n\n            # Broken links can't be fixed and\n            # I am not sure what do with the local ones.\n            if errortype.lower() in [\"broken\", \"local\"]:\n                print(\"Not Fixed: \" + line)\n            else:\n                # If this is a new file\n                if newfilename != _filename:\n                    # Update the previous file\n                    if _filename:\n                        Path(_filename).write_text(_contents, encoding=\"utf-8\")\n\n                    _filename = newfilename\n\n                    # Read the new file to memory\n                    _contents = Path(_filename).read_text(encoding=\"utf-8\")\n\n                _contents = _contents.replace(match.group(3), match.group(4))\n        else:\n            # We don't understand what the current line means!\n            print(\"Not Understood: \" + line)\n\n\nif __name__ == \"__main__\":\n    main()\n", "n_tokens": 455, "byte_len": 1975, "file_sha1": "62c35d56bd36083ac1d17474ee55342b3629e6cc", "start_line": 1, "end_line": 69}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/_ext/scrapydocs.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/_ext/scrapydocs.py", "rel_path": "docs/_ext/scrapydocs.py", "module": "docs._ext.scrapydocs", "ext": "py", "chunk_number": 1, "symbols": ["run", "is_setting_index", "get_setting_name_and_refid", "collect_scrapy_settings_refs", "make_setting_element", "replace_settingslist_nodes", "source_role", "issue_role", "commit_role", "rev_role", "setup", "SettingData", "SettingslistNode", "SettingsListDirective", "resolved", "doctree", "bool", "issue", "role", "append", "signal", "tagname", "name", "https", "directives", "settingslist", "node", "settin", "indextemplate", "directive", "document", "inliner", "issues", "child", "none", "type", "sorted", "http", "sequence", "add", "collect", "scrapy", "sphinx", "replace", "self", "setting", "data", "command", "targetid", "typing"], "ast_kind": "class_or_type", "text": "# pylint: disable=import-error\nfrom collections.abc import Sequence\nfrom operator import itemgetter\nfrom typing import Any, TypedDict\n\nfrom docutils import nodes\nfrom docutils.nodes import Element, General, Node, document\nfrom docutils.parsers.rst import Directive\nfrom sphinx.application import Sphinx\nfrom sphinx.util.nodes import make_refnode\n\n\nclass SettingData(TypedDict):\n    docname: str\n    setting_name: str\n    refid: str\n\n\nclass SettingslistNode(General, Element):\n    pass\n\n\nclass SettingsListDirective(Directive):\n    def run(self) -> Sequence[Node]:\n        return [SettingslistNode()]\n\n\ndef is_setting_index(node: Node) -> bool:\n    if node.tagname == \"index\" and node[\"entries\"]:  # type: ignore[index,attr-defined]\n        # index entries for setting directives look like:\n        # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n        entry_type, info, refid = node[\"entries\"][0][:3]  # type: ignore[index]\n        return entry_type == \"pair\" and info.endswith(\"; setting\")\n    return False\n\n\ndef get_setting_name_and_refid(node: Node) -> tuple[str, str]:\n    \"\"\"Extract setting name from directive index node\"\"\"\n    entry_type, info, refid = node[\"entries\"][0][:3]  # type: ignore[index]\n    return info.replace(\"; setting\", \"\"), refid\n\n\ndef collect_scrapy_settings_refs(app: Sphinx, doctree: document) -> None:\n    env = app.builder.env\n\n    if not hasattr(env, \"scrapy_all_settings\"):\n        emptyList: list[SettingData] = []\n        env.scrapy_all_settings = emptyList  # type: ignore[attr-defined]\n\n    for node in doctree.findall(is_setting_index):\n        setting_name, refid = get_setting_name_and_refid(node)\n\n        env.scrapy_all_settings.append(  # type: ignore[attr-defined]\n            SettingData(\n                docname=env.docname,\n                setting_name=setting_name,\n                refid=refid,\n            )\n        )\n\n\ndef make_setting_element(\n    setting_data: SettingData, app: Sphinx, fromdocname: str\n) -> Any:\n    refnode = make_refnode(\n        app.builder,\n        fromdocname,\n        todocname=setting_data[\"docname\"],\n        targetid=setting_data[\"refid\"],\n        child=nodes.Text(setting_data[\"setting_name\"]),\n    )\n    p = nodes.paragraph()\n    p += refnode\n\n    item = nodes.list_item()\n    item += p\n    return item\n\n\ndef replace_settingslist_nodes(\n    app: Sphinx, doctree: document, fromdocname: str\n) -> None:\n    env = app.builder.env\n\n    for node in doctree.findall(SettingslistNode):\n        settings_list = nodes.bullet_list()\n        settings_list.extend(\n            [\n                make_setting_element(d, app, fromdocname)\n                for d in sorted(env.scrapy_all_settings, key=itemgetter(\"setting_name\"))  # type: ignore[attr-defined]\n                if fromdocname != d[\"docname\"]\n            ]\n        )\n        node.replace_self(settings_list)\n\n\ndef source_role(\n    name, rawtext, text: str, lineno, inliner, options=None, content=None\n) -> tuple[list[Any], list[Any]]:\n    ref = \"https://github.com/scrapy/scrapy/blob/master/\" + text\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []\n\n\ndef issue_role(\n    name, rawtext, text: str, lineno, inliner, options=None, content=None\n) -> tuple[list[Any], list[Any]]:\n    ref = \"https://github.com/scrapy/scrapy/issues/\" + text\n    node = nodes.reference(rawtext, \"issue \" + text, refuri=ref)\n    return [node], []\n\n\ndef commit_role(\n    name, rawtext, text: str, lineno, inliner, options=None, content=None\n) -> tuple[list[Any], list[Any]]:\n    ref = \"https://github.com/scrapy/scrapy/commit/\" + text\n    node = nodes.reference(rawtext, \"commit \" + text, refuri=ref)\n    return [node], []\n\n\ndef rev_role(\n    name, rawtext, text: str, lineno, inliner, options=None, content=None\n) -> tuple[list[Any], list[Any]]:\n    ref = \"http://hg.scrapy.org/scrapy/changeset/\" + text\n    node = nodes.reference(rawtext, \"r\" + text, refuri=ref)\n    return [node], []\n\n\ndef setup(app: Sphinx) -> None:\n    app.add_crossref_type(\n        directivename=\"setting\",\n        rolename=\"setting\",\n        indextemplate=\"pair: %s; setting\",\n    )\n    app.add_crossref_type(\n        directivename=\"signal\",\n        rolename=\"signal\",\n        indextemplate=\"pair: %s; signal\",\n    )\n    app.add_crossref_type(\n        directivename=\"command\",\n        rolename=\"command\",\n        indextemplate=\"pair: %s; command\",\n    )\n    app.add_crossref_type(\n        directivename=\"reqmeta\",\n        rolename=\"reqmeta\",\n        indextemplate=\"pair: %s; reqmeta\",\n    )\n    app.add_role(\"source\", source_role)\n    app.add_role(\"commit\", commit_role)\n    app.add_role(\"issue\", issue_role)\n    app.add_role(\"rev\", rev_role)\n\n    app.add_node(SettingslistNode)\n    app.add_directive(\"settingslist\", SettingsListDirective)\n\n    app.connect(\"doctree-read\", collect_scrapy_settings_refs)\n    app.connect(\"doctree-resolved\", replace_settingslist_nodes)\n", "n_tokens": 1220, "byte_len": 4905, "file_sha1": "e9ce7006532c89f05e1ff9a8f4ff97c91119f385", "start_line": 1, "end_line": 160}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/docs/_ext/scrapyfixautodoc.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/docs/_ext/scrapyfixautodoc.py", "rel_path": "docs/_ext/scrapyfixautodoc.py", "module": "docs._ext.scrapyfixautodoc", "ext": "py", "chunk_number": 1, "symbols": ["maybe_skip_member", "setup", "sphinx", "text", "bool", "application", "default", "selector", "unwanted", "github", "member", "return", "after", "must", "name", "autodocs", "options", "item", "https", "pylint", "fixes", "connect", "members", "from", "autodoc", "what", "following", "issues", "none", "generating", "alias", "behavior", "import", "skip", "disable", "error", "included", "maybe"], "ast_kind": "function_or_method", "text": "\"\"\"\nMust be included after 'sphinx.ext.autodoc'. Fixes unwanted 'alias of' behavior.\nhttps://github.com/sphinx-doc/sphinx/issues/4422\n\"\"\"\n\n# pylint: disable=import-error\nfrom sphinx.application import Sphinx\n\n\ndef maybe_skip_member(app: Sphinx, what, name: str, obj, skip: bool, options) -> bool:\n    if not skip:\n        # autodocs was generating a text \"alias of\" for the following members\n        return name in {\"default_item_class\", \"default_selector_class\"}\n    return skip\n\n\ndef setup(app: Sphinx) -> None:\n    app.connect(\"autodoc-skip-member\", maybe_skip_member)\n", "n_tokens": 141, "byte_len": 572, "file_sha1": "e9fcca328ced32b586bf1b00ba5ad9957437c818", "start_line": 1, "end_line": 19}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/link.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/link.py", "rel_path": "scrapy/link.py", "module": "scrapy.link", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "__eq__", "__hash__", "__repr__", "Link", "used", "implementation", "anchor", "fragment", "presence", "text", "bool", "false", "below", "module", "hash", "return", "after", "objects", "absolute", "name", "class", "illustrate", "link", "symbol", "must", "part", "urls", "scrapy", "actual", "from", "https", "example", "not", "implemented", "value", "topics", "other", "type", "error", "init", "repr", "slots", "documentation", "defines", "href", "sample", "being", "attribute", "isinstance"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module defines the Link object used in Link extractors.\n\nFor actual link extractors implementation see scrapy.linkextractors, or\nits documentation in: docs/topics/link-extractors.rst\n\"\"\"\n\n\nclass Link:\n    \"\"\"Link objects represent an extracted link by the LinkExtractor.\n\n    Using the anchor tag sample below to illustrate the parameters::\n\n            <a href=\"https://example.com/nofollow.html#foo\" rel=\"nofollow\">Dont follow this one</a>\n\n    :param url: the absolute url being linked to in the anchor tag.\n                From the sample, this is ``https://example.com/nofollow.html``.\n\n    :param text: the text in the anchor tag. From the sample, this is ``Dont follow this one``.\n\n    :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.\n\n    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute\n                    of the anchor tag.\n    \"\"\"\n\n    __slots__ = [\"fragment\", \"nofollow\", \"text\", \"url\"]\n\n    def __init__(\n        self, url: str, text: str = \"\", fragment: str = \"\", nofollow: bool = False\n    ):\n        if not isinstance(url, str):\n            got = url.__class__.__name__\n            raise TypeError(f\"Link urls must be str objects, got {got}\")\n        self.url: str = url\n        self.text: str = text\n        self.fragment: str = fragment\n        self.nofollow: bool = nofollow\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Link):\n            raise NotImplementedError\n        return (\n            self.url == other.url\n            and self.text == other.text\n            and self.fragment == other.fragment\n            and self.nofollow == other.nofollow\n        )\n\n    def __hash__(self) -> int:\n        return (\n            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n        )\n\n    def __repr__(self) -> str:\n        return (\n            f\"Link(url={self.url!r}, text={self.text!r}, \"\n            f\"fragment={self.fragment!r}, nofollow={self.nofollow!r})\"\n        )\n", "n_tokens": 488, "byte_len": 2061, "file_sha1": "ce9e785b0c27642b5c498c088950cdbc16970ad1", "start_line": 1, "end_line": 60}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/mail.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/mail.py", "rel_path": "scrapy/mail.py", "module": "scrapy.mail", "ext": "py", "chunk_number": 1, "symbols": ["_to_bytes_or_none", "__init__", "from_settings", "from_crawler", "_from_settings", "send", "MailSender", "failure", "mailto", "bool", "add", "system", "sent", "failed", "smtphost", "python", "mailfrom", "sending", "sendmail", "deprecated", "mai", "pass", "helpers", "future", "typ", "checking", "https", "from", "debug", "settings", "_sent_ok", "_sent_failed", "_sendmail", "_create_sender_factory", "errstr", "imports", "encode", "base", "smtpport", "mail", "sender", "header", "none", "localhost", "ascii", "join", "docs", "misc", "string", "bytes"], "ast_kind": "class_or_type", "text": "\"\"\"\nMail sending helpers\n\nSee documentation in docs/topics/email.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport warnings\nfrom email import encoders as Encoders\nfrom email.mime.base import MIMEBase\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.nonmultipart import MIMENonMultipart\nfrom email.mime.text import MIMEText\nfrom email.utils import formatdate\nfrom io import BytesIO\nfrom typing import IO, TYPE_CHECKING, Any\n\nfrom twisted.internet import ssl\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Sequence\n\n    # imports twisted.internet.reactor\n    from twisted.mail.smtp import ESMTPSenderFactory\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nlogger = logging.getLogger(__name__)\n\n\n# Defined in the email.utils module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/email/utils.py#L42\nCOMMASPACE = \", \"\n\n\ndef _to_bytes_or_none(text: str | bytes | None) -> bytes | None:\n    if text is None:\n        return None\n    return to_bytes(text)\n\n\nclass MailSender:\n    def __init__(\n        self,\n        smtphost: str = \"localhost\",\n        mailfrom: str = \"scrapy@localhost\",\n        smtpuser: str | None = None,\n        smtppass: str | None = None,\n        smtpport: int = 25,\n        smtptls: bool = False,\n        smtpssl: bool = False,\n        debug: bool = False,\n    ):\n        self.smtphost: str = smtphost\n        self.smtpport: int = smtpport\n        self.smtpuser: bytes | None = _to_bytes_or_none(smtpuser)\n        self.smtppass: bytes | None = _to_bytes_or_none(smtppass)\n        self.smtptls: bool = smtptls\n        self.smtpssl: bool = smtpssl\n        self.mailfrom: str = mailfrom\n        self.debug: bool = debug\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        warnings.warn(\n            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return cls._from_settings(settings)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls._from_settings(crawler.settings)\n\n    @classmethod\n    def _from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(\n            smtphost=settings[\"MAIL_HOST\"],\n            mailfrom=settings[\"MAIL_FROM\"],\n            smtpuser=settings[\"MAIL_USER\"],\n            smtppass=settings[\"MAIL_PASS\"],\n            smtpport=settings.getint(\"MAIL_PORT\"),\n            smtptls=settings.getbool(\"MAIL_TLS\"),\n            smtpssl=settings.getbool(\"MAIL_SSL\"),\n        )\n\n    def send(\n        self,\n        to: str | list[str],\n        subject: str,\n        body: str,\n        cc: str | list[str] | None = None,\n        attachs: Sequence[tuple[str, str, IO[Any]]] = (),\n        mimetype: str = \"text/plain\",\n        charset: str | None = None,\n        _callback: Callable[..., None] | None = None,\n    ) -> Deferred[None] | None:\n        from twisted.internet import reactor\n\n        msg: MIMEBase = (\n            MIMEMultipart() if attachs else MIMENonMultipart(*mimetype.split(\"/\", 1))\n        )\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg[\"From\"] = self.mailfrom\n        msg[\"To\"] = COMMASPACE.join(to)\n        msg[\"Date\"] = formatdate(localtime=True)\n        msg[\"Subject\"] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg[\"Cc\"] = COMMASPACE.join(cc)\n\n        if attachs:\n            if charset:\n                msg.set_charset(charset)\n            msg.attach(MIMEText(body, \"plain\", charset or \"us-ascii\"))\n            for attach_name, attach_mimetype, f in attachs:\n                part = MIMEBase(*attach_mimetype.split(\"/\"))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header(\n                    \"Content-Disposition\", \"attachment\", filename=attach_name\n                )\n                msg.attach(part)\n        else:\n            msg.set_payload(body, charset)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug(\n                \"Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n                'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                {\n                    \"mailto\": to,\n                    \"mailcc\": cc,\n                    \"mailsubject\": subject,\n                    \"mailattachs\": len(attachs),\n                },\n            )\n            return None\n\n        dfd: Deferred[Any] = self._sendmail(\n            rcpts, msg.as_string().encode(charset or \"utf-8\")\n        )\n        dfd.addCallback(self._sent_ok, to, cc, subject, len(attachs))\n        dfd.addErrback(self._sent_failed, to, cc, subject, len(attachs))\n        reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n        return dfd\n", "n_tokens": 1244, "byte_len": 5267, "file_sha1": "1aee97a6223ea56488bd9092db8749275071cf4f", "start_line": 1, "end_line": 168}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/mail.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/mail.py", "rel_path": "scrapy/mail.py", "module": "scrapy.mail", "ext": "py", "chunk_number": 2, "symbols": ["_sent_ok", "_sent_failed", "_sendmail", "_create_sender_factory", "mailsubject", "mailto", "failure", "unable", "connect", "ssl", "false", "factory", "result", "mailerr", "internet", "smtppass", "client", "context", "sent", "failed", "smtphost", "twisted", "return", "dict", "require", "authentication", "subject", "attachs", "mailfrom", "hostname", "_to_bytes_or_none", "__init__", "from_settings", "from_crawler", "_from_settings", "send", "MailSender", "bool", "add", "system", "python", "sending", "sendmail", "deprecated", "mai", "pass", "helpers", "future", "typ", "checking"], "ast_kind": "function_or_method", "text": "    def _sent_ok(\n        self, result: Any, to: list[str], cc: list[str], subject: str, nattachs: int\n    ) -> None:\n        logger.info(\n            \"Mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n            'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n            {\n                \"mailto\": to,\n                \"mailcc\": cc,\n                \"mailsubject\": subject,\n                \"mailattachs\": nattachs,\n            },\n        )\n\n    def _sent_failed(\n        self,\n        failure: Failure,\n        to: list[str],\n        cc: list[str],\n        subject: str,\n        nattachs: int,\n    ) -> Failure:\n        errstr = str(failure.value)\n        logger.error(\n            \"Unable to send mail: To=%(mailto)s Cc=%(mailcc)s \"\n            'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n            \"- %(mailerr)s\",\n            {\n                \"mailto\": to,\n                \"mailcc\": cc,\n                \"mailsubject\": subject,\n                \"mailattachs\": nattachs,\n                \"mailerr\": errstr,\n            },\n        )\n        return failure\n\n    def _sendmail(self, to_addrs: list[str], msg: bytes) -> Deferred[Any]:\n        from twisted.internet import reactor\n\n        msg_io = BytesIO(msg)\n        d: Deferred[Any] = Deferred()\n\n        factory = self._create_sender_factory(to_addrs, msg_io, d)\n\n        if self.smtpssl:\n            reactor.connectSSL(\n                self.smtphost, self.smtpport, factory, ssl.ClientContextFactory()\n            )\n        else:\n            reactor.connectTCP(self.smtphost, self.smtpport, factory)\n\n        return d\n\n    def _create_sender_factory(\n        self, to_addrs: list[str], msg: IO[bytes], d: Deferred[Any]\n    ) -> ESMTPSenderFactory:\n        # imports twisted.internet.reactor\n        from twisted.mail.smtp import ESMTPSenderFactory  # noqa: PLC0415\n\n        factory_keywords: dict[str, Any] = {\n            \"heloFallback\": True,\n            \"requireAuthentication\": False,\n            \"requireTransportSecurity\": self.smtptls,\n            \"hostname\": self.smtphost,\n        }\n\n        factory = ESMTPSenderFactory(\n            self.smtpuser,\n            self.smtppass,\n            self.mailfrom,\n            to_addrs,\n            msg,\n            d,\n            **factory_keywords,\n        )\n        factory.noisy = False\n        return factory\n", "n_tokens": 547, "byte_len": 2323, "file_sha1": "1aee97a6223ea56488bd9092db8749275071cf4f", "start_line": 169, "end_line": 247}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/interfaces.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/interfaces.py", "rel_path": "scrapy/interfaces.py", "module": "scrapy.interfaces", "ext": "py", "chunk_number": 1, "symbols": ["from_settings", "load", "list", "find_by_request", "ISpiderLoader", "method", "argument", "spider", "instance", "loader", "name", "handle", "class", "with", "names", "spiders", "pylint", "given", "interface", "zope", "return", "key", "error", "from", "settings", "project", "find", "request", "import", "self", "disable", "found", "raise", "that", "must", "available"], "ast_kind": "class_or_type", "text": "# pylint: disable=no-method-argument,no-self-argument\n\nfrom zope.interface import Interface\n\n\nclass ISpiderLoader(Interface):\n    def from_settings(settings):\n        \"\"\"Return an instance of the class for the given settings\"\"\"\n\n    def load(spider_name):\n        \"\"\"Return the Spider class for the given spider name. If the spider\n        name is not found, it must raise a KeyError.\"\"\"\n\n    def list():\n        \"\"\"Return a list with the names of all spiders available in the\n        project\"\"\"\n\n    def find_by_request(request):\n        \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n", "n_tokens": 126, "byte_len": 612, "file_sha1": "3f81a1e6d3bb9fb7e13f9239b91c40bc6ad13130", "start_line": 1, "end_line": 20}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/robotstxt.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/robotstxt.py", "rel_path": "scrapy/robotstxt.py", "module": "scrapy.robotstxt", "ext": "py", "chunk_number": 1, "symbols": ["decode_robotstxt", "from_crawler", "allowed", "__init__", "RobotParser", "PythonRobotParser", "RerpRobotParser", "ProtegoRobotParser", "encoding", "failure", "parse", "method", "bool", "absolute", "instance", "made", "python", "robots", "spider", "txt", "future", "typ", "checking", "unicode", "state", "than", "can", "fetch", "none", "type", "robot", "parser", "either", "found", "exc", "info", "metaclass", "empty", "switch", "typing", "extensions", "garbage", "body", "decoded", "splitlines", "return", "treating", "exclusion", "annotations", "class"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom typing import TYPE_CHECKING\nfrom urllib.robotparser import RobotFileParser\n\nfrom protego import Protego\n\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef decode_robotstxt(\n    robotstxt_body: bytes, spider: Spider | None, to_native_str_type: bool = False\n) -> str:\n    try:\n        if to_native_str_type:\n            body_decoded = to_unicode(robotstxt_body)\n        else:\n            body_decoded = robotstxt_body.decode(\"utf-8\", errors=\"ignore\")\n    except UnicodeDecodeError:\n        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n        # Switch to 'allow all' state.\n        logger.warning(\n            \"Failure while parsing robots.txt. File either contains garbage or \"\n            \"is in an encoding other than UTF-8, treating it as an empty file.\",\n            exc_info=sys.exc_info(),\n            extra={\"spider\": spider},\n        )\n        body_decoded = \"\"\n    return body_decoded\n\n\nclass RobotParser(metaclass=ABCMeta):\n    @classmethod\n    @abstractmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n        It must return a new instance of the parser backend.\n\n        :param crawler: crawler which made the request\n        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n\n        :param robotstxt_body: content of a robots.txt_ file.\n        :type robotstxt_body: bytes\n        \"\"\"\n\n    @abstractmethod\n    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n\n        :param url: Absolute URL\n        :type url: str or bytes\n\n        :param user_agent: User agent\n        :type user_agent: str or bytes\n        \"\"\"\n\n\nclass PythonRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n        self.spider: Spider | None = spider\n        body_decoded = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n        self.rp: RobotFileParser = RobotFileParser()\n        self.rp.parse(body_decoded.splitlines())\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        return cls(robotstxt_body, spider)\n\n    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(user_agent, url)\n\n\nclass RerpRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n        from robotexclusionrulesparser import RobotExclusionRulesParser  # noqa: PLC0415\n\n        self.spider: Spider | None = spider\n        self.rp: RobotExclusionRulesParser = RobotExclusionRulesParser()\n        body_decoded = decode_robotstxt(robotstxt_body, spider)\n        self.rp.parse(body_decoded)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        return cls(robotstxt_body, spider)\n\n    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.is_allowed(user_agent, url)\n\n\nclass ProtegoRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n        self.spider: Spider | None = spider\n        body_decoded = decode_robotstxt(robotstxt_body, spider)\n        self.rp = Protego.parse(body_decoded)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        return cls(robotstxt_body, spider)\n\n    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(url, user_agent)\n", "n_tokens": 1045, "byte_len": 4338, "file_sha1": "f14ef52f75f4a58c85434e912cd91fabcbba2249", "start_line": 1, "end_line": 124}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/signals.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/signals.py", "rel_path": "scrapy/signals.py", "module": "scrapy.signals", "ext": "py", "chunk_number": 1, "symbols": ["request", "dropped", "engine", "stopped", "please", "documenting", "these", "scrapy", "spider", "closed", "item", "scraped", "topics", "feed", "exporter", "documented", "idle", "bytes", "received", "signals", "here", "response", "object", "them", "there", "scheduled", "opened", "error", "docs", "reached", "without", "started", "slot", "scheduler", "empty", "downloaded", "left", "headers"], "ast_kind": "unknown", "text": "\"\"\"\nScrapy signals\n\nThese signals are documented in docs/topics/signals.rst. Please don't add new\nsignals here without documenting them there.\n\"\"\"\n\nengine_started = object()\nengine_stopped = object()\nscheduler_empty = object()\nspider_opened = object()\nspider_idle = object()\nspider_closed = object()\nspider_error = object()\nrequest_scheduled = object()\nrequest_dropped = object()\nrequest_reached_downloader = object()\nrequest_left_downloader = object()\nresponse_received = object()\nresponse_downloaded = object()\nheaders_received = object()\nbytes_received = object()\nitem_scraped = object()\nitem_dropped = object()\nitem_error = object()\nfeed_slot_closed = object()\nfeed_exporter_closed = object()\n", "n_tokens": 151, "byte_len": 697, "file_sha1": "da770d841c8f52072a38fcedc30b18885127ea0a", "start_line": 1, "end_line": 28}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/dupefilters.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/dupefilters.py", "rel_path": "scrapy/dupefilters.py", "module": "scrapy.dupefilters", "ext": "py", "chunk_number": 1, "symbols": ["from_settings", "from_crawler", "request_seen", "open", "close", "log", "__init__", "_from_settings", "request_fingerprint", "BaseDupeFilter", "RFPDupeFilter", "encoding", "does", "method", "referer", "bool", "seen", "inc", "value", "filtered", "filtering", "python", "lib", "w3lib", "duplicate", "spider", "deprecated", "spiders", "more", "future", "typ", "checking", "string", "elif", "path", "dupefilter", "request", "debug", "settings", "fingerprint", "none", "returns", "reason", "uniquely", "http", "requests", "seek", "duplicates", "internet", "typing"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport warnings\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\nfrom warnings import warn\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.request import (\n    RequestFingerprinter,\n    RequestFingerprinterProtocol,\n    referer_str,\n)\n\nif TYPE_CHECKING:\n    from twisted.internet.defer import Deferred\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http.request import Request\n    from scrapy.settings import BaseSettings\n    from scrapy.spiders import Spider\n\n\nclass BaseDupeFilter:\n    \"\"\"Dummy duplicate request filtering class (:setting:`DUPEFILTER_CLASS`)\n    that does not filter out any request.\"\"\"\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        warnings.warn(\n            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return cls()\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls()\n\n    def request_seen(self, request: Request) -> bool:\n        return False\n\n    def open(self) -> Deferred[None] | None:\n        pass\n\n    def close(self, reason: str) -> Deferred[None] | None:\n        pass\n\n    def log(self, request: Request, spider: Spider) -> None:\n        \"\"\"Log that a request has been filtered\"\"\"\n        warn(\n            \"Calling BaseDupeFilter.log() is deprecated.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n\nclass RFPDupeFilter(BaseDupeFilter):\n    \"\"\"Duplicate request filtering class (:setting:`DUPEFILTER_CLASS`) that\n    filters out requests with the canonical\n    (:func:`w3lib.url.canonicalize_url`) :attr:`~scrapy.http.Request.url`,\n    :attr:`~scrapy.http.Request.method` and :attr:`~scrapy.http.Request.body`.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str | None = None,\n        debug: bool = False,\n        *,\n        fingerprinter: RequestFingerprinterProtocol | None = None,\n    ) -> None:\n        self.file = None\n        self.fingerprinter: RequestFingerprinterProtocol = (\n            fingerprinter or RequestFingerprinter()\n        )\n        self.fingerprints: set[str] = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = Path(path, \"requests.seen\").open(\"a+\", encoding=\"utf-8\")\n            self.file.seek(0)\n            self.fingerprints.update(x.rstrip() for x in self.file)\n\n    @classmethod\n    def from_settings(\n        cls,\n        settings: BaseSettings,\n        *,\n        fingerprinter: RequestFingerprinterProtocol | None = None,\n    ) -> Self:\n        warnings.warn(\n            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return cls._from_settings(settings, fingerprinter=fingerprinter)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.request_fingerprinter\n        return cls._from_settings(\n            crawler.settings,\n            fingerprinter=crawler.request_fingerprinter,\n        )\n\n    @classmethod\n    def _from_settings(\n        cls,\n        settings: BaseSettings,\n        *,\n        fingerprinter: RequestFingerprinterProtocol | None = None,\n    ) -> Self:\n        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n\n    def request_seen(self, request: Request) -> bool:\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + \"\\n\")\n        return False\n\n    def request_fingerprint(self, request: Request) -> str:\n        \"\"\"Returns a string that uniquely identifies the specified request.\"\"\"\n        return self.fingerprinter.fingerprint(request).hex()\n\n    def close(self, reason: str) -> None:\n        if self.file:\n            self.file.close()\n\n    def log(self, request: Request, spider: Spider) -> None:\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n            args = {\"request\": request, \"referer\": referer_str(request)}\n            self.logger.debug(msg, args, extra={\"spider\": spider})\n        elif self.logdupes:\n            msg = (\n                \"Filtered duplicate request: %(request)s\"\n                \" - no more duplicates will be shown\"\n                \" (see DUPEFILTER_DEBUG to show all duplicates)\"\n            )\n            self.logger.debug(msg, {\"request\": request}, extra={\"spider\": spider})\n            self.logdupes = False\n\n        assert spider.crawler.stats\n        spider.crawler.stats.inc_value(\"dupefilter/filtered\")\n", "n_tokens": 1101, "byte_len": 5015, "file_sha1": "04a889b7bf92df90aa570131e2aaac924ab6e873", "start_line": 1, "end_line": 156}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/addons.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/addons.py", "rel_path": "scrapy/addons.py", "module": "scrapy.addons", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "load_settings", "load_pre_crawler_settings", "AddonManager", "method", "loads", "configurations", "append", "load", "pre", "instance", "each", "settings", "doesn", "future", "typ", "checking", "addons", "object", "them", "none", "type", "methods", "misc", "addon", "access", "eargs", "present", "typing", "annotations", "class", "addoncls", "apply", "not", "configured", "classmethod", "facilitates", "build", "component", "clspath", "list", "every", "exceptions", "param", "calls", "such", "spide", "modules", "self", "this"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings, Settings\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass AddonManager:\n    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n\n    def __init__(self, crawler: Crawler) -> None:\n        self.crawler: Crawler = crawler\n        self.addons: list[Any] = []\n\n    def load_settings(self, settings: Settings) -> None:\n        \"\"\"Load add-ons and configurations from a settings object and apply them.\n\n        This will load the add-on for every add-on path in the\n        ``ADDONS`` setting and execute their ``update_settings`` methods.\n\n        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n            which to read the add-on configuration\n        :type settings: :class:`~scrapy.settings.Settings`\n        \"\"\"\n        for clspath in build_component_list(settings[\"ADDONS\"]):\n            try:\n                addoncls = load_object(clspath)\n                addon = build_from_crawler(addoncls, self.crawler)\n                if hasattr(addon, \"update_settings\"):\n                    addon.update_settings(settings)\n                self.addons.append(addon)\n            except NotConfigured as e:\n                if e.args:\n                    logger.warning(\n                        \"Disabled %(clspath)s: %(eargs)s\",\n                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                        extra={\"crawler\": self.crawler},\n                    )\n        logger.info(\n            \"Enabled addons:\\n%(addons)s\",\n            {\n                \"addons\": self.addons,\n            },\n            extra={\"crawler\": self.crawler},\n        )\n\n    @classmethod\n    def load_pre_crawler_settings(cls, settings: BaseSettings):\n        \"\"\"Update early settings that do not require a crawler instance, such as SPIDER_MODULES.\n\n        Similar to the load_settings method, this loads each add-on configured in the\n        ``ADDONS`` setting and calls their 'update_pre_crawler_settings' class method if present.\n        This method doesn't have access to the crawler instance or the addons list.\n\n        :param settings: The :class:`~scrapy.settings.BaseSettings` object from \\\n            which to read the early add-on configuration\n        :type settings: :class:`~scrapy.settings.Settings`\n        \"\"\"\n        for clspath in build_component_list(settings[\"ADDONS\"]):\n            addoncls = load_object(clspath)\n            if hasattr(addoncls, \"update_pre_crawler_settings\"):\n                addoncls.update_pre_crawler_settings(settings)\n", "n_tokens": 575, "byte_len": 2836, "file_sha1": "57caa3b3671d34f4e748f535df2a33d05e95c2f4", "start_line": 1, "end_line": 73}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pqueues.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pqueues.py", "rel_path": "scrapy/pqueues.py", "module": "scrapy.pqueues", "ext": "py", "chunk_number": 1, "symbols": ["_path_safe", "push", "pop", "close", "__len__", "from_crawler", "__init__", "init_prios", "qfactory", "_sqfactory", "priority", "QueueProtocol", "ScrapyPriorityQueue", "method", "those", "queues", "priorities", "each", "python", "start", "queue", "isalnum", "request", "startprios", "letters", "passed", "future", "typ", "checking", "string", "_update_curprio", "peek", "stats", "get_slot_key", "_active_downloads", "pqfactory", "__contains__", "DownloaderInterface", "DownloaderAwarePriorityQueue", "does", "takes", "bool", "raises", "instance", "active", "downloads", "resumed", "declare", "get", "slot"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport hashlib\nimport logging\nfrom typing import TYPE_CHECKING, Protocol, cast\n\nfrom scrapy.utils.misc import build_from_crawler\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request\n    from scrapy.core.downloader import Downloader\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef _path_safe(text: str) -> str:\n    \"\"\"\n    Return a filesystem-safe version of a string ``text``\n\n    >>> _path_safe('simple.org').startswith('simple.org')\n    True\n    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n    True\n    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n    True\n    \"\"\"\n    pathable_slot = \"\".join([c if c.isalnum() or c in \"-._\" else \"_\" for c in text])\n    # as we replace some letters we can get collision for different slots\n    # add we add unique part\n    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # noqa: S324\n    return f\"{pathable_slot}-{unique_slot}\"\n\n\nclass QueueProtocol(Protocol):\n    \"\"\"Protocol for downstream queues of ``ScrapyPriorityQueue``.\"\"\"\n\n    def push(self, request: Request) -> None: ...\n\n    def pop(self) -> Request | None: ...\n\n    def close(self) -> None: ...\n\n    def __len__(self) -> int: ...\n\n\nclass ScrapyPriorityQueue:\n    \"\"\"A priority queue implemented using multiple internal queues (typically,\n    FIFO queues). It uses one internal queue for each priority value. The internal\n    queue must implement the following methods:\n\n        * push(obj)\n        * pop()\n        * close()\n        * __len__()\n\n    Optionally, the queue could provide a ``peek`` method, that should return the\n    next object to be returned by ``pop``, but without removing it from the queue.\n\n    ``__init__`` method of ScrapyPriorityQueue receives a downstream_queue_cls\n    argument, which is a class used to instantiate a new (internal) queue when\n    a new priority is allocated.\n\n    Only integer priorities should be used. Lower numbers are higher\n    priorities.\n\n    startprios is a sequence of priorities to start with. If the queue was\n    previously closed leaving some priority buckets non-empty, those priorities\n    should be passed in startprios.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        downstream_queue_cls: type[QueueProtocol],\n        key: str,\n        startprios: Iterable[int] = (),\n        *,\n        start_queue_cls: type[QueueProtocol] | None = None,\n    ) -> Self:\n        return cls(\n            crawler,\n            downstream_queue_cls,\n            key,\n            startprios,\n            start_queue_cls=start_queue_cls,\n        )\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        downstream_queue_cls: type[QueueProtocol],\n        key: str,\n        startprios: Iterable[int] = (),\n        *,\n        start_queue_cls: type[QueueProtocol] | None = None,\n    ):\n        self.crawler: Crawler = crawler\n        self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n        self._start_queue_cls: type[QueueProtocol] | None = start_queue_cls\n        self.key: str = key\n        self.queues: dict[int, QueueProtocol] = {}\n        self._start_queues: dict[int, QueueProtocol] = {}\n        self.curprio: int | None = None\n        self.init_prios(startprios)\n\n    def init_prios(self, startprios: Iterable[int]) -> None:\n        if not startprios:\n            return\n\n        for priority in startprios:\n            q = self.qfactory(priority)\n            if q:\n                self.queues[priority] = q\n            if self._start_queue_cls:\n                q = self._sqfactory(priority)\n                if q:\n                    self._start_queues[priority] = q\n\n        self.curprio = min(startprios)\n\n    def qfactory(self, key: int) -> QueueProtocol:\n        return build_from_crawler(\n            self.downstream_queue_cls,\n            self.crawler,\n            self.key + \"/\" + str(key),\n        )\n\n    def _sqfactory(self, key: int) -> QueueProtocol:\n        assert self._start_queue_cls is not None\n        return build_from_crawler(\n            self._start_queue_cls,\n            self.crawler,\n            f\"{self.key}/{key}s\",\n        )\n\n    def priority(self, request: Request) -> int:\n        return -request.priority\n\n    def push(self, request: Request) -> None:\n        priority = self.priority(request)\n        is_start_request = request.meta.get(\"is_start_request\", False)\n        if is_start_request and self._start_queue_cls:\n            if priority not in self._start_queues:\n                self._start_queues[priority] = self._sqfactory(priority)\n            q = self._start_queues[priority]\n        else:\n            if priority not in self.queues:\n                self.queues[priority] = self.qfactory(priority)\n            q = self.queues[priority]\n        q.push(request)  # this may fail (eg. serialization error)\n        if self.curprio is None or priority < self.curprio:\n            self.curprio = priority\n", "n_tokens": 1163, "byte_len": 5106, "file_sha1": "8251c719d3ca1d1d56ff8f3284cf50f14153e142", "start_line": 1, "end_line": 160}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pqueues.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pqueues.py", "rel_path": "scrapy/pqueues.py", "module": "scrapy.pqueues", "ext": "py", "chunk_number": 2, "symbols": ["pop", "_update_curprio", "peek", "close", "__len__", "__init__", "stats", "get_slot_key", "_active_downloads", "from_crawler", "pqfactory", "DownloaderInterface", "DownloaderAwarePriorityQueue", "does", "method", "takes", "queues", "raises", "instance", "active", "downloads", "start", "queue", "resumed", "declare", "startprios", "get", "slot", "passed", "curprio", "_path_safe", "push", "init_prios", "qfactory", "_sqfactory", "priority", "__contains__", "QueueProtocol", "ScrapyPriorityQueue", "those", "bool", "priorities", "each", "python", "isalnum", "request", "letters", "future", "typ", "checking"], "ast_kind": "class_or_type", "text": "    def pop(self) -> Request | None:\n        while self.curprio is not None:\n            try:\n                q = self.queues[self.curprio]\n            except KeyError:\n                pass\n            else:\n                m = q.pop()\n                if not q:\n                    del self.queues[self.curprio]\n                    q.close()\n                    if not self._start_queues:\n                        self._update_curprio()\n                return m\n            if self._start_queues:\n                try:\n                    q = self._start_queues[self.curprio]\n                except KeyError:\n                    self._update_curprio()\n                else:\n                    m = q.pop()\n                    if not q:\n                        del self._start_queues[self.curprio]\n                        q.close()\n                        self._update_curprio()\n                    return m\n            else:\n                self._update_curprio()\n        return None\n\n    def _update_curprio(self) -> None:\n        prios = {\n            p\n            for queues in (self.queues, self._start_queues)\n            for p, q in queues.items()\n            if q\n        }\n        self.curprio = min(prios) if prios else None\n\n    def peek(self) -> Request | None:\n        \"\"\"Returns the next object to be returned by :meth:`pop`,\n        but without removing it from the queue.\n\n        Raises :exc:`NotImplementedError` if the underlying queue class does\n        not implement a ``peek`` method, which is optional for queues.\n        \"\"\"\n        if self.curprio is None:\n            return None\n        try:\n            queue = self._start_queues[self.curprio]\n        except KeyError:\n            queue = self.queues[self.curprio]\n        # Protocols can't declare optional members\n        return cast(\"Request\", queue.peek())  # type: ignore[attr-defined]\n\n    def close(self) -> list[int]:\n        active: set[int] = set()\n        for queues in (self.queues, self._start_queues):\n            for p, q in queues.items():\n                active.add(p)\n                q.close()\n        return list(active)\n\n    def __len__(self) -> int:\n        return (\n            sum(\n                len(x)\n                for queues in (self.queues, self._start_queues)\n                for x in queues.values()\n            )\n            if self.queues or self._start_queues\n            else 0\n        )\n\n\nclass DownloaderInterface:\n    def __init__(self, crawler: Crawler):\n        assert crawler.engine\n        self.downloader: Downloader = crawler.engine.downloader\n\n    def stats(self, possible_slots: Iterable[str]) -> list[tuple[int, str]]:\n        return [(self._active_downloads(slot), slot) for slot in possible_slots]\n\n    def get_slot_key(self, request: Request) -> str:\n        return self.downloader.get_slot_key(request)\n\n    def _active_downloads(self, slot: str) -> int:\n        \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n        if slot not in self.downloader.slots:\n            return 0\n        return len(self.downloader.slots[slot].active)\n\n\nclass DownloaderAwarePriorityQueue:\n    \"\"\"PriorityQueue which takes Downloader activity into account:\n    domains (slots) with the least amount of active downloads are dequeued\n    first.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        downstream_queue_cls: type[QueueProtocol],\n        key: str,\n        startprios: dict[str, Iterable[int]] | None = None,\n        *,\n        start_queue_cls: type[QueueProtocol] | None = None,\n    ) -> Self:\n        return cls(\n            crawler,\n            downstream_queue_cls,\n            key,\n            startprios,\n            start_queue_cls=start_queue_cls,\n        )\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        downstream_queue_cls: type[QueueProtocol],\n        key: str,\n        slot_startprios: dict[str, Iterable[int]] | None = None,\n        *,\n        start_queue_cls: type[QueueProtocol] | None = None,\n    ):\n        if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n            raise ValueError(\n                f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP'\n            )\n\n        if slot_startprios and not isinstance(slot_startprios, dict):\n            raise ValueError(\n                \"DownloaderAwarePriorityQueue accepts \"\n                \"``slot_startprios`` as a dict; \"\n                f\"{slot_startprios.__class__!r} instance \"\n                \"is passed. Most likely, it means the state is \"\n                \"created by an incompatible priority queue. \"\n                \"Only a crawl started with the same priority \"\n                \"queue class can be resumed.\"\n            )\n\n        self._downloader_interface: DownloaderInterface = DownloaderInterface(crawler)\n        self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n        self._start_queue_cls: type[QueueProtocol] | None = start_queue_cls\n        self.key: str = key\n        self.crawler: Crawler = crawler\n\n        self.pqueues: dict[str, ScrapyPriorityQueue] = {}  # slot -> priority queue\n        for slot, startprios in (slot_startprios or {}).items():\n            self.pqueues[slot] = self.pqfactory(slot, startprios)\n\n    def pqfactory(\n        self, slot: str, startprios: Iterable[int] = ()\n    ) -> ScrapyPriorityQueue:\n        return ScrapyPriorityQueue(\n            self.crawler,\n            self.downstream_queue_cls,\n            self.key + \"/\" + _path_safe(slot),\n            startprios,\n            start_queue_cls=self._start_queue_cls,\n        )\n", "n_tokens": 1206, "byte_len": 5594, "file_sha1": "8251c719d3ca1d1d56ff8f3284cf50f14153e142", "start_line": 161, "end_line": 323}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pqueues.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pqueues.py", "rel_path": "scrapy/pqueues.py", "module": "scrapy.pqueues", "ext": "py", "chunk_number": 3, "symbols": ["pop", "push", "peek", "close", "__len__", "__contains__", "underlying", "does", "method", "len", "bool", "implement", "raises", "queues", "pqfactory", "slot", "return", "dict", "class", "downloader", "interface", "get", "pqueues", "not", "implemented", "next", "contains", "meth", "returned", "from", "_path_safe", "from_crawler", "__init__", "init_prios", "qfactory", "_sqfactory", "priority", "_update_curprio", "stats", "get_slot_key", "_active_downloads", "QueueProtocol", "ScrapyPriorityQueue", "DownloaderInterface", "DownloaderAwarePriorityQueue", "takes", "those", "instance", "priorities", "each"], "ast_kind": "class_or_type", "text": "    def pop(self) -> Request | None:\n        stats = self._downloader_interface.stats(self.pqueues)\n\n        if not stats:\n            return None\n\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        request = queue.pop()\n        if len(queue) == 0:\n            del self.pqueues[slot]\n        return request\n\n    def push(self, request: Request) -> None:\n        slot = self._downloader_interface.get_slot_key(request)\n        if slot not in self.pqueues:\n            self.pqueues[slot] = self.pqfactory(slot)\n        queue = self.pqueues[slot]\n        queue.push(request)\n\n    def peek(self) -> Request | None:\n        \"\"\"Returns the next object to be returned by :meth:`pop`,\n        but without removing it from the queue.\n\n        Raises :exc:`NotImplementedError` if the underlying queue class does\n        not implement a ``peek`` method, which is optional for queues.\n        \"\"\"\n        stats = self._downloader_interface.stats(self.pqueues)\n        if not stats:\n            return None\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        return queue.peek()\n\n    def close(self) -> dict[str, list[int]]:\n        active = {slot: queue.close() for slot, queue in self.pqueues.items()}\n        self.pqueues.clear()\n        return active\n\n    def __len__(self) -> int:\n        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n\n    def __contains__(self, slot: str) -> bool:\n        return slot in self.pqueues\n", "n_tokens": 339, "byte_len": 1480, "file_sha1": "8251c719d3ca1d1d56ff8f3284cf50f14153e142", "start_line": 324, "end_line": 368}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/shell.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/shell.py", "rel_path": "scrapy/shell.py", "module": "scrapy.shell", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "start", "_schedule", "crawl_request", "fetch", "Shell", "async", "bool", "set", "asyncio", "signal", "lib", "w3lib", "interactive", "spider", "deferred", "from", "dont", "filter", "spiders", "open", "browser", "thread", "future", "typ", "checking", "elif", "detect", "contextlib", "handle", "populate_vars", "print_help", "get_help", "_is_relevant", "inspect_response", "_request_deferred", "_restore_callbacks", "append", "after", "selector", "httpstatus", "useful", "console", "know", "doing", "any", "uri", "save", "restore", "callbacks"], "ast_kind": "class_or_type", "text": "\"\"\"Scrapy Shell\n\nSee documentation in docs/topics/shell.rst\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport os\nimport signal\nfrom typing import TYPE_CHECKING, Any\n\nfrom itemadapter import is_item\nfrom twisted.internet import defer, threads\nfrom twisted.python import threadable\nfrom w3lib.url import any_to_uri\n\nimport scrapy\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.conf import get_config\nfrom scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.defer import _schedule_coro, deferred_f_from_coro_f\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\nfrom scrapy.utils.response import open_in_browser\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n\nclass Shell:\n    relevant_classes: tuple[type, ...] = (Crawler, Spider, Request, Response, Settings)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        update_vars: Callable[[dict[str, Any]], None] | None = None,\n        code: str | None = None,\n    ):\n        self.crawler: Crawler = crawler\n        self.update_vars: Callable[[dict[str, Any]], None] = update_vars or (\n            lambda x: None\n        )\n        self.item_class: type = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n        self.spider: Spider | None = None\n        self.inthread: bool = not threadable.isInIOThread()\n        self.code: str | None = code\n        self.vars: dict[str, Any] = {}\n\n    def start(\n        self,\n        url: str | None = None,\n        request: Request | None = None,\n        response: Response | None = None,\n        spider: Spider | None = None,\n        redirect: bool = True,\n    ) -> None:\n        # disable accidental Ctrl-C key press from shutting down the engine\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        if url:\n            self.fetch(url, spider, redirect=redirect)\n        elif request:\n            self.fetch(request, spider)\n        elif response:\n            request = response.request\n            self.populate_vars(response, request, spider)\n        else:\n            self.populate_vars()\n        if self.code:\n            # pylint: disable-next=eval-used\n            print(eval(self.code, globals(), self.vars))  # noqa: S307\n        else:\n            # Detect interactive shell setting in scrapy.cfg\n            # e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n            # [settings]\n            # # shell can be one of ipython, bpython or python;\n            # # to be used as the interactive python console, if available.\n            # # (default is ipython, fallbacks in the order listed above)\n            # shell = python\n            cfg = get_config()\n            section, option = \"settings\", \"shell\"\n            env = os.environ.get(\"SCRAPY_PYTHON_SHELL\")\n            shells = []\n            if env:\n                shells += env.strip().lower().split(\",\")\n            elif cfg.has_option(section, option):\n                shells += [cfg.get(section, option).strip().lower()]\n            else:  # try all by default\n                shells += DEFAULT_PYTHON_SHELLS.keys()\n            # always add standard shell as fallback\n            shells += [\"python\"]\n            start_python_console(\n                self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n            )\n\n    def _schedule(self, request: Request, spider: Spider | None) -> defer.Deferred[Any]:\n        if is_asyncio_reactor_installed():\n            # set the asyncio event loop for the current thread\n            event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n            set_asyncio_event_loop(event_loop_path)\n\n        def crawl_request(_):\n            assert self.crawler.engine is not None\n            self.crawler.engine.crawl(request)\n\n        d2 = self._open_spider(request, spider)\n        d2.addCallback(crawl_request)\n\n        d = _request_deferred(request)\n        d.addCallback(lambda x: (x, spider))\n        return d\n\n    @deferred_f_from_coro_f\n    async def _open_spider(self, request: Request, spider: Spider | None) -> None:\n        if self.spider:\n            return\n\n        if spider is None:\n            spider = self.crawler.spider or self.crawler._create_spider()\n\n        self.crawler.spider = spider\n        assert self.crawler.engine\n        await self.crawler.engine.open_spider_async(close_if_idle=False)\n        _schedule_coro(self.crawler.engine._start_request_processing())\n        self.spider = spider\n\n    def fetch(\n        self,\n        request_or_url: Request | str,\n        spider: Spider | None = None,\n        redirect: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        from twisted.internet import reactor\n\n        if isinstance(request_or_url, Request):\n            request = request_or_url\n        else:\n            url = any_to_uri(request_or_url)\n            request = Request(url, dont_filter=True, **kwargs)\n            if redirect:\n                request.meta[\"handle_httpstatus_list\"] = SequenceExclude(\n                    range(300, 400)\n                )\n            else:\n                request.meta[\"handle_httpstatus_all\"] = True\n        response = None\n        with contextlib.suppress(IgnoreRequest):\n            response, spider = threads.blockingCallFromThread(\n                reactor, self._schedule, request, spider\n            )\n        self.populate_vars(response, request, spider)\n", "n_tokens": 1212, "byte_len": 5627, "file_sha1": "5f1cbcc16a85c5ad4fac9b2d11bd035a6e06289c", "start_line": 1, "end_line": 159}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/shell.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/shell.py", "rel_path": "scrapy/shell.py", "module": "scrapy.shell", "ext": "py", "chunk_number": 2, "symbols": ["populate_vars", "print_help", "get_help", "_is_relevant", "inspect_response", "_request_deferred", "_restore_callbacks", "bool", "browser", "append", "signal", "after", "spider", "selector", "open", "useful", "doing", "know", "save", "restore", "callbacks", "getsignal", "settings", "items", "isinstance", "handler", "relevant", "classes", "none", "banner", "__init__", "start", "_schedule", "crawl_request", "fetch", "Shell", "async", "set", "asyncio", "lib", "w3lib", "interactive", "deferred", "from", "dont", "filter", "spiders", "thread", "future", "typ"], "ast_kind": "function_or_method", "text": "    def populate_vars(\n        self,\n        response: Response | None = None,\n        request: Request | None = None,\n        spider: Spider | None = None,\n    ) -> None:\n        self.vars[\"scrapy\"] = scrapy\n        self.vars[\"crawler\"] = self.crawler\n        self.vars[\"item\"] = self.item_class()\n        self.vars[\"settings\"] = self.crawler.settings\n        self.vars[\"spider\"] = spider\n        self.vars[\"request\"] = request\n        self.vars[\"response\"] = response\n        if self.inthread:\n            self.vars[\"fetch\"] = self.fetch\n        self.vars[\"view\"] = open_in_browser\n        self.vars[\"shelp\"] = self.print_help\n        self.update_vars(self.vars)\n        if not self.code:\n            self.vars[\"banner\"] = self.get_help()\n\n    def print_help(self) -> None:\n        print(self.get_help())\n\n    def get_help(self) -> str:\n        b = []\n        b.append(\"Available Scrapy objects:\")\n        b.append(\n            \"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\"\n        )\n        for k, v in sorted(self.vars.items()):\n            if self._is_relevant(v):\n                b.append(f\"  {k:<10} {v}\")\n        b.append(\"Useful shortcuts:\")\n        if self.inthread:\n            b.append(\n                \"  fetch(url[, redirect=True]) \"\n                \"Fetch URL and update local objects (by default, redirects are followed)\"\n            )\n            b.append(\n                \"  fetch(req)                  \"\n                \"Fetch a scrapy.Request and update local objects \"\n            )\n        b.append(\"  shelp()           Shell help (print this help)\")\n        b.append(\"  view(response)    View response in a browser\")\n\n        return \"\\n\".join(f\"[s] {line}\" for line in b) + \"\\n\"\n\n    def _is_relevant(self, value: Any) -> bool:\n        return isinstance(value, self.relevant_classes) or is_item(value)\n\n\ndef inspect_response(response: Response, spider: Spider) -> None:\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    # Shell.start removes the SIGINT handler, so save it and re-add it after\n    # the shell has closed\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    Shell(spider.crawler).start(response=response, spider=spider)\n    signal.signal(signal.SIGINT, sigint_handler)\n\n\ndef _request_deferred(request: Request) -> defer.Deferred[Any]:\n    \"\"\"Wrap a request inside a Deferred.\n\n    This function is harmful, do not use it until you know what you are doing.\n\n    This returns a Deferred whose first pair of callbacks are the request\n    callback and errback. The Deferred also triggers when the request\n    callback/errback is executed (i.e. when the request is downloaded)\n\n    WARNING: Do not call request.replace() until after the deferred is called.\n    \"\"\"\n    request_callback = request.callback\n    request_errback = request.errback\n\n    def _restore_callbacks(result: Any) -> Any:\n        request.callback = request_callback\n        request.errback = request_errback\n        return result\n\n    d: defer.Deferred[Any] = defer.Deferred()\n    d.addBoth(_restore_callbacks)\n    if request.callback:\n        d.addCallback(request.callback)\n    if request.errback:\n        d.addErrback(request.errback)\n\n    request.callback, request.errback = d.callback, d.errback\n    return d\n", "n_tokens": 735, "byte_len": 3259, "file_sha1": "5f1cbcc16a85c5ad4fac9b2d11bd035a6e06289c", "start_line": 160, "end_line": 249}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/cmdline.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/cmdline.py", "rel_path": "scrapy/cmdline.py", "module": "scrapy.cmdline", "ext": "py", "chunk_number": 1, "symbols": ["_parse_optional", "_iter_command_classes", "_get_commands_from_module", "_get_commands_from_entry_points", "_get_commands_dict", "_get_project_only_cmds", "_pop_command_name", "_print_header", "_print_commands", "_print_unknown_command_msg", "_print_unknown_command", "ScrapyArgumentParser", "entry", "points", "get", "commands", "usage", "error", "bool", "cmds", "python", "inproject", "name", "about", "iter", "command", "load", "these", "more", "future", "_run_print_help", "execute", "_run_command", "_run_command_profiled", "does", "syntax", "related", "typ", "checking", "merge", "elif", "forc", "crawle", "print", "unknown", "dump", "stats", "namespace", "add", "options"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport argparse\nimport cProfile\nimport inspect\nimport os\nimport sys\nfrom importlib.metadata import entry_points\nfrom typing import TYPE_CHECKING\n\nimport scrapy\nfrom scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\nfrom scrapy.crawler import AsyncCrawlerProcess, CrawlerProcess\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.misc import walk_modules\nfrom scrapy.utils.project import get_project_settings, inside_project\nfrom scrapy.utils.python import garbage_collect\nfrom scrapy.utils.reactor import _asyncio_reactor_path\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable\n\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    from scrapy.settings import BaseSettings, Settings\n\n    _P = ParamSpec(\"_P\")\n\n\nclass ScrapyArgumentParser(argparse.ArgumentParser):\n    def _parse_optional(\n        self, arg_string: str\n    ) -> tuple[argparse.Action | None, str, str | None] | None:\n        # Support something like ‘-o -:json’, where ‘-:json’ is a value for\n        # ‘-o’, not another parameter.\n        if arg_string.startswith(\"-:\"):\n            return None\n\n        return super()._parse_optional(arg_string)\n\n\ndef _iter_command_classes(module_name: str) -> Iterable[type[ScrapyCommand]]:\n    # TODO: add `name` attribute to commands and merge this function with\n    # scrapy.utils.spider.iter_spider_classes\n    for module in walk_modules(module_name):\n        for obj in vars(module).values():\n            if (\n                inspect.isclass(obj)\n                and issubclass(obj, ScrapyCommand)\n                and obj.__module__ == module.__name__\n                and obj not in (ScrapyCommand, BaseRunSpiderCommand)\n            ):\n                yield obj\n\n\ndef _get_commands_from_module(module: str, inproject: bool) -> dict[str, ScrapyCommand]:\n    d: dict[str, ScrapyCommand] = {}\n    for cmd in _iter_command_classes(module):\n        if inproject or not cmd.requires_project:\n            cmdname = cmd.__module__.split(\".\")[-1]\n            d[cmdname] = cmd()\n    return d\n\n\ndef _get_commands_from_entry_points(\n    inproject: bool, group: str = \"scrapy.commands\"\n) -> dict[str, ScrapyCommand]:\n    cmds: dict[str, ScrapyCommand] = {}\n    if sys.version_info >= (3, 10):\n        eps = entry_points(group=group)\n    else:\n        eps = entry_points().get(group, ())\n    for entry_point in eps:\n        obj = entry_point.load()\n        if inspect.isclass(obj):\n            cmds[entry_point.name] = obj()\n        else:\n            raise ValueError(f\"Invalid entry point {entry_point.name}\")\n    return cmds\n\n\ndef _get_commands_dict(\n    settings: BaseSettings, inproject: bool\n) -> dict[str, ScrapyCommand]:\n    cmds = _get_commands_from_module(\"scrapy.commands\", inproject)\n    cmds.update(_get_commands_from_entry_points(inproject))\n    cmds_module = settings[\"COMMANDS_MODULE\"]\n    if cmds_module:\n        cmds.update(_get_commands_from_module(cmds_module, inproject))\n    return cmds\n\n\ndef _get_project_only_cmds(settings: BaseSettings) -> set[str]:\n    return set(_get_commands_dict(settings, inproject=True)) - set(\n        _get_commands_dict(settings, inproject=False)\n    )\n\n\ndef _pop_command_name(argv: list[str]) -> str | None:\n    for i in range(1, len(argv)):\n        if not argv[i].startswith(\"-\"):\n            return argv.pop(i)\n    return None\n\n\ndef _print_header(settings: BaseSettings, inproject: bool) -> None:\n    version = scrapy.__version__\n    if inproject:\n        print(f\"Scrapy {version} - active project: {settings['BOT_NAME']}\\n\")\n\n    else:\n        print(f\"Scrapy {version} - no active project\\n\")\n\n\ndef _print_commands(settings: BaseSettings, inproject: bool) -> None:\n    _print_header(settings, inproject)\n    print(\"Usage:\")\n    print(\"  scrapy <command> [options] [args]\\n\")\n    print(\"Available commands:\")\n    cmds = _get_commands_dict(settings, inproject)\n    for cmdname, cmdclass in sorted(cmds.items()):\n        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n    if not inproject:\n        print()\n        print(\"  [ more ]      More commands available when run from project directory\")\n    print()\n    print('Use \"scrapy <command> -h\" to see more info about a command')\n\n\ndef _print_unknown_command_msg(\n    settings: BaseSettings, cmdname: str, inproject: bool\n) -> None:\n    proj_only_cmds = _get_project_only_cmds(settings)\n    if cmdname in proj_only_cmds and not inproject:\n        cmd_list = \", \".join(sorted(proj_only_cmds))\n        print(\n            f\"The {cmdname} command is not available from this location.\\n\"\n            f\"These commands are only available from within a project: {cmd_list}.\\n\"\n        )\n    else:\n        print(f\"Unknown command: {cmdname}\\n\")\n\n\ndef _print_unknown_command(\n    settings: BaseSettings, cmdname: str, inproject: bool\n) -> None:\n    _print_header(settings, inproject)\n    _print_unknown_command_msg(settings, cmdname, inproject)\n    print('Use \"scrapy\" to see available commands')\n\n", "n_tokens": 1170, "byte_len": 5029, "file_sha1": "830885ed77f820164f6191ff339ea2e8777c69ec", "start_line": 1, "end_line": 152}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/cmdline.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/cmdline.py", "rel_path": "scrapy/cmdline.py", "module": "scrapy.cmdline", "ext": "py", "chunk_number": 2, "symbols": ["_run_print_help", "execute", "_run_command", "_run_command_profiled", "does", "usage", "error", "cmds", "syntax", "inproject", "related", "elif", "forc", "crawle", "dump", "stats", "namespace", "add", "options", "default", "settings", "parse", "known", "strategies", "cmdname", "none", "twiste", "reactor", "html", "http", "_parse_optional", "_iter_command_classes", "_get_commands_from_module", "_get_commands_from_entry_points", "_get_commands_dict", "_get_project_only_cmds", "_pop_command_name", "_print_header", "_print_commands", "_print_unknown_command_msg", "_print_unknown_command", "ScrapyArgumentParser", "entry", "points", "get", "commands", "bool", "python", "name", "about"], "ast_kind": "function_or_method", "text": "def _run_print_help(\n    parser: argparse.ArgumentParser,\n    func: Callable[_P, None],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> None:\n    try:\n        func(*a, **kw)\n    except UsageError as e:\n        if str(e):\n            parser.error(str(e))\n        if e.print_help:\n            parser.print_help()\n        sys.exit(2)\n\n\ndef execute(argv: list[str] | None = None, settings: Settings | None = None) -> None:\n    if argv is None:\n        argv = sys.argv\n\n    if settings is None:\n        settings = get_project_settings()\n        # set EDITOR from environment if available\n        try:\n            editor = os.environ[\"EDITOR\"]\n        except KeyError:\n            pass\n        else:\n            settings[\"EDITOR\"] = editor\n\n    inproject = inside_project()\n    cmds = _get_commands_dict(settings, inproject)\n    cmdname = _pop_command_name(argv)\n    if not cmdname:\n        _print_commands(settings, inproject)\n        sys.exit(0)\n    elif cmdname not in cmds:\n        _print_unknown_command(settings, cmdname, inproject)\n        sys.exit(2)\n\n    cmd = cmds[cmdname]\n    parser = ScrapyArgumentParser(\n        formatter_class=ScrapyHelpFormatter,\n        usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n        conflict_handler=\"resolve\",\n        description=cmd.long_desc(),\n    )\n    settings.setdict(cmd.default_settings, priority=\"command\")\n    cmd.settings = settings\n    cmd.add_options(parser)\n    opts, args = parser.parse_known_args(args=argv[1:])\n    _run_print_help(parser, cmd.process_options, args, opts)\n\n    if cmd.requires_crawler_process:\n        if settings[\n            \"TWISTED_REACTOR\"\n        ] == _asyncio_reactor_path and not settings.getbool(\"FORCE_CRAWLER_PROCESS\"):\n            cmd.crawler_process = AsyncCrawlerProcess(settings)\n        else:\n            cmd.crawler_process = CrawlerProcess(settings)\n    _run_print_help(parser, _run_command, cmd, args, opts)\n    sys.exit(cmd.exitcode)\n\n\ndef _run_command(cmd: ScrapyCommand, args: list[str], opts: argparse.Namespace) -> None:\n    if opts.profile:\n        _run_command_profiled(cmd, args, opts)\n    else:\n        cmd.run(args, opts)\n\n\ndef _run_command_profiled(\n    cmd: ScrapyCommand, args: list[str], opts: argparse.Namespace\n) -> None:\n    if opts.profile:\n        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n    loc = locals()\n    p = cProfile.Profile()\n    p.runctx(\"cmd.run(args, opts)\", globals(), loc)\n    if opts.profile:\n        p.dump_stats(opts.profile)\n\n\nif __name__ == \"__main__\":\n    try:\n        execute()\n    finally:\n        # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect() on exit:\n        # http://doc.pypy.org/en/latest/cpython_differences.html\n        # ?highlight=gc.collect#differences-related-to-garbage-collection-strategies\n        garbage_collect()\n", "n_tokens": 676, "byte_len": 2819, "file_sha1": "830885ed77f820164f6191ff339ea2e8777c69ec", "start_line": 153, "end_line": 244}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/__init__.py", "rel_path": "scrapy/__init__.py", "module": "scrapy.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__getattr__", "micro", "framework", "deprecation", "module", "scrapy", "warning", "version", "info", "declare", "python", "twisted", "getattr", "return", "spider", "selector", "item", "name", "reimported", "instead", "form", "request", "deprecated", "ignore", "spiders", "crawling", "all", "pylint", "scraping", "category", "field", "versions", "decode", "warnings", "plc0415", "txv", "noqa", "from", "filterwarnings", "tuple", "split", "get", "data", "attribute", "exceptions", "written", "strip", "ascii", "minor", "noisy"], "ast_kind": "function_or_method", "text": "\"\"\"\nScrapy - a web crawling and web scraping framework written for Python\n\"\"\"\n\nimport pkgutil\nimport sys\nimport warnings\n\n# Declare top-level shortcuts\nfrom scrapy.http import FormRequest, Request\nfrom scrapy.item import Field, Item\nfrom scrapy.selector import Selector\nfrom scrapy.spiders import Spider\n\n__all__ = [\n    \"Field\",\n    \"FormRequest\",\n    \"Item\",\n    \"Request\",\n    \"Selector\",\n    \"Spider\",\n    \"__version__\",\n    \"version_info\",\n]\n\n\n# Scrapy and Twisted versions\n__version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\nversion_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\n\n\ndef __getattr__(name: str):\n    if name == \"twisted_version\":\n        import warnings  # noqa: PLC0415  # pylint: disable=reimported\n\n        from twisted import version as _txv  # noqa: PLC0415\n\n        from scrapy.exceptions import ScrapyDeprecationWarning  # noqa: PLC0415\n\n        warnings.warn(\n            \"The scrapy.twisted_version attribute is deprecated, use twisted.version instead\",\n            ScrapyDeprecationWarning,\n        )\n        return _txv.major, _txv.minor, _txv.micro\n\n    raise AttributeError\n\n\n# Ignore noisy twisted deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\n\n\ndel pkgutil\ndel sys\ndel warnings\n", "n_tokens": 318, "byte_len": 1333, "file_sha1": "82ce1b89f866d436dd145208653c5a58391bdd06", "start_line": 1, "end_line": 56}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/logformatter.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/logformatter.py", "rel_path": "scrapy/logformatter.py", "module": "scrapy.logformatter", "ext": "py", "chunk_number": 1, "symbols": ["dropped", "crawled", "scraped", "item_error", "LogFormatterResult", "LogFormatter", "PoliteLogFormatter", "failure", "method", "while", "referer", "those", "library", "log", "formatter", "each", "python", "omit", "spider", "error", "polite", "going", "passing", "future", "typ", "checking", "https", "string", "elif", "listing", "spider_error", "download_error", "from_crawler", "settings", "order", "provided", "lower", "isinstance", "level", "issues", "causes", "none", "dictionary", "computed", "docs", "html", "methods", "http", "customize", "spidererrormsg"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport os\nfrom typing import TYPE_CHECKING, Any, TypedDict\n\nfrom twisted.python.failure import Failure\n\n# working around https://github.com/sphinx-doc/sphinx/issues/10400\nfrom scrapy import Request, Spider  # noqa: TC001\nfrom scrapy.http import Response  # noqa: TC001\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.request import referer_str\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nSCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\nDROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\nCRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\nITEMERRORMSG = \"Error processing %(item)s\"\nSPIDERERRORMSG = \"Spider error processing %(request)s (referer: %(referer)s)\"\nDOWNLOADERRORMSG_SHORT = \"Error downloading %(request)s\"\nDOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n\n\nclass LogFormatterResult(TypedDict):\n    level: int\n    msg: str\n    args: dict[str, Any] | tuple[Any, ...]\n\n\nclass LogFormatter:\n    \"\"\"Class for generating log messages for different actions.\n\n    All methods must return a dictionary listing the parameters ``level``, ``msg``\n    and ``args`` which are going to be used for constructing the log message when\n    calling ``logging.log``.\n\n    Dictionary keys for the method outputs:\n\n    *   ``level`` is the log level for that action, you can use those from the\n        `python logging library <https://docs.python.org/3/library/logging.html>`_ :\n        ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR``\n        and ``logging.CRITICAL``.\n    *   ``msg`` should be a string that can contain different formatting placeholders.\n        This string, formatted with the provided ``args``, is going to be the long message\n        for that action.\n    *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.\n        The final log message is computed as ``msg % args``.\n\n    Users can define their own ``LogFormatter`` class if they want to customize how\n    each action is logged or if they want to omit it entirely. In order to omit\n    logging an action the method must return ``None``.\n\n    Here is an example on how to create a custom log formatter to lower the severity level of\n    the log message when an item is dropped from the pipeline::\n\n            class PoliteLogFormatter(logformatter.LogFormatter):\n                def dropped(self, item, exception, response, spider):\n                    return {\n                        'level': logging.INFO, # lowering the level from logging.WARNING\n                        'msg': \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\",\n                        'args': {\n                            'exception': exception,\n                            'item': item,\n                        }\n                    }\n    \"\"\"\n\n    def crawled(\n        self, request: Request, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n        request_flags = f\" {request.flags!s}\" if request.flags else \"\"\n        response_flags = f\" {response.flags!s}\" if response.flags else \"\"\n        return {\n            \"level\": logging.DEBUG,\n            \"msg\": CRAWLEDMSG,\n            \"args\": {\n                \"status\": response.status,\n                \"request\": request,\n                \"request_flags\": request_flags,\n                \"referer\": referer_str(request),\n                \"response_flags\": response_flags,\n                # backward compatibility with Scrapy logformatter below 1.4 version\n                \"flags\": response_flags,\n            },\n        }\n\n    def scraped(\n        self, item: Any, response: Response | Failure | None, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n        src: Any\n        if response is None:\n            src = f\"{global_object_name(spider.__class__)}.start\"\n        elif isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            \"level\": logging.DEBUG,\n            \"msg\": SCRAPEDMSG,\n            \"args\": {\n                \"src\": src,\n                \"item\": item,\n            },\n        }\n\n    def dropped(\n        self,\n        item: Any,\n        exception: BaseException,\n        response: Response | Failure | None,\n        spider: Spider,\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n        if (level := getattr(exception, \"log_level\", None)) is None:\n            level = spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"]\n        if isinstance(level, str):\n            level = getattr(logging, level)\n        return {\n            \"level\": level,\n            \"msg\": DROPPEDMSG,\n            \"args\": {\n                \"exception\": exception,\n                \"item\": item,\n            },\n        }\n\n    def item_error(\n        self,\n        item: Any,\n        exception: BaseException,\n        response: Response | Failure | None,\n        spider: Spider,\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item causes an error while it is passing\n        through the item pipeline.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": ITEMERRORMSG,\n            \"args\": {\n                \"item\": item,\n            },\n        }\n", "n_tokens": 1215, "byte_len": 5630, "file_sha1": "76d9dfb3f98c5d11e00b4b2e69a35d85555098e9", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/logformatter.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/logformatter.py", "rel_path": "scrapy/logformatter.py", "module": "scrapy.logformatter", "ext": "py", "chunk_number": 2, "symbols": ["spider_error", "download_error", "from_crawler", "failure", "download", "error", "referer", "str", "logs", "spider", "return", "dict", "downloaderrorms", "long", "args", "versionadded", "coming", "classmethod", "typically", "logging", "from", "crawler", "log", "formatter", "engine", "request", "none", "self", "level", "message", "dropped", "crawled", "scraped", "item_error", "LogFormatterResult", "LogFormatter", "PoliteLogFormatter", "method", "while", "those", "library", "each", "python", "omit", "polite", "going", "passing", "future", "typ", "checking"], "ast_kind": "function_or_method", "text": "    def spider_error(\n        self,\n        failure: Failure,\n        request: Request,\n        response: Response | Failure,\n        spider: Spider,\n    ) -> LogFormatterResult:\n        \"\"\"Logs an error message from a spider.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": SPIDERERRORMSG,\n            \"args\": {\n                \"request\": request,\n                \"referer\": referer_str(request),\n            },\n        }\n\n    def download_error(\n        self,\n        failure: Failure,\n        request: Request,\n        spider: Spider,\n        errmsg: str | None = None,\n    ) -> LogFormatterResult:\n        \"\"\"Logs a download error message from a spider (typically coming from\n        the engine).\n\n        .. versionadded:: 2.0\n        \"\"\"\n        args: dict[str, Any] = {\"request\": request}\n        if errmsg:\n            msg = DOWNLOADERRORMSG_LONG\n            args[\"errmsg\"] = errmsg\n        else:\n            msg = DOWNLOADERRORMSG_SHORT\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": msg,\n            \"args\": args,\n        }\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls()\n", "n_tokens": 264, "byte_len": 1218, "file_sha1": "76d9dfb3f98c5d11e00b4b2e69a35d85555098e9", "start_line": 156, "end_line": 203}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiderloader.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiderloader.py", "rel_path": "scrapy/spiderloader.py", "module": "scrapy.spiderloader", "ext": "py", "chunk_number": 1, "symbols": ["get_spider_loader", "from_settings", "load", "list", "find_by_request", "__init__", "_check_name_duplicates", "_load_spiders", "_load_all_spiders", "SpiderLoaderProtocol", "SpiderLoader", "DummySpiderLoader", "does", "loads", "bool", "spider", "loader", "handles", "request", "append", "instance", "python", "traceback", "name", "doesn", "runtime", "warning", "cause", "unexpected", "interfaces", "spiders", "future", "typ", "checking", "format", "exc", "get", "interface", "settings", "spcls", "dupes", "items", "none", "join", "type", "misc", "verify", "found", "defaultdict", "details"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport traceback\nimport warnings\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Protocol, cast\n\nfrom zope.interface import implementer\nfrom zope.interface.verify import verifyClass\n\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.utils.misc import load_object, walk_modules\nfrom scrapy.utils.spider import iter_spider_classes\n\nif TYPE_CHECKING:\n    from types import ModuleType\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.settings import BaseSettings\n\n\ndef get_spider_loader(settings: BaseSettings) -> SpiderLoaderProtocol:\n    \"\"\"Get SpiderLoader instance from settings\"\"\"\n    cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n    loader_cls = load_object(cls_path)\n    verifyClass(ISpiderLoader, loader_cls)\n    return cast(\"SpiderLoaderProtocol\", loader_cls.from_settings(settings.frozencopy()))\n\n\nclass SpiderLoaderProtocol(Protocol):\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        \"\"\"Return an instance of the class for the given settings\"\"\"\n\n    def load(self, spider_name: str) -> type[Spider]:\n        \"\"\"Return the Spider class for the given spider name. If the spider\n        name is not found, it must raise a KeyError.\"\"\"\n\n    def list(self) -> list[str]:\n        \"\"\"Return a list with the names of all spiders available in the\n        project\"\"\"\n\n    def find_by_request(self, request: Request) -> __builtins__.list[str]:\n        \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n\n\n@implementer(ISpiderLoader)\nclass SpiderLoader:\n    \"\"\"\n    SpiderLoader is a class which locates and loads spiders\n    in a Scrapy project.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        self.spider_modules: list[str] = settings.getlist(\"SPIDER_MODULES\")\n        self.warn_only: bool = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n        self._spiders: dict[str, type[Spider]] = {}\n        self._found: defaultdict[str, list[tuple[str, str]]] = defaultdict(list)\n        self._load_all_spiders()\n\n    def _check_name_duplicates(self) -> None:\n        dupes = []\n        for name, locations in self._found.items():\n            dupes.extend(\n                [\n                    f\"  {cls} named {name!r} (in {mod})\"\n                    for mod, cls in locations\n                    if len(locations) > 1\n                ]\n            )\n\n        if dupes:\n            dupes_string = \"\\n\\n\".join(dupes)\n            warnings.warn(\n                \"There are several spiders with the same name:\\n\\n\"\n                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                category=UserWarning,\n            )\n\n    def _load_spiders(self, module: ModuleType) -> None:\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls\n\n    def _load_all_spiders(self) -> None:\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except (ImportError, SyntaxError):\n                if self.warn_only:\n                    warnings.warn(\n                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n                        f\"from module '{name}'. \"\n                        \"See above traceback for details.\",\n                        category=RuntimeWarning,\n                    )\n                else:\n                    raise\n        self._check_name_duplicates()\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(settings)\n\n    def load(self, spider_name: str) -> type[Spider]:\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(f\"Spider not found: {spider_name}\")\n\n    def find_by_request(self, request: Request) -> list[str]:\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [\n            name for name, cls in self._spiders.items() if cls.handles_request(request)\n        ]\n\n    def list(self) -> list[str]:\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())\n\n\n@implementer(ISpiderLoader)\nclass DummySpiderLoader:\n    \"\"\"A dummy spider loader that does not load any spiders.\"\"\"\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls()\n\n    def load(self, spider_name: str) -> type[Spider]:\n        raise KeyError(\"DummySpiderLoader doesn't load any spiders\")\n\n    def list(self) -> list[str]:\n        return []\n\n    def find_by_request(self, request: Request) -> __builtins__.list[str]:\n        return []\n", "n_tokens": 1079, "byte_len": 5037, "file_sha1": "cfeba39718733d9df10108981f1d8341352c1c50", "start_line": 1, "end_line": 150}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/statscollectors.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/statscollectors.py", "rel_path": "scrapy/statscollectors.py", "module": "scrapy.statscollectors", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "__getattribute__", "get_value", "get_stats", "set_value", "set_stats", "inc_value", "max_value", "min_value", "clear_stats", "open_spider", "close_spider", "_persist_stats", "StatsCollector", "MemoryStatsCollector", "DummyStatsCollector", "set", "value", "stats", "bool", "setdefault", "getbool", "pass", "open", "spider", "statst", "inc", "max", "extra", "typing", "dump", "return", "dict", "name", "annotations", "memory", "class", "close", "warn", "scrapy", "logger", "future", "typ", "checking", "dumping", "get", "extension", "scraping", "init", "logging"], "ast_kind": "class_or_type", "text": "\"\"\"\nScrapy extension for collecting scraping stats\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport pprint\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.utils.decorators import _warn_spider_arg\n\nif TYPE_CHECKING:\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\nStatsT = dict[str, Any]\n\n\nclass StatsCollector:\n    def __init__(self, crawler: Crawler):\n        self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n        self._stats: StatsT = {}\n        self._crawler: Crawler = crawler\n\n    def __getattribute__(self, name):\n        original_attr = super().__getattribute__(name)\n\n        if name in (\n            \"get_value\",\n            \"get_stats\",\n            \"set_value\",\n            \"set_stats\",\n            \"inc_value\",\n            \"max_value\",\n            \"min_value\",\n            \"clear_stats\",\n            \"open_spider\",\n            \"close_spider\",\n        ) and callable(original_attr):\n            return _warn_spider_arg(original_attr)\n\n        return original_attr\n\n    def get_value(\n        self, key: str, default: Any = None, spider: Spider | None = None\n    ) -> Any:\n        return self._stats.get(key, default)\n\n    def get_stats(self, spider: Spider | None = None) -> StatsT:\n        return self._stats\n\n    def set_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n        self._stats[key] = value\n\n    def set_stats(self, stats: StatsT, spider: Spider | None = None) -> None:\n        self._stats = stats\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Spider | None = None\n    ) -> None:\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count\n\n    def max_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n        self._stats[key] = max(self._stats.setdefault(key, value), value)\n\n    def min_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n        self._stats[key] = min(self._stats.setdefault(key, value), value)\n\n    def clear_stats(self, spider: Spider | None = None) -> None:\n        self._stats.clear()\n\n    def open_spider(self, spider: Spider | None = None) -> None:\n        pass\n\n    def close_spider(\n        self, spider: Spider | None = None, reason: str | None = None\n    ) -> None:\n        if self._dump:\n            logger.info(\n                \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                extra={\"spider\": self._crawler.spider},\n            )\n        self._persist_stats(self._stats)\n\n    def _persist_stats(self, stats: StatsT) -> None:\n        pass\n\n\nclass MemoryStatsCollector(StatsCollector):\n    def __init__(self, crawler: Crawler):\n        super().__init__(crawler)\n        self.spider_stats: dict[str, StatsT] = {}\n\n    def _persist_stats(self, stats: StatsT) -> None:\n        if self._crawler.spider:\n            self.spider_stats[self._crawler.spider.name] = stats\n\n\nclass DummyStatsCollector(StatsCollector):\n    def get_value(\n        self, key: str, default: Any = None, spider: Spider | None = None\n    ) -> Any:\n        return default\n\n    def set_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n        pass\n\n    def set_stats(self, stats: StatsT, spider: Spider | None = None) -> None:\n        pass\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Spider | None = None\n    ) -> None:\n        pass\n\n    def max_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n        pass\n\n    def min_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n        pass\n", "n_tokens": 919, "byte_len": 3652, "file_sha1": "a7dfc0a3b462035f201eb3d30d6c1886abcf5491", "start_line": 1, "end_line": 127}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extension.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extension.py", "rel_path": "scrapy/extension.py", "module": "scrapy.extension", "ext": "py", "chunk_number": 1, "symbols": ["_get_mwlist_from_settings", "ExtensionManager", "getwithbase", "extensions", "typing", "return", "extension", "middleware", "annotations", "class", "get", "mwlist", "scrapy", "future", "typ", "checking", "classmethod", "manager", "topics", "conf", "documentation", "build", "component", "from", "settings", "list", "docs", "utils", "import", "name"], "ast_kind": "class_or_type", "text": "\"\"\"\nThe Extension Manager\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.utils.conf import build_component_list\n\nif TYPE_CHECKING:\n    from scrapy.settings import Settings\n\n\nclass ExtensionManager(MiddlewareManager):\n    component_name = \"extension\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n        return build_component_list(settings.getwithbase(\"EXTENSIONS\"))\n", "n_tokens": 109, "byte_len": 555, "file_sha1": "db504839af738ea68343fcc4b372bb073f4cd210", "start_line": 1, "end_line": 24}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/resolver.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/resolver.py", "rel_path": "scrapy/resolver.py", "module": "scrapy.resolver", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "install_on_reactor", "getHostByName", "_cache_result", "cancel", "resolutionBegan", "addressResolved", "resolutionComplete", "resolveHostName", "CachingThreadedResolver", "HostResolution", "_CachingResolutionReceiver", "CachingHostnameResolver", "does", "address", "enforce", "host", "resolution", "append", "python", "declarations", "transport", "semantics", "name", "interfaces", "passed", "future", "typ", "checking", "ipv4", "hostname", "resolver", "succeed", "interface", "settings", "install", "reactor", "timeout", "dns", "base", "caching", "provider", "none", "type", "receiver", "default", "sequence", "addr", "requests"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom twisted.internet import defer\nfrom twisted.internet.base import ReactorBase, ThreadedResolver\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostnameResolver,\n    IHostResolution,\n    IResolutionReceiver,\n    IResolverSimple,\n)\nfrom zope.interface.declarations import implementer, provider\n\nfrom scrapy.utils.datatypes import LocalCache\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n    from twisted.internet.defer import Deferred\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n# TODO: cache misses\ndnscache: LocalCache[str, Any] = LocalCache(10000)\n\n\n@implementer(IResolverSimple)\nclass CachingThreadedResolver(ThreadedResolver):\n    \"\"\"\n    Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.\n    \"\"\"\n\n    def __init__(self, reactor: ReactorBase, cache_size: int, timeout: float):\n        super().__init__(reactor)\n        dnscache.limit = cache_size\n        self.timeout = timeout\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size, crawler.settings.getfloat(\"DNS_TIMEOUT\"))\n\n    def install_on_reactor(self) -> None:\n        self.reactor.installResolver(self)\n\n    def getHostByName(self, name: str, timeout: Sequence[int] = ()) -> Deferred[str]:\n        if name in dnscache:\n            return defer.succeed(dnscache[name])\n        # in Twisted<=16.6, getHostByName() is always called with\n        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n        # so the input argument above is simply overridden\n        # to enforce Scrapy's DNS_TIMEOUT setting's value\n        # The timeout arg is typed as Sequence[int] but supports floats.\n        timeout = (self.timeout,)  # type: ignore[assignment]\n        d = super().getHostByName(name, timeout)\n        if dnscache.limit:\n            d.addCallback(self._cache_result, name)\n        return d\n\n    def _cache_result(self, result: Any, name: str) -> Any:\n        dnscache[name] = result\n        return result\n\n\n@implementer(IHostResolution)\nclass HostResolution:\n    def __init__(self, name: str):\n        self.name: str = name\n\n    def cancel(self) -> None:\n        raise NotImplementedError\n\n\n@provider(IResolutionReceiver)\nclass _CachingResolutionReceiver:\n    def __init__(self, resolutionReceiver: IResolutionReceiver, hostName: str):\n        self.resolutionReceiver: IResolutionReceiver = resolutionReceiver\n        self.hostName: str = hostName\n        self.addresses: list[IAddress] = []\n\n    def resolutionBegan(self, resolution: IHostResolution) -> None:\n        self.resolutionReceiver.resolutionBegan(resolution)\n        self.resolution = resolution\n\n    def addressResolved(self, address: IAddress) -> None:\n        self.resolutionReceiver.addressResolved(address)\n        self.addresses.append(address)\n\n    def resolutionComplete(self) -> None:\n        self.resolutionReceiver.resolutionComplete()\n        if self.addresses:\n            dnscache[self.hostName] = self.addresses\n\n\n@implementer(IHostnameResolver)\nclass CachingHostnameResolver:\n    \"\"\"\n    Experimental caching resolver. Resolves IPv4 and IPv6 addresses,\n    does not support setting a timeout value for DNS requests.\n    \"\"\"\n\n    def __init__(self, reactor: ReactorBase, cache_size: int):\n        self.reactor: ReactorBase = reactor\n        self.original_resolver: IHostnameResolver = reactor.nameResolver\n        dnscache.limit = cache_size\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size)\n\n    def install_on_reactor(self) -> None:\n        self.reactor.installNameResolver(self)\n\n    def resolveHostName(\n        self,\n        resolutionReceiver: IResolutionReceiver,\n        hostName: str,\n        portNumber: int = 0,\n        addressTypes: Sequence[type[IAddress]] | None = None,\n        transportSemantics: str = \"TCP\",\n    ) -> IHostResolution:\n        try:\n            addresses = dnscache[hostName]\n        except KeyError:\n            return self.original_resolver.resolveHostName(\n                _CachingResolutionReceiver(resolutionReceiver, hostName),\n                hostName,\n                portNumber,\n                addressTypes,\n                transportSemantics,\n            )\n        resolutionReceiver.resolutionBegan(HostResolution(hostName))\n        for addr in addresses:\n            resolutionReceiver.addressResolved(addr)\n        resolutionReceiver.resolutionComplete()\n        return resolutionReceiver\n", "n_tokens": 1083, "byte_len": 5002, "file_sha1": "59158bb4d93de09f773c81f242d6852d7c8255f6", "start_line": 1, "end_line": 149}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/responsetypes.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/responsetypes.py", "rel_path": "scrapy/responsetypes.py", "module": "scrapy.responsetypes", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_mimetype", "from_content_type", "from_content_disposition", "from_headers", "from_filename", "from_body", "from_args", "ResponseTypes", "encoding", "method", "guess", "type", "content", "disposition", "future", "mimetypes", "magic", "from", "filename", "stream", "name", "more", "typ", "checking", "unicode", "lower", "chunk", "items", "headers", "index", "error", "binary", "text", "none", "html", "methods", "misc", "looking", "http", "bytes", "found", "response", "could", "based", "utf", "utf8", "mimedata", "appropriate", "typing"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements a class which returns the appropriate Response class\nbased on different criteria.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom io import StringIO\nfrom mimetypes import MimeTypes\nfrom pkgutil import get_data\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.http import Response\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Mapping\n\n\nclass ResponseTypes:\n    CLASSES = {\n        \"text/html\": \"scrapy.http.HtmlResponse\",\n        \"application/atom+xml\": \"scrapy.http.XmlResponse\",\n        \"application/rdf+xml\": \"scrapy.http.XmlResponse\",\n        \"application/rss+xml\": \"scrapy.http.XmlResponse\",\n        \"application/xhtml+xml\": \"scrapy.http.HtmlResponse\",\n        \"application/vnd.wap.xhtml+xml\": \"scrapy.http.HtmlResponse\",\n        \"application/xml\": \"scrapy.http.XmlResponse\",\n        \"application/json\": \"scrapy.http.JsonResponse\",\n        \"application/x-json\": \"scrapy.http.JsonResponse\",\n        \"application/json-amazonui-streaming\": \"scrapy.http.JsonResponse\",\n        \"application/javascript\": \"scrapy.http.TextResponse\",\n        \"application/x-javascript\": \"scrapy.http.TextResponse\",\n        \"text/xml\": \"scrapy.http.XmlResponse\",\n        \"text/*\": \"scrapy.http.TextResponse\",\n    }\n\n    def __init__(self) -> None:\n        self.classes: dict[str, type[Response]] = {}\n        self.mimetypes: MimeTypes = MimeTypes()\n        mimedata = get_data(\"scrapy\", \"mime.types\")\n        if not mimedata:\n            raise ValueError(\n                \"The mime.types file is not found in the Scrapy installation\"\n            )\n        self.mimetypes.readfp(StringIO(mimedata.decode(\"utf8\")))\n        for mimetype, cls in self.CLASSES.items():\n            self.classes[mimetype] = load_object(cls)\n\n    def from_mimetype(self, mimetype: str) -> type[Response]:\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        if mimetype in self.classes:\n            return self.classes[mimetype]\n        basetype = f\"{mimetype.split('/')[0]}/*\"\n        return self.classes.get(basetype, Response)\n\n    def from_content_type(\n        self, content_type: str | bytes, content_encoding: bytes | None = None\n    ) -> type[Response]:\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header\"\"\"\n        if content_encoding:\n            return Response\n        mimetype = (\n            to_unicode(content_type, encoding=\"latin-1\").split(\";\")[0].strip().lower()\n        )\n        return self.from_mimetype(mimetype)\n\n    def from_content_disposition(\n        self, content_disposition: str | bytes\n    ) -> type[Response]:\n        try:\n            filename = (\n                to_unicode(content_disposition, encoding=\"latin-1\", errors=\"replace\")\n                .split(\";\")[1]\n                .split(\"=\")[1]\n                .strip(\"\\\"'\")\n            )\n            return self.from_filename(filename)\n        except IndexError:\n            return Response\n\n    def from_headers(self, headers: Mapping[bytes, bytes]) -> type[Response]:\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b\"Content-Type\" in headers:\n            cls = self.from_content_type(\n                content_type=headers[b\"Content-Type\"],\n                content_encoding=headers.get(b\"Content-Encoding\"),\n            )\n        if cls is Response and b\"Content-Disposition\" in headers:\n            cls = self.from_content_disposition(headers[b\"Content-Disposition\"])\n        return cls\n\n    def from_filename(self, filename: str) -> type[Response]:\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        return Response\n\n    def from_body(self, body: bytes) -> type[Response]:\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype(\"application/octet-stream\")\n        lowercase_chunk = chunk.lower()\n        if b\"<html>\" in lowercase_chunk:\n            return self.from_mimetype(\"text/html\")\n        if b\"<?xml\" in lowercase_chunk:\n            return self.from_mimetype(\"text/xml\")\n        if b\"<!doctype html>\" in lowercase_chunk:\n            return self.from_mimetype(\"text/html\")\n        return self.from_mimetype(\"text\")\n\n    def from_args(\n        self,\n        headers: Mapping[bytes, bytes] | None = None,\n        url: str | None = None,\n        filename: str | None = None,\n        body: bytes | None = None,\n    ) -> type[Response]:\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls\n\n\nresponsetypes = ResponseTypes()\n", "n_tokens": 1181, "byte_len": 5621, "file_sha1": "73cfcad249f91b84a893a8967cbcc0c5b920a9b9", "start_line": 1, "end_line": 146}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exceptions.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exceptions.py", "rel_path": "scrapy/exceptions.py", "module": "scrapy.exceptions", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "NotConfigured", "_InvalidOutput", "IgnoreRequest", "DontCloseSpider", "CloseSpider", "StopDownload", "DropItem", "NotSupported", "UsageError", "ScrapyDeprecationWarning", "ContractFail", "method", "usage", "error", "bool", "deprecation", "warning", "case", "made", "python", "silenced", "indicates", "deprecated", "these", "invalid", "output", "future", "missing", "process", "here", "log", "level", "them", "parameter", "none", "docs", "failing", "without", "code", "reason", "line", "since", "drop", "item", "fail", "default", "note", "commands", "not"], "ast_kind": "class_or_type", "text": "\"\"\"\nScrapy core exceptions\n\nThese exceptions are documented in docs/topics/exceptions.rst. Please don't add\nnew exceptions here without documenting them there.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\n\n# Internal\n\n\nclass NotConfigured(Exception):\n    \"\"\"Indicates a missing configuration situation\"\"\"\n\n\nclass _InvalidOutput(TypeError):\n    \"\"\"\n    Indicates an invalid value has been returned by a middleware's processing method.\n    Internal and undocumented, it should not be raised or caught by user code.\n    \"\"\"\n\n\n# HTTP and crawling\n\n\nclass IgnoreRequest(Exception):\n    \"\"\"Indicates a decision was made not to process a request\"\"\"\n\n\nclass DontCloseSpider(Exception):\n    \"\"\"Request the spider not to be closed yet\"\"\"\n\n\nclass CloseSpider(Exception):\n    \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n\n    def __init__(self, reason: str = \"cancelled\"):\n        super().__init__()\n        self.reason = reason\n\n\nclass StopDownload(Exception):\n    \"\"\"\n    Stop the download of the body for a given response.\n    The 'fail' boolean parameter indicates whether or not the resulting partial response\n    should be handled by the request errback. Note that 'fail' is a keyword-only argument.\n    \"\"\"\n\n    def __init__(self, *, fail: bool = True):\n        super().__init__()\n        self.fail = fail\n\n\n# Items\n\n\nclass DropItem(Exception):\n    \"\"\"Drop item from the item pipeline\"\"\"\n\n    def __init__(self, message: str, log_level: str | None = None):\n        super().__init__(message)\n        self.log_level = log_level\n\n\nclass NotSupported(Exception):\n    \"\"\"Indicates a feature or method is not supported\"\"\"\n\n\n# Commands\n\n\nclass UsageError(Exception):\n    \"\"\"To indicate a command-line usage error\"\"\"\n\n    def __init__(self, *a: Any, **kw: Any):\n        self.print_help = kw.pop(\"print_help\", True)\n        super().__init__(*a, **kw)\n\n\nclass ScrapyDeprecationWarning(Warning):\n    \"\"\"Warning category for deprecated features, since the default\n    DeprecationWarning is silenced on Python 2.7+\n    \"\"\"\n\n\nclass ContractFail(AssertionError):\n    \"\"\"Error raised in case of a failing contract\"\"\"\n", "n_tokens": 458, "byte_len": 2137, "file_sha1": "b7017436105b3383b79db548e3a0155f5b37ec6e", "start_line": 1, "end_line": 91}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exporters.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exporters.py", "rel_path": "scrapy/exporters.py", "module": "scrapy.exporters", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_configure", "export_item", "serialize_field", "start_exporting", "finish_exporting", "_get_serialized_fields", "_beautify_newline", "_add_comma_after_first", "BaseItemExporter", "JsonLinesItemExporter", "JsonItemExporter", "encoding", "bool", "subclasses", "xml", "item", "serialize", "pprint", "serialized", "value", "name", "small", "exporter", "unexpected", "xmlreader", "kwargs", "between", "future", "typ", "_beautify_indent", "_export_xml_field", "_join_if_needed", "_build_row", "_write_headers_and_set_fields_to_export", "_serialize_value", "_serialize_item", "XmlItemExporter", "CsvItemExporter", "PickleItemExporter", "MarshalItemExporter", "PprintItemExporter", "PythonItemExporter", "writer", "method", "protocol", "build", "row", "library", "root"], "ast_kind": "class_or_type", "text": "\"\"\"\nItem Exporters are used to export/serialize items into different formats.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport csv\nimport marshal\nimport pickle\nimport pprint\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Callable, Iterable, Mapping\nfrom io import BytesIO, TextIOWrapper\nfrom typing import TYPE_CHECKING, Any\nfrom xml.sax.saxutils import XMLGenerator\nfrom xml.sax.xmlreader import AttributesImpl\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.python import is_listlike, to_bytes, to_unicode\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\nif TYPE_CHECKING:\n    from json import JSONEncoder\n\n__all__ = [\n    \"BaseItemExporter\",\n    \"CsvItemExporter\",\n    \"JsonItemExporter\",\n    \"JsonLinesItemExporter\",\n    \"MarshalItemExporter\",\n    \"PickleItemExporter\",\n    \"PprintItemExporter\",\n    \"XmlItemExporter\",\n]\n\n\nclass BaseItemExporter(ABC):\n    def __init__(self, *, dont_fail: bool = False, **kwargs: Any):\n        self._kwargs: dict[str, Any] = kwargs\n        self._configure(kwargs, dont_fail=dont_fail)\n\n    def _configure(self, options: dict[str, Any], dont_fail: bool = False) -> None:\n        \"\"\"Configure the exporter by popping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding: str | None = options.pop(\"encoding\", None)\n        self.fields_to_export: Mapping[str, str] | Iterable[str] | None = options.pop(\n            \"fields_to_export\", None\n        )\n        self.export_empty_fields: bool = options.pop(\"export_empty_fields\", False)\n        self.indent: int | None = options.pop(\"indent\", None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n\n    @abstractmethod\n    def export_item(self, item: Any) -> None:\n        raise NotImplementedError\n\n    def serialize_field(\n        self, field: Mapping[str, Any] | Field, name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\"serializer\", lambda x: x)\n        return serializer(value)\n\n    def start_exporting(self) -> None:  # noqa: B027\n        pass\n\n    def finish_exporting(self) -> None:  # noqa: B027\n        pass\n\n    def _get_serialized_fields(\n        self, item: Any, default_value: Any = None, include_empty: bool | None = None\n    ) -> Iterable[tuple[str, Any]]:\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            field_iter = item.field_names() if include_empty else item.keys()\n        elif isinstance(self.fields_to_export, Mapping):\n            if include_empty:\n                field_iter = self.fields_to_export.items()\n            else:\n                field_iter = (\n                    (x, y) for x, y in self.fields_to_export.items() if x in item\n                )\n        elif include_empty:\n            field_iter = self.fields_to_export\n        else:\n            field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if isinstance(field_name, str):\n                item_field, output_field = field_name, field_name\n            else:\n                item_field, output_field = field_name\n            if item_field in item:\n                field_meta = item.get_field_meta(item_field)\n                value = self.serialize_field(field_meta, output_field, item[item_field])\n            else:\n                value = default_value\n\n            yield output_field, value\n\n\nclass JsonLinesItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file: BytesIO = file\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder: JSONEncoder = ScrapyJSONEncoder(**self._kwargs)\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + \"\\n\"\n        self.file.write(to_bytes(data, self.encoding))\n\n\nclass JsonItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file: BytesIO = file\n        # there is a small difference between the behaviour or JsonItemExporter.indent\n        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n        # the addition of newlines everywhere\n        json_indent = (\n            self.indent if self.indent is not None and self.indent > 0 else None\n        )\n        self._kwargs.setdefault(\"indent\", json_indent)\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n        self.first_item = True\n\n    def _beautify_newline(self) -> None:\n        if self.indent is not None:\n            self.file.write(b\"\\n\")\n\n    def _add_comma_after_first(self) -> None:\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b\",\")\n            self._beautify_newline()\n\n    def start_exporting(self) -> None:\n        self.file.write(b\"[\")\n        self._beautify_newline()\n", "n_tokens": 1230, "byte_len": 5505, "file_sha1": "e916e26f2d0cccda83db2d6a905eebf2bb26596f", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exporters.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exporters.py", "rel_path": "scrapy/exporters.py", "module": "scrapy.exporters", "ext": "py", "chunk_number": 2, "symbols": ["finish_exporting", "export_item", "__init__", "_beautify_newline", "_beautify_indent", "start_exporting", "_export_xml_field", "serialize_field", "_join_if_needed", "_build_row", "XmlItemExporter", "CsvItemExporter", "encoding", "writer", "build", "row", "bool", "root", "element", "write", "headers", "xml", "item", "serialized", "value", "stream", "name", "kwargs", "join", "multivalued", "_configure", "_get_serialized_fields", "_add_comma_after_first", "_write_headers_and_set_fields_to_export", "_serialize_value", "_serialize_item", "BaseItemExporter", "JsonLinesItemExporter", "JsonItemExporter", "PickleItemExporter", "MarshalItemExporter", "PprintItemExporter", "PythonItemExporter", "method", "protocol", "library", "subclasses", "python", "serialize", "pprint"], "ast_kind": "class_or_type", "text": "    def finish_exporting(self) -> None:\n        self._beautify_newline()\n        self.file.write(b\"]\")\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n        self._add_comma_after_first()\n        self.file.write(data)\n\n\nclass XmlItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        self.item_element = kwargs.pop(\"item_element\", \"item\")\n        self.root_element = kwargs.pop(\"root_element\", \"items\")\n        super().__init__(**kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.xg = XMLGenerator(file, encoding=self.encoding)\n\n    def _beautify_newline(self, new_item: bool = False) -> None:\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters(\"\\n\")\n\n    def _beautify_indent(self, depth: int = 1) -> None:\n        if self.indent:\n            self.xg.characters(\" \" * self.indent * depth)\n\n    def start_exporting(self) -> None:\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, AttributesImpl({}))\n        self._beautify_newline(new_item=True)\n\n    def export_item(self, item: Any) -> None:\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, AttributesImpl({}))\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=\"\"):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)\n\n    def finish_exporting(self) -> None:\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()\n\n    def _export_xml_field(self, name: str, serialized_value: Any, depth: int) -> None:\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, AttributesImpl({}))\n        if hasattr(serialized_value, \"items\"):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field(\"value\", value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()\n\n\nclass CsvItemExporter(BaseItemExporter):\n    def __init__(\n        self,\n        file: BytesIO,\n        include_headers_line: bool = True,\n        join_multivalued: str = \",\",\n        errors: str | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(dont_fail=True, **kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.include_headers_line = include_headers_line\n        self.stream = TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding,\n            newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n            errors=errors,\n        )\n        self.csv_writer = csv.writer(self.stream, **self._kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued\n\n    def serialize_field(\n        self, field: Mapping[str, Any] | Field, name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\"serializer\", self._join_if_needed)\n        return serializer(value)\n\n    def _join_if_needed(self, value: Any) -> Any:\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value\n\n    def export_item(self, item: Any) -> None:\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)\n\n    def finish_exporting(self) -> None:\n        self.stream.detach()  # Avoid closing the wrapped file.\n\n    def _build_row(self, values: Iterable[Any]) -> Iterable[Any]:\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s\n", "n_tokens": 1095, "byte_len": 4806, "file_sha1": "e916e26f2d0cccda83db2d6a905eebf2bb26596f", "start_line": 156, "end_line": 281}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exporters.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/exporters.py", "rel_path": "scrapy/exporters.py", "module": "scrapy.exporters", "ext": "py", "chunk_number": 3, "symbols": ["_write_headers_and_set_fields_to_export", "__init__", "export_item", "_configure", "serialize_field", "_serialize_value", "_serialize_item", "PickleItemExporter", "MarshalItemExporter", "PprintItemExporter", "PythonItemExporter", "encoding", "method", "protocol", "build", "row", "library", "bool", "write", "headers", "python", "serialize", "value", "pprint", "item", "name", "https", "csv", "writer", "pickle", "start_exporting", "finish_exporting", "_get_serialized_fields", "_beautify_newline", "_add_comma_after_first", "_beautify_indent", "_export_xml_field", "_join_if_needed", "_build_row", "BaseItemExporter", "JsonLinesItemExporter", "JsonItemExporter", "XmlItemExporter", "CsvItemExporter", "subclasses", "root", "element", "xml", "serialized", "small"], "ast_kind": "class_or_type", "text": "    def _write_headers_and_set_fields_to_export(self, item: Any) -> None:\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            fields: Iterable[str]\n            if isinstance(self.fields_to_export, Mapping):\n                fields = self.fields_to_export.values()\n            else:\n                assert self.fields_to_export\n                fields = self.fields_to_export\n            row = list(self._build_row(fields))\n            self.csv_writer.writerow(row)\n\n\nclass PickleItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, protocol: int = 4, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n        self.protocol: int = protocol\n\n    def export_item(self, item: Any) -> None:\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)\n\n\nclass MarshalItemExporter(BaseItemExporter):\n    \"\"\"Exports items in a Python-specific binary format (see\n    :mod:`marshal`).\n\n    :param file: The file-like object to use for exporting the data. Its\n                 ``write`` method should accept :class:`bytes` (a disk file\n                 opened in binary mode, a :class:`~io.BytesIO` object, etc)\n    \"\"\"\n\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n\n    def export_item(self, item: Any) -> None:\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n\n\nclass PprintItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n\n\nclass PythonItemExporter(BaseItemExporter):\n    \"\"\"This is a base class for item exporters that extends\n    :class:`BaseItemExporter` with support for nested items.\n\n    It serializes items to built-in Python types, so that any serialization\n    library (e.g. :mod:`json` or msgpack_) can be used on top of it.\n\n    .. _msgpack: https://pypi.org/project/msgpack/\n    \"\"\"\n\n    def _configure(self, options: dict[str, Any], dont_fail: bool = False) -> None:\n        super()._configure(options, dont_fail)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n\n    def serialize_field(\n        self, field: Mapping[str, Any] | Field, name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\n            \"serializer\", self._serialize_value\n        )\n        return serializer(value)\n\n    def _serialize_value(self, value: Any) -> Any:\n        if isinstance(value, Item):\n            return self.export_item(value)\n        if isinstance(value, (str, bytes)):\n            return to_unicode(value, encoding=self.encoding)\n        if is_item(value):\n            return dict(self._serialize_item(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        return value\n\n    def _serialize_item(self, item: Any) -> Iterable[tuple[str | bytes, Any]]:\n        for key, value in ItemAdapter(item).items():\n            yield key, self._serialize_value(value)\n\n    def export_item(self, item: Any) -> dict[str | bytes, Any]:  # type: ignore[override]\n        result: dict[str | bytes, Any] = dict(self._get_serialized_fields(item))\n        return result\n", "n_tokens": 828, "byte_len": 3606, "file_sha1": "e916e26f2d0cccda83db2d6a905eebf2bb26596f", "start_line": 282, "end_line": 376}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/signalmanager.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/signalmanager.py", "rel_path": "scrapy/signalmanager.py", "module": "scrapy.signalmanager", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "connect", "disconnect", "send_catch_log", "send_catch_log_deferred", "disconnect_all", "handle", "SignalManager", "effect", "method", "async", "send", "signal", "like", "passed", "future", "catch", "object", "them", "none", "handlers", "returns", "type", "lazy", "callable", "fired", "requests", "await", "internet", "opposite", "asynchronous", "typing", "return", "annotations", "class", "example", "some", "sender", "function", "list", "dispatcher", "signals", "kwargs", "through", "exceptions", "connected", "param", "self", "this", "comes"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom pydispatch import dispatcher\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils import signal as _signal\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass SignalManager:\n    def __init__(self, sender: Any = dispatcher.Anonymous):\n        self.sender: Any = sender\n\n    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.connect(receiver, signal, **kwargs)\n\n    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.disconnect(receiver, signal, **kwargs)\n\n    def send_catch_log(self, signal: Any, **kwargs: Any) -> list[tuple[Any, Any]]:\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log(signal, **kwargs)\n\n    def send_catch_log_deferred(\n        self, signal: Any, **kwargs: Any\n    ) -> Deferred[list[tuple[Any, Any]]]:\n        \"\"\"\n        Like :meth:`send_catch_log` but supports :ref:`asynchronous signal\n        handlers <signal-deferred>`.\n\n        Returns a Deferred that gets fired once all signal handlers\n        have finished. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)\n\n    async def send_catch_log_async(\n        self, signal: Any, **kwargs: Any\n    ) -> list[tuple[Any, Any]]:\n        \"\"\"\n        Like :meth:`send_catch_log` but supports :ref:`asynchronous signal\n        handlers <signal-deferred>`.\n\n        Returns a coroutine that completes once all signal handlers\n        have finished. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n\n        .. versionadded:: VERSION\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return await _signal.send_catch_log_async(signal, **kwargs)\n\n    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        _signal.disconnect_all(signal, **kwargs)\n\n    async def wait_for(self, signal):\n        \"\"\"Await the next *signal*.\n\n        See :ref:`start-requests-lazy` for an example.\n        \"\"\"\n        d = Deferred()\n\n        def handle():\n            self.disconnect(handle, signal)\n            d.callback(None)\n\n        self.connect(handle, signal)\n        await maybe_deferred_to_future(d)\n", "n_tokens": 786, "byte_len": 3665, "file_sha1": "e67b9e963e2b0fd84aa2ae364a163cf013e37810", "start_line": 1, "end_line": 109}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/squeues.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/squeues.py", "rel_path": "scrapy/squeues.py", "module": "scrapy.squeues", "ext": "py", "chunk_number": 1, "symbols": ["_with_mkdir", "__init__", "_serializable_queue", "push", "pop", "peek", "_scrapy_serialization_queue", "from_crawler", "_scrapy_non_serialization_queue", "DirectoriesCreated", "SerializableQueue", "ScrapyRequestQueue", "does", "method", "queues", "raises", "python", "serialize", "directories", "created", "future", "typ", "checking", "path", "request", "dict", "queuelib", "from", "object", "serializable", "_pickle_serialize", "dumps", "protocol", "subclasses", "loads", "selector", "pickle", "lifo", "fifo", "both", "queue", "none", "returns", "parents", "scrapy", "serialization", "type", "without", "misc", "like"], "ast_kind": "class_or_type", "text": "\"\"\"\nScheduler queues\n\"\"\"\n\nfrom __future__ import annotations\n\nimport marshal\nimport pickle\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nfrom queuelib import queue\n\nfrom scrapy.utils.request import request_from_dict\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n    from os import PathLike\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request\n    from scrapy.crawler import Crawler\n\n\ndef _with_mkdir(queue_class: type[queue.BaseQueue]) -> type[queue.BaseQueue]:\n    class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n        def __init__(self, path: str | PathLike, *args: Any, **kwargs: Any):\n            dirname = Path(path).parent\n            if not dirname.exists():\n                dirname.mkdir(parents=True, exist_ok=True)\n            super().__init__(path, *args, **kwargs)\n\n    return DirectoriesCreated\n\n\ndef _serializable_queue(\n    queue_class: type[queue.BaseQueue],\n    serialize: Callable[[Any], bytes],\n    deserialize: Callable[[bytes], Any],\n) -> type[queue.BaseQueue]:\n    class SerializableQueue(queue_class):  # type: ignore[valid-type,misc]\n        def push(self, obj: Any) -> None:\n            s = serialize(obj)\n            super().push(s)\n\n        def pop(self) -> Any | None:\n            s = super().pop()\n            if s:\n                return deserialize(s)\n            return None\n\n        def peek(self) -> Any | None:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            try:\n                s = super().peek()\n            except AttributeError as ex:\n                raise NotImplementedError(\n                    \"The underlying queue class does not implement 'peek'\"\n                ) from ex\n            if s:\n                return deserialize(s)\n            return None\n\n    return SerializableQueue\n\n\ndef _scrapy_serialization_queue(\n    queue_class: type[queue.BaseQueue],\n) -> type[queue.BaseQueue]:\n    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n        def __init__(self, crawler: Crawler, key: str):\n            self.spider = crawler.spider\n            super().__init__(key)\n\n        @classmethod\n        def from_crawler(\n            cls, crawler: Crawler, key: str, *args: Any, **kwargs: Any\n        ) -> Self:\n            return cls(crawler, key)\n\n        def push(self, request: Request) -> None:\n            request_dict = request.to_dict(spider=self.spider)\n            super().push(request_dict)\n\n        def pop(self) -> Request | None:\n            request = super().pop()\n            if not request:\n                return None\n            return request_from_dict(request, spider=self.spider)\n\n        def peek(self) -> Request | None:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            request = super().peek()\n            if not request:\n                return None\n            return request_from_dict(request, spider=self.spider)\n\n    return ScrapyRequestQueue\n\n\ndef _scrapy_non_serialization_queue(\n    queue_class: type[queue.BaseQueue],\n) -> type[queue.BaseQueue]:\n    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n        @classmethod\n        def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n            return cls()\n\n        def peek(self) -> Any | None:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            try:\n                s = super().peek()\n            except AttributeError as ex:\n                raise NotImplementedError(\n                    \"The underlying queue class does not implement 'peek'\"\n                ) from ex\n            return s\n\n    return ScrapyRequestQueue\n\n", "n_tokens": 936, "byte_len": 4407, "file_sha1": "8c968e537e6195d6443dfc14a8e498d7d8f76fe8", "start_line": 1, "end_line": 138}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/squeues.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/squeues.py", "rel_path": "scrapy/squeues.py", "module": "scrapy.squeues", "ext": "py", "chunk_number": 2, "symbols": ["_pickle_serialize", "dumps", "protocol", "pickling", "error", "subclasses", "loads", "except", "scrapy", "non", "value", "return", "selector", "marshal", "fifo", "pickle", "serialize", "with", "mkdir", "classes", "lifo", "disk", "ignore", "queue", "type", "memory", "from", "parsel", "both", "aren", "_with_mkdir", "__init__", "_serializable_queue", "push", "pop", "peek", "_scrapy_serialization_queue", "from_crawler", "_scrapy_non_serialization_queue", "DirectoriesCreated", "SerializableQueue", "ScrapyRequestQueue", "does", "method", "queues", "raises", "python", "directories", "created", "future"], "ast_kind": "function_or_method", "text": "def _pickle_serialize(obj: Any) -> bytes:\n    try:\n        return pickle.dumps(obj, protocol=4)\n    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n    # TypeError is raised from parsel.Selector\n    except (pickle.PicklingError, AttributeError, TypeError) as e:\n        raise ValueError(str(e)) from e\n\n\n# queue.*Queue aren't subclasses of queue.BaseQueue\n_PickleFifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.FifoDiskQueue),  # type: ignore[arg-type]\n    _pickle_serialize,\n    pickle.loads,\n)\n_PickleLifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.LifoDiskQueue),  # type: ignore[arg-type]\n    _pickle_serialize,\n    pickle.loads,\n)\n_MarshalFifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.FifoDiskQueue),  # type: ignore[arg-type]\n    marshal.dumps,\n    marshal.loads,\n)\n_MarshalLifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.LifoDiskQueue),  # type: ignore[arg-type]\n    marshal.dumps,\n    marshal.loads,\n)\n\n# public queue classes\nPickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQueue)\nPickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)\nMarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)\nMarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)\nFifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)  # type: ignore[arg-type]\nLifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)  # type: ignore[arg-type]\n", "n_tokens": 397, "byte_len": 1605, "file_sha1": "8c968e537e6195d6443dfc14a8e498d7d8f76fe8", "start_line": 139, "end_line": 177}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/__main__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/__main__.py", "rel_path": "scrapy/__main__.py", "module": "scrapy.__main__", "ext": "py", "chunk_number": 1, "symbols": ["name", "cmdline", "execute", "from", "import", "main", "scrapy"], "ast_kind": "imports", "text": "from scrapy.cmdline import execute\n\nif __name__ == \"__main__\":\n    execute()\n", "n_tokens": 18, "byte_len": 77, "file_sha1": "503691a71c517691ba5e8b48412d5ac77a95481b", "start_line": 1, "end_line": 5}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/middleware.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/middleware.py", "rel_path": "scrapy/middleware.py", "module": "scrapy.middleware", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_spider", "_set_compat_spider", "_warn_spider_arg", "_get_mwlist_from_settings", "_build_from_settings", "from_settings", "from_crawler", "MiddlewareManager", "different", "build", "from", "qualname", "instance", "were", "future", "python", "spider", "async", "deprecated", "warn", "passed", "removed", "typ", "checking", "elif", "settings", "passing", "responsibility", "implementing", "_from_settings", "_add_middleware", "_check_mw_method_spider_arg", "open_spider", "close_spider", "method", "bool", "append", "enabled", "save", "methods", "requiring", "none", "argument", "required", "without", "type", "misc", "runtime", "error"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport pprint\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict, deque\nfrom typing import TYPE_CHECKING, Any, TypeVar, cast\n\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.utils.defer import ensure_awaitable\nfrom scrapy.utils.deprecate import argument_is_required\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import global_object_name\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable\n\n    from twisted.internet.defer import Deferred\n\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    # typing.Self requires Python 3.11\n    from typing_extensions import Concatenate, ParamSpec, Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings, Settings\n\n    _P = ParamSpec(\"_P\")\n\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass MiddlewareManager(ABC):\n    \"\"\"Base class for implementing middleware managers\"\"\"\n\n    component_name: str\n    _compat_spider: Spider | None = None\n\n    def __init__(self, *middlewares: Any, crawler: Crawler | None = None) -> None:\n        self.crawler: Crawler | None = crawler\n        if crawler is None:\n            warnings.warn(\n                f\"MiddlewareManager.__init__() was called without the crawler argument\"\n                f\" when creating {global_object_name(self.__class__)}.\"\n                f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n        self.middlewares: tuple[Any, ...] = middlewares\n        # Only process_spider_output and process_spider_exception can be None.\n        # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n        self.methods: dict[str, deque[Callable | tuple[Callable, Callable] | None]] = (\n            defaultdict(deque)\n        )\n        self._mw_methods_requiring_spider: set[Callable] = set()\n        for mw in middlewares:\n            self._add_middleware(mw)\n\n    @property\n    def _spider(self) -> Spider:\n        if self.crawler is not None:\n            if self.crawler.spider is None:\n                raise ValueError(\n                    f\"{type(self).__name__} needs to access self.crawler.spider but it is None.\"\n                )\n            return self.crawler.spider\n        if self._compat_spider is not None:\n            return self._compat_spider\n        raise ValueError(f\"{type(self).__name__} has no known Spider instance.\")\n\n    def _set_compat_spider(self, spider: Spider | None) -> None:\n        if spider is None or self.crawler is not None:\n            return\n        # printing a deprecation warning is the caller's responsibility\n        if self._compat_spider is None:\n            self._compat_spider = spider\n        elif self._compat_spider is not spider:\n            raise RuntimeError(\n                f\"Different instances of Spider were passed to {type(self).__name__}:\"\n                f\" {self._compat_spider} and {spider}\"\n            )\n\n    def _warn_spider_arg(self, method_name: str) -> None:\n        if self.crawler:\n            msg = (\n                f\"Passing a spider argument to {type(self).__name__}.{method_name}() is deprecated\"\n                \" and the passed value is ignored.\"\n            )\n        else:\n            msg = (\n                f\"Passing a spider argument to {type(self).__name__}.{method_name}() is deprecated,\"\n                f\" {type(self).__name__} should be instantiated with a Crawler instance instead.\"\n            )\n        warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=3)\n\n    @classmethod\n    @abstractmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n        if hasattr(objcls, \"from_settings\"):\n            instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n            method_name = \"from_settings\"\n        else:\n            instance = objcls()\n            method_name = \"__new__\"\n        if instance is None:\n            raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n        return cast(\"_T\", instance)\n\n    @classmethod\n    def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n        warnings.warn(\n            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return cls._from_settings(settings, crawler)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls._from_settings(crawler.settings, crawler)\n", "n_tokens": 1078, "byte_len": 4936, "file_sha1": "1fb1b838e267cf4fbac8b691f64a5c61b1537034", "start_line": 1, "end_line": 129}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/middleware.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/middleware.py", "rel_path": "scrapy/middleware.py", "module": "scrapy.middleware", "ext": "py", "chunk_number": 2, "symbols": ["_from_settings", "_add_middleware", "_check_mw_method_spider_arg", "open_spider", "close_spider", "method", "async", "bool", "build", "from", "qualname", "append", "instance", "future", "spider", "enabled", "deprecated", "passed", "removed", "save", "settings", "methods", "requiring", "none", "argument", "required", "callable", "access", "eargs", "always", "__init__", "_spider", "_set_compat_spider", "_warn_spider_arg", "_get_mwlist_from_settings", "_build_from_settings", "from_settings", "from_crawler", "MiddlewareManager", "different", "were", "python", "warn", "typ", "checking", "elif", "passing", "responsibility", "implementing", "without"], "ast_kind": "function_or_method", "text": "    @classmethod\n    def _from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                if crawler is not None:\n                    mw = build_from_crawler(mwcls, crawler)\n                else:\n                    mw = MiddlewareManager._build_from_settings(mwcls, settings)\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    logger.warning(\n                        \"Disabled %(clspath)s: %(eargs)s\",\n                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                        extra={\"crawler\": crawler},\n                    )\n\n        logger.info(\n            \"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n            {\n                \"componentname\": cls.component_name,\n                \"enabledlist\": pprint.pformat(enabled),\n            },\n            extra={\"crawler\": crawler},\n        )\n        return cls(*middlewares, crawler=crawler)\n\n    def _add_middleware(self, mw: Any) -> None:  # noqa: B027\n        pass\n\n    def _check_mw_method_spider_arg(self, method: Callable) -> None:\n        if argument_is_required(method, \"spider\"):\n            warnings.warn(\n                f\"{method.__qualname__}() requires a spider argument,\"\n                f\" this is deprecated and the argument will not be passed in future Scrapy versions.\"\n                f\" If you need to access the spider instance you can save the crawler instance\"\n                f\" passed to from_crawler() and use its spider attribute.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            self._mw_methods_requiring_spider.add(method)\n\n    async def _process_chain(\n        self,\n        methodname: str,\n        obj: _T,\n        *args: Any,\n        add_spider: bool = False,\n        always_add_spider: bool = False,\n    ) -> _T:\n        methods = cast(\n            \"Iterable[Callable[Concatenate[_T, _P], _T]]\", self.methods[methodname]\n        )\n        for method in methods:\n            if always_add_spider or (\n                add_spider and method in self._mw_methods_requiring_spider\n            ):\n                obj = await ensure_awaitable(method(obj, *(*args, self._spider)))\n            else:\n                obj = await ensure_awaitable(method(obj, *args))\n        return obj\n\n    def open_spider(\n        self, spider: Spider | None = None\n    ) -> Deferred[list[None]]:  # pragma: no cover\n        raise NotImplementedError(\n            \"MiddlewareManager.open_spider() is no longer implemented\"\n            \" and will be removed in a future Scrapy version.\"\n        )\n\n    def close_spider(\n        self, spider: Spider | None = None\n    ) -> Deferred[list[None]]:  # pragma: no cover\n        raise NotImplementedError(\n            \"MiddlewareManager.close_spider() is no longer implemented\"\n            \" and will be removed in a future Scrapy version.\"\n        )\n", "n_tokens": 679, "byte_len": 3175, "file_sha1": "1fb1b838e267cf4fbac8b691f64a5c61b1537034", "start_line": 130, "end_line": 212}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/item.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/item.py", "rel_path": "scrapy/item.py", "module": "scrapy.item", "ext": "py", "chunk_number": 1, "symbols": ["__new__", "__init__", "__getitem__", "__setitem__", "__delitem__", "__getattr__", "__setattr__", "__len__", "__iter__", "keys", "__repr__", "copy", "deepcopy", "Field", "ItemMeta", "Item", "processed", "does", "those", "library", "containing", "case", "python", "itemadapter", "name", "about", "declare", "passed", "future", "typ", "checking", "https", "elif", "definitions", "referring", "debug", "getitem", "isinstance", "items", "object", "item", "none", "common", "docs", "type", "setattr", "default", "metaclass", "memory", "delitem"], "ast_kind": "class_or_type", "text": "\"\"\"\nScrapy Item\n\nSee documentation in docs/topics/item.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABCMeta\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nfrom pprint import pformat\nfrom typing import TYPE_CHECKING, Any, NoReturn\n\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator, KeysView\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass Field(dict[str, Any]):\n    \"\"\"Container of field metadata\"\"\"\n\n\nclass ItemMeta(ABCMeta):\n    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n\n    .. _metaclass: https://realpython.com/python-metaclasses\n    \"\"\"\n\n    def __new__(\n        mcs, class_name: str, bases: tuple[type, ...], attrs: dict[str, Any]\n    ) -> ItemMeta:\n        classcell = attrs.pop(\"__classcell__\", None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n        _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n\n        fields = getattr(_class, \"fields\", {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs[\"fields\"] = fields\n        new_attrs[\"_class\"] = _class\n        if classcell is not None:\n            new_attrs[\"__classcell__\"] = classcell\n        return super().__new__(mcs, class_name, bases, new_attrs)\n\n\nclass Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n    \"\"\"Base class for scraped items.\n\n    In Scrapy, an object is considered an ``item`` if it's supported by the\n    `itemadapter`_ library. For example, when the output of a spider callback\n    is evaluated, only such objects are passed to :ref:`item pipelines\n    <topics-item-pipeline>`. :class:`Item` is one of the classes supported by\n    `itemadapter`_ by default.\n\n    Items must declare :class:`Field` attributes, which are processed and stored\n    in the ``fields`` attribute. This restricts the set of allowed field names\n    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n    Additionally, fields can be used to define metadata and control the way\n    data is processed internally. Please refer to the :ref:`documentation\n    about fields <topics-items-fields>` for additional information.\n\n    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n\n    .. _itemadapter: https://github.com/scrapy/itemadapter\n    \"\"\"\n\n    #: A dictionary containing *all declared fields* for this Item, not only\n    #: those populated. The keys are the field names and the values are the\n    #: :class:`Field` objects used in the :ref:`Item declaration\n    #: <topics-items-declaring>`.\n    fields: dict[str, Field]\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        self._values: dict[str, Any] = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in dict(*args, **kwargs).items():\n                self[k] = v\n\n    def __getitem__(self, key: str) -> Any:\n        return self._values[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n\n    def __delitem__(self, key: str) -> None:\n        del self._values[key]\n\n    def __getattr__(self, name: str) -> NoReturn:\n        if name in self.fields:\n            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n        raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        if not name.startswith(\"_\"):\n            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n        super().__setattr__(name, value)\n\n    def __len__(self) -> int:\n        return len(self._values)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._values)\n\n    __hash__ = object_ref.__hash__\n\n    def keys(self) -> KeysView[str]:\n        return self._values.keys()\n\n    def __repr__(self) -> str:\n        return pformat(dict(self))\n\n    def copy(self) -> Self:\n        return self.__class__(self)\n\n    def deepcopy(self) -> Self:\n        \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n        return deepcopy(self)\n", "n_tokens": 1086, "byte_len": 4466, "file_sha1": "647410b8081fda9c2c2e45e2991e7c448cf2e679", "start_line": 1, "end_line": 133}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py", "rel_path": "scrapy/crawler.py", "module": "scrapy.crawler", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_update_root_log_handler", "_apply_settings", "Crawler", "spidercls", "bool", "spider", "loader", "log", "formatter", "signal", "load", "settings", "after", "future", "typ", "checking", "signalmanager", "contextlib", "install", "scrapy", "get", "reques", "fingerprinte", "stats", "collector", "imports", "cls", "isinstance", "execution", "crawl", "_create_spider", "_create_engine", "stop", "_get_component", "get_addon", "get_downloader_middleware", "get_extension", "get_item_pipeline", "get_spider_middleware", "crawlers", "create_crawler", "_create_crawler", "_crawl", "join", "_done", "start", "_signal_shutdown", "_signal_kill", "_setup_reactor"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport logging\nimport pprint\nimport signal\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nfrom twisted.internet.defer import Deferred, DeferredList, inlineCallbacks\n\nfrom scrapy import Spider, signals\nfrom scrapy.addons import AddonManager\nfrom scrapy.core.engine import ExecutionEngine\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.extension import ExtensionManager\nfrom scrapy.settings import Settings, overridden_settings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.spiderloader import SpiderLoaderProtocol, get_spider_loader\nfrom scrapy.utils.asyncio import is_asyncio_available\nfrom scrapy.utils.defer import deferred_from_coro\nfrom scrapy.utils.log import (\n    LogCounterHandler,\n    configure_logging,\n    get_scrapy_root_handler,\n    install_scrapy_root_handler,\n    log_reactor_info,\n    log_scrapy_info,\n)\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.ossignal import install_shutdown_handlers, signal_names\nfrom scrapy.utils.reactor import (\n    _asyncio_reactor_path,\n    install_reactor,\n    is_asyncio_reactor_installed,\n    is_reactor_installed,\n    verify_installed_asyncio_event_loop,\n    verify_installed_reactor,\n)\n\nif TYPE_CHECKING:\n    from collections.abc import Awaitable, Generator, Iterable\n\n    from scrapy.logformatter import LogFormatter\n    from scrapy.statscollectors import StatsCollector\n    from scrapy.utils.request import RequestFingerprinterProtocol\n\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass Crawler:\n    def __init__(\n        self,\n        spidercls: type[Spider],\n        settings: dict[str, Any] | Settings | None = None,\n        init_reactor: bool = False,\n    ):\n        if isinstance(spidercls, Spider):\n            raise ValueError(\"The spidercls argument must be a class, not an object\")\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        self.spidercls: type[Spider] = spidercls\n        self.settings: Settings = settings.copy()\n        self.spidercls.update_settings(self.settings)\n        self._update_root_log_handler()\n\n        self.addons: AddonManager = AddonManager(self)\n        self.signals: SignalManager = SignalManager(self)\n\n        self._init_reactor: bool = init_reactor\n        self.crawling: bool = False\n        self._started: bool = False\n\n        self.extensions: ExtensionManager | None = None\n        self.stats: StatsCollector | None = None\n        self.logformatter: LogFormatter | None = None\n        self.request_fingerprinter: RequestFingerprinterProtocol | None = None\n        self.spider: Spider | None = None\n        self.engine: ExecutionEngine | None = None\n\n    def _update_root_log_handler(self) -> None:\n        if get_scrapy_root_handler() is not None:\n            # scrapy root handler already installed: update it with new settings\n            install_scrapy_root_handler(self.settings)\n\n    def _apply_settings(self) -> None:\n        if self.settings.frozen:\n            return\n\n        self.addons.load_settings(self.settings)\n        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n\n        handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n        logging.root.addHandler(handler)\n        # lambda is assigned to Crawler attribute because this way it is not\n        # garbage collected after leaving the scope\n        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n\n        lf_cls: type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n        self.logformatter = lf_cls.from_crawler(self)\n\n        self.request_fingerprinter = build_from_crawler(\n            load_object(self.settings[\"REQUEST_FINGERPRINTER_CLASS\"]),\n            self,\n        )\n\n        reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n        event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n        if self._init_reactor:\n            # this needs to be done after the spider settings are merged,\n            # but before something imports twisted.internet.reactor\n            if reactor_class:\n                install_reactor(reactor_class, event_loop)\n            else:\n                from twisted.internet import reactor  # noqa: F401\n        if reactor_class:\n            verify_installed_reactor(reactor_class)\n            if is_asyncio_reactor_installed() and event_loop:\n                verify_installed_asyncio_event_loop(event_loop)\n\n        if self._init_reactor or reactor_class:\n            log_reactor_info()\n\n        self.extensions = ExtensionManager.from_crawler(self)\n        self.settings.freeze()\n\n        d = dict(overridden_settings(self.settings))\n        logger.info(\n            \"Overridden settings:\\n%(settings)s\", {\"settings\": pprint.pformat(d)}\n        )\n", "n_tokens": 1028, "byte_len": 4972, "file_sha1": "e8a41eccf996268239584b66f29126a093c82760", "start_line": 1, "end_line": 139}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py", "rel_path": "scrapy/crawler.py", "module": "scrapy.crawler", "ext": "py", "chunk_number": 2, "symbols": ["crawl", "_create_spider", "_create_engine", "stop", "_get_component", "get_addon", "get_downloader_middleware", "get_extension", "method", "async", "spidercls", "signal", "instance", "subclass", "after", "spider", "deferred", "from", "deprecated", "more", "should", "create", "engine", "get", "extension", "isinstance", "execution", "than", "created", "none", "__init__", "_update_root_log_handler", "_apply_settings", "get_item_pipeline", "get_spider_middleware", "crawlers", "create_crawler", "_create_crawler", "_crawl", "join", "_done", "start", "_signal_shutdown", "_signal_kill", "_setup_reactor", "_stop_dfd", "_graceful_stop_reactor", "_stop_reactor", "Crawler", "CrawlerRunnerBase"], "ast_kind": "class_or_type", "text": "    # Cannot use @deferred_f_from_coro_f because that relies on the reactor\n    # being installed already, which is done within _apply_settings(), inside\n    # this method.\n    @inlineCallbacks\n    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred[Any], Any, None]:\n        \"\"\"Start the crawler by instantiating its spider class with the given\n        *args* and *kwargs* arguments, while setting the execution engine in\n        motion. Should be called only once.\n\n        Return a deferred that is fired when the crawl is finished.\n        \"\"\"\n        if self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        if self._started:\n            raise RuntimeError(\n                \"Cannot run Crawler.crawl() more than once on the same instance.\"\n            )\n        self.crawling = self._started = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self._apply_settings()\n            self._update_root_log_handler()\n            self.engine = self._create_engine()\n            yield deferred_from_coro(self.engine.open_spider_async())\n            yield deferred_from_coro(self.engine.start_async())\n        except Exception:\n            self.crawling = False\n            if self.engine is not None:\n                yield deferred_from_coro(self.engine.close_async())\n            raise\n\n    async def crawl_async(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Start the crawler by instantiating its spider class with the given\n        *args* and *kwargs* arguments, while setting the execution engine in\n        motion. Should be called only once.\n\n        .. versionadded:: VERSION\n\n        Complete when the crawl is finished.\n\n        This function requires\n        :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor` to be\n        installed.\n        \"\"\"\n        if not is_asyncio_available():\n            raise RuntimeError(\"Crawler.crawl_async() requires AsyncioSelectorReactor.\")\n        if self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        if self._started:\n            raise RuntimeError(\n                \"Cannot run Crawler.crawl_async() more than once on the same instance.\"\n            )\n        self.crawling = self._started = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self._apply_settings()\n            self._update_root_log_handler()\n            self.engine = self._create_engine()\n            await self.engine.open_spider_async()\n            await self.engine.start_async()\n        except Exception:\n            self.crawling = False\n            if self.engine is not None:\n                await self.engine.close_async()\n            raise\n\n    def _create_spider(self, *args: Any, **kwargs: Any) -> Spider:\n        return self.spidercls.from_crawler(self, *args, **kwargs)\n\n    def _create_engine(self) -> ExecutionEngine:\n        return ExecutionEngine(self, lambda _: self.stop_async())\n\n    def stop(self) -> Deferred[None]:\n        \"\"\"Start a graceful stop of the crawler and return a deferred that is\n        fired when the crawler is stopped.\"\"\"\n        warnings.warn(\n            \"Crawler.stop() is deprecated, use stop_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.stop_async())\n\n    async def stop_async(self) -> None:\n        \"\"\"Start a graceful stop of the crawler and complete when the crawler is stopped.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        if self.crawling:\n            self.crawling = False\n            assert self.engine\n            if self.engine.running:\n                await self.engine.stop_async()\n\n    @staticmethod\n    def _get_component(\n        component_class: type[_T], components: Iterable[Any]\n    ) -> _T | None:\n        for component in components:\n            if isinstance(component, component_class):\n                return component\n        return None\n\n    def get_addon(self, cls: type[_T]) -> _T | None:\n        \"\"\"Return the run-time instance of an :ref:`add-on <topics-addons>` of\n        the specified class or a subclass, or ``None`` if none is found.\n\n        .. versionadded:: 2.12\n        \"\"\"\n        return self._get_component(cls, self.addons.addons)\n\n    def get_downloader_middleware(self, cls: type[_T]) -> _T | None:\n        \"\"\"Return the run-time instance of a :ref:`downloader middleware\n        <topics-downloader-middleware>` of the specified class or a subclass,\n        or ``None`` if none is found.\n\n        .. versionadded:: 2.12\n\n        This method can only be called after the crawl engine has been created,\n        e.g. at signals :signal:`engine_started` or :signal:`spider_opened`.\n        \"\"\"\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_downloader_middleware() can only be called after \"\n                \"the crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n\n    def get_extension(self, cls: type[_T]) -> _T | None:\n        \"\"\"Return the run-time instance of an :ref:`extension\n        <topics-extensions>` of the specified class or a subclass,\n        or ``None`` if none is found.\n\n        .. versionadded:: 2.12\n\n        This method can only be called after the extension manager has been\n        created, e.g. at signals :signal:`engine_started` or\n        :signal:`spider_opened`.\n        \"\"\"\n        if not self.extensions:\n            raise RuntimeError(\n                \"Crawler.get_extension() can only be called after the \"\n                \"extension manager has been created.\"\n            )\n        return self._get_component(cls, self.extensions.middlewares)\n", "n_tokens": 1225, "byte_len": 5784, "file_sha1": "e8a41eccf996268239584b66f29126a093c82760", "start_line": 140, "end_line": 286}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py", "rel_path": "scrapy/crawler.py", "module": "scrapy.crawler", "ext": "py", "chunk_number": 3, "symbols": ["get_item_pipeline", "get_spider_middleware", "__init__", "crawlers", "create_crawler", "_create_crawler", "crawl", "CrawlerRunnerBase", "CrawlerRunner", "method", "spidercls", "spider", "loader", "managed", "signal", "instance", "itemproc", "load", "pre", "subclass", "after", "crawler", "runner", "name", "string", "get", "responsible", "settings", "isinstance", "process", "_update_root_log_handler", "_apply_settings", "_create_spider", "_create_engine", "stop", "_get_component", "get_addon", "get_downloader_middleware", "get_extension", "_crawl", "join", "_done", "start", "_signal_shutdown", "_signal_kill", "_setup_reactor", "_stop_dfd", "_graceful_stop_reactor", "_stop_reactor", "Crawler"], "ast_kind": "class_or_type", "text": "    def get_item_pipeline(self, cls: type[_T]) -> _T | None:\n        \"\"\"Return the run-time instance of a :ref:`item pipeline\n        <topics-item-pipeline>` of the specified class or a subclass, or\n        ``None`` if none is found.\n\n        .. versionadded:: 2.12\n\n        This method can only be called after the crawl engine has been created,\n        e.g. at signals :signal:`engine_started` or :signal:`spider_opened`.\n        \"\"\"\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_item_pipeline() can only be called after the \"\n                \"crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n\n    def get_spider_middleware(self, cls: type[_T]) -> _T | None:\n        \"\"\"Return the run-time instance of a :ref:`spider middleware\n        <topics-spider-middleware>` of the specified class or a subclass, or\n        ``None`` if none is found.\n\n        .. versionadded:: 2.12\n\n        This method can only be called after the crawl engine has been created,\n        e.g. at signals :signal:`engine_started` or :signal:`spider_opened`.\n        \"\"\"\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_spider_middleware() can only be called after the \"\n                \"crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.scraper.spidermw.middlewares)\n\n\nclass CrawlerRunnerBase(ABC):\n    def __init__(self, settings: dict[str, Any] | Settings | None = None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        AddonManager.load_pre_crawler_settings(settings)\n        self.settings: Settings = settings\n        self.spider_loader: SpiderLoaderProtocol = get_spider_loader(settings)\n        self._crawlers: set[Crawler] = set()\n        self.bootstrap_failed = False\n\n    @property\n    def crawlers(self) -> set[Crawler]:\n        \"\"\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by\n        :meth:`crawl` and managed by this class.\"\"\"\n        return self._crawlers\n\n    def create_crawler(\n        self, crawler_or_spidercls: type[Spider] | str | Crawler\n    ) -> Crawler:\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If ``crawler_or_spidercls`` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)\n\n    def _create_crawler(self, spidercls: str | type[Spider]) -> Crawler:\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)\n\n    @abstractmethod\n    def crawl(\n        self,\n        crawler_or_spidercls: type[Spider] | str | Crawler,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Awaitable[None]:\n        raise NotImplementedError\n\n\nclass CrawlerRunner(CrawlerRunnerBase):\n    \"\"\"\n    This is a convenient helper class that keeps track of, manages and runs\n    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n\n    The CrawlerRunner object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n\n    This class provides Deferred-based APIs. Use :class:`AsyncCrawlerRunner`\n    for modern coroutine APIs.\n    \"\"\"\n\n    def __init__(self, settings: dict[str, Any] | Settings | None = None):\n        super().__init__(settings)\n        self._active: set[Deferred[None]] = set()\n", "n_tokens": 994, "byte_len": 4316, "file_sha1": "e8a41eccf996268239584b66f29126a093c82760", "start_line": 287, "end_line": 394}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py", "rel_path": "scrapy/crawler.py", "module": "scrapy.crawler", "ext": "py", "chunk_number": 4, "symbols": ["crawl", "_crawl", "stop", "join", "__init__", "AsyncCrawlerRunner", "method", "initialize", "managed", "instance", "subclass", "crawler", "runner", "spider", "name", "spiders", "string", "spidercls", "responsible", "settings", "loop", "provided", "jobs", "isinstance", "process", "task", "object", "created", "none", "parameter", "_update_root_log_handler", "_apply_settings", "_create_spider", "_create_engine", "_get_component", "get_addon", "get_downloader_middleware", "get_extension", "get_item_pipeline", "get_spider_middleware", "crawlers", "create_crawler", "_create_crawler", "_done", "start", "_signal_shutdown", "_signal_kill", "_setup_reactor", "_stop_dfd", "_graceful_stop_reactor"], "ast_kind": "class_or_type", "text": "    def crawl(\n        self,\n        crawler_or_spidercls: type[Spider] | str | Crawler,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Deferred[None]:\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param args: arguments to initialize the spider\n\n        :param kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)\n\n    @inlineCallbacks\n    def _crawl(\n        self, crawler: Crawler, *args: Any, **kwargs: Any\n    ) -> Generator[Deferred[Any], Any, None]:\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n        try:\n            yield d\n        finally:\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n\n    def stop(self) -> Deferred[Any]:\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return DeferredList(deferred_from_coro(c.stop_async()) for c in self.crawlers)\n\n    @inlineCallbacks\n    def join(self) -> Generator[Deferred[Any], Any, None]:\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield DeferredList(self._active)\n\n\nclass AsyncCrawlerRunner(CrawlerRunnerBase):\n    \"\"\"\n    This is a convenient helper class that keeps track of, manages and runs\n    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n\n    The AsyncCrawlerRunner object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n\n    This class provides coroutine APIs. It requires\n    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n    \"\"\"\n\n    def __init__(self, settings: dict[str, Any] | Settings | None = None):\n        super().__init__(settings)\n        self._active: set[asyncio.Task[None]] = set()\n\n    def crawl(\n        self,\n        crawler_or_spidercls: type[Spider] | str | Crawler,\n        *args: Any,\n        **kwargs: Any,\n    ) -> asyncio.Task[None]:\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a :class:`~asyncio.Task` object which completes when the\n        crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param args: arguments to initialize the spider\n\n        :param kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\n                f\"{type(self).__name__} requires AsyncioSelectorReactor.\"\n            )\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)\n\n    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> asyncio.Task[None]:\n        # At this point the asyncio loop has been installed either by the user\n        # or by AsyncCrawlerProcess (but it isn't running yet, so no asyncio.create_task()).\n        loop = asyncio.get_event_loop()\n        self.crawlers.add(crawler)\n        task = loop.create_task(crawler.crawl_async(*args, **kwargs))\n        self._active.add(task)\n", "n_tokens": 1193, "byte_len": 5293, "file_sha1": "e8a41eccf996268239584b66f29126a093c82760", "start_line": 395, "end_line": 531}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py", "rel_path": "scrapy/crawler.py", "module": "scrapy.crawler", "ext": "py", "chunk_number": 5, "symbols": ["_done", "__init__", "start", "_signal_shutdown", "_signal_kill", "_setup_reactor", "_stop_dfd", "_graceful_stop_reactor", "_stop_reactor", "_create_crawler", "CrawlerProcessBase", "CrawlerProcess", "add", "done", "async", "spidercls", "starting", "bool", "send", "system", "your", "maxthreads", "managed", "signal", "setup", "reactor", "after", "crawler", "runner", "spider", "_update_root_log_handler", "_apply_settings", "crawl", "_create_spider", "_create_engine", "stop", "_get_component", "get_addon", "get_downloader_middleware", "get_extension", "get_item_pipeline", "get_spider_middleware", "crawlers", "create_crawler", "_crawl", "join", "Crawler", "CrawlerRunnerBase", "CrawlerRunner", "AsyncCrawlerRunner"], "ast_kind": "class_or_type", "text": "        def _done(_: asyncio.Task[None]) -> None:\n            self.crawlers.discard(crawler)\n            self._active.discard(task)\n            self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n\n        task.add_done_callback(_done)\n        return task\n\n    async def stop(self) -> None:\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Completes when they all have ended.\n        \"\"\"\n        if self.crawlers:\n            await asyncio.wait(\n                [asyncio.create_task(c.stop_async()) for c in self.crawlers]\n            )\n\n    async def join(self) -> None:\n        \"\"\"\n        Completes when all managed :attr:`crawlers` have completed their\n        executions.\n        \"\"\"\n        while self._active:\n            await asyncio.wait(self._active)\n\n\nclass CrawlerProcessBase(CrawlerRunnerBase):\n    def __init__(\n        self,\n        settings: dict[str, Any] | Settings | None = None,\n        install_root_handler: bool = True,\n    ):\n        super().__init__(settings)\n        configure_logging(self.settings, install_root_handler)\n        log_scrapy_info(self.settings)\n\n    @abstractmethod\n    def start(\n        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n    ) -> None:\n        raise NotImplementedError\n\n    def _signal_shutdown(self, signum: int, _: Any) -> None:\n        from twisted.internet import reactor\n\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\n            \"Received %(signame)s, shutting down gracefully. Send again to force \",\n            {\"signame\": signame},\n        )\n        reactor.callFromThread(self._graceful_stop_reactor)\n\n    def _signal_kill(self, signum: int, _: Any) -> None:\n        from twisted.internet import reactor\n\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info(\n            \"Received %(signame)s twice, forcing unclean shutdown\", {\"signame\": signame}\n        )\n        reactor.callFromThread(self._stop_reactor)\n\n    def _setup_reactor(self, install_signal_handlers: bool) -> None:\n        from twisted.internet import reactor\n\n        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n        # We pass self, which is CrawlerProcess, instead of Crawler here,\n        # which works because the default resolvers only use crawler.settings.\n        resolver = build_from_crawler(resolver_class, self, reactor=reactor)  # type: ignore[arg-type]\n        resolver.install_on_reactor()\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n        reactor.addSystemEventTrigger(\"before\", \"shutdown\", self._stop_dfd)\n        if install_signal_handlers:\n            reactor.addSystemEventTrigger(\n                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n            )\n\n    @abstractmethod\n    def _stop_dfd(self) -> Deferred[Any]:\n        raise NotImplementedError\n\n    @inlineCallbacks\n    def _graceful_stop_reactor(self) -> Generator[Deferred[Any], Any, None]:\n        try:\n            yield self._stop_dfd()\n        finally:\n            self._stop_reactor()\n\n    def _stop_reactor(self, _: Any = None) -> None:\n        from twisted.internet import reactor\n\n        # raised if already stopped or in shutdown stage\n        with contextlib.suppress(RuntimeError):\n            reactor.stop()\n\n\nclass CrawlerProcess(CrawlerProcessBase, CrawlerRunner):\n    \"\"\"\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n    signals, like the keyboard interrupt command Ctrl-C. It also configures\n    top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n    :mod:`~twisted.internet.reactor` within your application.\n\n    The CrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n\n    This class provides Deferred-based APIs. Use :class:`AsyncCrawlerProcess`\n    for modern coroutine APIs.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: dict[str, Any] | Settings | None = None,\n        install_root_handler: bool = True,\n    ):\n        super().__init__(settings, install_root_handler)\n        self._initialized_reactor: bool = False\n        logger.debug(\"Using CrawlerProcess\")\n\n    def _create_crawler(self, spidercls: type[Spider] | str) -> Crawler:\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        init_reactor = not self._initialized_reactor\n        self._initialized_reactor = True\n        return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n\n    def _stop_dfd(self) -> Deferred[Any]:\n        return self.stop()\n", "n_tokens": 1147, "byte_len": 5264, "file_sha1": "e8a41eccf996268239584b66f29126a093c82760", "start_line": 532, "end_line": 677}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/crawler.py", "rel_path": "scrapy/crawler.py", "module": "scrapy.crawler", "ext": "py", "chunk_number": 6, "symbols": ["start", "__init__", "_stop_dfd", "AsyncCrawlerProcess", "add", "done", "method", "starting", "bool", "your", "signal", "setup", "reactor", "after", "spiders", "correct", "responsible", "make", "deferreds", "debug", "settings", "loop", "handler", "process", "install", "root", "than", "object", "here", "none", "_update_root_log_handler", "_apply_settings", "crawl", "_create_spider", "_create_engine", "stop", "_get_component", "get_addon", "get_downloader_middleware", "get_extension", "get_item_pipeline", "get_spider_middleware", "crawlers", "create_crawler", "_create_crawler", "_crawl", "join", "_done", "_signal_shutdown", "_signal_kill"], "ast_kind": "class_or_type", "text": "    def start(\n        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n    ) -> None:\n        \"\"\"\n        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param bool stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n\n        :param bool install_signal_handlers: whether to install the OS signal\n            handlers from Twisted and Scrapy (default: True)\n        \"\"\"\n        from twisted.internet import reactor\n\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(self._stop_reactor)\n\n        self._setup_reactor(install_signal_handlers)\n        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n\n\nclass AsyncCrawlerProcess(CrawlerProcessBase, AsyncCrawlerRunner):\n    \"\"\"\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.AsyncCrawlerRunner` by adding support\n    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n    signals, like the keyboard interrupt command Ctrl-C. It also configures\n    top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.AsyncCrawlerRunner` if you aren't running another\n    :mod:`~twisted.internet.reactor` within your application.\n\n    The AsyncCrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n\n    This class provides coroutine APIs. It requires\n    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: dict[str, Any] | Settings | None = None,\n        install_root_handler: bool = True,\n    ):\n        super().__init__(settings, install_root_handler)\n        logger.debug(\"Using AsyncCrawlerProcess\")\n        # We want the asyncio event loop to be installed early, so that it's\n        # always the correct one. And as we do that, we can also install the\n        # reactor here.\n        # The ASYNCIO_EVENT_LOOP setting cannot be overridden by add-ons and\n        # spiders when using AsyncCrawlerProcess.\n        loop_path = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n        if is_reactor_installed():\n            # The user could install a reactor before this class is instantiated.\n            # We need to make sure the reactor is the correct one and the loop\n            # type matches the setting.\n            verify_installed_reactor(_asyncio_reactor_path)\n            if loop_path:\n                verify_installed_asyncio_event_loop(loop_path)\n        else:\n            install_reactor(_asyncio_reactor_path, loop_path)\n        self._initialized_reactor = True\n\n    def _stop_dfd(self) -> Deferred[Any]:\n        return deferred_from_coro(self.stop())\n\n    def start(\n        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n    ) -> None:\n        \"\"\"\n        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param bool stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n\n        :param bool install_signal_handlers: whether to install the OS signal\n            handlers from Twisted and Scrapy (default: True)\n        \"\"\"\n        from twisted.internet import reactor\n\n        if stop_after_crawl:\n            loop = asyncio.get_event_loop()\n            join_task = loop.create_task(self.join())\n            join_task.add_done_callback(self._stop_reactor)\n\n        self._setup_reactor(install_signal_handlers)\n        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n", "n_tokens": 1000, "byte_len": 4620, "file_sha1": "e8a41eccf996268239584b66f29126a093c82760", "start_line": 678, "end_line": 788}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/linkextractors/lxmlhtml.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/linkextractors/lxmlhtml.py", "rel_path": "scrapy/linkextractors/lxmlhtml.py", "module": "scrapy.linkextractors.lxmlhtml", "ext": "py", "chunk_number": 1, "symbols": ["_nons", "_identity", "_canonicalize_link_url", "__init__", "_iter_links", "_extract_links", "extract_links", "_process_links", "_deduplicate_if_needed", "LxmlParserLinkExtractor", "LxmlLinkExtractor", "encoding", "skipping", "html", "element", "bool", "xhtm", "namespace", "translator", "scan", "tag", "append", "unique", "list", "subclass", "lib", "w3lib", "extraction", "after", "selector", "_compile_regexes", "_link_allowed", "matches", "method", "css", "xpath", "duplicate", "lxml", "link", "doesn", "bogus", "passed", "future", "typ", "checking", "string", "debug", "settings", "href", "document"], "ast_kind": "class_or_type", "text": "\"\"\"\nLink extractor based on lxml.html\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport operator\nimport re\nfrom collections.abc import Callable, Iterable\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any, Union, cast\nfrom urllib.parse import urljoin, urlparse\n\nfrom lxml import etree\nfrom parsel.csstranslator import HTMLTranslator\nfrom w3lib.html import strip_html5_whitespace\nfrom w3lib.url import canonicalize_url, safe_url_string\n\nfrom scrapy.link import Link\nfrom scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches\nfrom scrapy.utils.misc import arg_to_iter, rel_has_nofollow\nfrom scrapy.utils.python import unique as unique_list\nfrom scrapy.utils.response import get_base_url\nfrom scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n\nif TYPE_CHECKING:\n    from lxml.html import HtmlElement\n\n    from scrapy import Selector\n    from scrapy.http import TextResponse\n\n\nlogger = logging.getLogger(__name__)\n\n# from lxml/src/lxml/html/__init__.py\nXHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n\n_collect_string_content = etree.XPath(\"string()\")\n\n\ndef _nons(tag: Any) -> Any:\n    if (\n        isinstance(tag, str)\n        and tag[0] == \"{\"\n        and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE\n    ):\n        return tag.split(\"}\")[-1]\n    return tag\n\n\ndef _identity(x: Any) -> Any:\n    return x\n\n\ndef _canonicalize_link_url(link: Link) -> str:\n    return canonicalize_url(link.url, keep_fragments=True)\n\n\nclass LxmlParserLinkExtractor:\n    def __init__(\n        self,\n        tag: str | Callable[[str], bool] = \"a\",\n        attr: str | Callable[[str], bool] = \"href\",\n        process: Callable[[Any], Any] | None = None,\n        unique: bool = False,\n        strip: bool = True,\n        canonicalized: bool = False,\n    ):\n        # mypy doesn't infer types for operator.* and also for partial()\n        self.scan_tag: Callable[[str], bool] = (\n            tag\n            if callable(tag)\n            else cast(\"Callable[[str], bool]\", partial(operator.eq, tag))\n        )\n        self.scan_attr: Callable[[str], bool] = (\n            attr\n            if callable(attr)\n            else cast(\"Callable[[str], bool]\", partial(operator.eq, attr))\n        )\n        self.process_attr: Callable[[Any], Any] = (\n            process if callable(process) else _identity\n        )\n        self.unique: bool = unique\n        self.strip: bool = strip\n        self.link_key: Callable[[Link], str] = (\n            cast(\"Callable[[Link], str]\", operator.attrgetter(\"url\"))\n            if canonicalized\n            else _canonicalize_link_url\n        )\n\n    def _iter_links(\n        self, document: HtmlElement\n    ) -> Iterable[tuple[HtmlElement, str, str]]:\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield el, attrib, attribs[attrib]\n\n    def _extract_links(\n        self,\n        selector: Selector,\n        response_url: str,\n        response_encoding: str,\n        base_url: str,\n    ) -> list[Link]:\n        links: list[Link] = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                if self.strip:\n                    attr_val = strip_html5_whitespace(attr_val)\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue  # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            try:\n                url = safe_url_string(url, encoding=response_encoding)\n            except ValueError:\n                logger.debug(f\"Skipping extraction of link with bad URL {url!r}\")\n                continue\n\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(\n                url,\n                _collect_string_content(el) or \"\",\n                nofollow=rel_has_nofollow(el.get(\"rel\")),\n            )\n            links.append(link)\n        return self._deduplicate_if_needed(links)\n\n    def extract_links(self, response: TextResponse) -> list[Link]:\n        base_url = get_base_url(response)\n        return self._extract_links(\n            response.selector, response.url, response.encoding, base_url\n        )\n\n    def _process_links(self, links: list[Link]) -> list[Link]:\n        \"\"\"Normalize and filter extracted links\n\n        The subclass should override it if necessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)\n\n    def _deduplicate_if_needed(self, links: list[Link]) -> list[Link]:\n        if self.unique:\n            return unique_list(links, key=self.link_key)\n        return links\n\n\n_RegexT = Union[str, re.Pattern[str]]\n_RegexOrSeveralT = Union[_RegexT, Iterable[_RegexT]]\n\n\nclass LxmlLinkExtractor:\n    _csstranslator = HTMLTranslator()\n", "n_tokens": 1141, "byte_len": 5192, "file_sha1": "9557f0a1eeb2a88bc5be55227d9f915768ea5ea5", "start_line": 1, "end_line": 166}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/linkextractors/lxmlhtml.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/linkextractors/lxmlhtml.py", "rel_path": "scrapy/linkextractors/lxmlhtml.py", "module": "scrapy.linkextractors.lxmlhtml", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "_compile_regexes", "_link_allowed", "matches", "_process_links", "_extract_links", "extract_links", "encoding", "method", "bool", "xpath", "css", "unique", "list", "duplicate", "passed", "settings", "href", "deny", "res", "canonicalize", "extensions", "isinstance", "process", "selector", "denied", "none", "returns", "base", "url", "_nons", "_identity", "_canonicalize_link_url", "_iter_links", "_deduplicate_if_needed", "LxmlParserLinkExtractor", "LxmlLinkExtractor", "skipping", "html", "element", "xhtm", "namespace", "translator", "scan", "tag", "append", "subclass", "lib", "w3lib", "extraction"], "ast_kind": "function_or_method", "text": "    def __init__(\n        self,\n        allow: _RegexOrSeveralT = (),\n        deny: _RegexOrSeveralT = (),\n        allow_domains: str | Iterable[str] = (),\n        deny_domains: str | Iterable[str] = (),\n        restrict_xpaths: str | Iterable[str] = (),\n        tags: str | Iterable[str] = (\"a\", \"area\"),\n        attrs: str | Iterable[str] = (\"href\",),\n        canonicalize: bool = False,\n        unique: bool = True,\n        process_value: Callable[[Any], Any] | None = None,\n        deny_extensions: str | Iterable[str] | None = None,\n        restrict_css: str | Iterable[str] = (),\n        strip: bool = True,\n        restrict_text: _RegexOrSeveralT | None = None,\n    ):\n        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n        self.link_extractor = LxmlParserLinkExtractor(\n            tag=partial(operator.contains, tags),\n            attr=partial(operator.contains, attrs),\n            unique=unique,\n            process=process_value,\n            strip=strip,\n            canonicalized=not canonicalize,\n        )\n        self.allow_res: list[re.Pattern[str]] = self._compile_regexes(allow)\n        self.deny_res: list[re.Pattern[str]] = self._compile_regexes(deny)\n\n        self.allow_domains: set[str] = set(arg_to_iter(allow_domains))\n        self.deny_domains: set[str] = set(arg_to_iter(deny_domains))\n\n        self.restrict_xpaths: tuple[str, ...] = tuple(arg_to_iter(restrict_xpaths))\n        self.restrict_xpaths += tuple(\n            map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n        )\n\n        if deny_extensions is None:\n            deny_extensions = IGNORED_EXTENSIONS\n        self.canonicalize: bool = canonicalize\n        self.deny_extensions: set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n        self.restrict_text: list[re.Pattern[str]] = self._compile_regexes(restrict_text)\n\n    @staticmethod\n    def _compile_regexes(value: _RegexOrSeveralT | None) -> list[re.Pattern[str]]:\n        return [\n            x if isinstance(x, re.Pattern) else re.compile(x)\n            for x in arg_to_iter(value)\n        ]\n\n    def _link_allowed(self, link: Link) -> bool:\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(\n            parsed_url, self.allow_domains\n        ):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(\n            parsed_url, self.deny_extensions\n        ):\n            return False\n        return not self.restrict_text or _matches(link.text, self.restrict_text)\n\n    def matches(self, url: str) -> bool:\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (\n            (regex.search(url) for regex in self.allow_res)\n            if self.allow_res\n            else [True]\n        )\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)\n\n    def _process_links(self, links: list[Link]) -> list[Link]:\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(link.url)\n        return self.link_extractor._process_links(links)\n\n    def _extract_links(self, *args: Any, **kwargs: Any) -> list[Link]:\n        return self.link_extractor._extract_links(*args, **kwargs)\n\n    def extract_links(self, response: TextResponse) -> list[Link]:\n        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n        specified :class:`response <scrapy.http.Response>`.\n\n        Only links that match the settings passed to the ``__init__`` method of\n        the link extractor are returned.\n\n        Duplicate links are omitted if the ``unique`` attribute is set to ``True``,\n        otherwise they are returned.\n        \"\"\"\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [\n                subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)\n            ]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        if self.link_extractor.unique:\n            return unique_list(all_links, key=self.link_extractor.link_key)\n        return all_links\n", "n_tokens": 1109, "byte_len": 4919, "file_sha1": "9557f0a1eeb2a88bc5be55227d9f915768ea5ea5", "start_line": 167, "end_line": 285}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/linkextractors/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/linkextractors/__init__.py", "rel_path": "scrapy/linkextractors/__init__.py", "module": "scrapy.linkextractors.__init__", "ext": "py", "chunk_number": 1, "symbols": ["_matches", "_is_valid_url", "pattern", "bool", "xlsm", "images", "docb", "followed", "tiff", "dotm", "potm", "typing", "potx", "pptx", "return", "jpeg", "xltx", "lxml", "link", "matches", "annotations", "video", "scrapy", "extractors", "more", "future", "typ", "checking", "collections", "links", "ignore", "extensions", "https", "all", "topics", "archives", "webp", "docx", "other", "info", "valid", "url", "lxmlhtml", "webm", "from", "they", "xlsx", "pptm", "regexs", "imports"], "ast_kind": "function_or_method", "text": "\"\"\"\nscrapy.linkextractors\n\nThis package contains a collection of Link Extractors.\n\nFor more info see docs/topics/link-extractors.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n    from re import Pattern\n\n# common file extensions that are not followed if they occur in links\nIGNORED_EXTENSIONS = [\n    # archives\n    \"7z\",\n    \"7zip\",\n    \"bz2\",\n    \"rar\",\n    \"tar\",\n    \"tar.gz\",\n    \"xz\",\n    \"zip\",\n    # images\n    \"mng\",\n    \"pct\",\n    \"bmp\",\n    \"gif\",\n    \"jpg\",\n    \"jpeg\",\n    \"png\",\n    \"pst\",\n    \"psp\",\n    \"tif\",\n    \"tiff\",\n    \"ai\",\n    \"drw\",\n    \"dxf\",\n    \"eps\",\n    \"ps\",\n    \"svg\",\n    \"cdr\",\n    \"ico\",\n    \"webp\",\n    # audio\n    \"mp3\",\n    \"wma\",\n    \"ogg\",\n    \"wav\",\n    \"ra\",\n    \"aac\",\n    \"mid\",\n    \"au\",\n    \"aiff\",\n    # video\n    \"3gp\",\n    \"asf\",\n    \"asx\",\n    \"avi\",\n    \"mov\",\n    \"mp4\",\n    \"mpg\",\n    \"qt\",\n    \"rm\",\n    \"swf\",\n    \"wmv\",\n    \"m4a\",\n    \"m4v\",\n    \"flv\",\n    \"webm\",\n    # office suites\n    \"xls\",\n    \"xlsm\",\n    \"xlsx\",\n    \"xltm\",\n    \"xltx\",\n    \"potm\",\n    \"potx\",\n    \"ppt\",\n    \"pptm\",\n    \"pptx\",\n    \"pps\",\n    \"doc\",\n    \"docb\",\n    \"docm\",\n    \"docx\",\n    \"dotm\",\n    \"dotx\",\n    \"odt\",\n    \"ods\",\n    \"odg\",\n    \"odp\",\n    # other\n    \"css\",\n    \"pdf\",\n    \"exe\",\n    \"bin\",\n    \"rss\",\n    \"dmg\",\n    \"iso\",\n    \"apk\",\n    \"jar\",\n    \"sh\",\n    \"rb\",\n    \"js\",\n    \"hta\",\n    \"bat\",\n    \"cpl\",\n    \"msi\",\n    \"msp\",\n    \"py\",\n]\n\n\ndef _matches(url: str, regexs: Iterable[Pattern[str]]) -> bool:\n    return any(r.search(url) for r in regexs)\n\n\ndef _is_valid_url(url: str) -> bool:\n    return url.split(\"://\", 1)[0] in {\"http\", \"https\", \"file\", \"ftp\"}\n\n\n# Top-level imports\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n\n__all__ = [\n    \"IGNORED_EXTENSIONS\",\n    \"LinkExtractor\",\n]\n", "n_tokens": 627, "byte_len": 1868, "file_sha1": "1f513eaf97aae7f0156af1b92e12554abecf2c96", "start_line": 1, "end_line": 134}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/cookies.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/cookies.py", "rel_path": "scrapy/downloadermiddlewares/cookies.py", "module": "scrapy.downloadermiddlewares.cookies", "ext": "py", "chunk_number": 1, "symbols": ["_is_public_domain", "__init__", "from_crawler", "_process_cookies", "process_request", "process_response", "_debug_cookie", "_debug_set_cookie", "CookiesMiddleware", "bool", "cookies", "python", "spider", "domain", "split", "warn", "future", "typ", "checking", "urlparse", "cached", "cookiejarkey", "unset", "debug", "settings", "unicode", "lower", "extract", "object", "none", "_format_cookie", "_get_request_cookies", "decoded", "latin", "latin1", "name", "string", "https", "missing", "cookie", "str", "encoded", "isinstance", "items", "get", "request", "join", "verbose", "http", "sequence"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Any\n\nfrom tldextract import TLDExtract\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.http.cookies import CookieJar\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n    from http.cookiejar import Cookie\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http.request import VerboseCookie\n\n\nlogger = logging.getLogger(__name__)\n\n\n_split_domain = TLDExtract(include_psl_private_domains=True)\n_UNSET = object()\n\n\ndef _is_public_domain(domain: str) -> bool:\n    parts = _split_domain(domain)\n    return not parts.domain\n\n\nclass CookiesMiddleware:\n    \"\"\"This middleware enables working with sites that need cookies\"\"\"\n\n    crawler: Crawler\n\n    def __init__(self, debug: bool = False):\n        self.jars: defaultdict[Any, CookieJar] = defaultdict(CookieJar)\n        self.debug: bool = debug\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"COOKIES_ENABLED\"):\n            raise NotConfigured\n        o = cls(crawler.settings.getbool(\"COOKIES_DEBUG\"))\n        o.crawler = crawler\n        return o\n\n    def _process_cookies(\n        self, cookies: Iterable[Cookie], *, jar: CookieJar, request: Request\n    ) -> None:\n        for cookie in cookies:\n            cookie_domain = cookie.domain\n            cookie_domain = cookie_domain.removeprefix(\".\")\n\n            hostname = urlparse_cached(request).hostname\n            assert hostname is not None\n            request_domain = hostname.lower()\n\n            if cookie_domain and _is_public_domain(cookie_domain):\n                if cookie_domain != request_domain:\n                    continue\n                cookie.domain = request_domain\n\n            jar.set_cookie_if_ok(cookie, request)\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        if request.meta.get(\"dont_merge_cookies\", False):\n            return None\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        self._process_cookies(cookies, jar=jar, request=request)\n\n        # set Cookie header\n        request.headers.pop(\"Cookie\", None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request)\n        return None\n\n    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        if request.meta.get(\"dont_merge_cookies\", False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = jar.make_cookies(response, request)\n        self._process_cookies(cookies, jar=jar, request=request)\n\n        self._debug_set_cookie(response)\n\n        return response\n\n    def _debug_cookie(self, request: Request) -> None:\n        if self.debug:\n            cl = [\n                to_unicode(c, errors=\"replace\")\n                for c in request.headers.getlist(\"Cookie\")\n            ]\n            if cl:\n                cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n                msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                logger.debug(msg, extra={\"spider\": self.crawler.spider})\n\n    def _debug_set_cookie(self, response: Response) -> None:\n        if self.debug:\n            cl = [\n                to_unicode(c, errors=\"replace\")\n                for c in response.headers.getlist(\"Set-Cookie\")\n            ]\n            if cl:\n                cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n                msg = f\"Received cookies from: {response}\\n{cookies}\"\n                logger.debug(msg, extra={\"spider\": self.crawler.spider})\n", "n_tokens": 917, "byte_len": 4263, "file_sha1": "cd70415379d587dbdb1dd5605fba74a69f6dd524", "start_line": 1, "end_line": 131}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/cookies.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/cookies.py", "rel_path": "scrapy/downloadermiddlewares/cookies.py", "module": "scrapy.downloadermiddlewares.cookies", "ext": "py", "chunk_number": 2, "symbols": ["_format_cookie", "_get_request_cookies", "utf", "utf8", "necessary", "given", "representation", "make", "cookies", "bool", "unicode", "decode", "except", "setdefault", "decoded", "errors", "latin", "latin1", "return", "dict", "extract", "name", "domain", "replace", "consisting", "warning", "path", "logger", "continue", "string", "_is_public_domain", "__init__", "from_crawler", "_process_cookies", "process_request", "process_response", "_debug_cookie", "_debug_set_cookie", "CookiesMiddleware", "python", "spider", "split", "warn", "future", "typ", "checking", "https", "urlparse", "cached", "missing"], "ast_kind": "function_or_method", "text": "    def _format_cookie(self, cookie: VerboseCookie, request: Request) -> str | None:\n        \"\"\"\n        Given a dict consisting of cookie components, return its string representation.\n        Decode from bytes if necessary.\n        \"\"\"\n        decoded = {}\n        flags = set()\n        for key in (\"name\", \"value\", \"path\", \"domain\"):\n            value = cookie.get(key)\n            if value is None:\n                if key in (\"name\", \"value\"):\n                    msg = f\"Invalid cookie found in request {request}: {cookie} ('{key}' is missing)\"\n                    logger.warning(msg)\n                    return None\n                continue\n            if isinstance(value, (bool, float, int, str)):\n                decoded[key] = str(value)\n            else:\n                assert isinstance(value, bytes)\n                try:\n                    decoded[key] = value.decode(\"utf8\")\n                except UnicodeDecodeError:\n                    logger.warning(\n                        \"Non UTF-8 encoded cookie found in request %s: %s\",\n                        request,\n                        cookie,\n                    )\n                    decoded[key] = value.decode(\"latin1\", errors=\"replace\")\n        for flag in (\"secure\",):\n            value = cookie.get(flag, _UNSET)\n            if value is _UNSET or not value:\n                continue\n            flags.add(flag)\n        cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n        for key, value in decoded.items():  # path, domain\n            cookie_str += f\"; {key.capitalize()}={value}\"\n        for flag in flags:  # secure\n            cookie_str += f\"; {flag.capitalize()}\"\n        return cookie_str\n\n    def _get_request_cookies(\n        self, jar: CookieJar, request: Request\n    ) -> Sequence[Cookie]:\n        \"\"\"\n        Extract cookies from the Request.cookies attribute\n        \"\"\"\n        if not request.cookies:\n            return []\n        cookies: Iterable[VerboseCookie]\n        if isinstance(request.cookies, dict):\n            cookies = tuple({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n        else:\n            cookies = request.cookies\n        for cookie in cookies:\n            cookie.setdefault(\"secure\", urlparse_cached(request).scheme == \"https\")\n        formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n        response = Response(request.url, headers={\"Set-Cookie\": formatted})\n        return jar.make_cookies(response, request)\n", "n_tokens": 497, "byte_len": 2481, "file_sha1": "cd70415379d587dbdb1dd5605fba74a69f6dd524", "start_line": 132, "end_line": 190}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/robotstxt.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/robotstxt.py", "rel_path": "scrapy/downloadermiddlewares/robotstxt.py", "module": "scrapy.downloadermiddlewares.robotstxt", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_request_2", "_parse_robots", "_robots_error", "RobotsTxtMiddleware", "throw", "downloa", "priority", "async", "robotstx", "use", "inc", "value", "agent", "netloc", "python", "robots", "spider", "callback", "error", "warn", "future", "typ", "checking", "robotstxt", "policies", "urlparse", "cached", "download", "forbidden", "debug", "settings", "isinstance", "none", "type", "robot", "parser", "misc", "response", "count", "http", "exc", "info", "request", "await", "internet", "ignore", "typing", "extensions"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis is a middleware to respect robots.txt policies. To activate it you must\nenable this middleware and enable the ROBOTSTXT_OBEY setting.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.robotstxt import RobotParser\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass RobotsTxtMiddleware:\n    DOWNLOAD_PRIORITY: int = 1000\n\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"ROBOTSTXT_OBEY\"):\n            raise NotConfigured\n        self._default_useragent: str = crawler.settings[\"USER_AGENT\"]\n        self._robotstxt_useragent: str | None = crawler.settings[\"ROBOTSTXT_USER_AGENT\"]\n        self.crawler: Crawler = crawler\n        self._parsers: dict[str, RobotParser | Deferred[RobotParser | None] | None] = {}\n        self._parserimpl: RobotParser = load_object(\n            crawler.settings.get(\"ROBOTSTXT_PARSER\")\n        )\n\n        # check if parser dependencies are met, this should throw an error otherwise.\n        self._parserimpl.from_crawler(self.crawler, b\"\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    @_warn_spider_arg\n    async def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> None:\n        if request.meta.get(\"dont_obey_robotstxt\"):\n            return\n        if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n            return\n        rp = await self.robot_parser(request)\n        self.process_request_2(rp, request)\n\n    def process_request_2(self, rp: RobotParser | None, request: Request) -> None:\n        if rp is None:\n            return\n\n        useragent: str | bytes | None = self._robotstxt_useragent\n        if not useragent:\n            useragent = request.headers.get(b\"User-Agent\", self._default_useragent)\n            assert useragent is not None\n        if not rp.allowed(request.url, useragent):\n            logger.debug(\n                \"Forbidden by robots.txt: %(request)s\",\n                {\"request\": request},\n                extra={\"spider\": self.crawler.spider},\n            )\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(\"robotstxt/forbidden\")\n            raise IgnoreRequest(\"Forbidden by robots.txt\")\n\n    async def robot_parser(self, request: Request) -> RobotParser | None:\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = f\"{url.scheme}://{url.netloc}/robots.txt\"\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={\"dont_obey_robotstxt\": True},\n                callback=NO_CALLBACK,\n            )\n            assert self.crawler.engine\n            assert self.crawler.stats\n            try:\n                resp = await self.crawler.engine.download_async(robotsreq)\n                self._parse_robots(resp, netloc)\n            except Exception as e:\n                if not isinstance(e, IgnoreRequest):\n                    logger.error(\n                        \"Error downloading %(request)s: %(f_exception)s\",\n                        {\"request\": request, \"f_exception\": e},\n                        exc_info=True,\n                        extra={\"spider\": self.crawler.spider},\n                    )\n                self._robots_error(e, netloc)\n            self.crawler.stats.inc_value(\"robotstxt/request_count\")\n\n        parser = self._parsers[netloc]\n        if isinstance(parser, Deferred):\n            return await maybe_deferred_to_future(parser)\n        return parser\n\n    def _parse_robots(self, response: Response, netloc: str) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\"robotstxt/response_count\")\n        self.crawler.stats.inc_value(\n            f\"robotstxt/response_status_count/{response.status}\"\n        )\n        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n        rp_dfd = self._parsers[netloc]\n        assert isinstance(rp_dfd, Deferred)\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)\n\n    def _robots_error(self, exc: Exception, netloc: str) -> None:\n        if not isinstance(exc, IgnoreRequest):\n            key = f\"robotstxt/exception_count/{type(exc)}\"\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(key)\n        rp_dfd = self._parsers[netloc]\n        assert isinstance(rp_dfd, Deferred)\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)\n", "n_tokens": 1113, "byte_len": 5147, "file_sha1": "0dccb7be912d51f9fa5f5aa2bc808737186b5045", "start_line": 1, "end_line": 139}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/useragent.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/useragent.py", "rel_path": "scrapy/downloadermiddlewares/useragent.py", "module": "scrapy.downloadermiddlewares.useragent", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "process_request", "UserAgentMiddleware", "user", "agent", "setdefault", "requires", "spider", "typing", "extensions", "signal", "python", "override", "return", "middleware", "annotations", "class", "allows", "warn", "spiders", "scrapy", "future", "typ", "checking", "classmethod", "headers", "value", "init", "connect", "decorators", "from", "crawler", "settings", "getattr", "signals", "request", "process", "none", "opened", "utils", "self", "import", "header", "default", "http", "this", "response", "use"], "ast_kind": "class_or_type", "text": "\"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.decorators import _warn_spider_arg\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nclass UserAgentMiddleware:\n    \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n\n    def __init__(self, user_agent: str = \"Scrapy\"):\n        self.user_agent = user_agent\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings[\"USER_AGENT\"])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.user_agent = getattr(spider, \"user_agent\", self.user_agent)\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        if self.user_agent:\n            request.headers.setdefault(b\"User-Agent\", self.user_agent)\n        return None\n", "n_tokens": 272, "byte_len": 1211, "file_sha1": "92ad5d5353e2bef4073d6dcbc565fc7d4a5a026c", "start_line": 1, "end_line": 40}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/ajaxcrawl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/ajaxcrawl.py", "rel_path": "scrapy/downloadermiddlewares/ajaxcrawl.py", "module": "scrapy.downloadermiddlewares.ajaxcrawl", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_response", "_has_ajax_crawlable_variant", "_has_ajaxcrawlable_meta", "AjaxCrawlMiddleware", "method", "ajax", "crawlable", "bool", "future", "python", "lib", "w3lib", "slow", "spider", "name", "about", "pages", "deprecated", "between", "removed", "typ", "checking", "string", "turns", "debug", "settings", "ajaxcrawl", "isinstance", "than", "crawl", "insufficient", "none", "html", "without", "methods", "script", "http", "default", "either", "fail", "prevent", "could", "response", "quick", "lookup", "bytes", "typing", "extensions"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport re\nfrom typing import TYPE_CHECKING\nfrom warnings import warn\n\nfrom w3lib import html\n\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.utils.url import escape_ajax\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass AjaxCrawlMiddleware:\n    \"\"\"\n    Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(\"AJAXCRAWL_ENABLED\"):\n            raise NotConfigured\n\n        warn(\n            \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware is deprecated\"\n            \" and will be removed in a future Scrapy version.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n        # XXX: Google parses at least first 100k bytes; scrapy's redirect\n        # middleware parses first 4k. 4k turns out to be insufficient\n        # for this middleware, and parsing 100k could be slow.\n        # We use something in between (32K) by default.\n        self.lookup_bytes: int = settings.getint(\"AJAXCRAWL_MAXSIZE\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Request | Response:\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != \"GET\":\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if \"ajax_crawlable\" in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        ajax_crawl_request = request.replace(url=escape_ajax(request.url + \"#!\"))\n        logger.debug(\n            \"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n            {\"ajax_crawl_request\": ajax_crawl_request, \"request\": request},\n            extra={\"spider\": spider},\n        )\n\n        ajax_crawl_request.meta[\"ajax_crawlable\"] = True\n        return ajax_crawl_request\n\n    def _has_ajax_crawlable_variant(self, response: Response) -> bool:\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\".\n        \"\"\"\n        body = response.text[: self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)\n\n\n_ajax_crawlable_re: re.Pattern[str] = re.compile(\n    r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'\n)\n\n\ndef _has_ajaxcrawlable_meta(text: str) -> bool:\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if \"fragment\" not in text:\n        return False\n    if \"content\" not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, (\"script\", \"noscript\"))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None\n", "n_tokens": 894, "byte_len": 3805, "file_sha1": "ee805257c215910c6e2803276364c97381336ddc", "start_line": 1, "end_line": 115}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpauth.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpauth.py", "rel_path": "scrapy/downloadermiddlewares/httpauth.py", "module": "scrapy.downloadermiddlewares.httpauth", "ext": "py", "chunk_number": 1, "symbols": ["from_crawler", "spider_opened", "process_request", "HttpAuthMiddleware", "downloader", "requires", "spider", "typing", "extensions", "response", "authorization", "signal", "http", "auth", "lib", "w3lib", "python", "return", "middleware", "annotations", "domain", "class", "warn", "ignore", "attr", "scrapy", "defined", "future", "typ", "checking", "classmethod", "headers", "topics", "connect", "documentation", "decorators", "from", "crawler", "pass", "getattr", "signals", "request", "process", "basic", "none", "opened", "docs", "attributes", "utils", "self"], "ast_kind": "class_or_type", "text": "\"\"\"\nHTTP basic auth downloader middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.url import url_is_from_any_domain\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nclass HttpAuthMiddleware:\n    \"\"\"Set Basic HTTP Authorization header\n    (http_user and http_pass spider class attributes)\"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        usr = getattr(spider, \"http_user\", \"\")\n        pwd = getattr(spider, \"http_pass\", \"\")\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)\n            self.domain = spider.http_auth_domain  # type: ignore[attr-defined]\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        auth = getattr(self, \"auth\", None)\n        if (\n            auth\n            and b\"Authorization\" not in request.headers\n            and (not self.domain or url_is_from_any_domain(request.url, [self.domain]))\n        ):\n            request.headers[b\"Authorization\"] = auth\n        return None\n", "n_tokens": 357, "byte_len": 1604, "file_sha1": "9fe0a88fd04e5c88e1f9000e9402b9e6c9015eb3", "start_line": 1, "end_line": 54}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/redirect.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/redirect.py", "rel_path": "scrapy/downloadermiddlewares/redirect.py", "module": "scrapy.downloadermiddlewares.redirect", "ext": "py", "chunk_number": 1, "symbols": ["_build_redirect_request", "__init__", "from_crawler", "_redirect", "_redirect_request_using_get", "BaseRedirectMiddleware", "RedirectMiddleware", "method", "cors", "redirect", "host", "cookies", "redirection", "source", "spec", "python", "lib", "w3lib", "spider", "name", "redirec", "priorit", "dont", "filter", "warn", "future", "typ", "checking", "https", "port", "process_response", "MetaRefreshMiddleware", "ignore", "tags", "allowed", "status", "request", "scheme", "priority", "adjust", "handle", "httpstatus", "urlparse", "cached", "parsed", "meta", "refresh", "debug", "settings", "maxdelay"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, cast\nfrom urllib.parse import urljoin\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.response import get_meta_refresh\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _build_redirect_request(\n    source_request: Request, *, url: str, **kwargs: Any\n) -> Request:\n    redirect_request = source_request.replace(\n        url=url,\n        **kwargs,\n        cls=None,\n        cookies=None,\n    )\n    if \"_scheme_proxy\" in redirect_request.meta:\n        source_request_scheme = urlparse_cached(source_request).scheme\n        redirect_request_scheme = urlparse_cached(redirect_request).scheme\n        if source_request_scheme != redirect_request_scheme:\n            redirect_request.meta.pop(\"_scheme_proxy\")\n            redirect_request.meta.pop(\"proxy\", None)\n            redirect_request.meta.pop(\"_auth_proxy\", None)\n            redirect_request.headers.pop(b\"Proxy-Authorization\", None)\n    has_cookie_header = \"Cookie\" in redirect_request.headers\n    has_authorization_header = \"Authorization\" in redirect_request.headers\n    if has_cookie_header or has_authorization_header:\n        default_ports = {\"http\": 80, \"https\": 443}\n\n        parsed_source_request = urlparse_cached(source_request)\n        source_scheme, source_host, source_port = (\n            parsed_source_request.scheme,\n            parsed_source_request.hostname,\n            parsed_source_request.port\n            or default_ports.get(parsed_source_request.scheme),\n        )\n\n        parsed_redirect_request = urlparse_cached(redirect_request)\n        redirect_scheme, redirect_host, redirect_port = (\n            parsed_redirect_request.scheme,\n            parsed_redirect_request.hostname,\n            parsed_redirect_request.port\n            or default_ports.get(parsed_redirect_request.scheme),\n        )\n\n        if has_cookie_header and (\n            redirect_scheme not in {source_scheme, \"https\"}\n            or source_host != redirect_host\n        ):\n            del redirect_request.headers[\"Cookie\"]\n\n        # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name\n        if has_authorization_header and (\n            source_scheme != redirect_scheme\n            or source_host != redirect_host\n            or source_port != redirect_port\n        ):\n            del redirect_request.headers[\"Authorization\"]\n\n    return redirect_request\n\n\nclass BaseRedirectMiddleware:\n    crawler: Crawler\n    enabled_setting: str = \"REDIRECT_ENABLED\"\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(self.enabled_setting):\n            raise NotConfigured\n\n        self.max_redirect_times: int = settings.getint(\"REDIRECT_MAX_TIMES\")\n        self.priority_adjust: int = settings.getint(\"REDIRECT_PRIORITY_ADJUST\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings)\n        o.crawler = crawler\n        return o\n\n    def _redirect(self, redirected: Request, request: Request, reason: Any) -> Request:\n        ttl = request.meta.setdefault(\"redirect_ttl\", self.max_redirect_times)\n        redirects = request.meta.get(\"redirect_times\", 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta[\"redirect_times\"] = redirects\n            redirected.meta[\"redirect_ttl\"] = ttl - 1\n            redirected.meta[\"redirect_urls\"] = [\n                *request.meta.get(\"redirect_urls\", []),\n                request.url,\n            ]\n            redirected.meta[\"redirect_reasons\"] = [\n                *request.meta.get(\"redirect_reasons\", []),\n                reason,\n            ]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\n                \"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                {\"reason\": reason, \"redirected\": redirected, \"request\": request},\n                extra={\"spider\": self.crawler.spider},\n            )\n            return redirected\n        logger.debug(\n            \"Discarding %(request)s: max redirections reached\",\n            {\"request\": request},\n            extra={\"spider\": self.crawler.spider},\n        )\n        raise IgnoreRequest(\"max redirections reached\")\n\n    def _redirect_request_using_get(\n        self, request: Request, redirect_url: str\n    ) -> Request:\n        redirect_request = _build_redirect_request(\n            request,\n            url=redirect_url,\n            method=\"GET\",\n            body=\"\",\n        )\n        redirect_request.headers.pop(\"Content-Type\", None)\n        redirect_request.headers.pop(\"Content-Length\", None)\n        return redirect_request\n\n\nclass RedirectMiddleware(BaseRedirectMiddleware):\n    \"\"\"\n    Handle redirection of requests based on response status\n    and meta-refresh html tag.\n    \"\"\"\n", "n_tokens": 1043, "byte_len": 5335, "file_sha1": "4531634f6c2419e537d4778ff387f0399b2ea8d2", "start_line": 1, "end_line": 148}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/redirect.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/redirect.py", "rel_path": "scrapy/downloadermiddlewares/redirect.py", "module": "scrapy.downloadermiddlewares.redirect", "ext": "py", "chunk_number": 2, "symbols": ["process_response", "__init__", "MetaRefreshMiddleware", "ignore", "tags", "method", "false", "allowed", "status", "spider", "cast", "metarefres", "ignor", "refresh", "return", "getint", "meta", "request", "scheme", "urljoin", "startswith", "class", "getlist", "warn", "redirected", "url", "headers", "handle", "httpstatus", "https", "_build_redirect_request", "from_crawler", "_redirect", "_redirect_request_using_get", "BaseRedirectMiddleware", "RedirectMiddleware", "cors", "redirect", "host", "cookies", "redirection", "source", "spec", "python", "lib", "w3lib", "name", "redirec", "priorit", "dont"], "ast_kind": "class_or_type", "text": "    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        if (\n            request.meta.get(\"dont_redirect\", False)\n            or response.status\n            in getattr(self.crawler.spider, \"handle_httpstatus_list\", [])\n            or response.status in request.meta.get(\"handle_httpstatus_list\", [])\n            or request.meta.get(\"handle_httpstatus_all\", False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if \"Location\" not in response.headers or response.status not in allowed_status:\n            return response\n\n        assert response.headers[\"Location\"] is not None\n        location = safe_url_string(response.headers[\"Location\"])\n        if response.headers[\"Location\"].startswith(b\"//\"):\n            request_scheme = urlparse_cached(request).scheme\n            location = request_scheme + \"://\" + location.lstrip(\"/\")\n\n        redirected_url = urljoin(request.url, location)\n        redirected = _build_redirect_request(request, url=redirected_url)\n        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n            return response\n\n        if response.status in (301, 307, 308) or request.method == \"HEAD\":\n            return self._redirect(redirected, request, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, response.status)\n\n\nclass MetaRefreshMiddleware(BaseRedirectMiddleware):\n    enabled_setting = \"METAREFRESH_ENABLED\"\n\n    def __init__(self, settings: BaseSettings):\n        super().__init__(settings)\n        self._ignore_tags: list[str] = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n        self._maxdelay: int = settings.getint(\"METAREFRESH_MAXDELAY\")\n\n    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        if (\n            request.meta.get(\"dont_redirect\", False)\n            or request.method == \"HEAD\"\n            or not isinstance(response, HtmlResponse)\n            or urlparse_cached(request).scheme not in {\"http\", \"https\"}\n        ):\n            return response\n\n        interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)\n        if not url:\n            return response\n        redirected = self._redirect_request_using_get(request, url)\n        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n            return response\n        if cast(\"float\", interval) < self._maxdelay:\n            return self._redirect(redirected, request, \"meta refresh\")\n        return response\n", "n_tokens": 576, "byte_len": 2732, "file_sha1": "4531634f6c2419e537d4778ff387f0399b2ea8d2", "start_line": 149, "end_line": 213}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/retry.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/retry.py", "rel_path": "scrapy/downloadermiddlewares/retry.py", "module": "scrapy.downloadermiddlewares.retry", "ext": "py", "chunk_number": 1, "symbols": ["get_retry_request", "parse", "__init__", "from_crawler", "RetryMiddleware", "new", "request", "problems", "stats", "base", "inc", "value", "instance", "get", "retry", "python", "connection", "spider", "name", "retr", "htt", "related", "pages", "dont", "filter", "temporary", "warn", "indicates", "spiders", "future", "process_response", "process_exception", "_retry", "typ", "checking", "priority", "adjust", "string", "failed", "changes", "debug", "settings", "reason", "count", "isinstance", "process", "object", "none", "returns", "type"], "ast_kind": "class_or_type", "text": "\"\"\"\nAn extension to retry failed requests that are potentially caused by temporary\nproblems such as a connection timeout or HTTP 500 error.\n\nYou can change the behaviour of this middleware by modifying the scraping settings:\nRETRY_TIMES - how many times to retry a failed page\nRETRY_HTTP_CODES - which HTTP response codes to retry\n\nFailed pages are collected on the scraping process and rescheduled at the end,\nonce the spider has finished crawling all regular (non-failed) pages.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom logging import Logger, getLogger\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.response import response_status_message\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.http.request import Request\n    from scrapy.settings import BaseSettings\n    from scrapy.spiders import Spider\n\n\nretry_logger = getLogger(__name__)\n\n\ndef get_retry_request(\n    request: Request,\n    *,\n    spider: Spider,\n    reason: str | Exception | type[Exception] = \"unspecified\",\n    max_retry_times: int | None = None,\n    priority_adjust: int | None = None,\n    logger: Logger = retry_logger,\n    stats_base_key: str = \"retry\",\n) -> Request | None:\n    \"\"\"\n    Returns a new :class:`~scrapy.Request` object to retry the specified\n    request, or ``None`` if retries of the specified request have been\n    exhausted.\n\n    For example, in a :class:`~scrapy.Spider` callback, you could use it as\n    follows::\n\n        def parse(self, response):\n            if not response.text:\n                new_request_or_none = get_retry_request(\n                    response.request,\n                    spider=self,\n                    reason='empty',\n                )\n                return new_request_or_none\n\n    *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n    retry request. It is used to access the :ref:`settings <topics-settings>`\n    and :ref:`stats <topics-stats>`, and to provide extra logging context (see\n    :func:`logging.debug`).\n\n    *reason* is a string or an :class:`Exception` object that indicates the\n    reason why the request needs to be retried. It is used to name retry stats.\n\n    *max_retry_times* is a number that determines the maximum number of times\n    that *request* can be retried. If not specified or ``None``, the number is\n    read from the :reqmeta:`max_retry_times` meta key of the request. If the\n    :reqmeta:`max_retry_times` meta key is not defined or ``None``, the number\n    is read from the :setting:`RETRY_TIMES` setting.\n\n    *priority_adjust* is a number that determines how the priority of the new\n    request changes in relation to *request*. If not specified, the number is\n    read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n\n    *logger* is the logging.Logger object to be used when logging messages\n\n    *stats_base_key* is a string to be used as the base key for the\n    retry-related job stats\n    \"\"\"\n    settings = spider.crawler.settings\n    assert spider.crawler.stats\n    stats = spider.crawler.stats\n    retry_times = request.meta.get(\"retry_times\", 0) + 1\n    if max_retry_times is None:\n        max_retry_times = request.meta.get(\"max_retry_times\")\n        if max_retry_times is None:\n            max_retry_times = settings.getint(\"RETRY_TIMES\")\n    if retry_times <= max_retry_times:\n        logger.debug(\n            \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n            {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n            extra={\"spider\": spider},\n        )\n        new_request: Request = request.copy()\n        new_request.meta[\"retry_times\"] = retry_times\n        new_request.dont_filter = True\n        if priority_adjust is None:\n            priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n        new_request.priority = request.priority + priority_adjust\n\n        if callable(reason):\n            reason = reason()\n        if isinstance(reason, Exception):\n            reason = global_object_name(reason.__class__)\n\n        stats.inc_value(f\"{stats_base_key}/count\")\n        stats.inc_value(f\"{stats_base_key}/reason_count/{reason}\")\n        return new_request\n    stats.inc_value(f\"{stats_base_key}/max_reached\")\n    logger.error(\n        \"Gave up retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n        {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n        extra={\"spider\": spider},\n    )\n    return None\n\n\nclass RetryMiddleware:\n    crawler: Crawler\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(\"RETRY_ENABLED\"):\n            raise NotConfigured\n        self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n        self.retry_http_codes = {int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")}\n        self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n        self.exceptions_to_retry = tuple(\n            load_object(x) if isinstance(x, str) else x\n            for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n        )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings)\n        o.crawler = crawler\n        return o\n", "n_tokens": 1212, "byte_len": 5464, "file_sha1": "621a32aa86342c4b13b85ef2974f5c310a3df024", "start_line": 1, "end_line": 145}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/retry.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/retry.py", "rel_path": "scrapy/downloadermiddlewares/retry.py", "module": "scrapy.downloadermiddlewares.retry", "ext": "py", "chunk_number": 2, "symbols": ["process_response", "process_exception", "_retry", "response", "status", "process", "exception", "false", "exceptions", "retry", "spider", "get", "dont", "return", "meta", "warn", "priority", "adjust", "assert", "request", "isinstance", "none", "type", "max", "reason", "crawler", "self", "http", "get_retry_request", "parse", "__init__", "from_crawler", "RetryMiddleware", "new", "problems", "stats", "base", "inc", "value", "instance", "python", "connection", "name", "retr", "htt", "related", "pages", "filter", "temporary", "indicates"], "ast_kind": "function_or_method", "text": "    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        if request.meta.get(\"dont_retry\", False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason) or response\n        return response\n\n    @_warn_spider_arg\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider | None = None\n    ) -> Request | Response | None:\n        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n            \"dont_retry\", False\n        ):\n            return self._retry(request, exception)\n        return None\n\n    def _retry(\n        self, request: Request, reason: str | Exception | type[Exception]\n    ) -> Request | None:\n        max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n        priority_adjust = request.meta.get(\"priority_adjust\", self.priority_adjust)\n        assert self.crawler.spider\n        return get_retry_request(\n            request,\n            reason=reason,\n            spider=self.crawler.spider,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )\n", "n_tokens": 275, "byte_len": 1344, "file_sha1": "621a32aa86342c4b13b85ef2974f5c310a3df024", "start_line": 146, "end_line": 180}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/downloadtimeout.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/downloadtimeout.py", "rel_path": "scrapy/downloadermiddlewares/downloadtimeout.py", "module": "scrapy.downloadermiddlewares.downloadtimeout", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "process_request", "DownloadTimeoutMiddleware", "downloader", "setdefault", "requires", "signal", "typing", "extensions", "spider", "python", "getfloat", "return", "middleware", "annotations", "class", "meta", "warn", "scrapy", "future", "typ", "checking", "classmethod", "topics", "init", "timeout", "connect", "documentation", "decorators", "from", "crawler", "downloa", "settings", "getattr", "signals", "request", "process", "none", "opened", "float", "download", "docs", "utils", "self", "import", "http", "response"], "ast_kind": "class_or_type", "text": "\"\"\"\nDownload timeout middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.decorators import _warn_spider_arg\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nclass DownloadTimeoutMiddleware:\n    def __init__(self, timeout: float = 180):\n        self._timeout: float = timeout\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings.getfloat(\"DOWNLOAD_TIMEOUT\"))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self._timeout = getattr(spider, \"download_timeout\", self._timeout)\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        if self._timeout:\n            request.meta.setdefault(\"download_timeout\", self._timeout)\n        return None\n", "n_tokens": 263, "byte_len": 1179, "file_sha1": "41bea5ce6a822339983dc93126b87080a0ade290", "start_line": 1, "end_line": 42}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/stats.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/stats.py", "rel_path": "scrapy/downloadermiddlewares/stats.py", "module": "scrapy.downloadermiddlewares.stats", "ext": "py", "chunk_number": 1, "symbols": ["get_header_size", "get_status_size", "__init__", "from_crawler", "process_request", "process_response", "process_exception", "DownloaderStats", "method", "inc", "value", "python", "get", "header", "spider", "warn", "future", "typ", "checking", "stats", "collector", "settings", "status", "items", "isinstance", "response", "none", "class", "count", "http", "bytes", "request", "process", "exception", "typing", "extensions", "return", "annotations", "responses", "global", "object", "resp", "not", "configured", "headers", "classmethod", "decorators", "httprepr", "from", "crawler"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom twisted.web import http\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.python import global_object_name, to_bytes\nfrom scrapy.utils.request import request_httprepr\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.statscollectors import StatsCollector\n\n\ndef get_header_size(\n    headers: dict[str, list[str | bytes] | tuple[str | bytes, ...]],\n) -> int:\n    size = 0\n    for key, value in headers.items():\n        if isinstance(value, (list, tuple)):\n            for v in value:\n                size += len(b\": \") + len(key) + len(v)\n    return size + len(b\"\\r\\n\") * (len(headers.keys()) - 1)\n\n\ndef get_status_size(response_status: int) -> int:\n    return len(to_bytes(http.RESPONSES.get(response_status, b\"\"))) + 15\n    # resp.status + b\"\\r\\n\" + b\"HTTP/1.1 <100-599> \"\n\n\nclass DownloaderStats:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"DOWNLOADER_STATS\"):\n            raise NotConfigured\n        assert crawler.stats\n        return cls(crawler.stats)\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        self.stats.inc_value(\"downloader/request_count\")\n        self.stats.inc_value(f\"downloader/request_method_count/{request.method}\")\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value(\"downloader/request_bytes\", reqlen)\n        return None\n\n    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        self.stats.inc_value(\"downloader/response_count\")\n        self.stats.inc_value(f\"downloader/response_status_count/{response.status}\")\n        reslen = (\n            len(response.body)\n            + get_header_size(response.headers)\n            + get_status_size(response.status)\n            + 4\n        )\n        # response.body + b\"\\r\\n\"+ response.header + b\"\\r\\n\" + response.status\n        self.stats.inc_value(\"downloader/response_bytes\", reslen)\n        return response\n\n    @_warn_spider_arg\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider | None = None\n    ) -> Request | Response | None:\n        ex_class = global_object_name(exception.__class__)\n        self.stats.inc_value(\"downloader/exception_count\")\n        self.stats.inc_value(f\"downloader/exception_type_count/{ex_class}\")\n        return None\n", "n_tokens": 661, "byte_len": 2876, "file_sha1": "1b72382988f5be56c1551e0562c8365d56de9c32", "start_line": 1, "end_line": 83}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpcompression.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpcompression.py", "rel_path": "scrapy/downloadermiddlewares/httpcompression.py", "module": "scrapy.downloadermiddlewares.httpcompression", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "open_spider", "process_request", "process_response", "_handle_encoding", "HttpCompressionMiddleware", "encoding", "method", "append", "inc", "value", "compressio", "enabled", "zstandard", "python", "after", "decompression", "max", "spider", "name", "warn", "unknown", "deprecated", "future", "typ", "checking", "decode", "make", "sent", "_split_encodings", "_decode", "_warn_unknown_encoding", "stats", "collector", "settings", "split", "encodings", "lower", "isinstance", "size", "during", "than", "none", "join", "accept", "from", "iterable", "response", "count"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils._compression import (\n    _DecompressionMaxSizeExceeded,\n    _inflate,\n    _unbrotli,\n    _unzstd,\n)\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.gz import gunzip\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nlogger = getLogger(__name__)\n\nACCEPTED_ENCODINGS: list[bytes] = [b\"gzip\", b\"deflate\"]\n\ntry:\n    try:\n        import brotli  # noqa: F401\n    except ImportError:\n        import brotlicffi  # noqa: F401\nexcept ImportError:\n    pass\nelse:\n    ACCEPTED_ENCODINGS.append(b\"br\")\n\ntry:\n    import zstandard  # noqa: F401\nexcept ImportError:\n    pass\nelse:\n    ACCEPTED_ENCODINGS.append(b\"zstd\")\n\n\nclass HttpCompressionMiddleware:\n    \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n    sent/received from websites\"\"\"\n\n    def __init__(\n        self,\n        stats: StatsCollector | None = None,\n        *,\n        crawler: Crawler | None = None,\n    ):\n        if not crawler:\n            self.stats = stats\n            self._max_size = 1073741824\n            self._warn_size = 33554432\n            return\n        self.stats = crawler.stats\n        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n        crawler.signals.connect(self.open_spider, signals.spider_opened)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n            raise NotConfigured\n        return cls(crawler=crawler)\n\n    def open_spider(self, spider: Spider) -> None:\n        if hasattr(spider, \"download_maxsize\"):\n            warnings.warn(\n                \"The 'download_maxsize' spider attribute is deprecated. \"\n                \"Use Spider.custom_settings or Spider.update_settings() instead. \"\n                \"The corresponding setting name is 'DOWNLOAD_MAXSIZE'.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            self._max_size = spider.download_maxsize\n        if hasattr(spider, \"download_warnsize\"):\n            warnings.warn(\n                \"The 'download_warnsize' spider attribute is deprecated. \"\n                \"Use Spider.custom_settings or Spider.update_settings() instead. \"\n                \"The corresponding setting name is 'DOWNLOAD_WARNSIZE'.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            self._warn_size = spider.download_warnsize\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n        return None\n\n    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        if request.method == \"HEAD\":\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist(\"Content-Encoding\")\n            if content_encoding:\n                max_size = request.meta.get(\"download_maxsize\", self._max_size)\n                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\n                try:\n                    decoded_body, content_encoding = self._handle_encoding(\n                        response.body, content_encoding, max_size\n                    )\n                except _DecompressionMaxSizeExceeded:\n                    raise IgnoreRequest(\n                        f\"Ignored response {response} because its body \"\n                        f\"({len(response.body)} B compressed) exceeded \"\n                        f\"DOWNLOAD_MAXSIZE ({max_size} B) during \"\n                        f\"decompression.\"\n                    )\n                if len(response.body) < warn_size <= len(decoded_body):\n                    logger.warning(\n                        f\"{response} body size after decompression \"\n                        f\"({len(decoded_body)} B) is larger than the \"\n                        f\"download warning size ({warn_size} B).\"\n                    )\n                if content_encoding:\n                    self._warn_unknown_encoding(response, content_encoding)\n                response.headers[\"Content-Encoding\"] = content_encoding\n                if self.stats:\n                    self.stats.inc_value(\n                        \"httpcompression/response_bytes\",\n                        len(decoded_body),\n                    )\n                    self.stats.inc_value(\"httpcompression/response_count\")\n                respcls = responsetypes.from_args(\n                    headers=response.headers, url=response.url, body=decoded_body\n                )\n                kwargs: dict[str, Any] = {\"body\": decoded_body}\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs[\"encoding\"] = None\n                response = response.replace(cls=respcls, **kwargs)\n                if not content_encoding:\n                    del response.headers[\"Content-Encoding\"]\n\n        return response\n\n    def _handle_encoding(\n        self, body: bytes, content_encoding: list[bytes], max_size: int\n    ) -> tuple[bytes, list[bytes]]:\n        to_decode, to_keep = self._split_encodings(content_encoding)\n        for encoding in to_decode:\n            body = self._decode(body, encoding, max_size)\n        return body, to_keep\n", "n_tokens": 1237, "byte_len": 6113, "file_sha1": "0cb3461f0de535f85d5d0869080c1a9f4ea667d4", "start_line": 1, "end_line": 162}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpcompression.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpcompression.py", "rel_path": "scrapy/downloadermiddlewares/httpcompression.py", "module": "scrapy.downloadermiddlewares.httpcompression", "ext": "py", "chunk_number": 2, "symbols": ["_split_encodings", "_decode", "_warn_unknown_encoding", "encoding", "chain", "gzip", "pragma", "decode", "append", "response", "zstandard", "cover", "return", "content", "name", "cannot", "warn", "unknown", "warning", "encodings", "str", "logger", "staticmethod", "body", "deflate", "shouldn", "accepte", "gunzip", "from", "list", "__init__", "from_crawler", "open_spider", "process_request", "process_response", "_handle_encoding", "HttpCompressionMiddleware", "method", "inc", "value", "compressio", "enabled", "python", "after", "decompression", "max", "spider", "deprecated", "future", "typ"], "ast_kind": "function_or_method", "text": "    @staticmethod\n    def _split_encodings(\n        content_encoding: list[bytes],\n    ) -> tuple[list[bytes], list[bytes]]:\n        supported_encodings = {*ACCEPTED_ENCODINGS, b\"x-gzip\"}\n        to_keep: list[bytes] = [\n            encoding.strip().lower()\n            for encoding in chain.from_iterable(\n                encodings.split(b\",\") for encodings in content_encoding\n            )\n        ]\n        to_decode: list[bytes] = []\n        while to_keep:\n            encoding = to_keep.pop()\n            if encoding not in supported_encodings:\n                to_keep.append(encoding)\n                return to_decode, to_keep\n            to_decode.append(encoding)\n        return to_decode, to_keep\n\n    @staticmethod\n    def _decode(body: bytes, encoding: bytes, max_size: int) -> bytes:\n        if encoding in {b\"gzip\", b\"x-gzip\"}:\n            return gunzip(body, max_size=max_size)\n        if encoding == b\"deflate\":\n            return _inflate(body, max_size=max_size)\n        if encoding == b\"br\":\n            return _unbrotli(body, max_size=max_size)\n        if encoding == b\"zstd\":\n            return _unzstd(body, max_size=max_size)\n        # shouldn't be reached\n        return body  # pragma: no cover\n\n    def _warn_unknown_encoding(\n        self, response: Response, encodings: list[bytes]\n    ) -> None:\n        encodings_str = b\",\".join(encodings).decode()\n        msg = (\n            f\"{self.__class__.__name__} cannot decode the response for {response.url} \"\n            f\"from unsupported encoding(s) '{encodings_str}'.\"\n        )\n        if b\"br\" in encodings:\n            msg += \" You need to install brotli or brotlicffi to decode 'br'.\"\n        if b\"zstd\" in encodings:\n            msg += \" You need to install zstandard to decode 'zstd'.\"\n        logger.warning(msg)\n", "n_tokens": 425, "byte_len": 1797, "file_sha1": "0cb3461f0de535f85d5d0869080c1a9f4ea667d4", "start_line": 163, "end_line": 209}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/offsite.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/offsite.py", "rel_path": "scrapy/downloadermiddlewares/offsite.py", "module": "scrapy.downloadermiddlewares.offsite", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "request_scheduled", "process_request", "should_follow", "get_host_regex", "OffsiteMiddleware", "method", "bool", "inc", "value", "domains", "seen", "signal", "filtered", "append", "python", "spider", "domain", "dont", "filter", "warn", "future", "typ", "checking", "https", "elif", "urlparse", "cached", "debug", "stats", "collector", "none", "join", "without", "default", "match", "policy", "ignoring", "implement", "ignore", "request", "typing", "extensions", "return", "annotations", "class", "meta", "classmethod"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport re\nimport warnings\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware:\n    crawler: Crawler\n\n    def __init__(self, stats: StatsCollector):\n        self.stats = stats\n        self.domains_seen: set[str] = set()\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n        o.crawler = crawler\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n\n    def request_scheduled(self, request: Request, spider: Spider) -> None:\n        self.process_request(request)\n\n    @_warn_spider_arg\n    def process_request(self, request: Request, spider: Spider | None = None) -> None:\n        assert self.crawler.spider\n        if (\n            request.dont_filter\n            or request.meta.get(\"allow_offsite\")\n            or self.should_follow(request, self.crawler.spider)\n        ):\n            return\n        domain = urlparse_cached(request).hostname\n        if domain and domain not in self.domains_seen:\n            self.domains_seen.add(domain)\n            logger.debug(\n                \"Filtered offsite request to %(domain)r: %(request)s\",\n                {\"domain\": domain, \"request\": request},\n                extra={\"spider\": self.crawler.spider},\n            )\n            self.stats.inc_value(\"offsite/domains\")\n        self.stats.inc_value(\"offsite/filtered\")\n        raise IgnoreRequest\n\n    def should_follow(self, request: Request, spider: Spider) -> bool:\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or \"\"\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, \"allowed_domains\", None)\n        if not allowed_domains:\n            return re.compile(\"\")  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            if url_pattern.match(domain):\n                message = (\n                    \"allowed_domains accepts only domains, not URLs. \"\n                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message)\n            elif port_pattern.search(domain):\n                message = (\n                    \"allowed_domains accepts only domains without ports. \"\n                    f\"Ignoring entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message)\n            else:\n                domains.append(re.escape(domain))\n        regex = rf\"^(.*\\.)?({'|'.join(domains)})$\"\n        return re.compile(regex)\n", "n_tokens": 737, "byte_len": 3583, "file_sha1": "56f691a419558f847a90f83dbc66cb779d23297e", "start_line": 1, "end_line": 100}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/defaultheaders.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/defaultheaders.py", "rel_path": "scrapy/downloadermiddlewares/defaultheaders.py", "module": "scrapy.downloadermiddlewares.defaultheaders", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_request", "DefaultHeadersMiddleware", "downloader", "setdefault", "requires", "default", "headers", "python", "spider", "typing", "extensions", "return", "middleware", "annotations", "class", "warn", "scrapy", "future", "typ", "checking", "collections", "classmethod", "topics", "init", "documentation", "decorators", "from", "crawler", "settings", "tuple", "iterable", "request", "items", "process", "defaul", "reques", "none", "docs", "utils", "self", "import", "http", "without", "response"], "ast_kind": "class_or_type", "text": "\"\"\"\nDefaultHeaders downloader middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nclass DefaultHeadersMiddleware:\n    def __init__(self, headers: Iterable[tuple[str, str]]):\n        self._headers: Iterable[tuple[str, str]] = headers\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        headers = without_none_values(crawler.settings[\"DEFAULT_REQUEST_HEADERS\"])\n        return cls(headers.items())\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)\n        return None\n", "n_tokens": 243, "byte_len": 1124, "file_sha1": "33ea10dee10477b215c1ebf3ed27fcc9dc969d0c", "start_line": 1, "end_line": 41}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpproxy.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpproxy.py", "rel_path": "scrapy/downloadermiddlewares/httpproxy.py", "module": "scrapy.downloadermiddlewares.httpproxy", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "_basic_auth_header", "_get_proxy", "process_request", "_set_proxy_and_creds", "HttpProxyMiddleware", "encoding", "auth", "orig", "type", "proxy", "python", "spider", "warn", "future", "typ", "checking", "elif", "https", "urlparse", "cached", "username", "urlunparse", "settings", "proxies", "items", "none", "skipped", "parse", "http", "bytes", "creds", "response", "values", "set", "typing", "extensions", "authorization", "return", "sock", "annotations", "class", "meta", "ignore", "not", "configured", "headers", "classmethod", "httpprox"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport base64\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import unquote, urlunparse\nfrom urllib.request import (  # type: ignore[attr-defined]\n    _parse_proxy,\n    getproxies,\n    proxy_bypass,\n)\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nclass HttpProxyMiddleware:\n    def __init__(self, auth_encoding: str | None = \"latin-1\"):\n        self.auth_encoding: str | None = auth_encoding\n        self.proxies: dict[str, tuple[bytes | None, str]] = {}\n        for type_, url in getproxies().items():\n            try:\n                self.proxies[type_] = self._get_proxy(url, type_)\n            # some values such as '/var/run/docker.sock' can't be parsed\n            # by _parse_proxy and as such should be skipped\n            except ValueError:\n                continue\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n            raise NotConfigured\n        auth_encoding: str | None = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n        return cls(auth_encoding)\n\n    def _basic_auth_header(self, username: str, password: str) -> bytes:\n        user_pass = to_bytes(\n            f\"{unquote(username)}:{unquote(password)}\", encoding=self.auth_encoding\n        )\n        return base64.b64encode(user_pass)\n\n    def _get_proxy(self, url: str, orig_type: str) -> tuple[bytes | None, str]:\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n\n        creds = self._basic_auth_header(user, password) if user else None\n\n        return creds, proxy_url\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        creds, proxy_url, scheme = None, None, None\n        if \"proxy\" in request.meta:\n            if request.meta[\"proxy\"] is not None:\n                creds, proxy_url = self._get_proxy(request.meta[\"proxy\"], \"\")\n        elif self.proxies:\n            parsed = urlparse_cached(request)\n            _scheme = parsed.scheme\n            if (\n                # 'no_proxy' is only supported by http schemes\n                _scheme not in (\"http\", \"https\")\n                or (parsed.hostname and not proxy_bypass(parsed.hostname))\n            ) and _scheme in self.proxies:\n                scheme = _scheme\n                creds, proxy_url = self.proxies[scheme]\n\n        self._set_proxy_and_creds(request, proxy_url, creds, scheme)\n        return None\n\n    def _set_proxy_and_creds(\n        self,\n        request: Request,\n        proxy_url: str | None,\n        creds: bytes | None,\n        scheme: str | None,\n    ) -> None:\n        if scheme:\n            request.meta[\"_scheme_proxy\"] = True\n        if proxy_url:\n            request.meta[\"proxy\"] = proxy_url\n        elif request.meta.get(\"proxy\") is not None:\n            request.meta[\"proxy\"] = None\n        if creds:\n            request.headers[b\"Proxy-Authorization\"] = b\"Basic \" + creds\n            request.meta[\"_auth_proxy\"] = proxy_url\n        elif \"_auth_proxy\" in request.meta:\n            if proxy_url != request.meta[\"_auth_proxy\"]:\n                if b\"Proxy-Authorization\" in request.headers:\n                    del request.headers[b\"Proxy-Authorization\"]\n                del request.meta[\"_auth_proxy\"]\n        elif b\"Proxy-Authorization\" in request.headers:\n            if proxy_url:\n                request.meta[\"_auth_proxy\"] = proxy_url\n            else:\n                del request.headers[b\"Proxy-Authorization\"]\n", "n_tokens": 886, "byte_len": 3966, "file_sha1": "c49e4774796d5b7d7e0e7a380d5bd86b9a6973d3", "start_line": 1, "end_line": 107}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpcache.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/downloadermiddlewares/httpcache.py", "rel_path": "scrapy/downloadermiddlewares/httpcache.py", "module": "scrapy.downloadermiddlewares.httpcache", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "spider_closed", "process_request", "process_response", "process_exception", "_cache_response", "HttpCacheMiddleware", "rfc", "rfc2616", "inc", "value", "append", "signal", "origin", "python", "miss", "spider", "responses", "should", "cache", "warn", "httpcach", "enabled", "spiders", "error", "oserror", "future", "typ", "checking", "https", "validate", "settings", "stats", "collector", "isinstance", "none", "timeout", "server", "ignor", "html", "misc", "http", "response", "policy", "requests", "cached", "process", "exception"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom email.utils import formatdate\nfrom typing import TYPE_CHECKING\n\nfrom twisted.internet import defer\nfrom twisted.internet.error import (\n    ConnectError,\n    ConnectionDone,\n    ConnectionLost,\n    DNSLookupError,\n    TCPTimedOutError,\n)\nfrom twisted.internet.error import ConnectionRefusedError as TxConnectionRefusedError\nfrom twisted.internet.error import TimeoutError as TxTimeoutError\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy import signals\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http.request import Request\n    from scrapy.http.response import Response\n    from scrapy.settings import Settings\n    from scrapy.spiders import Spider\n    from scrapy.statscollectors import StatsCollector\n\n\nclass HttpCacheMiddleware:\n    DOWNLOAD_EXCEPTIONS = (\n        defer.TimeoutError,\n        TxTimeoutError,\n        DNSLookupError,\n        TxConnectionRefusedError,\n        ConnectionDone,\n        ConnectError,\n        ConnectionLost,\n        TCPTimedOutError,\n        ResponseFailed,\n        OSError,\n    )\n\n    crawler: Crawler\n\n    def __init__(self, settings: Settings, stats: StatsCollector) -> None:\n        if not settings.getbool(\"HTTPCACHE_ENABLED\"):\n            raise NotConfigured\n        self.policy = load_object(settings[\"HTTPCACHE_POLICY\"])(settings)\n        self.storage = load_object(settings[\"HTTPCACHE_STORAGE\"])(settings)\n        self.ignore_missing = settings.getbool(\"HTTPCACHE_IGNORE_MISSING\")\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        o.crawler = crawler\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.storage.open_spider(spider)\n\n    def spider_closed(self, spider: Spider) -> None:\n        self.storage.close_spider(spider)\n\n    @_warn_spider_arg\n    def process_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Request | Response | None:\n        if request.meta.get(\"dont_cache\", False):\n            return None\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta[\"_dont_cache\"] = True  # flag as uncacheable\n            return None\n\n        # Look for cached response and check if expired\n        cachedresponse: Response | None = self.storage.retrieve_response(\n            self.crawler.spider, request\n        )\n        if cachedresponse is None:\n            self.stats.inc_value(\"httpcache/miss\")\n            if self.ignore_missing:\n                self.stats.inc_value(\"httpcache/ignore\")\n                raise IgnoreRequest(f\"Ignored request not in cache: {request}\")\n            return None  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append(\"cached\")\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value(\"httpcache/hit\")\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta[\"cached_response\"] = cachedresponse\n\n        return None\n\n    @_warn_spider_arg\n    def process_response(\n        self, request: Request, response: Response, spider: Spider | None = None\n    ) -> Request | Response:\n        if request.meta.get(\"dont_cache\", False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if \"cached\" in response.flags or \"_dont_cache\" in request.meta:\n            request.meta.pop(\"_dont_cache\", None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if \"Date\" not in response.headers:\n            response.headers[\"Date\"] = formatdate(usegmt=True)\n\n        # Do not validate first-hand responses\n        cachedresponse: Response | None = request.meta.pop(\"cached_response\", None)\n        if cachedresponse is None:\n            self.stats.inc_value(\"httpcache/firsthand\")\n            self._cache_response(response, request)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value(\"httpcache/revalidate\")\n            return cachedresponse\n\n        self.stats.inc_value(\"httpcache/invalidate\")\n        self._cache_response(response, request)\n        return response\n\n    @_warn_spider_arg\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider | None = None\n    ) -> Request | Response | None:\n        cachedresponse: Response | None = request.meta.pop(\"cached_response\", None)\n        if cachedresponse is not None and isinstance(\n            exception, self.DOWNLOAD_EXCEPTIONS\n        ):\n            self.stats.inc_value(\"httpcache/errorrecovery\")\n            return cachedresponse\n        return None\n\n    def _cache_response(self, response: Response, request: Request) -> None:\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value(\"httpcache/store\")\n            self.storage.store_response(self.crawler.spider, request, response)\n        else:\n            self.stats.inc_value(\"httpcache/uncacheable\")\n", "n_tokens": 1193, "byte_len": 5772, "file_sha1": "d45b80fc478c24a7a501def3dc33bb6bc11d91d4", "start_line": 1, "end_line": 159}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py", "rel_path": "scrapy/settings/__init__.py", "module": "scrapy.settings.__init__", "ext": "py", "chunk_number": 1, "symbols": ["get_settings_priority", "__init__", "set", "__repr__", "__getitem__", "__contains__", "add_to_list", "remove_from_list", "SettingsAttribute", "BaseSettings", "method", "bool", "instance", "priorities", "retrieved", "case", "python", "getpriority", "related", "name", "once", "levels", "import", "module", "numerical", "passed", "future", "typ", "checking", "https", "get", "getbool", "getint", "getfloat", "getlist", "getdict", "getdictorlist", "getwithbase", "maxpriority", "replace_in_component_priority_dict", "__setitem__", "set_in_component_priority_dict", "setdefault", "setdefault_in_component_priority_dict", "setdict", "setmodule", "update", "delete", "__delitem__", "_assert_mutability"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport copy\nimport json\nimport warnings\nfrom collections.abc import Iterable, Iterator, Mapping, MutableMapping\nfrom importlib import import_module\nfrom pprint import pformat\nfrom typing import TYPE_CHECKING, Any, Union, cast\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.settings import default_settings\nfrom scrapy.utils.misc import load_object\n\n# The key types are restricted in BaseSettings._get_key() to ones supported by JSON,\n# see https://github.com/scrapy/scrapy/issues/5383.\n_SettingsKeyT = Union[bool, float, int, str, None]\n\nif TYPE_CHECKING:\n    from types import ModuleType\n\n    # https://github.com/python/typing/issues/445#issuecomment-1131458824\n    from _typeshed import SupportsItems\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    _SettingsInputT = Union[SupportsItems[_SettingsKeyT, Any], str, None]\n\n\nSETTINGS_PRIORITIES: dict[str, int] = {\n    \"default\": 0,\n    \"command\": 10,\n    \"addon\": 15,\n    \"project\": 20,\n    \"spider\": 30,\n    \"cmdline\": 40,\n}\n\n\ndef get_settings_priority(priority: int | str) -> int:\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, str):\n        return SETTINGS_PRIORITIES[priority]\n    return priority\n\n\nclass SettingsAttribute:\n    \"\"\"Class for storing data related to settings attributes.\n\n    This class is intended for internal usage, you should try Settings class\n    for settings configuration, not this one.\n    \"\"\"\n\n    def __init__(self, value: Any, priority: int):\n        self.value: Any = value\n        self.priority: int\n        if isinstance(self.value, BaseSettings):\n            self.priority = max(self.value.maxpriority(), priority)\n        else:\n            self.priority = priority\n\n    def set(self, value: Any, priority: int) -> None:\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority\n\n    def __repr__(self) -> str:\n        return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"\n\n\nclass BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n    \"\"\"\n    Instances of this class behave like dictionaries, but store priorities\n    along with their ``(key, value)`` pairs, and can be frozen (i.e. marked\n    immutable).\n\n    Key-value entries can be passed on initialization with the ``values``\n    argument, and they would take the ``priority`` level (unless ``values`` is\n    already an instance of :class:`~scrapy.settings.BaseSettings`, in which\n    case the existing priority levels will be kept).  If the ``priority``\n    argument is a string, the priority name will be looked up in\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer\n    should be provided.\n\n    Once the object is created, new settings can be loaded or updated with the\n    :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with\n    the square bracket notation of dictionaries, or with the\n    :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its\n    value conversion variants. When requesting a stored key, the value with the\n    highest priority will be retrieved.\n    \"\"\"\n\n    __default = object()\n\n    def __init__(self, values: _SettingsInputT = None, priority: int | str = \"project\"):\n        self.frozen: bool = False\n        self.attributes: dict[_SettingsKeyT, SettingsAttribute] = {}\n        if values:\n            self.update(values, priority)\n\n    def __getitem__(self, opt_name: _SettingsKeyT) -> Any:\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value\n\n    def __contains__(self, name: Any) -> bool:\n        return name in self.attributes\n\n    def add_to_list(self, name: _SettingsKeyT, item: Any) -> None:\n        \"\"\"Append *item* to the :class:`list` setting with the specified *name*\n        if *item* is not already in that list.\n\n        This change is applied regardless of the priority of the *name*\n        setting. The setting priority is not affected by this change either.\n        \"\"\"\n        value: list[str] = self.getlist(name)\n        if item not in value:\n            self.set(name, [*value, item], self.getpriority(name) or 0)\n\n    def remove_from_list(self, name: _SettingsKeyT, item: Any) -> None:\n        \"\"\"Remove *item* from the :class:`list` setting with the specified\n        *name*.\n\n        If *item* is missing, raise :exc:`ValueError`.\n\n        This change is applied regardless of the priority of the *name*\n        setting. The setting priority is not affected by this change either.\n        \"\"\"\n        value: list[str] = self.getlist(name)\n        if item not in value:\n            raise ValueError(f\"{item!r} not found in the {name} setting ({value!r}).\")\n        self.set(name, [v for v in value if v != item], self.getpriority(name) or 0)\n", "n_tokens": 1204, "byte_len": 5226, "file_sha1": "34809cf372c8def8e57b978d711c90ab9ec16f5a", "start_line": 1, "end_line": 141}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py", "rel_path": "scrapy/settings/__init__.py", "module": "scrapy.settings.__init__", "ext": "py", "chunk_number": 2, "symbols": ["get", "getbool", "getint", "getfloat", "getlist", "getdict", "method", "bool", "loads", "containing", "instance", "case", "name", "about", "deprecated", "false", "string", "settings", "isinstance", "object", "none", "json", "type", "without", "default", "found", "empty", "values", "return", "concurren", "get_settings_priority", "__init__", "set", "__repr__", "__getitem__", "__contains__", "add_to_list", "remove_from_list", "getdictorlist", "getwithbase", "getpriority", "maxpriority", "replace_in_component_priority_dict", "__setitem__", "set_in_component_priority_dict", "setdefault", "setdefault_in_component_priority_dict", "setdict", "setmodule", "update"], "ast_kind": "function_or_method", "text": "    def get(self, name: _SettingsKeyT, default: Any = None) -> Any:\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        if name == \"CONCURRENT_REQUESTS_PER_IP\" and (\n            isinstance(self[name], int) and self[name] != 0\n        ):\n            warnings.warn(\n                \"The CONCURRENT_REQUESTS_PER_IP setting is deprecated, use CONCURRENT_REQUESTS_PER_DOMAIN instead.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n\n        return self[name] if self[name] is not None else default\n\n    def getbool(self, name: _SettingsKeyT, default: bool = False) -> bool:\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\n                \"Supported values for boolean settings \"\n                \"are 0/1, True/False, '0'/'1', \"\n                \"'True'/'False' and 'true'/'false'\"\n            )\n\n    def getint(self, name: _SettingsKeyT, default: int = 0) -> int:\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return int(self.get(name, default))\n\n    def getfloat(self, name: _SettingsKeyT, default: float = 0.0) -> float:\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return float(self.get(name, default))\n\n    def getlist(\n        self, name: _SettingsKeyT, default: list[Any] | None = None\n    ) -> list[Any]:\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list,\n        a copy of it will be returned. If it's a string it will be split by\n        \",\". If it is an empty string, an empty list will be returned.\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or [])\n        if not value:\n            return []\n        if isinstance(value, str):\n            value = value.split(\",\")\n        return list(value)\n\n    def getdict(\n        self, name: _SettingsKeyT, default: dict[Any, Any] | None = None\n    ) -> dict[Any, Any]:\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, str):\n            value = json.loads(value)\n        return dict(value)\n", "n_tokens": 994, "byte_len": 4303, "file_sha1": "34809cf372c8def8e57b978d711c90ab9ec16f5a", "start_line": 142, "end_line": 263}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py", "rel_path": "scrapy/settings/__init__.py", "module": "scrapy.settings.__init__", "ext": "py", "chunk_number": 3, "symbols": ["getdictorlist", "getwithbase", "getpriority", "maxpriority", "replace_in_component_priority_dict", "__setitem__", "does", "those", "loads", "possible", "name", "about", "key", "key1", "numerical", "more", "string", "missing", "settings", "replaced", "old", "cls", "fallback", "isinstance", "than", "them", "exist", "none", "json", "type", "get_settings_priority", "__init__", "set", "__repr__", "__getitem__", "__contains__", "add_to_list", "remove_from_list", "get", "getbool", "getint", "getfloat", "getlist", "getdict", "set_in_component_priority_dict", "setdefault", "setdefault_in_component_priority_dict", "setdict", "setmodule", "update"], "ast_kind": "function_or_method", "text": "    def getdictorlist(\n        self,\n        name: _SettingsKeyT,\n        default: dict[Any, Any] | list[Any] | tuple[Any] | None = None,\n    ) -> dict[Any, Any] | list[Any]:\n        \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n\n        If the setting is already a dict or a list, a copy of it will be\n        returned.\n\n        If it is a string it will be evaluated as JSON, or as a comma-separated\n        list of strings as a fallback.\n\n        For example, settings populated from the command line will return:\n\n        -   ``{'key1': 'value1', 'key2': 'value2'}`` if set to\n            ``'{\"key1\": \"value1\", \"key2\": \"value2\"}'``\n\n        -   ``['one', 'two']`` if set to ``'[\"one\", \"two\"]'`` or ``'one,two'``\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default)\n        if value is None:\n            return {}\n        if isinstance(value, str):\n            try:\n                value_loaded = json.loads(value)\n                assert isinstance(value_loaded, (dict, list))\n                return value_loaded\n            except ValueError:\n                return value.split(\",\")\n        if isinstance(value, tuple):\n            return list(value)\n        assert isinstance(value, (dict, list))\n        return copy.deepcopy(value)\n\n    def getwithbase(self, name: _SettingsKeyT) -> BaseSettings:\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: str\n        \"\"\"\n        if not isinstance(name, str):\n            raise ValueError(f\"Base setting key must be a string, got {name}\")\n        compbs = BaseSettings()\n        compbs.update(self[name + \"_BASE\"])\n        compbs.update(self[name])\n        return compbs\n\n    def getpriority(self, name: _SettingsKeyT) -> int | None:\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: str\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority\n\n    def maxpriority(self) -> int:\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(cast(\"int\", self.getpriority(name)) for name in self)\n        return get_settings_priority(\"default\")\n\n    def replace_in_component_priority_dict(\n        self,\n        name: _SettingsKeyT,\n        old_cls: type,\n        new_cls: type,\n        priority: int | None = None,\n    ) -> None:\n        \"\"\"Replace *old_cls* with *new_cls* in the *name* :ref:`component\n        priority dictionary <component-priority-dictionaries>`.\n\n        If *old_cls* is missing, or has :data:`None` as value, :exc:`KeyError`\n        is raised.\n\n        If *old_cls* was present as an import string, even more than once,\n        those keys are dropped and replaced by *new_cls*.\n\n        If *priority* is specified, that is the value assigned to *new_cls* in\n        the component priority dictionary. Otherwise, the value of *old_cls* is\n        used. If *old_cls* was present multiple times (possible with import\n        strings) with different values, the value assigned to *new_cls* is one\n        of them, with no guarantee about which one it is.\n\n        This change is applied regardless of the priority of the *name*\n        setting. The setting priority is not affected by this change either.\n        \"\"\"\n        component_priority_dict = self.getdict(name)\n        old_priority = None\n        for cls_or_path in tuple(component_priority_dict):\n            if load_object(cls_or_path) != old_cls:\n                continue\n            if (old_priority := component_priority_dict.pop(cls_or_path)) is None:\n                break\n        if old_priority is None:\n            raise KeyError(\n                f\"{old_cls} not found in the {name} setting ({component_priority_dict!r}).\"\n            )\n        component_priority_dict[new_cls] = (\n            old_priority if priority is None else priority\n        )\n        self.set(name, component_priority_dict, priority=self.getpriority(name) or 0)\n\n    def __setitem__(self, name: _SettingsKeyT, value: Any) -> None:\n        self.set(name, value)\n", "n_tokens": 1054, "byte_len": 4625, "file_sha1": "34809cf372c8def8e57b978d711c90ab9ec16f5a", "start_line": 264, "end_line": 385}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py", "rel_path": "scrapy/settings/__init__.py", "module": "scrapy.settings.__init__", "ext": "py", "chunk_number": 4, "symbols": ["set", "set_in_component_priority_dict", "setdefault", "setdefault_in_component_priority_dict", "setdict", "setmodule", "effect", "method", "those", "configuring", "globally", "getpriority", "name", "import", "module", "more", "string", "variable", "should", "settings", "replaced", "provided", "isinstance", "than", "object", "none", "type", "component", "either", "default", "get_settings_priority", "__init__", "__repr__", "__getitem__", "__contains__", "add_to_list", "remove_from_list", "get", "getbool", "getint", "getfloat", "getlist", "getdict", "getdictorlist", "getwithbase", "maxpriority", "replace_in_component_priority_dict", "__setitem__", "update", "delete"], "ast_kind": "function_or_method", "text": "    def set(\n        self, name: _SettingsKeyT, value: Any, priority: int | str = \"project\"\n    ) -> None:\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: str\n\n        :param value: the value to associate with the setting\n        :type value: object\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)\n\n    def set_in_component_priority_dict(\n        self, name: _SettingsKeyT, cls: type, priority: int | None\n    ) -> None:\n        \"\"\"Set the *cls* component in the *name* :ref:`component priority\n        dictionary <component-priority-dictionaries>` setting with *priority*.\n\n        If *cls* already exists, its value is updated.\n\n        If *cls* was present as an import string, even more than once, those\n        keys are dropped and replaced by *cls*.\n\n        This change is applied regardless of the priority of the *name*\n        setting. The setting priority is not affected by this change either.\n        \"\"\"\n        component_priority_dict = self.getdict(name)\n        for cls_or_path in tuple(component_priority_dict):\n            if not isinstance(cls_or_path, str):\n                continue\n            _cls = load_object(cls_or_path)\n            if _cls == cls:\n                del component_priority_dict[cls_or_path]\n        component_priority_dict[cls] = priority\n        self.set(name, component_priority_dict, self.getpriority(name) or 0)\n\n    def setdefault(\n        self,\n        name: _SettingsKeyT,\n        default: Any = None,\n        priority: int | str = \"project\",\n    ) -> Any:\n        if name not in self:\n            self.set(name, default, priority)\n            return default\n\n        return self.attributes[name].value\n\n    def setdefault_in_component_priority_dict(\n        self, name: _SettingsKeyT, cls: type, priority: int | None\n    ) -> None:\n        \"\"\"Set the *cls* component in the *name* :ref:`component priority\n        dictionary <component-priority-dictionaries>` setting with *priority*\n        if not already defined (even as an import string).\n\n        If *cls* is not already defined, it is set regardless of the priority\n        of the *name* setting. The setting priority is not affected by this\n        change either.\n        \"\"\"\n        component_priority_dict = self.getdict(name)\n        for cls_or_path in tuple(component_priority_dict):\n            if load_object(cls_or_path) == cls:\n                return\n        component_priority_dict[cls] = priority\n        self.set(name, component_priority_dict, self.getpriority(name) or 0)\n\n    def setdict(self, values: _SettingsInputT, priority: int | str = \"project\") -> None:\n        self.update(values, priority)\n\n    def setmodule(\n        self, module: ModuleType | str, priority: int | str = \"project\"\n    ) -> None:\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: types.ModuleType or str\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, str):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)\n", "n_tokens": 939, "byte_len": 4329, "file_sha1": "34809cf372c8def8e57b978d711c90ab9ec16f5a", "start_line": 386, "end_line": 496}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py", "rel_path": "scrapy/settings/__init__.py", "module": "scrapy.settings.__init__", "ext": "py", "chunk_number": 5, "symbols": ["update", "delete", "__delitem__", "_assert_mutability", "copy", "freeze", "frozencopy", "__iter__", "__len__", "_to_dict", "_get_key", "copy_to_dict", "_repr_pretty_", "pop", "Settings", "method", "inputs", "loads", "bool", "instance", "priorities", "updating", "subclass", "getpriority", "after", "possible", "name", "doesn", "make", "repr", "get_settings_priority", "__init__", "set", "__repr__", "__getitem__", "__contains__", "add_to_list", "remove_from_list", "get", "getbool", "getint", "getfloat", "getlist", "getdict", "getdictorlist", "getwithbase", "maxpriority", "replace_in_component_priority_dict", "__setitem__", "set_in_component_priority_dict"], "ast_kind": "class_or_type", "text": "    # BaseSettings.update() doesn't support all inputs that MutableMapping.update() supports\n    def update(self, values: _SettingsInputT, priority: int | str = \"project\") -> None:  # type: ignore[override]\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, str):\n            values = cast(\"dict[_SettingsKeyT, Any]\", json.loads(values))\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in values.items():\n                    self.set(name, value, cast(\"int\", values.getpriority(name)))\n            else:\n                for name, value in values.items():\n                    self.set(name, value, priority)\n\n    def delete(self, name: _SettingsKeyT, priority: int | str = \"project\") -> None:\n        if name not in self:\n            raise KeyError(name)\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= cast(\"int\", self.getpriority(name)):\n            del self.attributes[name]\n\n    def __delitem__(self, name: _SettingsKeyT) -> None:\n        self._assert_mutability()\n        del self.attributes[name]\n\n    def _assert_mutability(self) -> None:\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")\n\n    def copy(self) -> Self:\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)\n\n    def freeze(self) -> None:\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True\n\n    def frozencopy(self) -> Self:\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy\n\n    def __iter__(self) -> Iterator[_SettingsKeyT]:\n        return iter(self.attributes)\n\n    def __len__(self) -> int:\n        return len(self.attributes)\n\n    def _to_dict(self) -> dict[_SettingsKeyT, Any]:\n        return {\n            self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n            for k, v in self.items()\n        }\n\n    def _get_key(self, key_value: Any) -> _SettingsKeyT:\n        return (\n            key_value\n            if isinstance(key_value, (bool, float, int, str, type(None)))\n            else str(key_value)\n        )\n\n    def copy_to_dict(self) -> dict[_SettingsKeyT, Any]:\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()\n\n    # https://ipython.readthedocs.io/en/stable/config/integrating.html#pretty-printing\n    def _repr_pretty_(self, p: Any, cycle: bool) -> None:\n        if cycle:\n            p.text(repr(self))\n        else:\n            p.text(pformat(self.copy_to_dict()))\n\n    def pop(self, name: _SettingsKeyT, default: Any = __default) -> Any:\n        try:\n            value = self.attributes[name].value\n        except KeyError:\n            if default is self.__default:\n                raise\n            return default\n        self.__delitem__(name)\n        return value\n\n\nclass Settings(BaseSettings):\n    \"\"\"\n    This object stores Scrapy settings for the configuration of internal\n    components, and can be used for any further customization.\n\n    It is a direct subclass and supports all methods of\n    :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation\n    of this class, the new object will have the global default settings\n    described on :ref:`topics-settings-ref` already populated.\n    \"\"\"\n", "n_tokens": 1141, "byte_len": 5328, "file_sha1": "34809cf372c8def8e57b978d711c90ab9ec16f5a", "start_line": 497, "end_line": 642}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/__init__.py", "rel_path": "scrapy/settings/__init__.py", "module": "scrapy.settings.__init__", "ext": "py", "chunk_number": 6, "symbols": ["__init__", "iter_default_settings", "overridden_settings", "been", "user", "promote", "overridden", "pass", "mapping", "tuples", "priority", "priorities", "settings", "dict", "replace", "update", "name", "isupper", "iter", "default", "with", "defined", "iterator", "kwarg", "value", "given", "init", "return", "yield", "project", "get_settings_priority", "set", "__repr__", "__getitem__", "__contains__", "add_to_list", "remove_from_list", "get", "getbool", "getint", "getfloat", "getlist", "getdict", "getdictorlist", "getwithbase", "getpriority", "maxpriority", "replace_in_component_priority_dict", "__setitem__", "set_in_component_priority_dict"], "ast_kind": "function_or_method", "text": "    def __init__(self, values: _SettingsInputT = None, priority: int | str = \"project\"):\n        # Do not pass kwarg values here. We don't want to promote user-defined\n        # dicts, and we want to update, not replace, default dicts with the\n        # values given by the user\n        super().__init__()\n        self.setmodule(default_settings, \"default\")\n        # Promote default dictionaries to BaseSettings instances for per-key\n        # priorities\n        for name, val in self.items():\n            if isinstance(val, dict):\n                self.set(name, BaseSettings(val, \"default\"), \"default\")\n        self.update(values, priority)\n\n\ndef iter_default_settings() -> Iterable[tuple[str, Any]]:\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)\n\n\ndef overridden_settings(\n    settings: Mapping[_SettingsKeyT, Any],\n) -> Iterable[tuple[str, Any]]:\n    \"\"\"Return an iterable of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value\n", "n_tokens": 270, "byte_len": 1252, "file_sha1": "34809cf372c8def8e57b978d711c90ab9ec16f5a", "start_line": 643, "end_line": 672}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/default_settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/default_settings.py", "rel_path": "scrapy/settings/default_settings.py", "module": "scrapy.settings.default_settings", "ext": "py", "chunk_number": 1, "symbols": ["used", "usual", "user", "module", "these", "enabling", "exception", "password", "about", "remember", "their", "with", "available", "like", "more", "port", "scrapy", "topics", "values", "other", "host", "documentation", "similar", "first", "settings", "order", "pairs", "developers", "leaving", "blank", "__getattr__", "downloader", "contextfactory", "autothrottl", "star", "lib", "w3lib", "memusag", "warnin", "spider", "concurren", "requests", "mai", "pass", "error", "oserror", "robotstxt", "addons", "anonymous", "trying"], "ast_kind": "unknown", "text": "\"\"\"This module contains the default values for all settings used by Scrapy.\n\nFor more information about these settings you can read the settings\ndocumentation in docs/topics/settings.rst\n\nScrapy developers, if you add a setting here remember to:\n\n* add it in alphabetical order, with the exception that enabling flags and\n  other high-level settings for a group should come first in their group\n  and pairs like host/port and user/password should be in the usual order\n* group similar settings without leaving blank lines\n* add its documentation to the available settings documentation\n  (docs/topics/settings.rst)\n\"\"\"\n", "n_tokens": 122, "byte_len": 619, "file_sha1": "f3d0fe83c1be8c211326407e625dd9eb37740b72", "start_line": 1, "end_line": 15}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/default_settings.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/default_settings.py", "rel_path": "scrapy/settings/default_settings.py", "module": "scrapy.settings.default_settings", "ext": "py", "chunk_number": 2, "symbols": ["downloader", "contextfactory", "autothrottl", "star", "lib", "w3lib", "memusag", "warnin", "spider", "concurren", "requests", "mai", "pass", "error", "oserror", "robotstxt", "addons", "anonymous", "trying", "reques", "fingerprinte", "periodi", "stats", "telnetconsol", "enabled", "pickle", "item", "ajaxcrawl", "idle", "fifo", "__getattr__", "port", "language", "spiderloader", "here", "targe", "twiste", "reactor", "accept", "feed", "exporter", "http", "schedule", "dis", "engine", "template", "dir", "come", "empty", "values"], "ast_kind": "imports", "text": "import sys\nfrom importlib import import_module\nfrom pathlib import Path\n\n__all__ = [\n    \"ADDONS\",\n    \"AJAXCRAWL_ENABLED\",\n    \"AJAXCRAWL_MAXSIZE\",\n    \"ASYNCIO_EVENT_LOOP\",\n    \"AUTOTHROTTLE_DEBUG\",\n    \"AUTOTHROTTLE_ENABLED\",\n    \"AUTOTHROTTLE_MAX_DELAY\",\n    \"AUTOTHROTTLE_START_DELAY\",\n    \"AUTOTHROTTLE_TARGET_CONCURRENCY\",\n    \"BOT_NAME\",\n    \"CLOSESPIDER_ERRORCOUNT\",\n    \"CLOSESPIDER_ITEMCOUNT\",\n    \"CLOSESPIDER_PAGECOUNT\",\n    \"CLOSESPIDER_TIMEOUT\",\n    \"COMMANDS_MODULE\",\n    \"COMPRESSION_ENABLED\",\n    \"CONCURRENT_ITEMS\",\n    \"CONCURRENT_REQUESTS\",\n    \"CONCURRENT_REQUESTS_PER_DOMAIN\",\n    \"COOKIES_DEBUG\",\n    \"COOKIES_ENABLED\",\n    \"CRAWLSPIDER_FOLLOW_LINKS\",\n    \"DEFAULT_DROPITEM_LOG_LEVEL\",\n    \"DEFAULT_ITEM_CLASS\",\n    \"DEFAULT_REQUEST_HEADERS\",\n    \"DEPTH_LIMIT\",\n    \"DEPTH_PRIORITY\",\n    \"DEPTH_STATS_VERBOSE\",\n    \"DNSCACHE_ENABLED\",\n    \"DNSCACHE_SIZE\",\n    \"DNS_RESOLVER\",\n    \"DNS_TIMEOUT\",\n    \"DOWNLOADER\",\n    \"DOWNLOADER_CLIENTCONTEXTFACTORY\",\n    \"DOWNLOADER_CLIENT_TLS_CIPHERS\",\n    \"DOWNLOADER_CLIENT_TLS_METHOD\",\n    \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\",\n    \"DOWNLOADER_HTTPCLIENTFACTORY\",\n    \"DOWNLOADER_MIDDLEWARES\",\n    \"DOWNLOADER_MIDDLEWARES_BASE\",\n    \"DOWNLOADER_STATS\",\n    \"DOWNLOAD_DELAY\",\n    \"DOWNLOAD_FAIL_ON_DATALOSS\",\n    \"DOWNLOAD_HANDLERS\",\n    \"DOWNLOAD_HANDLERS_BASE\",\n    \"DOWNLOAD_MAXSIZE\",\n    \"DOWNLOAD_TIMEOUT\",\n    \"DOWNLOAD_WARNSIZE\",\n    \"DUPEFILTER_CLASS\",\n    \"EDITOR\",\n    \"EXTENSIONS\",\n    \"EXTENSIONS_BASE\",\n    \"FEEDS\",\n    \"FEED_EXPORTERS\",\n    \"FEED_EXPORTERS_BASE\",\n    \"FEED_EXPORT_BATCH_ITEM_COUNT\",\n    \"FEED_EXPORT_ENCODING\",\n    \"FEED_EXPORT_FIELDS\",\n    \"FEED_EXPORT_INDENT\",\n    \"FEED_FORMAT\",\n    \"FEED_STORAGES\",\n    \"FEED_STORAGES_BASE\",\n    \"FEED_STORAGE_FTP_ACTIVE\",\n    \"FEED_STORAGE_GCS_ACL\",\n    \"FEED_STORAGE_S3_ACL\",\n    \"FEED_STORE_EMPTY\",\n    \"FEED_TEMPDIR\",\n    \"FEED_URI_PARAMS\",\n    \"FILES_STORE_GCS_ACL\",\n    \"FILES_STORE_S3_ACL\",\n    \"FORCE_CRAWLER_PROCESS\",\n    \"FTP_PASSIVE_MODE\",\n    \"FTP_PASSWORD\",\n    \"FTP_USER\",\n    \"GCS_PROJECT_ID\",\n    \"HTTPCACHE_ALWAYS_STORE\",\n    \"HTTPCACHE_DBM_MODULE\",\n    \"HTTPCACHE_DIR\",\n    \"HTTPCACHE_ENABLED\",\n    \"HTTPCACHE_EXPIRATION_SECS\",\n    \"HTTPCACHE_GZIP\",\n    \"HTTPCACHE_IGNORE_HTTP_CODES\",\n    \"HTTPCACHE_IGNORE_MISSING\",\n    \"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\",\n    \"HTTPCACHE_IGNORE_SCHEMES\",\n    \"HTTPCACHE_POLICY\",\n    \"HTTPCACHE_STORAGE\",\n    \"HTTPPROXY_AUTH_ENCODING\",\n    \"HTTPPROXY_ENABLED\",\n    \"IMAGES_STORE_GCS_ACL\",\n    \"IMAGES_STORE_S3_ACL\",\n    \"ITEM_PIPELINES\",\n    \"ITEM_PIPELINES_BASE\",\n    \"ITEM_PROCESSOR\",\n    \"JOBDIR\",\n    \"LOGSTATS_INTERVAL\",\n    \"LOG_DATEFORMAT\",\n    \"LOG_ENABLED\",\n    \"LOG_ENCODING\",\n    \"LOG_FILE\",\n    \"LOG_FILE_APPEND\",\n    \"LOG_FORMAT\",\n    \"LOG_FORMATTER\",\n    \"LOG_LEVEL\",\n    \"LOG_SHORT_NAMES\",\n    \"LOG_STDOUT\",\n    \"LOG_VERSIONS\",\n    \"MAIL_FROM\",\n    \"MAIL_HOST\",\n    \"MAIL_PASS\",\n    \"MAIL_PORT\",\n    \"MAIL_USER\",\n    \"MEMDEBUG_ENABLED\",\n    \"MEMDEBUG_NOTIFY\",\n    \"MEMUSAGE_CHECK_INTERVAL_SECONDS\",\n    \"MEMUSAGE_ENABLED\",\n    \"MEMUSAGE_LIMIT_MB\",\n    \"MEMUSAGE_NOTIFY_MAIL\",\n    \"MEMUSAGE_WARNING_MB\",\n    \"METAREFRESH_ENABLED\",\n    \"METAREFRESH_IGNORE_TAGS\",\n    \"METAREFRESH_MAXDELAY\",\n    \"NEWSPIDER_MODULE\",\n    \"PERIODIC_LOG_DELTA\",\n    \"PERIODIC_LOG_STATS\",\n    \"PERIODIC_LOG_TIMING_ENABLED\",\n    \"RANDOMIZE_DOWNLOAD_DELAY\",\n    \"REACTOR_THREADPOOL_MAXSIZE\",\n    \"REDIRECT_ENABLED\",\n    \"REDIRECT_MAX_TIMES\",\n    \"REDIRECT_PRIORITY_ADJUST\",\n    \"REFERER_ENABLED\",\n    \"REFERRER_POLICY\",\n    \"REQUEST_FINGERPRINTER_CLASS\",\n    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\",\n    \"RETRY_ENABLED\",\n    \"RETRY_EXCEPTIONS\",\n    \"RETRY_HTTP_CODES\",\n    \"RETRY_PRIORITY_ADJUST\",\n    \"RETRY_TIMES\",\n    \"ROBOTSTXT_OBEY\",\n    \"ROBOTSTXT_PARSER\",\n    \"ROBOTSTXT_USER_AGENT\",\n    \"SCHEDULER\",\n    \"SCHEDULER_DEBUG\",\n    \"SCHEDULER_DISK_QUEUE\",\n    \"SCHEDULER_MEMORY_QUEUE\",\n    \"SCHEDULER_PRIORITY_QUEUE\",\n    \"SCHEDULER_START_DISK_QUEUE\",\n    \"SCHEDULER_START_MEMORY_QUEUE\",\n    \"SCRAPER_SLOT_MAX_ACTIVE_SIZE\",\n    \"SPIDER_CONTRACTS\",\n    \"SPIDER_CONTRACTS_BASE\",\n    \"SPIDER_LOADER_CLASS\",\n    \"SPIDER_LOADER_WARN_ONLY\",\n    \"SPIDER_MIDDLEWARES\",\n    \"SPIDER_MIDDLEWARES_BASE\",\n    \"SPIDER_MODULES\",\n    \"STATSMAILER_RCPTS\",\n    \"STATS_CLASS\",\n    \"STATS_DUMP\",\n    \"TELNETCONSOLE_ENABLED\",\n    \"TELNETCONSOLE_HOST\",\n    \"TELNETCONSOLE_PASSWORD\",\n    \"TELNETCONSOLE_PORT\",\n    \"TELNETCONSOLE_USERNAME\",\n    \"TEMPLATES_DIR\",\n    \"TWISTED_REACTOR\",\n    \"URLLENGTH_LIMIT\",\n    \"USER_AGENT\",\n    \"WARN_ON_GENERATOR_RETURN_VALUE\",\n]\n\nADDONS = {}\n\nAJAXCRAWL_ENABLED = False\nAJAXCRAWL_MAXSIZE = 32768\n\nASYNCIO_EVENT_LOOP = None\n\nAUTOTHROTTLE_ENABLED = False\nAUTOTHROTTLE_DEBUG = False\nAUTOTHROTTLE_MAX_DELAY = 60.0\nAUTOTHROTTLE_START_DELAY = 5.0\nAUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n\nBOT_NAME = \"scrapybot\"\n\nCLOSESPIDER_ERRORCOUNT = 0\nCLOSESPIDER_ITEMCOUNT = 0\nCLOSESPIDER_PAGECOUNT = 0\nCLOSESPIDER_TIMEOUT = 0\n\nCOMMANDS_MODULE = \"\"\n\nCOMPRESSION_ENABLED = True\n\nCONCURRENT_ITEMS = 100\n\nCONCURRENT_REQUESTS = 16\nCONCURRENT_REQUESTS_PER_DOMAIN = 8\n\nCOOKIES_ENABLED = True\nCOOKIES_DEBUG = False\n\nCRAWLSPIDER_FOLLOW_LINKS = True\n\nDEFAULT_DROPITEM_LOG_LEVEL = \"WARNING\"\n\nDEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n\nDEFAULT_REQUEST_HEADERS = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"en\",\n}\n\nDEPTH_LIMIT = 0\nDEPTH_PRIORITY = 0\nDEPTH_STATS_VERBOSE = False\n\nDNSCACHE_ENABLED = True\nDNSCACHE_SIZE = 10000\nDNS_RESOLVER = \"scrapy.resolver.CachingThreadedResolver\"\nDNS_TIMEOUT = 60\n\nDOWNLOAD_DELAY = 0\n\nDOWNLOAD_FAIL_ON_DATALOSS = True\n\nDOWNLOAD_HANDLERS = {}\nDOWNLOAD_HANDLERS_BASE = {\n    \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n    \"file\": \"scrapy.core.downloader.handlers.file.FileDownloadHandler\",\n    \"http\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"https\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"s3\": \"scrapy.core.downloader.handlers.s3.S3DownloadHandler\",\n    \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\",\n}\n\nDOWNLOAD_MAXSIZE = 1024 * 1024 * 1024  # 1024m\nDOWNLOAD_WARNSIZE = 32 * 1024 * 1024  # 32m\n\nDOWNLOAD_TIMEOUT = 180  # 3mins\n\nDOWNLOADER = \"scrapy.core.downloader.Downloader\"\n\nDOWNLOADER_CLIENTCONTEXTFACTORY = (\n    \"scrapy.core.downloader.contextfactory.ScrapyClientContextFactory\"\n)\nDOWNLOADER_CLIENT_TLS_CIPHERS = \"DEFAULT\"\n# Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:\nDOWNLOADER_CLIENT_TLS_METHOD = \"TLS\"\nDOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n\nDOWNLOADER_HTTPCLIENTFACTORY = (\n    \"scrapy.core.downloader.webclient.ScrapyHTTPClientFactory\"\n)\n\nDOWNLOADER_MIDDLEWARES = {}\nDOWNLOADER_MIDDLEWARES_BASE = {\n    # Engine side\n    \"scrapy.downloadermiddlewares.offsite.OffsiteMiddleware\": 50,\n    \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n    \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n    \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n    \"scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\": 400,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": 500,\n    \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 550,\n    \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware\": 560,\n    \"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\": 580,\n    \"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\": 590,\n    \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 600,\n    \"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\": 700,\n    \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\": 750,\n    \"scrapy.downloadermiddlewares.stats.DownloaderStats\": 850,\n    \"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\": 900,\n    # Downloader side\n}\n\nDOWNLOADER_STATS = True\n\nDUPEFILTER_CLASS = \"scrapy.dupefilters.RFPDupeFilter\"\n\nEDITOR = \"vi\"\nif sys.platform == \"win32\":\n    EDITOR = \"%s -m idlelib.idle\"\n\nEXTENSIONS = {}\nEXTENSIONS_BASE = {\n    \"scrapy.extensions.corestats.CoreStats\": 0,\n    \"scrapy.extensions.telnet.TelnetConsole\": 0,\n    \"scrapy.extensions.memusage.MemoryUsage\": 0,\n    \"scrapy.extensions.memdebug.MemoryDebugger\": 0,\n    \"scrapy.extensions.closespider.CloseSpider\": 0,\n    \"scrapy.extensions.feedexport.FeedExporter\": 0,\n    \"scrapy.extensions.logstats.LogStats\": 0,\n    \"scrapy.extensions.spiderstate.SpiderState\": 0,\n    \"scrapy.extensions.throttle.AutoThrottle\": 0,\n}\n\nFEEDS = {}\nFEED_EXPORT_BATCH_ITEM_COUNT = 0\nFEED_EXPORT_ENCODING = None\nFEED_EXPORT_FIELDS = None\nFEED_EXPORT_INDENT = 0\nFEED_EXPORTERS = {}\nFEED_EXPORTERS_BASE = {\n    \"json\": \"scrapy.exporters.JsonItemExporter\",\n    \"jsonlines\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"jsonl\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"jl\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"csv\": \"scrapy.exporters.CsvItemExporter\",\n    \"xml\": \"scrapy.exporters.XmlItemExporter\",\n    \"marshal\": \"scrapy.exporters.MarshalItemExporter\",\n    \"pickle\": \"scrapy.exporters.PickleItemExporter\",\n}\nFEED_FORMAT = \"jsonlines\"\nFEED_STORE_EMPTY = True\nFEED_STORAGES = {}\nFEED_STORAGES_BASE = {\n    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n    \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n}\nFEED_STORAGE_FTP_ACTIVE = False\nFEED_STORAGE_GCS_ACL = \"\"\nFEED_STORAGE_S3_ACL = \"\"\nFEED_TEMPDIR = None\nFEED_URI_PARAMS = None  # a function to extend uri arguments\n\nFILES_STORE_GCS_ACL = \"\"\nFILES_STORE_S3_ACL = \"private\"\n\nFORCE_CRAWLER_PROCESS = False\n\nFTP_PASSIVE_MODE = True\nFTP_USER = \"anonymous\"\nFTP_PASSWORD = \"guest\"  # noqa: S105\n\nGCS_PROJECT_ID = None\n\nHTTPCACHE_ENABLED = False\nHTTPCACHE_ALWAYS_STORE = False\nHTTPCACHE_DBM_MODULE = \"dbm\"\nHTTPCACHE_DIR = \"httpcache\"\nHTTPCACHE_EXPIRATION_SECS = 0\nHTTPCACHE_GZIP = False\nHTTPCACHE_IGNORE_HTTP_CODES = []\nHTTPCACHE_IGNORE_MISSING = False\nHTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\nHTTPCACHE_IGNORE_SCHEMES = [\"file\"]\nHTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\nHTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n\nHTTPPROXY_ENABLED = True\nHTTPPROXY_AUTH_ENCODING = \"latin-1\"\n\nIMAGES_STORE_GCS_ACL = \"\"\nIMAGES_STORE_S3_ACL = \"private\"\n\nITEM_PIPELINES = {}\nITEM_PIPELINES_BASE = {}\n\nITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n\nJOBDIR = None\n\nLOG_ENABLED = True\nLOG_DATEFORMAT = \"%Y-%m-%d %H:%M:%S\"\nLOG_ENCODING = \"utf-8\"\nLOG_FILE = None\nLOG_FILE_APPEND = True\nLOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\"\nLOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\nLOG_LEVEL = \"DEBUG\"\nLOG_SHORT_NAMES = False\nLOG_STDOUT = False\nLOG_VERSIONS = [\n    \"lxml\",\n    \"libxml2\",\n    \"cssselect\",\n    \"parsel\",\n    \"w3lib\",\n    \"Twisted\",\n    \"Python\",\n    \"pyOpenSSL\",\n    \"cryptography\",\n    \"Platform\",\n]\n\nLOGSTATS_INTERVAL = 60.0\n\nMAIL_FROM = \"scrapy@localhost\"\nMAIL_HOST = \"localhost\"\nMAIL_PORT = 25\nMAIL_USER = None\nMAIL_PASS = None\n\nMEMDEBUG_ENABLED = False  # enable memory debugging\nMEMDEBUG_NOTIFY = []  # send memory debugging report by mail at engine shutdown\n\nMEMUSAGE_ENABLED = True\nMEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\nMEMUSAGE_LIMIT_MB = 0\nMEMUSAGE_NOTIFY_MAIL = []\nMEMUSAGE_WARNING_MB = 0\n\nMETAREFRESH_ENABLED = True\nMETAREFRESH_IGNORE_TAGS = [\"noscript\"]\nMETAREFRESH_MAXDELAY = 100\n\nNEWSPIDER_MODULE = \"\"\n\nPERIODIC_LOG_DELTA = None\nPERIODIC_LOG_STATS = None\nPERIODIC_LOG_TIMING_ENABLED = False\n\nRANDOMIZE_DOWNLOAD_DELAY = True\n\nREACTOR_THREADPOOL_MAXSIZE = 10\n\nREDIRECT_ENABLED = True\nREDIRECT_MAX_TIMES = 20  # uses Firefox default setting\nREDIRECT_PRIORITY_ADJUST = +2\n\nREFERER_ENABLED = True\nREFERRER_POLICY = \"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"\n\nREQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\nREQUEST_FINGERPRINTER_IMPLEMENTATION = \"SENTINEL\"\n\nRETRY_ENABLED = True\nRETRY_EXCEPTIONS = [\n    \"twisted.internet.defer.TimeoutError\",\n    \"twisted.internet.error.TimeoutError\",\n    \"twisted.internet.error.DNSLookupError\",\n    \"twisted.internet.error.ConnectionRefusedError\",\n    \"twisted.internet.error.ConnectionDone\",\n    \"twisted.internet.error.ConnectError\",\n    \"twisted.internet.error.ConnectionLost\",\n    \"twisted.internet.error.TCPTimedOutError\",\n    \"twisted.web.client.ResponseFailed\",\n    # OSError is raised by the HttpCompression middleware when trying to\n    # decompress an empty response\n    OSError,\n    \"scrapy.core.downloader.handlers.http11.TunnelError\",\n]\nRETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\nRETRY_PRIORITY_ADJUST = -1\nRETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n\nROBOTSTXT_OBEY = False\nROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\nROBOTSTXT_USER_AGENT = None\n\nSCHEDULER = \"scrapy.core.scheduler.Scheduler\"\nSCHEDULER_DEBUG = False\nSCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleLifoDiskQueue\"\nSCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.LifoMemoryQueue\"\nSCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.ScrapyPriorityQueue\"\nSCHEDULER_START_DISK_QUEUE = \"scrapy.squeues.PickleFifoDiskQueue\"\nSCHEDULER_START_MEMORY_QUEUE = \"scrapy.squeues.FifoMemoryQueue\"\n\nSCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n\nSPIDER_CONTRACTS = {}\nSPIDER_CONTRACTS_BASE = {\n    \"scrapy.contracts.default.UrlContract\": 1,\n    \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n    \"scrapy.contracts.default.MetadataContract\": 1,\n    \"scrapy.contracts.default.ReturnsContract\": 2,\n    \"scrapy.contracts.default.ScrapesContract\": 3,\n}\n\nSPIDER_LOADER_CLASS = \"scrapy.spiderloader.SpiderLoader\"\nSPIDER_LOADER_WARN_ONLY = False\n\nSPIDER_MIDDLEWARES = {}\nSPIDER_MIDDLEWARES_BASE = {\n    # Engine side\n    \"scrapy.spidermiddlewares.start.StartSpiderMiddleware\": 25,\n    \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n    \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n    \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n    \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900,\n    # Spider side\n}\n\nSPIDER_MODULES = []\n\nSTATS_CLASS = \"scrapy.statscollectors.MemoryStatsCollector\"\nSTATS_DUMP = True\n\nSTATSMAILER_RCPTS = []\n\nTELNETCONSOLE_ENABLED = 1\nTELNETCONSOLE_HOST = \"127.0.0.1\"\nTELNETCONSOLE_PORT = [6023, 6073]\nTELNETCONSOLE_USERNAME = \"scrapy\"\nTELNETCONSOLE_PASSWORD = None\n\nTEMPLATES_DIR = str((Path(__file__).parent / \"..\" / \"templates\").resolve())\n\nTWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n\nURLLENGTH_LIMIT = 2083\n\nUSER_AGENT = f\"Scrapy/{import_module('scrapy').__version__} (+https://scrapy.org)\"\n\nWARN_ON_GENERATOR_RETURN_VALUE = True\n\n", "n_tokens": 4067, "byte_len": 14750, "file_sha1": "f3d0fe83c1be8c211326407e625dd9eb37740b72", "start_line": 16, "end_line": 535}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/default_settings.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/settings/default_settings.py", "rel_path": "scrapy/settings/default_settings.py", "module": "scrapy.settings.default_settings", "ext": "py", "chunk_number": 3, "symbols": ["__getattr__", "scrapy", "deprecation", "getattr", "return", "name", "concurren", "request", "instead", "deprecated", "warnings", "plc0415", "noqa", "default", "settings", "from", "stacklevel", "attribute", "exceptions", "import", "raise", "warn", "error", "downloader", "contextfactory", "autothrottl", "star", "lib", "w3lib", "memusag", "warnin", "spider", "requests", "mai", "pass", "oserror", "port", "robotstxt", "addons", "anonymous", "trying", "reques", "fingerprinte", "periodi", "stats", "telnetconsol", "enabled", "pickle", "item", "ajaxcrawl"], "ast_kind": "function_or_method", "text": "def __getattr__(name: str):\n    if name == \"CONCURRENT_REQUESTS_PER_IP\":\n        import warnings  # noqa: PLC0415\n\n        from scrapy.exceptions import ScrapyDeprecationWarning  # noqa: PLC0415\n\n        warnings.warn(\n            \"The scrapy.settings.default_settings.CONCURRENT_REQUESTS_PER_IP attribute is deprecated, use scrapy.settings.default_settings.CONCURRENT_REQUESTS_PER_DOMAIN instead.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return 0\n\n    raise AttributeError\n", "n_tokens": 109, "byte_len": 517, "file_sha1": "f3d0fe83c1be8c211326407e625dd9eb37740b72", "start_line": 536, "end_line": 550}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/selector/unified.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/selector/unified.py", "rel_path": "scrapy/selector/unified.py", "module": "scrapy.selector.unified", "ext": "py", "chunk_number": 1, "symbols": ["_st", "_response_from_text", "__init__", "SelectorList", "Selector", "encoding", "instance", "subclass", "case", "selector", "about", "passed", "future", "list", "string", "detection", "slots", "defines", "encoded", "isinstance", "object", "anything", "none", "base", "url", "html", "type", "methods", "together", "http", "default", "bytes", "automatically", "using", "based", "below", "typing", "ref", "return", "over", "annotations", "class", "follows", "noqa", "parsel", "parts", "xml", "response", "kwargs", "blank"], "ast_kind": "class_or_type", "text": "\"\"\"\nXPath selectors based on lxml\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom parsel import Selector as _ParselSelector\n\nfrom scrapy.http import HtmlResponse, TextResponse, XmlResponse\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.response import get_base_url\nfrom scrapy.utils.trackref import object_ref\n\n__all__ = [\"Selector\", \"SelectorList\"]\n\n_NOT_SET = object()\n\n\ndef _st(response: TextResponse | None, st: str | None) -> str:\n    if st is None:\n        return \"xml\" if isinstance(response, XmlResponse) else \"html\"\n    return st\n\n\ndef _response_from_text(text: str | bytes, st: str | None) -> TextResponse:\n    rt: type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n    return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n\n\nclass SelectorList(_ParselSelector.selectorlist_cls, object_ref):\n    \"\"\"\n    The :class:`SelectorList` class is a subclass of the builtin ``list``\n    class, which provides a few additional methods.\n    \"\"\"\n\n\nclass Selector(_ParselSelector, object_ref):\n    \"\"\"\n    An instance of :class:`Selector` is a wrapper over response to select\n    certain parts of its content.\n\n    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an\n    :class:`~scrapy.http.XmlResponse` object that will be used for selecting\n    and extracting data.\n\n    ``text`` is a unicode string or utf-8 encoded text for cases when a\n    ``response`` isn't available. Using ``text`` and ``response`` together is\n    undefined behavior.\n\n    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``, ``\"json\"``\n    or ``None`` (default).\n\n    If ``type`` is ``None``, the selector automatically chooses the best type\n    based on ``response`` type (see below), or defaults to ``\"html\"`` in case it\n    is used together with ``text``.\n\n    If ``type`` is ``None`` and a ``response`` is passed, the selector type is\n    inferred from the response type as follows:\n\n    * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n    * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n    * ``\"json\"`` for :class:`~scrapy.http.TextResponse` type\n    * ``\"html\"`` for anything else\n\n    Otherwise, if ``type`` is set, the selector type will be forced and no\n    detection will occur.\n    \"\"\"\n\n    __slots__ = [\"response\"]\n    selectorlist_cls = SelectorList\n\n    def __init__(\n        self,\n        response: TextResponse | None = None,\n        text: str | None = None,\n        type: str | None = None,  # noqa: A002\n        root: Any | None = _NOT_SET,\n        **kwargs: Any,\n    ):\n        if response is not None and text is not None:\n            raise ValueError(\n                f\"{self.__class__.__name__}.__init__() received both response and text\"\n            )\n\n        st = _st(response, type)\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            text = response.text\n            kwargs.setdefault(\"base_url\", get_base_url(response))\n\n        self.response = response\n\n        if root is not _NOT_SET:\n            kwargs[\"root\"] = root\n\n        super().__init__(text=text, type=st, **kwargs)\n", "n_tokens": 783, "byte_len": 3170, "file_sha1": "1c86e7449a7d52f466e3b93ae8b43435b7257163", "start_line": 1, "end_line": 102}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/selector/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/selector/__init__.py", "rel_path": "scrapy/selector/__init__.py", "module": "scrapy.selector.__init__", "ext": "py", "chunk_number": 1, "symbols": ["unified", "selector", "level", "from", "selectors", "import", "imports", "scrapy", "list", "all"], "ast_kind": "imports", "text": "\"\"\"\nSelectors\n\"\"\"\n\n# top-level imports\nfrom scrapy.selector.unified import Selector, SelectorList\n\n__all__ = [\n    \"Selector\",\n    \"SelectorList\",\n]\n", "n_tokens": 35, "byte_len": 149, "file_sha1": "0a45dd2dfe1af74bb57cbaff54db681500ab6247", "start_line": 1, "end_line": 12}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py", "rel_path": "scrapy/core/scraper.py", "module": "scrapy.core.scraper", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "add_response_request", "next_response_request_deferred", "finish_response", "is_idle", "needs_backout", "Slot", "Scraper", "failure", "bool", "parallel", "log", "formatter", "append", "finish", "response", "spider", "responses", "popleft", "warn", "future", "typ", "checking", "signalmanager", "isinstance", "closing", "them", "spidermw", "none", "argument", "open_spider", "close_spider", "_check_if_closing", "enqueue_scrape", "_scrape_next", "call_spider", "handle_spider_error", "handle_spider_output", "_process_spidermw_output", "start_itemproc", "itemproc", "has", "method", "call", "async", "does", "scrape", "handle", "send", "inc"], "ast_kind": "class_or_type", "text": "\"\"\"This module implements the Scraper component which parses responses and\nextracts information from them\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport warnings\nfrom collections import deque\nfrom collections.abc import AsyncIterator\nfrom typing import TYPE_CHECKING, Any, TypeVar, Union\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider, signals\nfrom scrapy.core.spidermw import SpiderMiddlewareManager\nfrom scrapy.exceptions import (\n    CloseSpider,\n    DropItem,\n    IgnoreRequest,\n    ScrapyDeprecationWarning,\n)\nfrom scrapy.http import Request, Response\nfrom scrapy.pipelines import ItemPipelineManager\nfrom scrapy.utils.asyncio import _parallel_asyncio, is_asyncio_available\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.defer import (\n    _defer_sleep_async,\n    _schedule_coro,\n    aiter_errback,\n    deferred_from_coro,\n    ensure_awaitable,\n    iter_errback,\n    maybe_deferred_to_future,\n    parallel,\n    parallel_async,\n)\nfrom scrapy.utils.deprecate import argument_is_required, method_is_overridden\nfrom scrapy.utils.log import failure_to_exc_info, logformatter_adapter\nfrom scrapy.utils.misc import load_object, warn_on_generator_with_return_value\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from collections.abc import Generator, Iterable\n\n    from scrapy.crawler import Crawler\n    from scrapy.logformatter import LogFormatter\n    from scrapy.signalmanager import SignalManager\n\n\nlogger = logging.getLogger(__name__)\n\n\n_T = TypeVar(\"_T\")\nQueueTuple = tuple[Union[Response, Failure], Request, Deferred[None]]\n\n\nclass Slot:\n    \"\"\"Scraper slot (one per running spider)\"\"\"\n\n    MIN_RESPONSE_SIZE = 1024\n\n    def __init__(self, max_active_size: int = 5000000):\n        self.max_active_size: int = max_active_size\n        self.queue: deque[QueueTuple] = deque()\n        self.active: set[Request] = set()\n        self.active_size: int = 0\n        self.itemproc_size: int = 0\n        self.closing: Deferred[Spider] | None = None\n\n    def add_response_request(\n        self, result: Response | Failure, request: Request\n    ) -> Deferred[None]:\n        # this Deferred will be awaited in enqueue_scrape()\n        deferred: Deferred[None] = Deferred()\n        self.queue.append((result, request, deferred))\n        if isinstance(result, Response):\n            self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred\n\n    def next_response_request_deferred(self) -> QueueTuple:\n        result, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return result, request, deferred\n\n    def finish_response(self, result: Response | Failure, request: Request) -> None:\n        self.active.remove(request)\n        if isinstance(result, Response):\n            self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE\n\n    def is_idle(self) -> bool:\n        return not (self.queue or self.active)\n\n    def needs_backout(self) -> bool:\n        return self.active_size > self.max_active_size\n\n\nclass Scraper:", "n_tokens": 714, "byte_len": 3324, "file_sha1": "8c816b59023cb12bf7e56f9a8b23e8af2a421e4b", "start_line": 1, "end_line": 103}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py", "rel_path": "scrapy/core/scraper.py", "module": "scrapy.core.scraper", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "open_spider", "close_spider", "is_idle", "_check_if_closing", "itemproc", "has", "method", "async", "bool", "log", "formatter", "append", "future", "spider", "doesn", "deprecated", "spiders", "passed", "more", "elif", "settings", "close", "process", "methods", "closing", "spidermw", "none", "argument", "required", "add_response_request", "next_response_request_deferred", "finish_response", "needs_backout", "enqueue_scrape", "_scrape_next", "call_spider", "handle_spider_error", "handle_spider_output", "_process_spidermw_output", "start_itemproc", "Slot", "Scraper", "failure", "call", "does", "scrape", "handle", "parallel", "send"], "ast_kind": "function_or_method", "text": "    def __init__(self, crawler: Crawler) -> None:\n        self.slot: Slot | None = None\n        self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(\n            crawler\n        )\n        itemproc_cls: type[ItemPipelineManager] = load_object(\n            crawler.settings[\"ITEM_PROCESSOR\"]\n        )\n        self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n        itemproc_methods = [\n            \"open_spider\",\n            \"close_spider\",\n        ]\n        if not hasattr(self.itemproc, \"process_item_async\"):\n            warnings.warn(\n                f\"{global_object_name(itemproc_cls)} doesn't define a process_item_async() method,\"\n                f\" this is deprecated and the method will be required in future Scrapy versions.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            itemproc_methods.append(\"process_item\")\n            self._itemproc_has_process_async = False\n        elif (\n            issubclass(itemproc_cls, ItemPipelineManager)\n            and method_is_overridden(itemproc_cls, ItemPipelineManager, \"process_item\")\n            and not method_is_overridden(\n                itemproc_cls, ItemPipelineManager, \"process_item_async\"\n            )\n        ):\n            warnings.warn(\n                f\"{global_object_name(itemproc_cls)} overrides process_item() but doesn't override process_item_async().\"\n                f\" This is deprecated. process_item() will be used, but in future Scrapy versions process_item_async() will be used instead.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            itemproc_methods.append(\"process_item\")\n            self._itemproc_has_process_async = False\n        else:\n            self._itemproc_has_process_async = True\n        self._itemproc_needs_spider: dict[str, bool] = {}\n        for method in itemproc_methods:\n            self._itemproc_needs_spider[method] = argument_is_required(\n                getattr(self.itemproc, method), \"spider\"\n            )\n            if self._itemproc_needs_spider[method]:\n                warnings.warn(\n                    f\"The {method}() method of {global_object_name(itemproc_cls)} requires a spider argument,\"\n                    f\" this is deprecated and the argument will not be passed in future Scrapy versions.\",\n                    ScrapyDeprecationWarning,\n                    stacklevel=2,\n                )\n\n        self.concurrent_items: int = crawler.settings.getint(\"CONCURRENT_ITEMS\")\n        self.crawler: Crawler = crawler\n        self.signals: SignalManager = crawler.signals\n        assert crawler.logformatter\n        self.logformatter: LogFormatter = crawler.logformatter\n\n    def open_spider(self, spider: Spider | None = None) -> Deferred[None]:\n        warnings.warn(\n            \"Scraper.open_spider() is deprecated, use open_spider_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.open_spider_async())\n\n    async def open_spider_async(self) -> None:\n        \"\"\"Open the spider for scraping and allocate resources for it.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n        if not self.crawler.spider:\n            raise RuntimeError(\n                \"Scraper.open_spider() called before Crawler.spider is set.\"\n            )\n        if self._itemproc_needs_spider[\"open_spider\"]:\n            await maybe_deferred_to_future(\n                self.itemproc.open_spider(self.crawler.spider)\n            )\n        else:\n            await maybe_deferred_to_future(self.itemproc.open_spider())\n\n    def close_spider(self, spider: Spider | None = None) -> Deferred[None]:\n        warnings.warn(\n            \"Scraper.close_spider() is deprecated, use close_spider_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.close_spider_async())\n\n    async def close_spider_async(self) -> None:\n        \"\"\"Close the spider being scraped and release its resources.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        if self.slot is None:\n            raise RuntimeError(\"Scraper slot not assigned\")\n        self.slot.closing = Deferred()\n        self._check_if_closing()\n        await maybe_deferred_to_future(self.slot.closing)\n        if self._itemproc_needs_spider[\"close_spider\"]:\n            await maybe_deferred_to_future(\n                self.itemproc.close_spider(self.crawler.spider)\n            )\n        else:\n            await maybe_deferred_to_future(self.itemproc.close_spider())\n\n    def is_idle(self) -> bool:\n        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n        return not self.slot\n\n    def _check_if_closing(self) -> None:\n        assert self.slot is not None  # typing\n        if self.slot.closing and self.slot.is_idle():\n            assert self.crawler.spider\n            self.slot.closing.callback(self.crawler.spider)\n", "n_tokens": 1051, "byte_len": 5075, "file_sha1": "8c816b59023cb12bf7e56f9a8b23e8af2a421e4b", "start_line": 104, "end_line": 221}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py", "rel_path": "scrapy/core/scraper.py", "module": "scrapy.core.scraper", "ext": "py", "chunk_number": 3, "symbols": ["enqueue_scrape", "_scrape_next", "call_spider", "failure", "call", "spider", "async", "scrape", "finish", "response", "didn", "warn", "deprecated", "passed", "isinstance", "spidermw", "none", "type", "runtime", "error", "exc", "info", "fired", "expected", "await", "ignore", "request", "slot", "typing", "return", "__init__", "add_response_request", "next_response_request_deferred", "finish_response", "is_idle", "needs_backout", "open_spider", "close_spider", "_check_if_closing", "handle_spider_error", "handle_spider_output", "_process_spidermw_output", "start_itemproc", "Slot", "Scraper", "itemproc", "has", "method", "does", "handle"], "ast_kind": "function_or_method", "text": "    @inlineCallbacks\n    @_warn_spider_arg\n    def enqueue_scrape(\n        self, result: Response | Failure, request: Request, spider: Spider | None = None\n    ) -> Generator[Deferred[Any], Any, None]:\n        if self.slot is None:\n            raise RuntimeError(\"Scraper slot not assigned\")\n        dfd = self.slot.add_response_request(result, request)\n        self._scrape_next()\n        try:\n            yield dfd  # fired in _wait_for_processing()\n        except Exception:\n            logger.error(\n                \"Scraper bug processing %(request)s\",\n                {\"request\": request},\n                exc_info=True,\n                extra={\"spider\": self.crawler.spider},\n            )\n        finally:\n            self.slot.finish_response(result, request)\n            self._check_if_closing()\n            self._scrape_next()\n\n    def _scrape_next(self) -> None:\n        assert self.slot is not None  # typing\n        while self.slot.queue:\n            result, request, queue_dfd = self.slot.next_response_request_deferred()\n            _schedule_coro(self._wait_for_processing(result, request, queue_dfd))\n\n    async def _scrape(self, result: Response | Failure, request: Request) -> None:\n        \"\"\"Handle the downloaded response or failure through the spider callback/errback.\"\"\"\n        if not isinstance(result, (Response, Failure)):\n            raise TypeError(\n                f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n            )\n\n        output: Iterable[Any] | AsyncIterator[Any]\n        if isinstance(result, Response):\n            try:\n                # call the spider middlewares and the request callback with the response\n                output = await self.spidermw.scrape_response_async(\n                    self.call_spider_async, result, request\n                )\n            except Exception:\n                self.handle_spider_error(Failure(), request, result)\n            else:\n                await self.handle_spider_output_async(output, request, result)\n            return\n\n        try:\n            # call the request errback with the downloader error\n            output = await self.call_spider_async(result, request)\n        except Exception as spider_exc:\n            # the errback didn't silence the exception\n            assert self.crawler.spider\n            if not result.check(IgnoreRequest):\n                logkws = self.logformatter.download_error(\n                    result, request, self.crawler.spider\n                )\n                logger.log(\n                    *logformatter_adapter(logkws),\n                    extra={\"spider\": self.crawler.spider},\n                    exc_info=failure_to_exc_info(result),\n                )\n            if spider_exc is not result.value:\n                # the errback raised a different exception, handle it\n                self.handle_spider_error(Failure(), request, result)\n        else:\n            await self.handle_spider_output_async(output, request, result)\n\n    async def _wait_for_processing(\n        self, result: Response | Failure, request: Request, queue_dfd: Deferred[None]\n    ) -> None:\n        try:\n            await self._scrape(result, request)\n        except Exception:\n            queue_dfd.errback(Failure())\n        else:\n            queue_dfd.callback(None)  # awaited in enqueue_scrape()\n\n    def call_spider(\n        self, result: Response | Failure, request: Request, spider: Spider | None = None\n    ) -> Deferred[Iterable[Any] | AsyncIterator[Any]]:\n        warnings.warn(\n            \"Scraper.call_spider() is deprecated, use call_spider_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.call_spider_async(result, request))\n\n    async def call_spider_async(\n        self, result: Response | Failure, request: Request\n    ) -> Iterable[Any] | AsyncIterator[Any]:\n        \"\"\"Call the request callback or errback with the response or failure.\n\n        .. versionadded:: 2.13\n        \"\"\"\n        await _defer_sleep_async()\n        assert self.crawler.spider\n        if isinstance(result, Response):\n            if getattr(result, \"request\", None) is None:\n                result.request = request\n            assert result.request\n            callback = result.request.callback or self.crawler.spider._parse\n            warn_on_generator_with_return_value(self.crawler.spider, callback)\n            output = callback(result, **result.request.cb_kwargs)\n        else:  # result is a Failure\n            # TODO: properly type adding this attribute to a Failure\n            result.request = request  # type: ignore[attr-defined]\n            if not request.errback:\n                result.raiseException()\n            warn_on_generator_with_return_value(self.crawler.spider, request.errback)\n            output = request.errback(result)\n            if isinstance(output, Failure):\n                output.raiseException()\n            # else the errback returned actual output (like a callback),\n            # which needs to be passed to iterate_spider_output()\n        return await ensure_awaitable(iterate_spider_output(output))\n", "n_tokens": 1034, "byte_len": 5168, "file_sha1": "8c816b59023cb12bf7e56f9a8b23e8af2a421e4b", "start_line": 222, "end_line": 340}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py", "rel_path": "scrapy/core/scraper.py", "module": "scrapy.core.scraper", "ext": "py", "chunk_number": 4, "symbols": ["handle_spider_error", "handle_spider_output", "_process_spidermw_output", "start_itemproc", "failure", "does", "async", "handle", "spider", "send", "parallel", "inc", "value", "scheduled", "signal", "start", "itemproc", "each", "warn", "deprecated", "exceptions", "sent", "catch", "isinstance", "items", "item", "none", "parameter", "reason", "error", "__init__", "add_response_request", "next_response_request_deferred", "finish_response", "is_idle", "needs_backout", "open_spider", "close_spider", "_check_if_closing", "enqueue_scrape", "_scrape_next", "call_spider", "Slot", "Scraper", "has", "method", "call", "scrape", "bool", "log"], "ast_kind": "function_or_method", "text": "    @_warn_spider_arg\n    def handle_spider_error(\n        self,\n        _failure: Failure,\n        request: Request,\n        response: Response | Failure,\n        spider: Spider | None = None,\n    ) -> None:\n        \"\"\"Handle an exception raised by a spider callback or errback.\"\"\"\n        assert self.crawler.spider\n        exc = _failure.value\n        if isinstance(exc, CloseSpider):\n            assert self.crawler.engine is not None  # typing\n            _schedule_coro(\n                self.crawler.engine.close_spider_async(reason=exc.reason or \"cancelled\")\n            )\n            return\n        logkws = self.logformatter.spider_error(\n            _failure, request, response, self.crawler.spider\n        )\n        logger.log(\n            *logformatter_adapter(logkws),\n            exc_info=failure_to_exc_info(_failure),\n            extra={\"spider\": self.crawler.spider},\n        )\n        self.signals.send_catch_log(\n            signal=signals.spider_error,\n            failure=_failure,\n            response=response,\n            spider=self.crawler.spider,\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\"spider_exceptions/count\")\n        self.crawler.stats.inc_value(\n            f\"spider_exceptions/{_failure.value.__class__.__name__}\"\n        )\n\n    def handle_spider_output(\n        self,\n        result: Iterable[_T] | AsyncIterator[_T],\n        request: Request,\n        response: Response | Failure,\n        spider: Spider | None = None,\n    ) -> Deferred[None]:\n        \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n        warnings.warn(\n            \"Scraper.handle_spider_output() is deprecated, use handle_spider_output_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(\n            self.handle_spider_output_async(result, request, response)\n        )\n\n    async def handle_spider_output_async(\n        self,\n        result: Iterable[_T] | AsyncIterator[_T],\n        request: Request,\n        response: Response | Failure,\n    ) -> None:\n        \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\n\n        .. versionadded:: 2.13\n        \"\"\"\n        it: Iterable[_T] | AsyncIterator[_T]\n        if is_asyncio_available():\n            if isinstance(result, AsyncIterator):\n                it = aiter_errback(result, self.handle_spider_error, request, response)\n            else:\n                it = iter_errback(result, self.handle_spider_error, request, response)\n            await _parallel_asyncio(\n                it, self.concurrent_items, self._process_spidermw_output_async, response\n            )\n            return\n        if isinstance(result, AsyncIterator):\n            it = aiter_errback(result, self.handle_spider_error, request, response)\n            await maybe_deferred_to_future(\n                parallel_async(\n                    it,\n                    self.concurrent_items,\n                    self._process_spidermw_output,\n                    response,\n                )\n            )\n            return\n        it = iter_errback(result, self.handle_spider_error, request, response)\n        await maybe_deferred_to_future(\n            parallel(\n                it,\n                self.concurrent_items,\n                self._process_spidermw_output,\n                response,\n            )\n        )\n\n    def _process_spidermw_output(\n        self, output: Any, response: Response | Failure\n    ) -> Deferred[None]:\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider.\n\n        Items are sent to the item pipelines, requests are scheduled.\n        \"\"\"\n        return deferred_from_coro(self._process_spidermw_output_async(output, response))\n\n    async def _process_spidermw_output_async(\n        self, output: Any, response: Response | Failure\n    ) -> None:\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider.\n\n        Items are sent to the item pipelines, requests are scheduled.\n        \"\"\"\n        if isinstance(output, Request):\n            assert self.crawler.engine is not None  # typing\n            self.crawler.engine.crawl(request=output)\n            return\n        if output is not None:\n            await self.start_itemproc_async(output, response=response)\n\n    def start_itemproc(\n        self, item: Any, *, response: Response | Failure | None\n    ) -> Deferred[None]:\n        \"\"\"Send *item* to the item pipelines for processing.\n\n        *response* is the source of the item data. If the item does not come\n        from response data, e.g. it was hard-coded, set it to ``None``.\n        \"\"\"\n        warnings.warn(\n            \"Scraper.start_itemproc() is deprecated, use start_itemproc_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.start_itemproc_async(item, response=response))\n", "n_tokens": 1047, "byte_len": 5048, "file_sha1": "8c816b59023cb12bf7e56f9a8b23e8af2a421e4b", "start_line": 341, "end_line": 475}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scraper.py", "rel_path": "scrapy/core/scraper.py", "module": "scrapy.core.scraper", "ext": "py", "chunk_number": 5, "symbols": ["itemproc", "has", "failure", "does", "async", "send", "await", "except", "process", "item", "coded", "start", "processing", "spider", "signal", "exception", "slot", "typing", "extra", "scraped", "catch", "hard", "maybe", "deferred", "versionadded", "output", "logger", "version", "size", "true", "__init__", "add_response_request", "next_response_request_deferred", "finish_response", "is_idle", "needs_backout", "open_spider", "close_spider", "_check_if_closing", "enqueue_scrape", "_scrape_next", "call_spider", "handle_spider_error", "handle_spider_output", "_process_spidermw_output", "start_itemproc", "Slot", "Scraper", "method", "call"], "ast_kind": "imports", "text": "    async def start_itemproc_async(\n        self, item: Any, *, response: Response | Failure | None\n    ) -> None:\n        \"\"\"Send *item* to the item pipelines for processing.\n\n        *response* is the source of the item data. If the item does not come\n        from response data, e.g. it was hard-coded, set it to ``None``.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        assert self.slot is not None  # typing\n        assert self.crawler.spider is not None  # typing\n        self.slot.itemproc_size += 1\n        try:\n            if self._itemproc_has_process_async:\n                output = await self.itemproc.process_item_async(item)\n            else:\n                if self._itemproc_needs_spider[\"process_item\"]:\n                    d = self.itemproc.process_item(item, self.crawler.spider)\n                else:\n                    d = self.itemproc.process_item(item)\n                output = await maybe_deferred_to_future(d)\n        except DropItem as ex:\n            logkws = self.logformatter.dropped(item, ex, response, self.crawler.spider)\n            if logkws is not None:\n                logger.log(\n                    *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n                )\n            await self.signals.send_catch_log_async(\n                signal=signals.item_dropped,\n                item=item,\n                response=response,\n                spider=self.crawler.spider,\n                exception=ex,\n            )\n        except Exception as ex:\n            logkws = self.logformatter.item_error(\n                item, ex, response, self.crawler.spider\n            )\n            logger.log(\n                *logformatter_adapter(logkws),\n                extra={\"spider\": self.crawler.spider},\n                exc_info=True,\n            )\n            await self.signals.send_catch_log_async(\n                signal=signals.item_error,\n                item=item,\n                response=response,\n                spider=self.crawler.spider,\n                failure=Failure(),\n            )\n        else:\n            logkws = self.logformatter.scraped(output, response, self.crawler.spider)\n            if logkws is not None:\n                logger.log(\n                    *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n                )\n            await self.signals.send_catch_log_async(\n                signal=signals.item_scraped,\n                item=output,\n                response=response,\n                spider=self.crawler.spider,\n            )\n        finally:\n            self.slot.itemproc_size -= 1\n", "n_tokens": 526, "byte_len": 2604, "file_sha1": "8c816b59023cb12bf7e56f9a8b23e8af2a421e4b", "start_line": 476, "end_line": 541}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py", "rel_path": "scrapy/core/spidermw.py", "module": "scrapy.core.spidermw", "ext": "py", "chunk_number": 1, "symbols": ["_isiterable", "_get_mwlist_from_settings", "__init__", "_check_deprecated_process_start_requests_use", "_add_middleware", "SpiderMiddlewareManager", "failure", "method", "async", "those", "bool", "append", "middleware", "future", "coroutine", "spider", "possible", "enabled", "deprecated", "invalid", "output", "typ", "checking", "spide", "middlewares", "check", "https", "elif", "trying", "make", "_evaluate_iterable", "process_sync", "_process_spider_exception", "_process_spider_output", "scrape_response", "process_spider_exception", "_check_deprecated_start_requests_use", "_get_async_method_pair", "cls", "dict", "passed", "removed", "three", "back", "following", "methods", "requiring", "here", "them", "storing"], "ast_kind": "class_or_type", "text": "\"\"\"\nSpider Middleware manager\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import AsyncIterator, Callable, Coroutine, Iterable\nfrom functools import wraps\nfrom inspect import isasyncgenfunction, iscoroutine\nfrom itertools import islice\nfrom typing import TYPE_CHECKING, Any, TypeVar, Union, cast\nfrom warnings import warn\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import ScrapyDeprecationWarning, _InvalidOutput\nfrom scrapy.http import Response\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import (\n    _defer_sleep_async,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.python import MutableAsyncChain, MutableChain, global_object_name\n\nif TYPE_CHECKING:\n    from collections.abc import Generator\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nlogger = logging.getLogger(__name__)\n\n\n_T = TypeVar(\"_T\")\nScrapeFunc = Callable[\n    [Union[Response, Failure], Request],\n    Coroutine[Any, Any, Union[Iterable[_T], AsyncIterator[_T]]],\n]\n\n\ndef _isiterable(o: Any) -> bool:\n    return isinstance(o, (Iterable, AsyncIterator))\n\n\nclass SpiderMiddlewareManager(MiddlewareManager):\n    component_name = \"spider middleware\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> list[Any]:\n        return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n\n    def __init__(self, *middlewares: Any, crawler: Crawler | None = None) -> None:\n        self._check_deprecated_process_start_requests_use(middlewares)\n        super().__init__(*middlewares, crawler=crawler)\n\n    def _check_deprecated_process_start_requests_use(\n        self, middlewares: tuple[Any, ...]\n    ) -> None:\n        deprecated_middlewares = [\n            middleware\n            for middleware in middlewares\n            if hasattr(middleware, \"process_start_requests\")\n            and not hasattr(middleware, \"process_start\")\n        ]\n        modern_middlewares = [\n            middleware\n            for middleware in middlewares\n            if not hasattr(middleware, \"process_start_requests\")\n            and hasattr(middleware, \"process_start\")\n        ]\n        if deprecated_middlewares and modern_middlewares:\n            raise ValueError(\n                \"You are trying to combine spider middlewares that only \"\n                \"define the deprecated process_start_requests() method () \"\n                \"with spider middlewares that only define the \"\n                \"process_start() method (). This is not possible. You must \"\n                \"either disable or make universal 1 of those 2 sets of \"\n                \"spider middlewares. Making a spider middleware universal \"\n                \"means having it define both methods. See the release notes \"\n                \"of Scrapy 2.13 for details: \"\n                \"https://docs.scrapy.org/en/2.13/news.html\"\n            )\n\n        self._use_start_requests = bool(deprecated_middlewares)\n        if self._use_start_requests:\n            deprecated_middleware_list = \", \".join(\n                global_object_name(middleware.__class__)\n                for middleware in deprecated_middlewares\n            )\n            warn(\n                f\"The following enabled spider middlewares, directly or \"\n                f\"through their parent classes, define the deprecated \"\n                f\"process_start_requests() method: \"\n                f\"{deprecated_middleware_list}. process_start_requests() has \"\n                f\"been deprecated in favor of a new method, process_start(), \"\n                f\"to support asynchronous code execution. \"\n                f\"process_start_requests() will stop being called in a future \"\n                f\"version of Scrapy. If you use Scrapy 2.13 or higher \"\n                f\"only, replace process_start_requests() with \"\n                f\"process_start(); note that process_start() is a coroutine \"\n                f\"(async def). If you need to maintain compatibility with \"\n                f\"lower Scrapy versions, when defining \"\n                f\"process_start_requests() in a spider middleware class, \"\n                f\"define process_start() as well. See the release notes of \"\n                f\"Scrapy 2.13 for details: \"\n                f\"https://docs.scrapy.org/en/2.13/news.html\",\n                ScrapyDeprecationWarning,\n            )\n\n    def _add_middleware(self, mw: Any) -> None:\n        if hasattr(mw, \"process_spider_input\"):\n            self.methods[\"process_spider_input\"].append(mw.process_spider_input)\n            self._check_mw_method_spider_arg(mw.process_spider_input)\n\n        if self._use_start_requests:\n            if hasattr(mw, \"process_start_requests\"):\n                self.methods[\"process_start_requests\"].appendleft(\n                    mw.process_start_requests\n                )\n        elif hasattr(mw, \"process_start\"):\n            self.methods[\"process_start\"].appendleft(mw.process_start)\n\n        process_spider_output = self._get_async_method_pair(mw, \"process_spider_output\")\n        self.methods[\"process_spider_output\"].appendleft(process_spider_output)\n        if callable(process_spider_output):\n            self._check_mw_method_spider_arg(process_spider_output)\n        elif isinstance(process_spider_output, tuple):\n            for m in process_spider_output:\n                self._check_mw_method_spider_arg(m)\n\n        process_spider_exception = getattr(mw, \"process_spider_exception\", None)\n        self.methods[\"process_spider_exception\"].appendleft(process_spider_exception)\n        if process_spider_exception is not None:\n            self._check_mw_method_spider_arg(process_spider_exception)\n", "n_tokens": 1214, "byte_len": 5982, "file_sha1": "5dd09bac61486a34e4d515da3e181909700ccba0", "start_line": 1, "end_line": 144}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py", "rel_path": "scrapy/core/spidermw.py", "module": "scrapy.core.spidermw", "ext": "py", "chunk_number": 2, "symbols": ["_evaluate_iterable", "process_sync", "_process_spider_exception", "failure", "method", "async", "immediately", "index", "didn", "architecture", "invalid", "output", "removed", "process", "spider", "isinstance", "methods", "requiring", "here", "none", "stop", "type", "exception", "processor", "callable", "scrape", "func", "response", "await", "return", "_isiterable", "_get_mwlist_from_settings", "__init__", "_check_deprecated_process_start_requests_use", "_add_middleware", "_process_spider_output", "scrape_response", "process_spider_exception", "_check_deprecated_start_requests_use", "_get_async_method_pair", "SpiderMiddlewareManager", "those", "bool", "cls", "dict", "passed", "trying", "three", "back", "following"], "ast_kind": "function_or_method", "text": "    async def _process_spider_input(\n        self,\n        scrape_func: ScrapeFunc[_T],\n        response: Response,\n        request: Request,\n    ) -> Iterable[_T] | AsyncIterator[_T]:\n        for method in self.methods[\"process_spider_input\"]:\n            method = cast(\"Callable\", method)\n            try:\n                if method in self._mw_methods_requiring_spider:\n                    result = method(response=response, spider=self._spider)\n                else:\n                    result = method(response=response)\n                if result is not None:\n                    msg = (\n                        f\"{global_object_name(method)} must return None \"\n                        f\"or raise an exception, got {type(result)}\"\n                    )\n                    raise _InvalidOutput(msg)\n            except _InvalidOutput:\n                raise\n            except Exception:\n                return await scrape_func(Failure(), request)\n        return await scrape_func(response, request)\n\n    def _evaluate_iterable(\n        self,\n        response: Response,\n        iterable: Iterable[_T] | AsyncIterator[_T],\n        exception_processor_index: int,\n        recover_to: MutableChain[_T] | MutableAsyncChain[_T],\n    ) -> Iterable[_T] | AsyncIterator[_T]:\n        def process_sync(iterable: Iterable[_T]) -> Iterable[_T]:\n            try:\n                yield from iterable\n            except Exception as ex:\n                exception_result = cast(\n                    \"Union[Failure, MutableChain[_T]]\",\n                    self._process_spider_exception(\n                        response, ex, exception_processor_index\n                    ),\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                assert isinstance(recover_to, MutableChain)\n                recover_to.extend(exception_result)\n\n        async def process_async(iterable: AsyncIterator[_T]) -> AsyncIterator[_T]:\n            try:\n                async for r in iterable:\n                    yield r\n            except Exception as ex:\n                exception_result = cast(\n                    \"Union[Failure, MutableAsyncChain[_T]]\",\n                    self._process_spider_exception(\n                        response, ex, exception_processor_index\n                    ),\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                assert isinstance(recover_to, MutableAsyncChain)\n                recover_to.extend(exception_result)\n\n        if isinstance(iterable, AsyncIterator):\n            return process_async(iterable)\n        return process_sync(iterable)\n\n    def _process_spider_exception(\n        self,\n        response: Response,\n        exception: Exception,\n        start_index: int = 0,\n    ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n        # don't handle _InvalidOutput exception\n        if isinstance(exception, _InvalidOutput):\n            raise exception\n        method_list = islice(\n            self.methods[\"process_spider_exception\"], start_index, None\n        )\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            method = cast(\"Callable\", method)\n            if method in self._mw_methods_requiring_spider:\n                result = method(\n                    response=response, exception=exception, spider=self._spider\n                )\n            else:\n                result = method(response=response, exception=exception)\n            if _isiterable(result):\n                # stop exception handling by handing control over to the\n                # process_spider_output chain if an iterable has been returned\n                dfd: Deferred[MutableChain[_T] | MutableAsyncChain[_T]] = (\n                    self._process_spider_output(response, result, method_index + 1)\n                )\n                # _process_spider_output() returns a Deferred only because of downgrading so this can be\n                # simplified when downgrading is removed.\n                if dfd.called:\n                    # the result is available immediately if _process_spider_output didn't do downgrading\n                    return cast(\n                        \"Union[MutableChain[_T], MutableAsyncChain[_T]]\", dfd.result\n                    )\n                # we forbid waiting here because otherwise we would need to return a deferred from\n                # _process_spider_exception too, which complicates the architecture\n                msg = f\"Async iterable returned from {global_object_name(method)} cannot be downgraded\"\n                raise _InvalidOutput(msg)\n            if result is None:\n                continue\n            msg = (\n                f\"{global_object_name(method)} must return None \"\n                f\"or an iterable, got {type(result)}\"\n            )\n            raise _InvalidOutput(msg)\n        raise exception\n", "n_tokens": 909, "byte_len": 4968, "file_sha1": "5dd09bac61486a34e4d515da3e181909700ccba0", "start_line": 145, "end_line": 259}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py", "rel_path": "scrapy/core/spidermw.py", "module": "scrapy.core.spidermw", "ext": "py", "chunk_number": 3, "symbols": ["_process_spider_output", "failure", "method", "async", "immediately", "foo", "weird", "generator", "awaiting", "future", "made", "index", "coroutines", "about", "doesn", "deprecated", "passed", "removed", "elif", "https", "more", "invalid", "output", "three", "recovered", "collected", "back", "items", "isinstance", "sync", "_isiterable", "_get_mwlist_from_settings", "__init__", "_check_deprecated_process_start_requests_use", "_add_middleware", "_evaluate_iterable", "process_sync", "_process_spider_exception", "scrape_response", "process_spider_exception", "_check_deprecated_start_requests_use", "_get_async_method_pair", "SpiderMiddlewareManager", "those", "bool", "cls", "dict", "spider", "trying", "following"], "ast_kind": "function_or_method", "text": "    # This method cannot be made async def, as _process_spider_exception relies on the Deferred result\n    # being available immediately which doesn't work when it's a wrapped coroutine.\n    # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.\n    @inlineCallbacks\n    def _process_spider_output(\n        self,\n        response: Response,\n        result: Iterable[_T] | AsyncIterator[_T],\n        start_index: int = 0,\n    ) -> Generator[Deferred[Any], Any, MutableChain[_T] | MutableAsyncChain[_T]]:\n        # items in this iterable do not need to go through the process_spider_output\n        # chain, they went through it already from the process_spider_exception method\n        recovered: MutableChain[_T] | MutableAsyncChain[_T]\n        last_result_is_async = isinstance(result, AsyncIterator)\n        recovered = MutableAsyncChain() if last_result_is_async else MutableChain()\n\n        # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.\n        # 1. def foo. Sync iterables are passed as is, async ones are downgraded.\n        # 2. async def foo. Sync iterables are upgraded, async ones are passed as is.\n        # 3. def foo + async def foo_async. Iterables are passed to the respective method.\n        # Storing methods and method tuples in the same list is weird but we should be able to roll this back\n        # when we drop this compatibility feature.\n\n        method_list = islice(self.methods[\"process_spider_output\"], start_index, None)\n        for method_index, method_pair in enumerate(method_list, start=start_index):\n            if method_pair is None:\n                continue\n            need_upgrade = need_downgrade = False\n            if isinstance(method_pair, tuple):\n                # This tuple handling is only needed until _async compatibility methods are removed.\n                method_sync, method_async = method_pair\n                method = method_async if last_result_is_async else method_sync\n            else:\n                method = method_pair\n                if not last_result_is_async and isasyncgenfunction(method):\n                    need_upgrade = True\n                elif last_result_is_async and not isasyncgenfunction(method):\n                    need_downgrade = True\n            try:\n                if need_upgrade:\n                    # Iterable -> AsyncIterator\n                    result = as_async_generator(result)\n                elif need_downgrade:\n                    logger.warning(\n                        f\"Async iterable passed to {global_object_name(method)} was\"\n                        f\" downgraded to a non-async one. This is deprecated and will\"\n                        f\" stop working in a future version of Scrapy. Please see\"\n                        f\" https://docs.scrapy.org/en/latest/topics/coroutines.html#for-middleware-users\"\n                        f\" for more information.\"\n                    )\n                    assert isinstance(result, AsyncIterator)\n                    # AsyncIterator -> Iterable\n                    result = yield deferred_from_coro(collect_asyncgen(result))\n                    if isinstance(recovered, AsyncIterator):\n                        recovered_collected = yield deferred_from_coro(\n                            collect_asyncgen(recovered)\n                        )\n                        recovered = MutableChain(recovered_collected)\n                # might fail directly if the output value is not a generator\n                if method in self._mw_methods_requiring_spider:\n                    result = method(\n                        response=response, result=result, spider=self._spider\n                    )\n                else:\n                    result = method(response=response, result=result)\n            except Exception as ex:\n                exception_result: Failure | MutableChain[_T] | MutableAsyncChain[_T] = (\n                    self._process_spider_exception(response, ex, method_index + 1)\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                return exception_result\n            if _isiterable(result):\n                result = self._evaluate_iterable(\n                    response, result, method_index + 1, recovered\n                )\n            else:\n                if iscoroutine(result):\n                    result.close()  # Silence warning about not awaiting\n                    msg = (\n                        f\"{global_object_name(method)} must be an asynchronous \"\n                        f\"generator (i.e. use yield)\"\n                    )\n                else:\n                    msg = (\n                        f\"{global_object_name(method)} must return an iterable, got \"\n                        f\"{type(result)}\"\n                    )\n                raise _InvalidOutput(msg)\n            last_result_is_async = isinstance(result, AsyncIterator)\n\n        if last_result_is_async:\n            return MutableAsyncChain(result, recovered)\n        return MutableChain(result, recovered)  # type: ignore[arg-type]\n\n    async def _process_callback_output(\n        self,\n        response: Response,\n        result: Iterable[_T] | AsyncIterator[_T],\n    ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n        recovered: MutableChain[_T] | MutableAsyncChain[_T]\n        if isinstance(result, AsyncIterator):\n            recovered = MutableAsyncChain()\n        else:\n            recovered = MutableChain()\n        result = self._evaluate_iterable(response, result, 0, recovered)\n        result = await maybe_deferred_to_future(\n            cast(\n                \"Deferred[Iterable[_T] | AsyncIterator[_T]]\",\n                self._process_spider_output(response, result),\n            )\n        )\n        if isinstance(result, AsyncIterator):\n            return MutableAsyncChain(result, recovered)\n        if isinstance(recovered, AsyncIterator):\n            recovered_collected = await collect_asyncgen(recovered)\n            recovered = MutableChain(recovered_collected)\n        return MutableChain(result, recovered)\n", "n_tokens": 1148, "byte_len": 6139, "file_sha1": "5dd09bac61486a34e4d515da3e181909700ccba0", "start_line": 260, "end_line": 378}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py", "rel_path": "scrapy/core/spidermw.py", "module": "scrapy.core.spidermw", "ext": "py", "chunk_number": 4, "symbols": ["scrape_response", "process_spider_exception", "failure", "async", "wraps", "always", "add", "await", "result", "except", "spider", "scrape", "response", "scrapy", "deprecation", "func", "instance", "exception", "sync", "start", "return", "check", "deprecated", "middleware", "maybe", "deferred", "instead", "process", "else", "warn", "_isiterable", "_get_mwlist_from_settings", "__init__", "_check_deprecated_process_start_requests_use", "_add_middleware", "_evaluate_iterable", "process_sync", "_process_spider_exception", "_process_spider_output", "_check_deprecated_start_requests_use", "_get_async_method_pair", "SpiderMiddlewareManager", "method", "those", "bool", "cls", "dict", "passed", "removed", "trying"], "ast_kind": "function_or_method", "text": "    def scrape_response(\n        self,\n        scrape_func: Callable[\n            [Response | Failure, Request],\n            Deferred[Iterable[_T] | AsyncIterator[_T]],\n        ],\n        response: Response,\n        request: Request,\n        spider: Spider,\n    ) -> Deferred[MutableChain[_T] | MutableAsyncChain[_T]]:\n        warn(\n            \"SpiderMiddlewareManager.scrape_response() is deprecated, use scrape_response_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n        @wraps(scrape_func)\n        async def scrape_func_wrapped(\n            response: Response | Failure, request: Request\n        ) -> Iterable[_T] | AsyncIterator[_T]:\n            return await maybe_deferred_to_future(scrape_func(response, request))\n\n        self._set_compat_spider(spider)\n        return deferred_from_coro(\n            self.scrape_response_async(scrape_func_wrapped, response, request)\n        )\n\n    async def scrape_response_async(\n        self,\n        scrape_func: ScrapeFunc[_T],\n        response: Response,\n        request: Request,\n    ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n        if not self.crawler:\n            raise RuntimeError(\n                \"scrape_response_async() called on a SpiderMiddlewareManager\"\n                \" instance created without a crawler.\"\n            )\n\n        async def process_callback_output(\n            result: Iterable[_T] | AsyncIterator[_T],\n        ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n            return await self._process_callback_output(response, result)\n\n        def process_spider_exception(\n            exception: Exception,\n        ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n            return self._process_spider_exception(response, exception)\n\n        try:\n            it: Iterable[_T] | AsyncIterator[_T] = await self._process_spider_input(\n                scrape_func, response, request\n            )\n            return await process_callback_output(it)\n        except Exception as ex:\n            await _defer_sleep_async()\n            return process_spider_exception(ex)\n\n    async def process_start(\n        self, spider: Spider | None = None\n    ) -> AsyncIterator[Any] | None:\n        if spider:\n            self._warn_spider_arg(\"process_start\")\n            self._set_compat_spider(spider)\n        self._check_deprecated_start_requests_use()\n        if self._use_start_requests:\n            sync_start = iter(self._spider.start_requests())\n            sync_start = await self._process_chain(\n                \"process_start_requests\", sync_start, always_add_spider=True\n            )\n            start: AsyncIterator[Any] = as_async_generator(sync_start)\n        else:\n            start = self._spider.start()\n            start = await self._process_chain(\"process_start\", start)\n        return start\n", "n_tokens": 581, "byte_len": 2828, "file_sha1": "5dd09bac61486a34e4d515da3e181909700ccba0", "start_line": 379, "end_line": 454}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/spidermw.py", "rel_path": "scrapy/core/spidermw.py", "module": "scrapy.core.spidermw", "ext": "py", "chunk_number": 5, "symbols": ["_check_deprecated_start_requests_use", "_get_async_method_pair", "method", "does", "spidercls", "async", "skipping", "your", "generator", "middleware", "future", "cls", "dict", "spider", "coroutines", "doesn", "enabled", "mro", "deprecated", "spiders", "more", "removed", "https", "upgrade", "make", "defines", "compatible", "start", "requests", "lower", "_isiterable", "_get_mwlist_from_settings", "__init__", "_check_deprecated_process_start_requests_use", "_add_middleware", "_evaluate_iterable", "process_sync", "_process_spider_exception", "_process_spider_output", "scrape_response", "process_spider_exception", "SpiderMiddlewareManager", "failure", "those", "bool", "passed", "trying", "three", "back", "following"], "ast_kind": "function_or_method", "text": "    def _check_deprecated_start_requests_use(self):\n        start_requests_cls = None\n        start_cls = None\n        spidercls = self._spider.__class__\n        mro = spidercls.__mro__\n\n        for cls in mro:\n            cls_dict = cls.__dict__\n            if start_requests_cls is None and \"start_requests\" in cls_dict:\n                start_requests_cls = cls\n            if start_cls is None and \"start\" in cls_dict:\n                start_cls = cls\n            if start_requests_cls is not None and start_cls is not None:\n                break\n\n        # Spider defines both, start_requests and start.\n        assert start_requests_cls is not None\n        assert start_cls is not None\n\n        if (\n            start_requests_cls is not Spider\n            and start_cls is not start_requests_cls\n            and mro.index(start_requests_cls) < mro.index(start_cls)\n        ):\n            src = global_object_name(start_requests_cls)\n            if start_requests_cls is not spidercls:\n                src += f\" (inherited by {global_object_name(spidercls)})\"\n            warn(\n                f\"{src} defines the deprecated start_requests() method. \"\n                f\"start_requests() has been deprecated in favor of a new \"\n                f\"method, start(), to support asynchronous code \"\n                f\"execution. start_requests() will stop being called in a \"\n                f\"future version of Scrapy. If you use Scrapy 2.13 or \"\n                f\"higher only, replace start_requests() with start(); \"\n                f\"note that start() is a coroutine (async def). If you \"\n                f\"need to maintain compatibility with lower Scrapy versions, \"\n                f\"when overriding start_requests() in a spider class, \"\n                f\"override start() as well; you can use super() to \"\n                f\"reuse the inherited start() implementation without \"\n                f\"copy-pasting. See the release notes of Scrapy 2.13 for \"\n                f\"details: https://docs.scrapy.org/en/2.13/news.html\",\n                ScrapyDeprecationWarning,\n            )\n\n        if (\n            self._use_start_requests\n            and start_cls is not Spider\n            and start_requests_cls is not start_cls\n            and mro.index(start_cls) < mro.index(start_requests_cls)\n        ):\n            src = global_object_name(start_cls)\n            if start_cls is not spidercls:\n                src += f\" (inherited by {global_object_name(spidercls)})\"\n            raise ValueError(\n                f\"{src} does not define the deprecated start_requests() \"\n                f\"method. However, one or more of your enabled spider \"\n                f\"middlewares (reported in an earlier deprecation warning) \"\n                f\"define the process_start_requests() method, and not the \"\n                f\"process_start() method, making them only compatible with \"\n                f\"(deprecated) spiders that define the start_requests() \"\n                f\"method. To solve this issue, disable the offending spider \"\n                f\"middlewares, upgrade them as described in that earlier \"\n                f\"deprecation warning, or make your spider compatible with \"\n                f\"deprecated spider middlewares (and earlier Scrapy versions) \"\n                f\"by defining a sync start_requests() method that works \"\n                f\"similarly to its existing start() method. See the \"\n                f\"release notes of Scrapy 2.13 for details: \"\n                f\"https://docs.scrapy.org/en/2.13/news.html\"\n            )\n\n    # This method is only needed until _async compatibility methods are removed.\n    @staticmethod\n    def _get_async_method_pair(\n        mw: Any, methodname: str\n    ) -> Callable | tuple[Callable, Callable] | None:\n        normal_method: Callable | None = getattr(mw, methodname, None)\n        methodname_async = methodname + \"_async\"\n        async_method: Callable | None = getattr(mw, methodname_async, None)\n        if not async_method:\n            if normal_method and not isasyncgenfunction(normal_method):\n                logger.warning(\n                    f\"Middleware {global_object_name(mw.__class__)} doesn't support\"\n                    f\" asynchronous spider output, this is deprecated and will stop\"\n                    f\" working in a future version of Scrapy. The middleware should\"\n                    f\" be updated to support it. Please see\"\n                    f\" https://docs.scrapy.org/en/latest/topics/coroutines.html#for-middleware-users\"\n                    f\" for more information.\"\n                )\n            return normal_method\n        if not normal_method:\n            logger.error(\n                f\"Middleware {global_object_name(mw.__class__)} has {methodname_async} \"\n                f\"without {methodname}, skipping this method.\"\n            )\n            return None\n        if not isasyncgenfunction(async_method):\n            logger.error(\n                f\"{global_object_name(async_method)} is not \"\n                f\"an async generator function, skipping this method.\"\n            )\n            return normal_method\n        if isasyncgenfunction(normal_method):\n            logger.error(\n                f\"{global_object_name(normal_method)} is an async \"\n                f\"generator function while {methodname_async} exists, \"\n                f\"skipping both methods.\"\n            )\n            return None\n        return normal_method, async_method\n", "n_tokens": 1102, "byte_len": 5440, "file_sha1": "5dd09bac61486a34e4d515da3e181909700ccba0", "start_line": 455, "end_line": 564}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/__init__.py", "rel_path": "scrapy/core/__init__.py", "module": "scrapy.core.__init__", "ext": "py", "chunk_number": 1, "symbols": ["classes", "library", "core", "scrapy", "functions"], "ast_kind": "unknown", "text": "\"\"\"\nScrapy core library classes and functions.\n\"\"\"\n", "n_tokens": 10, "byte_len": 51, "file_sha1": "b6cd5da42870bfe3e61dcfa714a4d1d09e7623fd", "start_line": 1, "end_line": 4}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py", "rel_path": "scrapy/core/engine.py", "module": "scrapy.core.engine", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "add_request", "remove_request", "_maybe_fire_closing", "_get_scheduler_class", "start", "_Slot", "ExecutionEngine", "downloader", "failure", "method", "does", "async", "bool", "log", "formatter", "future", "coroutine", "spider", "traceback", "scheduler", "architecture", "maybe", "fire", "deprecated", "spiders", "passed", "more", "typ", "checking", "stop", "close", "pause", "unpause", "_start_scheduled_requests", "needs_backout", "_start_scheduled_request", "_remove_request", "_handle_downloader_output", "spider_is_idle", "crawl", "_schedule_request", "download", "_download", "open_spider", "_spider_idle", "close_spider", "log_failure", "continuing", "signal"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis is the Scrapy engine which controls the Scheduler, Downloader and Spider.\n\nFor more information see docs/topics/architecture.rst\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport warnings\nfrom time import time\nfrom traceback import format_exc\nfrom typing import TYPE_CHECKING, Any\n\nfrom twisted.internet.defer import CancelledError, Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import signals\nfrom scrapy.core.scheduler import BaseScheduler\nfrom scrapy.core.scraper import Scraper\nfrom scrapy.exceptions import (\n    CloseSpider,\n    DontCloseSpider,\n    IgnoreRequest,\n    ScrapyDeprecationWarning,\n)\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.asyncio import (\n    AsyncioLoopingCall,\n    create_looping_call,\n    is_asyncio_available,\n)\nfrom scrapy.utils.defer import (\n    _schedule_coro,\n    deferred_from_coro,\n    ensure_awaitable,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.deprecate import argument_is_required\nfrom scrapy.utils.log import failure_to_exc_info, logformatter_adapter\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.reactor import CallLaterOnce\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Callable, Coroutine, Generator\n\n    from twisted.internet.task import LoopingCall\n\n    from scrapy.core.downloader import Downloader\n    from scrapy.crawler import Crawler\n    from scrapy.logformatter import LogFormatter\n    from scrapy.settings import BaseSettings, Settings\n    from scrapy.signalmanager import SignalManager\n    from scrapy.spiders import Spider\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass _Slot:\n    def __init__(\n        self,\n        close_if_idle: bool,\n        nextcall: CallLaterOnce[None],\n        scheduler: BaseScheduler,\n    ) -> None:\n        self.closing: Deferred[None] | None = None\n        self.inprogress: set[Request] = set()\n        self.close_if_idle: bool = close_if_idle\n        self.nextcall: CallLaterOnce[None] = nextcall\n        self.scheduler: BaseScheduler = scheduler\n        self.heartbeat: AsyncioLoopingCall | LoopingCall = create_looping_call(\n            nextcall.schedule\n        )\n\n    def add_request(self, request: Request) -> None:\n        self.inprogress.add(request)\n\n    def remove_request(self, request: Request) -> None:\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()\n\n    async def close(self) -> None:\n        self.closing = Deferred()\n        self._maybe_fire_closing()\n        await maybe_deferred_to_future(self.closing)\n\n    def _maybe_fire_closing(self) -> None:\n        if self.closing is not None and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)\n\n\nclass ExecutionEngine:\n    _SLOT_HEARTBEAT_INTERVAL: float = 5.0\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        spider_closed_callback: Callable[\n            [Spider], Coroutine[Any, Any, None] | Deferred[None] | None\n        ],\n    ) -> None:\n        self.crawler: Crawler = crawler\n        self.settings: Settings = crawler.settings\n        self.signals: SignalManager = crawler.signals\n        assert crawler.logformatter\n        self.logformatter: LogFormatter = crawler.logformatter\n        self._slot: _Slot | None = None\n        self.spider: Spider | None = None\n        self.running: bool = False\n        self.paused: bool = False\n        self._spider_closed_callback: Callable[\n            [Spider], Coroutine[Any, Any, None] | Deferred[None] | None\n        ] = spider_closed_callback\n        self.start_time: float | None = None\n        self._start: AsyncIterator[Any] | None = None\n        self._closewait: Deferred[None] | None = None\n        self._start_request_processing_awaitable: (\n            asyncio.Future[None] | Deferred[None] | None\n        ) = None\n        downloader_cls: type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n        try:\n            self.scheduler_cls: type[BaseScheduler] = self._get_scheduler_class(\n                crawler.settings\n            )\n            self.downloader: Downloader = downloader_cls(crawler)\n            self._downloader_fetch_needs_spider: bool = argument_is_required(\n                self.downloader.fetch, \"spider\"\n            )\n            if self._downloader_fetch_needs_spider:\n                warnings.warn(\n                    f\"The fetch() method of {global_object_name(downloader_cls)} requires a spider argument,\"\n                    f\" this is deprecated and the argument will not be passed in future Scrapy versions.\",\n                    ScrapyDeprecationWarning,\n                    stacklevel=2,\n                )\n\n            self.scraper: Scraper = Scraper(crawler)\n        except Exception:\n            if hasattr(self, \"downloader\"):\n                self.downloader.close()\n            raise\n\n    def _get_scheduler_class(self, settings: BaseSettings) -> type[BaseScheduler]:\n        scheduler_cls: type[BaseScheduler] = load_object(settings[\"SCHEDULER\"])\n        if not issubclass(scheduler_cls, BaseScheduler):\n            raise TypeError(\n                f\"The provided scheduler class ({settings['SCHEDULER']})\"\n                \" does not fully implement the scheduler interface\"\n            )\n        return scheduler_cls\n\n    def start(self, _start_request_processing=True) -> Deferred[None]:\n        warnings.warn(\n            \"ExecutionEngine.start() is deprecated, use start_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(\n            self.start_async(_start_request_processing=_start_request_processing)\n        )\n", "n_tokens": 1232, "byte_len": 5850, "file_sha1": "7ea78a13b319d0972b87fa2382d8dbfb3670854f", "start_line": 1, "end_line": 169}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py", "rel_path": "scrapy/core/engine.py", "module": "scrapy.core.engine", "ext": "py", "chunk_number": 2, "symbols": ["stop", "close", "pause", "unpause", "_start_scheduled_requests", "method", "processed", "async", "start", "scheduled", "bool", "continuing", "itemproc", "signal", "consuming", "from", "coroutine", "spider", "scheduler", "sending", "error", "deprecated", "anext", "room", "elif", "https", "format", "exc", "process", "sent", "__init__", "add_request", "remove_request", "_maybe_fire_closing", "_get_scheduler_class", "needs_backout", "_start_scheduled_request", "_remove_request", "_handle_downloader_output", "spider_is_idle", "crawl", "_schedule_request", "download", "_download", "open_spider", "_spider_idle", "close_spider", "log_failure", "_Slot", "ExecutionEngine"], "ast_kind": "function_or_method", "text": "    async def start_async(self, *, _start_request_processing: bool = True) -> None:\n        \"\"\"Start the execution engine.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        if self.running:\n            raise RuntimeError(\"Engine already running\")\n        self.start_time = time()\n        await self.signals.send_catch_log_async(signal=signals.engine_started)\n        if _start_request_processing and self.spider is None:\n            # require an opened spider when not run in scrapy shell\n            return\n        self.running = True\n        self._closewait = Deferred()\n        if _start_request_processing:\n            coro = self._start_request_processing()\n            if is_asyncio_available():\n                # not wrapping in a Deferred here to avoid https://github.com/twisted/twisted/issues/12470\n                # (can happen when this is cancelled, e.g. in test_close_during_start_iteration())\n                self._start_request_processing_awaitable = asyncio.ensure_future(coro)\n            else:\n                self._start_request_processing_awaitable = Deferred.fromCoroutine(coro)\n        await maybe_deferred_to_future(self._closewait)\n\n    def stop(self) -> Deferred[None]:\n        warnings.warn(\n            \"ExecutionEngine.stop() is deprecated, use stop_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.stop_async())\n\n    async def stop_async(self) -> None:\n        \"\"\"Gracefully stop the execution engine.\n\n        .. versionadded:: VERSION\n        \"\"\"\n\n        if not self.running:\n            raise RuntimeError(\"Engine not running\")\n\n        self.running = False\n        if self._start_request_processing_awaitable is not None:\n            self._start_request_processing_awaitable.cancel()\n            self._start_request_processing_awaitable = None\n        if self.spider is not None:\n            await self.close_spider_async(reason=\"shutdown\")\n        await self.signals.send_catch_log_async(signal=signals.engine_stopped)\n        if self._closewait:\n            self._closewait.callback(None)\n\n    def close(self) -> Deferred[None]:\n        warnings.warn(\n            \"ExecutionEngine.close() is deprecated, use close_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.close_async())\n\n    async def close_async(self) -> None:\n        \"\"\"\n        Gracefully close the execution engine.\n        If it has already been started, stop it. In all cases, close the spider and the downloader.\n        \"\"\"\n        if self.running:\n            await self.stop_async()  # will also close spider and downloader\n        elif self.spider is not None:\n            await self.close_spider_async(\n                reason=\"shutdown\"\n            )  # will also close downloader\n        elif hasattr(self, \"downloader\"):\n            self.downloader.close()\n\n    def pause(self) -> None:\n        self.paused = True\n\n    def unpause(self) -> None:\n        self.paused = False\n\n    async def _process_start_next(self):\n        \"\"\"Processes the next item or request from Spider.start().\n\n        If a request, it is scheduled. If an item, it is sent to item\n        pipelines.\n        \"\"\"\n        try:\n            item_or_request = await self._start.__anext__()\n        except StopAsyncIteration:\n            self._start = None\n        except Exception as exception:\n            self._start = None\n            exception_traceback = format_exc()\n            logger.error(\n                f\"Error while reading start items and requests: {exception}.\\n{exception_traceback}\",\n                exc_info=True,\n            )\n        else:\n            if not self.spider:\n                return  # spider already closed\n            if isinstance(item_or_request, Request):\n                self.crawl(item_or_request)\n            else:\n                _schedule_coro(\n                    self.scraper.start_itemproc_async(item_or_request, response=None)\n                )\n                self._slot.nextcall.schedule()\n\n    async def _start_request_processing(self) -> None:\n        \"\"\"Starts consuming Spider.start() output and sending scheduled\n        requests.\"\"\"\n        # Starts the processing of scheduled requests, as well as a periodic\n        # call to that processing method for scenarios where the scheduler\n        # reports having pending requests but returns none.\n        try:\n            assert self._slot is not None  # typing\n            self._slot.nextcall.schedule()\n            self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n\n            while self._start and self.spider:\n                await self._process_start_next()\n                if not self.needs_backout():\n                    # Give room for the outcome of self._process_start_next() to be\n                    # processed before continuing with the next iteration.\n                    self._slot.nextcall.schedule()\n                    await self._slot.nextcall.wait()\n        except (asyncio.exceptions.CancelledError, CancelledError):\n            # self.stop() has cancelled us, nothing to do\n            return\n        except Exception:\n            # an error happened, log it and stop the engine\n            self._start_request_processing_awaitable = None\n            logger.error(\n                \"Error while processing requests from start()\",\n                exc_info=True,\n                extra={\"spider\": self.spider},\n            )\n            await self.stop_async()\n\n    def _start_scheduled_requests(self) -> None:\n        if self._slot is None or self._slot.closing is not None or self.paused:\n            return\n\n        while not self.needs_backout():\n            if not self._start_scheduled_request():\n                break\n\n        if self.spider_is_idle() and self._slot.close_if_idle:\n            self._spider_idle()\n", "n_tokens": 1167, "byte_len": 5898, "file_sha1": "7ea78a13b319d0972b87fa2382d8dbfb3670854f", "start_line": 170, "end_line": 318}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py", "rel_path": "scrapy/core/engine.py", "module": "scrapy.core.engine", "ext": "py", "chunk_number": 3, "symbols": ["needs_backout", "_start_scheduled_request", "_remove_request", "_handle_downloader_output", "spider_is_idle", "crawl", "_schedule_request", "download", "failure", "async", "start", "scheduled", "bool", "scheduling", "scheduler", "error", "deprecated", "more", "sent", "response", "request", "send", "catch", "scrape", "isinstance", "handler", "execution", "engine", "closing", "schedule", "__init__", "add_request", "remove_request", "_maybe_fire_closing", "_get_scheduler_class", "stop", "close", "pause", "unpause", "_start_scheduled_requests", "_download", "open_spider", "_spider_idle", "close_spider", "log_failure", "_Slot", "ExecutionEngine", "downloader", "method", "continuing"], "ast_kind": "function_or_method", "text": "    def needs_backout(self) -> bool:\n        \"\"\"Returns ``True`` if no more requests can be sent at the moment, or\n        ``False`` otherwise.\n\n        See :ref:`start-requests-lazy` for an example.\n        \"\"\"\n        assert self.scraper.slot is not None  # typing\n        return (\n            not self.running\n            or not self._slot\n            or bool(self._slot.closing)\n            or self.downloader.needs_backout()\n            or self.scraper.slot.needs_backout()\n        )\n\n    def _start_scheduled_request(self) -> bool:\n        assert self._slot is not None  # typing\n        assert self.spider is not None  # typing\n\n        request = self._slot.scheduler.next_request()\n        if request is None:\n            self.signals.send_catch_log(signals.scheduler_empty)\n            return False\n\n        d: Deferred[Response | Request] = self._download(request)\n        d.addBoth(self._handle_downloader_output, request)\n        d.addErrback(\n            lambda f: logger.info(\n                \"Error while handling downloader output\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n\n        def _remove_request(_: Any) -> None:\n            assert self._slot\n            self._slot.remove_request(request)\n\n        d2: Deferred[None] = d.addBoth(_remove_request)\n        d2.addErrback(\n            lambda f: logger.info(\n                \"Error while removing request from slot\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        slot = self._slot\n        d2.addBoth(lambda _: slot.nextcall.schedule())\n        d2.addErrback(\n            lambda f: logger.info(\n                \"Error while scheduling new request\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        return True\n\n    @inlineCallbacks\n    def _handle_downloader_output(\n        self, result: Request | Response | Failure, request: Request\n    ) -> Generator[Deferred[Any], Any, None]:\n        if not isinstance(result, (Request, Response, Failure)):\n            raise TypeError(\n                f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\"\n            )\n\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(result, Request):\n            self.crawl(result)\n            return\n\n        try:\n            yield self.scraper.enqueue_scrape(result, request)\n        except Exception:\n            assert self.spider is not None\n            logger.error(\n                \"Error while enqueuing scrape\",\n                exc_info=True,\n                extra={\"spider\": self.spider},\n            )\n\n    def spider_is_idle(self) -> bool:\n        if self._slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n        if not self.scraper.slot.is_idle():  # type: ignore[union-attr]\n            return False\n        if self.downloader.active:  # downloader has pending requests\n            return False\n        if self._start is not None:  # not all start requests are handled\n            return False\n        return not self._slot.scheduler.has_pending_requests()\n\n    def crawl(self, request: Request) -> None:\n        \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        self._schedule_request(request)\n        self._slot.nextcall.schedule()  # type: ignore[union-attr]\n\n    def _schedule_request(self, request: Request) -> None:\n        request_scheduled_result = self.signals.send_catch_log(\n            signals.request_scheduled,\n            request=request,\n            spider=self.spider,\n            dont_log=IgnoreRequest,\n        )\n        for handler, result in request_scheduled_result:\n            if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n                return\n        if not self._slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n            self.signals.send_catch_log(\n                signals.request_dropped, request=request, spider=self.spider\n            )\n\n    def download(self, request: Request) -> Deferred[Response]:\n        \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n        warnings.warn(\n            \"ExecutionEngine.download() is deprecated, use download_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.download_async(request))\n\n    async def download_async(self, request: Request) -> Response:\n        \"\"\"Return a coroutine which fires with a Response as result.\n\n         Only downloader middlewares are applied.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        try:\n            response_or_request = await maybe_deferred_to_future(\n                self._download(request)\n            )\n        finally:\n            assert self._slot is not None\n            self._slot.remove_request(request)\n        if isinstance(response_or_request, Request):\n            return await self.download_async(response_or_request)\n        return response_or_request\n", "n_tokens": 1092, "byte_len": 5408, "file_sha1": "7ea78a13b319d0972b87fa2382d8dbfb3670854f", "start_line": 319, "end_line": 461}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py", "rel_path": "scrapy/core/engine.py", "module": "scrapy.core.engine", "ext": "py", "chunk_number": 4, "symbols": ["_download", "open_spider", "_spider_idle", "close_spider", "failure", "async", "bool", "guaranteed", "signal", "start", "scheduled", "spider", "name", "scheduler", "deprecated", "send", "catch", "close", "loop", "idle", "isinstance", "execution", "engine", "handler", "closing", "spidermw", "none", "process", "type", "reason", "__init__", "add_request", "remove_request", "_maybe_fire_closing", "_get_scheduler_class", "stop", "pause", "unpause", "_start_scheduled_requests", "needs_backout", "_start_scheduled_request", "_remove_request", "_handle_downloader_output", "spider_is_idle", "crawl", "_schedule_request", "download", "log_failure", "_Slot", "ExecutionEngine"], "ast_kind": "function_or_method", "text": "    @inlineCallbacks\n    def _download(\n        self, request: Request\n    ) -> Generator[Deferred[Any], Any, Response | Request]:\n        assert self._slot is not None  # typing\n        assert self.spider is not None\n\n        self._slot.add_request(request)\n        try:\n            result: Response | Request\n            if self._downloader_fetch_needs_spider:\n                result = yield self.downloader.fetch(request, self.spider)\n            else:\n                result = yield self.downloader.fetch(request)\n            if not isinstance(result, (Response, Request)):\n                raise TypeError(\n                    f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\"\n                )\n            if isinstance(result, Response):\n                if result.request is None:\n                    result.request = request\n                logkws = self.logformatter.crawled(result.request, result, self.spider)\n                if logkws is not None:\n                    logger.log(\n                        *logformatter_adapter(logkws), extra={\"spider\": self.spider}\n                    )\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=result,\n                    request=result.request,\n                    spider=self.spider,\n                )\n            return result\n        finally:\n            self._slot.nextcall.schedule()\n\n    def open_spider(self, spider: Spider, close_if_idle: bool = True) -> Deferred[None]:\n        warnings.warn(\n            \"ExecutionEngine.open_spider() is deprecated, use open_spider_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.open_spider_async(close_if_idle=close_if_idle))\n\n    async def open_spider_async(self, *, close_if_idle: bool = True) -> None:\n        assert self.crawler.spider\n        if self._slot is not None:\n            raise RuntimeError(\n                f\"No free spider slot when opening {self.crawler.spider.name!r}\"\n            )\n        logger.info(\"Spider opened\", extra={\"spider\": self.crawler.spider})\n        self.spider = self.crawler.spider\n        nextcall = CallLaterOnce(self._start_scheduled_requests)\n        scheduler = build_from_crawler(self.scheduler_cls, self.crawler)\n        self._slot = _Slot(close_if_idle, nextcall, scheduler)\n        self._start = await self.scraper.spidermw.process_start()\n        if hasattr(scheduler, \"open\") and (d := scheduler.open(self.crawler.spider)):\n            await maybe_deferred_to_future(d)\n        await self.scraper.open_spider_async()\n        assert self.crawler.stats\n        self.crawler.stats.open_spider()\n        await self.signals.send_catch_log_async(\n            signals.spider_opened, spider=self.crawler.spider\n        )\n\n    def _spider_idle(self) -> None:\n        \"\"\"\n        Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n        It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n        exception, the spider is not closed until the next loop and this function is guaranteed to be called\n        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.\n        \"\"\"\n        assert self.spider is not None  # typing\n        expected_ex = (DontCloseSpider, CloseSpider)\n        res = self.signals.send_catch_log(\n            signals.spider_idle, spider=self.spider, dont_log=expected_ex\n        )\n        detected_ex = {\n            ex: x.value\n            for _, x in res\n            for ex in expected_ex\n            if isinstance(x, Failure) and isinstance(x.value, ex)\n        }\n        if DontCloseSpider in detected_ex:\n            return\n        if self.spider_is_idle():\n            ex = detected_ex.get(CloseSpider, CloseSpider(reason=\"finished\"))\n            assert isinstance(ex, CloseSpider)  # typing\n            _schedule_coro(self.close_spider_async(reason=ex.reason))\n\n    def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred[None]:\n        warnings.warn(\n            \"ExecutionEngine.close_spider() is deprecated, use close_spider_async() instead\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.close_spider_async(reason=reason))\n\n    async def close_spider_async(self, *, reason: str = \"cancelled\") -> None:\n        \"\"\"Close (cancel) spider and clear all its outstanding requests.\n\n        .. versionadded:: VERSION\n        \"\"\"\n        if self.spider is None:\n            raise RuntimeError(\"Spider not opened\")\n\n        if self._slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n\n        if self._slot.closing is not None:\n            await maybe_deferred_to_future(self._slot.closing)\n            return\n\n        spider = self.spider\n\n        logger.info(\n            \"Closing spider (%(reason)s)\", {\"reason\": reason}, extra={\"spider\": spider}\n        )\n", "n_tokens": 1058, "byte_len": 5057, "file_sha1": "7ea78a13b319d0972b87fa2382d8dbfb3670854f", "start_line": 462, "end_line": 580}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/engine.py", "rel_path": "scrapy/core/engine.py", "module": "scrapy.core.engine", "ext": "py", "chunk_number": 5, "symbols": ["log_failure", "scraper", "downloader", "ensure", "awaitable", "slot", "hasattr", "await", "except", "close", "spider", "signal", "closed", "stats", "extra", "send", "catch", "maybe", "deferred", "scheduler", "log", "failure", "error", "sending", "logger", "exception", "running", "info", "true", "noqa", "__init__", "add_request", "remove_request", "_maybe_fire_closing", "_get_scheduler_class", "start", "stop", "pause", "unpause", "_start_scheduled_requests", "needs_backout", "_start_scheduled_request", "_remove_request", "_handle_downloader_output", "spider_is_idle", "crawl", "_schedule_request", "download", "_download", "open_spider"], "ast_kind": "function_or_method", "text": "        def log_failure(msg: str) -> None:\n            logger.error(msg, exc_info=True, extra={\"spider\": spider})  # noqa: LOG014\n\n        try:\n            await self._slot.close()\n        except Exception:\n            log_failure(\"Slot close failure\")\n\n        try:\n            self.downloader.close()\n        except Exception:\n            log_failure(\"Downloader close failure\")\n\n        try:\n            await self.scraper.close_spider_async()\n        except Exception:\n            log_failure(\"Scraper close failure\")\n\n        if hasattr(self._slot.scheduler, \"close\"):\n            try:\n                if (d := self._slot.scheduler.close(reason)) is not None:\n                    await maybe_deferred_to_future(d)\n            except Exception:\n                log_failure(\"Scheduler close failure\")\n\n        try:\n            await self.signals.send_catch_log_async(\n                signal=signals.spider_closed,\n                spider=spider,\n                reason=reason,\n            )\n        except Exception:\n            log_failure(\"Error while sending spider_close signal\")\n\n        assert self.crawler.stats\n        try:\n            self.crawler.stats.close_spider(reason=reason)\n        except Exception:\n            log_failure(\"Stats close failure\")\n\n        logger.info(\n            \"Spider closed (%(reason)s)\",\n            {\"reason\": reason},\n            extra={\"spider\": spider},\n        )\n\n        self._slot = None\n        self.spider = None\n\n        try:\n            await ensure_awaitable(self._spider_closed_callback(spider))\n        except Exception:\n            log_failure(\"Error running spider_closed_callback\")\n", "n_tokens": 313, "byte_len": 1641, "file_sha1": "7ea78a13b319d0972b87fa2382d8dbfb3670854f", "start_line": 581, "end_line": 634}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py", "rel_path": "scrapy/core/scheduler.py", "module": "scrapy.core.scheduler", "ext": "py", "chunk_number": 1, "symbols": ["__instancecheck__", "__subclasscheck__", "from_crawler", "open", "close", "has_pending_requests", "enqueue_request", "next_request", "BaseSchedulerMeta", "BaseScheduler", "method", "processed", "those", "bool", "signal", "instance", "subclass", "python", "spider", "scheduler", "spiders", "describes", "future", "typ", "checking", "https", "string", "path", "dupefilter", "interface", "__init__", "_get_start_queue_cls", "__len__", "_dqpush", "_mqpush", "_dqpop", "_mq", "_dq", "_dqdir", "_read_dqs_state", "_write_dqs_state", "Scheduler", "falling", "concurren", "requests", "load", "filtered", "startprios", "stack", "sent"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, cast\nfrom warnings import warn\n\n# working around https://github.com/sphinx-doc/sphinx/issues/10400\nfrom twisted.internet.defer import Deferred  # noqa: TC002\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.spiders import Spider  # noqa: TC001\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import global_object_name\n\nif TYPE_CHECKING:\n    # requires queuelib >= 1.6.2\n    from queuelib.queue import BaseQueue\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.dupefilters import BaseDupeFilter\n    from scrapy.http.request import Request\n    from scrapy.pqueues import ScrapyPriorityQueue\n    from scrapy.statscollectors import StatsCollector\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseSchedulerMeta(type):\n    \"\"\"\n    Metaclass to check scheduler classes against the necessary interface\n    \"\"\"\n\n    def __instancecheck__(cls, instance: Any) -> bool:\n        return cls.__subclasscheck__(type(instance))\n\n    def __subclasscheck__(cls, subclass: type) -> bool:\n        return (\n            hasattr(subclass, \"has_pending_requests\")\n            and callable(subclass.has_pending_requests)\n            and hasattr(subclass, \"enqueue_request\")\n            and callable(subclass.enqueue_request)\n            and hasattr(subclass, \"next_request\")\n            and callable(subclass.next_request)\n        )\n\n\nclass BaseScheduler(metaclass=BaseSchedulerMeta):\n    \"\"\"The scheduler component is responsible for storing requests received\n    from the engine, and feeding them back upon request (also to the engine).\n\n    The original sources of said requests are:\n\n    * Spider: ``start`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n    * Spider middleware: ``process_spider_output`` and ``process_spider_exception`` methods\n    * Downloader middleware: ``process_request``, ``process_response`` and ``process_exception`` methods\n\n    The order in which the scheduler returns its stored requests (via the ``next_request`` method)\n    plays a great part in determining the order in which those requests are downloaded. See :ref:`request-order`.\n\n    The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.\n        \"\"\"\n        return cls()\n\n    def open(self, spider: Spider) -> Deferred[None] | None:\n        \"\"\"\n        Called when the spider is opened by the engine. It receives the spider\n        instance as argument and it's useful to execute initialization code.\n\n        :param spider: the spider object for the current crawl\n        :type spider: :class:`~scrapy.spiders.Spider`\n        \"\"\"\n\n    def close(self, reason: str) -> Deferred[None] | None:\n        \"\"\"\n        Called when the spider is closed by the engine. It receives the reason why the crawl\n        finished as argument and it's useful to execute cleaning code.\n\n        :param reason: a string which describes the reason why the spider was closed\n        :type reason: :class:`str`\n        \"\"\"\n\n    @abstractmethod\n    def has_pending_requests(self) -> bool:\n        \"\"\"\n        ``True`` if the scheduler has enqueued requests, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Process a request received by the engine.\n\n        Return ``True`` if the request is stored correctly, ``False`` otherwise.\n\n        If ``False``, the engine will fire a ``request_dropped`` signal, and\n        will not make further attempts to schedule the request at a later time.\n        For reference, the default Scrapy scheduler returns ``False`` when the\n        request is rejected by the dupefilter.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def next_request(self) -> Request | None:\n        \"\"\"\n        Return the next :class:`~scrapy.Request` to be processed, or ``None``\n        to indicate that there are no requests to be considered ready at the moment.\n\n        Returning ``None`` implies that no request from the scheduler will be sent\n        to the downloader in the current reactor cycle. The engine will continue\n        calling ``next_request`` until ``has_pending_requests`` is ``False``.\n        \"\"\"\n        raise NotImplementedError\n\n", "n_tokens": 992, "byte_len": 4778, "file_sha1": "b5d172596db78f2dc9159e0fefdcc02f408a56d6", "start_line": 1, "end_line": 129}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py", "rel_path": "scrapy/core/scheduler.py", "module": "scrapy.core.scheduler", "ext": "py", "chunk_number": 2, "symbols": ["from_crawler", "Scheduler", "those", "schedule", "memor", "separate", "first", "search", "enforce", "queues", "determine", "instance", "duplicate", "spider", "scheduler", "concurren", "requests", "filtered", "abstract", "data", "stack", "https", "dupefilter", "ordered", "sent", "squeues", "each", "concurrency", "settings", "order", "__instancecheck__", "__subclasscheck__", "open", "close", "has_pending_requests", "enqueue_request", "next_request", "__init__", "_get_start_queue_cls", "__len__", "_dqpush", "_mqpush", "_dqpop", "_mq", "_dq", "_dqdir", "_read_dqs_state", "_write_dqs_state", "BaseSchedulerMeta", "BaseScheduler"], "ast_kind": "class_or_type", "text": "class Scheduler(BaseScheduler):\n    \"\"\"Default scheduler.\n\n    Requests are stored into priority queues\n    (:setting:`SCHEDULER_PRIORITY_QUEUE`) that sort requests by\n    :attr:`~scrapy.http.Request.priority`.\n\n    By default, a single, memory-based priority queue is used for all requests.\n    When using :setting:`JOBDIR`, a disk-based priority queue is also created,\n    and only unserializable requests are stored in the memory-based priority\n    queue. For a given priority value, requests in memory take precedence over\n    requests in disk.\n\n    Each priority queue stores requests in separate internal queues, one per\n    priority value. The memory priority queue uses\n    :setting:`SCHEDULER_MEMORY_QUEUE` queues, while the disk priority queue\n    uses :setting:`SCHEDULER_DISK_QUEUE` queues. The internal queues determine\n    :ref:`request order <request-order>` when requests have the same priority.\n    :ref:`Start requests <start-requests>` are stored into separate internal\n    queues by default, and :ref:`ordered differently <start-request-order>`.\n\n    Duplicate requests are filtered out with an instance of\n    :setting:`DUPEFILTER_CLASS`.\n\n    .. _request-order:\n\n    Request order\n    =============\n\n    With default settings, pending requests are stored in a LIFO_ queue\n    (:ref:`except for start requests <start-request-order>`). As a result,\n    crawling happens in `DFO order`_, which is usually the most convenient\n    crawl order. However, you can enforce :ref:`BFO <bfo>` or :ref:`a custom\n    order <custom-request-order>` (:ref:`except for the first few requests\n    <concurrency-v-order>`).\n\n    .. _LIFO: https://en.wikipedia.org/wiki/Stack_(abstract_data_type)\n    .. _DFO order: https://en.wikipedia.org/wiki/Depth-first_search\n\n    .. _start-request-order:\n\n    Start request order\n    -------------------\n\n    :ref:`Start requests <start-requests>` are sent in the order they are\n    yielded from :meth:`~scrapy.Spider.start`, and given the same\n    :attr:`~scrapy.http.Request.priority`, other requests take precedence over\n    start requests.\n\n    You can set :setting:`SCHEDULER_START_MEMORY_QUEUE` and\n    :setting:`SCHEDULER_START_DISK_QUEUE` to ``None`` to handle start requests\n    the same as other requests when it comes to order and priority.\n\n\n    .. _bfo:\n\n    Crawling in BFO order\n    ---------------------\n\n    If you do want to crawl in `BFO order`_, you can do it by setting the\n    following :ref:`settings <topics-settings>`:\n\n    | :setting:`DEPTH_PRIORITY` = ``1``\n    | :setting:`SCHEDULER_DISK_QUEUE` = ``\"scrapy.squeues.PickleFifoDiskQueue\"``\n    | :setting:`SCHEDULER_MEMORY_QUEUE` = ``\"scrapy.squeues.FifoMemoryQueue\"``\n\n    .. _BFO order: https://en.wikipedia.org/wiki/Breadth-first_search\n\n\n    .. _custom-request-order:\n\n    Crawling in a custom order\n    --------------------------\n\n    You can manually set :attr:`~scrapy.http.Request.priority` on requests to\n    force a specific request order.\n\n\n    .. _concurrency-v-order:\n\n    Concurrency affects order\n    -------------------------\n\n    While pending requests are below the configured values of\n    :setting:`CONCURRENT_REQUESTS`, :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`\n    or :setting:`CONCURRENT_REQUESTS_PER_IP`, those requests are sent\n    concurrently.\n\n    As a result, the first few requests of a crawl may not follow the desired\n    order. Lowering those settings to ``1`` enforces the desired order except\n    for the very first request, but it significantly slows down the crawl as a\n    whole.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n        return cls(\n            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n            jobdir=job_dir(crawler.settings),\n            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n            stats=crawler.stats,\n            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n            crawler=crawler,\n        )\n", "n_tokens": 962, "byte_len": 4191, "file_sha1": "b5d172596db78f2dc9159e0fefdcc02f408a56d6", "start_line": 130, "end_line": 237}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py", "rel_path": "scrapy/core/scheduler.py", "module": "scrapy.core.scheduler", "ext": "py", "chunk_number": 3, "symbols": ["__init__", "_get_start_queue_cls", "has_pending_requests", "open", "close", "enqueue_request", "method", "initialize", "schedule", "memor", "bool", "scheduling", "inc", "value", "filtering", "instance", "spider", "about", "falling", "scheduler", "stat", "class", "dont", "filter", "indicates", "filtered", "dupefilter", "interface", "responsible", "dqdir", "__instancecheck__", "__subclasscheck__", "from_crawler", "next_request", "__len__", "_dqpush", "_mqpush", "_dqpop", "_mq", "_dq", "_dqdir", "_read_dqs_state", "_write_dqs_state", "BaseSchedulerMeta", "BaseScheduler", "Scheduler", "those", "signal", "concurren", "requests"], "ast_kind": "class_or_type", "text": "    def __init__(\n        self,\n        dupefilter: BaseDupeFilter,\n        jobdir: str | None = None,\n        dqclass: type[BaseQueue] | None = None,\n        mqclass: type[BaseQueue] | None = None,\n        logunser: bool = False,\n        stats: StatsCollector | None = None,\n        pqclass: type[ScrapyPriorityQueue] | None = None,\n        crawler: Crawler | None = None,\n    ):\n        \"\"\"Initialize the scheduler.\n\n        :param dupefilter: An object responsible for checking and filtering duplicate requests.\n                        The value for the :setting:`DUPEFILTER_CLASS` setting is used by default.\n        :type dupefilter: :class:`scrapy.dupefilters.BaseDupeFilter` instance or similar:\n                        any class that implements the `BaseDupeFilter` interface\n\n        :param jobdir: The path of a directory to be used for persisting the crawl's state.\n                    The value for the :setting:`JOBDIR` setting is used by default.\n                    See :ref:`topics-jobs`.\n        :type jobdir: :class:`str` or ``None``\n\n        :param dqclass: A class to be used as persistent request queue.\n                        The value for the :setting:`SCHEDULER_DISK_QUEUE` setting is used by default.\n        :type dqclass: class\n\n        :param mqclass: A class to be used as non-persistent request queue.\n                        The value for the :setting:`SCHEDULER_MEMORY_QUEUE` setting is used by default.\n        :type mqclass: class\n\n        :param logunser: A boolean that indicates whether or not unserializable requests should be logged.\n                        The value for the :setting:`SCHEDULER_DEBUG` setting is used by default.\n        :type logunser: bool\n\n        :param stats: A stats collector object to record stats about the request scheduling process.\n                    The value for the :setting:`STATS_CLASS` setting is used by default.\n        :type stats: :class:`scrapy.statscollectors.StatsCollector` instance or similar:\n                    any class that implements the `StatsCollector` interface\n\n        :param pqclass: A class to be used as priority queue for requests.\n                        The value for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting is used by default.\n        :type pqclass: class\n\n        :param crawler: The crawler object corresponding to the current crawl.\n        :type crawler: :class:`scrapy.crawler.Crawler`\n        \"\"\"\n        self.df: BaseDupeFilter = dupefilter\n        self.dqdir: str | None = self._dqdir(jobdir)\n        self.pqclass: type[ScrapyPriorityQueue] | None = pqclass\n        self.dqclass: type[BaseQueue] | None = dqclass\n        self.mqclass: type[BaseQueue] | None = mqclass\n        self.logunser: bool = logunser\n        self.stats: StatsCollector | None = stats\n        self.crawler: Crawler | None = crawler\n        self._sdqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n            crawler, \"DISK\"\n        )\n        self._smqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n            crawler, \"MEMORY\"\n        )\n\n    def _get_start_queue_cls(\n        self, crawler: Crawler | None, queue: str\n    ) -> type[BaseQueue] | None:\n        if crawler is None:\n            return None\n        cls = crawler.settings[f\"SCHEDULER_START_{queue}_QUEUE\"]\n        if not cls:\n            return None\n        return load_object(cls)\n\n    def has_pending_requests(self) -> bool:\n        return len(self) > 0\n\n    def open(self, spider: Spider) -> Deferred[None] | None:\n        \"\"\"\n        (1) initialize the memory queue\n        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n        (3) return the result of the dupefilter's ``open`` method\n        \"\"\"\n        self.spider: Spider = spider\n        self.mqs: ScrapyPriorityQueue = self._mq()\n        self.dqs: ScrapyPriorityQueue | None = self._dq() if self.dqdir else None\n        return self.df.open()\n\n    def close(self, reason: str) -> Deferred[None] | None:\n        \"\"\"\n        (1) dump pending requests to disk if there is a disk queue\n        (2) return the result of the dupefilter's ``close`` method\n        \"\"\"\n        if self.dqs is not None:\n            state = self.dqs.close()\n            assert isinstance(self.dqdir, str)\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)\n\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Unless the received request is filtered out by the Dupefilter, attempt to push\n        it into the disk queue, falling back to pushing it into the memory queue.\n\n        Increment the appropriate stats, such as: ``scheduler/enqueued``,\n        ``scheduler/enqueued/disk``, ``scheduler/enqueued/memory``.\n\n        Return ``True`` if the request was stored successfully, ``False`` otherwise.\n        \"\"\"\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        assert self.stats is not None\n        if dqok:\n            self.stats.inc_value(\"scheduler/enqueued/disk\")\n        else:\n            self._mqpush(request)\n            self.stats.inc_value(\"scheduler/enqueued/memory\")\n        self.stats.inc_value(\"scheduler/enqueued\")\n        return True\n", "n_tokens": 1225, "byte_len": 5293, "file_sha1": "b5d172596db78f2dc9159e0fefdcc02f408a56d6", "start_line": 238, "end_line": 357}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/scheduler.py", "rel_path": "scrapy/core/scheduler.py", "module": "scrapy.core.scheduler", "ext": "py", "chunk_number": 4, "symbols": ["next_request", "__len__", "_dqpush", "_mqpush", "_dqpop", "_mq", "_dq", "_dqdir", "_read_dqs_state", "_write_dqs_state", "encoding", "method", "does", "bool", "inc", "value", "scheduled", "instance", "serialize", "name", "falling", "scheduler", "load", "startprios", "more", "path", "amount", "dqdir", "back", "mqpush", "__instancecheck__", "__subclasscheck__", "from_crawler", "open", "close", "has_pending_requests", "enqueue_request", "__init__", "_get_start_queue_cls", "BaseSchedulerMeta", "BaseScheduler", "Scheduler", "those", "signal", "spider", "concurren", "requests", "filtered", "stack", "sent"], "ast_kind": "function_or_method", "text": "    def next_request(self) -> Request | None:\n        \"\"\"\n        Return a :class:`~scrapy.Request` object from the memory queue,\n        falling back to the disk queue if the memory queue is empty.\n        Return ``None`` if there are no more enqueued requests.\n\n        Increment the appropriate stats, such as: ``scheduler/dequeued``,\n        ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n        \"\"\"\n        request: Request | None = self.mqs.pop()\n        assert self.stats is not None\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued/memory\")\n        else:\n            request = self._dqpop()\n            if request is not None:\n                self.stats.inc_value(\"scheduler/dequeued/disk\")\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued\")\n        return request\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the total amount of enqueued requests\n        \"\"\"\n        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)\n\n    def _dqpush(self, request: Request) -> bool:\n        if self.dqs is None:\n            return False\n        try:\n            self.dqs.push(request)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\n                    \"Unable to serialize request: %(request)s - reason:\"\n                    \" %(reason)s - no more unserializable requests will be\"\n                    \" logged (stats being collected)\"\n                )\n                logger.warning(\n                    msg,\n                    {\"request\": request, \"reason\": e},\n                    exc_info=True,\n                    extra={\"spider\": self.spider},\n                )\n                self.logunser = False\n            assert self.stats is not None\n            self.stats.inc_value(\"scheduler/unserializable\")\n            return False\n        return True\n\n    def _mqpush(self, request: Request) -> None:\n        self.mqs.push(request)\n\n    def _dqpop(self) -> Request | None:\n        if self.dqs is not None:\n            return self.dqs.pop()\n        return None\n\n    def _mq(self) -> ScrapyPriorityQueue:\n        \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n        assert self.crawler\n        assert self.pqclass\n        try:\n            return build_from_crawler(\n                self.pqclass,\n                self.crawler,\n                downstream_queue_cls=self.mqclass,\n                key=\"\",\n                start_queue_cls=self._smqclass,\n            )\n        except TypeError:\n            warn(\n                f\"The __init__ method of {global_object_name(self.pqclass)} \"\n                f\"does not support a `start_queue_cls` keyword-only \"\n                f\"parameter.\",\n                ScrapyDeprecationWarning,\n            )\n            return build_from_crawler(\n                self.pqclass,\n                self.crawler,\n                downstream_queue_cls=self.mqclass,\n                key=\"\",\n            )\n\n    def _dq(self) -> ScrapyPriorityQueue:\n        \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n        assert self.crawler\n        assert self.dqdir\n        assert self.pqclass\n        state = self._read_dqs_state(self.dqdir)\n        try:\n            q = build_from_crawler(\n                self.pqclass,\n                self.crawler,\n                downstream_queue_cls=self.dqclass,\n                key=self.dqdir,\n                startprios=state,\n                start_queue_cls=self._sdqclass,\n            )\n        except TypeError:\n            warn(\n                f\"The __init__ method of {global_object_name(self.pqclass)} \"\n                f\"does not support a `start_queue_cls` keyword-only \"\n                f\"parameter.\",\n                ScrapyDeprecationWarning,\n            )\n            q = build_from_crawler(\n                self.pqclass,\n                self.crawler,\n                downstream_queue_cls=self.dqclass,\n                key=self.dqdir,\n                startprios=state,\n            )\n        if q:\n            logger.info(\n                \"Resuming crawl (%(queuesize)d requests scheduled)\",\n                {\"queuesize\": len(q)},\n                extra={\"spider\": self.spider},\n            )\n        return q\n\n    def _dqdir(self, jobdir: str | None) -> str | None:\n        \"\"\"Return a folder name to keep disk queue state at\"\"\"\n        if jobdir:\n            dqdir = Path(jobdir, \"requests.queue\")\n            if not dqdir.exists():\n                dqdir.mkdir(parents=True)\n            return str(dqdir)\n        return None\n\n    def _read_dqs_state(self, dqdir: str) -> list[int]:\n        path = Path(dqdir, \"active.json\")\n        if not path.exists():\n            return []\n        with path.open(encoding=\"utf-8\") as f:\n            return cast(\"list[int]\", json.load(f))\n\n    def _write_dqs_state(self, dqdir: str, state: list[int]) -> None:\n        with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(state, f)\n", "n_tokens": 1073, "byte_len": 5049, "file_sha1": "b5d172596db78f2dc9159e0fefdcc02f408a56d6", "start_line": 358, "end_line": 499}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/__init__.py", "rel_path": "scrapy/core/downloader/__init__.py", "module": "scrapy.core.downloader.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "free_transfer_slots", "download_delay", "close", "__repr__", "__str__", "_get_concurrency_delay", "fetch", "needs_backout", "Slot", "Downloader", "failure", "bool", "call", "later", "uniform", "spider", "name", "concurren", "requests", "warn", "deprecated", "future", "typ", "checking", "signalmanager", "cls", "urlparse", "cached", "delay", "_get_slot", "get_slot_key", "_get_slot_key", "_enqueue_request", "_process_queue", "_latercall", "_slot_gc", "came", "method", "consider", "async", "those", "append", "signal", "were", "about", "popleft", "recent", "get", "slot"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport random\nimport warnings\nfrom collections import deque\nfrom datetime import datetime\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader.handlers import DownloadHandlers\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.resolver import dnscache\nfrom scrapy.utils.asyncio import (\n    AsyncioLoopingCall,\n    CallLaterResult,\n    call_later,\n    create_looping_call,\n)\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.defer import (\n    _defer_sleep_async,\n    _schedule_coro,\n    maybe_deferred_to_future,\n)\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    from collections.abc import Generator\n\n    from twisted.internet.task import LoopingCall\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.settings import BaseSettings\n    from scrapy.signalmanager import SignalManager\n\n\nclass Slot:\n    \"\"\"Downloader slot\"\"\"\n\n    def __init__(\n        self,\n        concurrency: int,\n        delay: float,\n        randomize_delay: bool,\n    ):\n        self.concurrency: int = concurrency\n        self.delay: float = delay\n        self.randomize_delay: bool = randomize_delay\n\n        self.active: set[Request] = set()\n        self.queue: deque[tuple[Request, Deferred[Response]]] = deque()\n        self.transferring: set[Request] = set()\n        self.lastseen: float = 0\n        self.latercall: CallLaterResult | None = None\n\n    def free_transfer_slots(self) -> int:\n        return self.concurrency - len(self.transferring)\n\n    def download_delay(self) -> float:\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)  # noqa: S311\n        return self.delay\n\n    def close(self) -> None:\n        if self.latercall:\n            self.latercall.cancel()\n            self.latercall = None\n\n    def __repr__(self) -> str:\n        cls_name = self.__class__.__name__\n        return (\n            f\"{cls_name}(concurrency={self.concurrency!r}, \"\n            f\"delay={self.delay:.2f}, \"\n            f\"randomize_delay={self.randomize_delay!r})\"\n        )\n\n    def __str__(self) -> str:\n        return (\n            f\"<downloader.Slot concurrency={self.concurrency!r} \"\n            f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n            f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n            f\"len(transferring)={len(self.transferring)} \"\n            f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n        )\n\n\ndef _get_concurrency_delay(\n    concurrency: int, spider: Spider, settings: BaseSettings\n) -> tuple[int, float]:\n    delay: float = settings.getfloat(\"DOWNLOAD_DELAY\")\n    if hasattr(spider, \"download_delay\"):\n        warnings.warn(\n            \"The 'download_delay' spider attribute is deprecated. \"\n            \"Use Spider.custom_settings or Spider.update_settings() instead. \"\n            \"The corresponding setting name is 'DOWNLOAD_DELAY'.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        delay = spider.download_delay\n\n    if hasattr(spider, \"max_concurrent_requests\"):\n        warnings.warn(\n            \"The 'max_concurrent_requests' spider attribute is deprecated. \"\n            \"Use Spider.custom_settings or Spider.update_settings() instead. \"\n            \"The corresponding setting name is 'CONCURRENT_REQUESTS'.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay\n\n\nclass Downloader:\n    DOWNLOAD_SLOT = \"download_slot\"\n\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        self.settings: BaseSettings = crawler.settings\n        self.signals: SignalManager = crawler.signals\n        self.slots: dict[str, Slot] = {}\n        self.active: set[Request] = set()\n        self.handlers: DownloadHandlers = DownloadHandlers(crawler)\n        self.total_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS\")\n        self.domain_concurrency: int = self.settings.getint(\n            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n        )\n        self.ip_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\")\n        self.randomize_delay: bool = self.settings.getbool(\"RANDOMIZE_DOWNLOAD_DELAY\")\n        self.middleware: DownloaderMiddlewareManager = (\n            DownloaderMiddlewareManager.from_crawler(crawler)\n        )\n        self._slot_gc_loop: AsyncioLoopingCall | LoopingCall = create_looping_call(\n            self._slot_gc\n        )\n        self._slot_gc_loop.start(60)\n        self.per_slot_settings: dict[str, dict[str, Any]] = self.settings.getdict(\n            \"DOWNLOAD_SLOTS\"\n        )\n\n    @inlineCallbacks\n    @_warn_spider_arg\n    def fetch(\n        self, request: Request, spider: Spider | None = None\n    ) -> Generator[Deferred[Any], Any, Response | Request]:\n        self.active.add(request)\n        try:\n            return (yield self.middleware.download(self._enqueue_request, request))\n        finally:\n            self.active.remove(request)\n\n    def needs_backout(self) -> bool:\n        return len(self.active) >= self.total_concurrency\n", "n_tokens": 1215, "byte_len": 5495, "file_sha1": "d2d3deb0d21647203065aa177199cab8a487f759", "start_line": 1, "end_line": 160}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/__init__.py", "rel_path": "scrapy/core/downloader/__init__.py", "module": "scrapy.core.downloader.__init__", "ext": "py", "chunk_number": 2, "symbols": ["_get_slot", "get_slot_key", "_get_slot_key", "_enqueue_request", "_process_queue", "_latercall", "close", "_slot_gc", "came", "consider", "method", "failure", "async", "those", "call", "later", "append", "signal", "were", "spider", "about", "popleft", "recent", "deprecated", "get", "slot", "passed", "urlparse", "cached", "delay", "__init__", "free_transfer_slots", "download_delay", "__repr__", "__str__", "_get_concurrency_delay", "fetch", "needs_backout", "Slot", "Downloader", "bool", "uniform", "name", "concurren", "requests", "warn", "future", "typ", "checking", "signalmanager"], "ast_kind": "function_or_method", "text": "    def _get_slot(self, request: Request) -> tuple[str, Slot]:\n        key = self.get_slot_key(request)\n        if key not in self.slots:\n            assert self.crawler.spider\n            slot_settings = self.per_slot_settings.get(key, {})\n            conc = (\n                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            )\n            conc, delay = _get_concurrency_delay(\n                conc, self.crawler.spider, self.settings\n            )\n            conc, delay = (\n                slot_settings.get(\"concurrency\", conc),\n                slot_settings.get(\"delay\", delay),\n            )\n            randomize_delay = slot_settings.get(\"randomize_delay\", self.randomize_delay)\n            new_slot = Slot(conc, delay, randomize_delay)\n            self.slots[key] = new_slot\n\n        return key, self.slots[key]\n\n    def get_slot_key(self, request: Request) -> str:\n        if self.DOWNLOAD_SLOT in request.meta:\n            return cast(\"str\", request.meta[self.DOWNLOAD_SLOT])\n\n        key = urlparse_cached(request).hostname or \"\"\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key\n\n    def _get_slot_key(self, request: Request, spider: Spider | None) -> str:\n        warnings.warn(\n            \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.get_slot_key(request)\n\n    # passed as download_func into self.middleware.download() in self.fetch()\n    @inlineCallbacks\n    def _enqueue_request(\n        self, request: Request\n    ) -> Generator[Deferred[Any], Any, Response]:\n        key, slot = self._get_slot(request)\n        request.meta[self.DOWNLOAD_SLOT] = key\n        slot.active.add(request)\n        self.signals.send_catch_log(\n            signal=signals.request_reached_downloader,\n            request=request,\n            spider=self.crawler.spider,\n        )\n        d: Deferred[Response] = Deferred()\n        slot.queue.append((request, d))\n        self._process_queue(slot)\n        try:\n            return (yield d)  # fired in _wait_for_download()\n        finally:\n            slot.active.remove(request)\n\n    def _process_queue(self, slot: Slot) -> None:\n        if slot.latercall:\n            # block processing until slot.latercall is called\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = call_later(penalty, self._latercall, slot)\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, queue_dfd = slot.queue.popleft()\n            _schedule_coro(self._wait_for_download(slot, request, queue_dfd))\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(slot)\n                break\n\n    def _latercall(self, slot: Slot) -> None:\n        slot.latercall = None\n        self._process_queue(slot)\n\n    async def _download(self, slot: Slot, request: Request) -> Response:\n        # The order is very important for the following logic. Do not change!\n        slot.transferring.add(request)\n        try:\n            # 1. Download the response\n            response: Response = await maybe_deferred_to_future(\n                self.handlers.download_request(request)\n            )\n            # 2. Notify response_downloaded listeners about the recent download\n            # before querying queue for next request\n            self.signals.send_catch_log(\n                signal=signals.response_downloaded,\n                response=response,\n                request=request,\n                spider=self.crawler.spider,\n            )\n            return response\n        except Exception:\n            await _defer_sleep_async()\n            raise\n        finally:\n            # 3. After response arrives, remove the request from transferring\n            # state to free up the transferring slot so it can be used by the\n            # following requests (perhaps those which came from the downloader\n            # middleware itself)\n            slot.transferring.remove(request)\n            self._process_queue(slot)\n            self.signals.send_catch_log(\n                signal=signals.request_left_downloader,\n                request=request,\n                spider=self.crawler.spider,\n            )\n\n    async def _wait_for_download(\n        self, slot: Slot, request: Request, queue_dfd: Deferred[Response]\n    ) -> None:\n        try:\n            response = await self._download(slot, request)\n        except Exception:\n            queue_dfd.errback(Failure())\n        else:\n            queue_dfd.callback(response)  # awaited in _enqueue_request()\n\n    def close(self) -> None:\n        self._slot_gc_loop.stop()\n        for slot in self.slots.values():\n            slot.close()\n\n    def _slot_gc(self, age: float = 60) -> None:\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()\n", "n_tokens": 1125, "byte_len": 5455, "file_sha1": "d2d3deb0d21647203065aa177199cab8a487f759", "start_line": 161, "end_line": 302}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/webclient.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/webclient.py", "rel_path": "scrapy/core/downloader/webclient.py", "module": "scrapy.core.downloader.webclient", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "connectionMade", "lineReceived", "handleHeader", "handleStatus", "handleEndHeaders", "connectionLost", "handleResponse", "timeout", "_build_response", "_set_connection_attributes", "ScrapyHTTPPageGetter", "ScrapyHTTPClientFactory", "encoding", "build", "response", "method", "protocol", "path", "str", "connections", "got", "status", "netloc", "send", "header", "future", "command", "https", "deprecated", "__repr__", "_cancelTimeout", "buildProtocol", "gotHeaders", "gotStatus", "page", "noPage", "clientConnectionFailed", "wait", "for", "issued", "disconnect", "case", "connection", "decides", "removed", "typ", "checking", "body", "elif"], "ast_kind": "class_or_type", "text": "\"\"\"Deprecated HTTP/1.0 helper classes used by HTTP10DownloadHandler.\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom time import time\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urldefrag, urlparse, urlunparse\n\nfrom twisted.internet import defer\nfrom twisted.internet.protocol import ClientFactory\nfrom twisted.web.http import HTTPClient\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    from scrapy import Request\n\n\nclass ScrapyHTTPPageGetter(HTTPClient):\n    delimiter = b\"\\n\"\n\n    def __init__(self):\n        warnings.warn(\n            \"ScrapyHTTPPageGetter is deprecated and will be removed in a future Scrapy version.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        super().__init__()\n\n    def connectionMade(self):\n        self.headers = Headers()  # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)\n\n    def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())\n\n    def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)\n\n    def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)\n\n    def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)\n\n    def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)\n\n    def handleResponse(self, response):\n        if self.factory.method.upper() == b\"HEAD\":\n            self.factory.page(b\"\")\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()\n\n    def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b\"https\"):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\n            defer.TimeoutError(\n                f\"Getting {self.factory.url} took longer \"\n                f\"than {self.factory.timeout} seconds.\"\n            )\n        )\n\n\n# This class used to inherit from Twisted’s\n# twisted.web.client.HTTPClientFactory. When that class was deprecated in\n# Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n# non-overridden code into this class.\nclass ScrapyHTTPClientFactory(ClientFactory):\n    protocol = ScrapyHTTPPageGetter\n\n    waiting = 1\n    noisy = False\n    followRedirect = False\n    afterFoundGet = False\n\n    def _build_response(self, body, request):\n        request.meta[\"download_latency\"] = self.headers_time - self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)\n        return respcls(\n            url=self._url,\n            status=status,\n            headers=headers,\n            body=body,\n            protocol=to_unicode(self.version),\n        )\n\n    def _set_connection_attributes(self, request):\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            proxy_parsed = urlparse(to_bytes(proxy, encoding=\"ascii\"))\n            self.scheme = proxy_parsed.scheme\n            self.host = proxy_parsed.hostname\n            self.port = proxy_parsed.port\n            self.netloc = proxy_parsed.netloc\n            if self.port is None:\n                self.port = 443 if proxy_parsed.scheme == b\"https\" else 80\n            self.path = self.url\n        else:\n            parsed = urlparse_cached(request)\n            path_str = urlunparse(\n                (\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\")\n            )\n            self.path = to_bytes(path_str, encoding=\"ascii\")\n            assert parsed.hostname is not None\n            self.host = to_bytes(parsed.hostname, encoding=\"ascii\")\n            self.port = parsed.port\n            self.scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n            self.netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n            if self.port is None:\n                self.port = 443 if self.scheme == b\"https\" else 80\n", "n_tokens": 979, "byte_len": 4776, "file_sha1": "2991810fc16f90551764988c6f9f345c92ace4c6", "start_line": 1, "end_line": 139}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/webclient.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/webclient.py", "rel_path": "scrapy/core/downloader/webclient.py", "module": "scrapy.core.downloader.webclient", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "__repr__", "_cancelTimeout", "buildProtocol", "gotHeaders", "gotStatus", "page", "noPage", "clientConnectionFailed", "encoding", "build", "response", "wait", "for", "method", "issued", "disconnect", "netloc", "got", "status", "future", "case", "connection", "decides", "deprecated", "removed", "elif", "more", "interface", "failed", "connectionMade", "lineReceived", "handleHeader", "handleStatus", "handleEndHeaders", "connectionLost", "handleResponse", "timeout", "_build_response", "_set_connection_attributes", "ScrapyHTTPPageGetter", "ScrapyHTTPClientFactory", "protocol", "path", "str", "connections", "send", "header", "command", "https"], "ast_kind": "function_or_method", "text": "    def __init__(self, request: Request, timeout: float = 180):\n        warnings.warn(\n            \"ScrapyHTTPClientFactory is deprecated and will be removed in a future Scrapy version.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n        self._url: str = urldefrag(request.url)[0]\n        # converting to bytes to comply to Twisted interface\n        self.url: bytes = to_bytes(self._url, encoding=\"ascii\")\n        self.method: bytes = to_bytes(request.method, encoding=\"ascii\")\n        self.body: bytes | None = request.body or None\n        self.headers: Headers = Headers(request.headers)\n        self.response_headers: Headers | None = None\n        self.timeout: float = request.meta.get(\"download_timeout\") or timeout\n        self.start_time: float = time()\n        self.deferred: defer.Deferred[Response] = defer.Deferred().addCallback(\n            self._build_response, request\n        )\n\n        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n        # to have _disconnectedDeferred. See Twisted r32329.\n        # As Scrapy implements it's own logic to handle redirects is not\n        # needed to add the callback _waitForDisconnect.\n        # Specifically this avoids the AttributeError exception when\n        # clientConnectionFailed method is called.\n        self._disconnectedDeferred: defer.Deferred[None] = defer.Deferred()\n\n        self._set_connection_attributes(request)\n\n        # set Host header based on url\n        self.headers.setdefault(\"Host\", self.netloc)\n\n        # set Content-Length based len of body\n        if self.body is not None:\n            self.headers[\"Content-Length\"] = len(self.body)\n            # just in case a broken http/1.1 decides to keep connection alive\n            self.headers.setdefault(\"Connection\", \"close\")\n        # Content-Length must be specified in POST method even with no body\n        elif self.method == b\"POST\":\n            self.headers[\"Content-Length\"] = 0\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self._url}>\"\n\n    def _cancelTimeout(self, result, timeoutCall):\n        if timeoutCall.active():\n            timeoutCall.cancel()\n        return result\n\n    def buildProtocol(self, addr):\n        p = ClientFactory.buildProtocol(self, addr)\n        p.followRedirect = self.followRedirect\n        p.afterFoundGet = self.afterFoundGet\n        if self.timeout:\n            from twisted.internet import reactor\n\n            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n        return p\n\n    def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers\n\n    def gotStatus(self, version, status, message):\n        \"\"\"\n        Set the status of the request on us.\n        @param version: The HTTP version.\n        @type version: L{bytes}\n        @param status: The HTTP status code, an integer represented as a\n        bytestring.\n        @type status: L{bytes}\n        @param message: The HTTP status message.\n        @type message: L{bytes}\n        \"\"\"\n        self.version, self.status, self.message = version, status, message\n\n    def page(self, page):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.callback(page)\n\n    def noPage(self, reason):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.errback(reason)\n\n    def clientConnectionFailed(self, _, reason):\n        \"\"\"\n        When a connection attempt fails, the request cannot be issued.  If no\n        result has yet been provided to the result Deferred, provide the\n        connection failure reason as an error result.\n        \"\"\"\n        if self.waiting:\n            self.waiting = 0\n            # If the connection attempt failed, there is nothing more to\n            # disconnect, so just fire that Deferred now.\n            self._disconnectedDeferred.callback(None)\n            self.deferred.errback(reason)\n", "n_tokens": 863, "byte_len": 4013, "file_sha1": "2991810fc16f90551764988c6f9f345c92ace4c6", "start_line": 140, "end_line": 240}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/tls.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/tls.py", "rel_path": "scrapy/core/downloader/tls.py", "module": "scrapy.core.downloader.tls", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_identityVerifyingInfoCallback", "ScrapyClientTLSOptions", "protocol", "bool", "connection", "https", "elif", "sslverify", "key", "info", "debug", "get", "none", "code", "handshak", "start", "hostname", "ascii", "verifying", "metho", "method", "ignoring", "internet", "certificate", "error", "sslv23", "typing", "class", "verbose", "logging", "bytes", "issuer", "warnings", "bugs", "cipher", "certificates", "twisted", "exceptions", "super", "same", "self", "creator", "scrapy", "client", "name", "string", "defaul", "ciphers", "caught"], "ast_kind": "class_or_type", "text": "import logging\nfrom typing import Any\n\nfrom OpenSSL import SSL\nfrom service_identity.exceptions import CertificateError\nfrom twisted.internet._sslverify import (\n    ClientTLSOptions,\n    VerificationError,\n    verifyHostname,\n)\nfrom twisted.internet.ssl import AcceptableCiphers\n\nfrom scrapy.utils.ssl import get_temp_key_info, x509name_to_string\n\nlogger = logging.getLogger(__name__)\n\n\nMETHOD_TLS = \"TLS\"\nMETHOD_TLSv10 = \"TLSv1.0\"\nMETHOD_TLSv11 = \"TLSv1.1\"\nMETHOD_TLSv12 = \"TLSv1.2\"\n\n\nopenssl_methods: dict[str, int] = {\n    METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n    METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only\n}\n\n\nclass ScrapyClientTLSOptions(ClientTLSOptions):\n    \"\"\"\n    SSL Client connection creator ignoring certificate verification errors\n    (for genuinely invalid certificates or bugs in verification code).\n\n    Same as Twisted's private _sslverify.ClientTLSOptions,\n    except that VerificationError, CertificateError and ValueError\n    exceptions are caught, so that the connection is not closed, only\n    logging warnings. Also, HTTPS connection parameters logging is added.\n    \"\"\"\n\n    def __init__(self, hostname: str, ctx: SSL.Context, verbose_logging: bool = False):\n        super().__init__(hostname, ctx)\n        self.verbose_logging: bool = verbose_logging\n\n    def _identityVerifyingInfoCallback(\n        self, connection: SSL.Connection, where: int, ret: Any\n    ) -> None:\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            connection.set_tlsext_host_name(self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                logger.debug(\n                    \"SSL connection to %s using protocol %s, cipher %s\",\n                    self._hostnameASCII,\n                    connection.get_protocol_version_name(),\n                    connection.get_cipher_name(),\n                )\n                server_cert = connection.get_peer_certificate()\n                if server_cert:\n                    logger.debug(\n                        'SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                        x509name_to_string(server_cert.get_issuer()),\n                        x509name_to_string(server_cert.get_subject()),\n                    )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug(\"SSL temp key: %s\", key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"%s\"; %s',\n                    self._hostnameASCII,\n                    e,\n                )\n\n            except ValueError as e:\n                logger.warning(\n                    \"Ignoring error while verifying certificate \"\n                    'from host \"%s\" (exception: %r)',\n                    self._hostnameASCII,\n                    e,\n                )\n\n\nDEFAULT_CIPHERS: AcceptableCiphers = AcceptableCiphers.fromOpenSSLCipherString(\n    \"DEFAULT\"\n)\n", "n_tokens": 674, "byte_len": 3247, "file_sha1": "2aa2fe19293f314de56ee66b6860ad9c872e5a26", "start_line": 1, "end_line": 92}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/contextfactory.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/contextfactory.py", "rel_path": "scrapy/core/downloader/contextfactory.py", "module": "scrapy.core.downloader.contextfactory", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_settings", "from_crawler", "_from_settings", "getCertificateOptions", "getContext", "creatorForNetloc", "ScrapyClientContextFactory", "BrowserLikeContextFactory", "method", "protocol", "bool", "agent", "future", "python", "declarations", "connection", "browser", "like", "https", "acceptable", "ciphers", "deprecated", "removed", "typ", "checking", "port", "interface", "sslverify", "downloade", "load_context_factory_from_settings", "AcceptableProtocolsContextFactory", "does", "your", "context", "factory", "doing", "protocols", "upgrade", "seldom", "clien", "settings", "legac", "serve", "creator", "for", "them", "quoting", "tls", "none"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any\n\nfrom OpenSSL import SSL\nfrom twisted.internet._sslverify import _setAcceptableProtocols\nfrom twisted.internet.ssl import (\n    AcceptableCiphers,\n    CertificateOptions,\n    optionsForClientTLS,\n    platformTrust,\n)\nfrom twisted.web.client import BrowserLikePolicyForHTTPS\nfrom twisted.web.iweb import IPolicyForHTTPS\nfrom zope.interface.declarations import implementer\nfrom zope.interface.verify import verifyObject\n\nfrom scrapy.core.downloader.tls import (\n    DEFAULT_CIPHERS,\n    ScrapyClientTLSOptions,\n    openssl_methods,\n)\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.deprecate import method_is_overridden\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    from twisted.internet._sslverify import ClientTLSOptions\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\n@implementer(IPolicyForHTTPS)\nclass ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n    \"\"\"\n    Non-peer-certificate verifying HTTPS context factory\n\n    Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)\n    which allows TLS protocol negotiation\n\n    'A TLS/SSL connection established with [this method] may\n     understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n    \"\"\"\n\n    def __init__(\n        self,\n        method: int = SSL.SSLv23_METHOD,\n        tls_verbose_logging: bool = False,\n        tls_ciphers: str | None = None,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n        self._ssl_method: int = method\n        self.tls_verbose_logging: bool = tls_verbose_logging\n        self.tls_ciphers: AcceptableCiphers\n        if tls_ciphers:\n            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n        else:\n            self.tls_ciphers = DEFAULT_CIPHERS\n        if method_is_overridden(type(self), ScrapyClientContextFactory, \"getContext\"):\n            warnings.warn(\n                \"Overriding ScrapyClientContextFactory.getContext() is deprecated and that method\"\n                \" will be removed in a future Scrapy version. Override creatorForNetloc() instead.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n\n    @classmethod\n    def from_settings(\n        cls,\n        settings: BaseSettings,\n        method: int = SSL.SSLv23_METHOD,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Self:\n        warnings.warn(\n            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return cls._from_settings(settings, method, *args, **kwargs)\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        method: int = SSL.SSLv23_METHOD,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Self:\n        return cls._from_settings(crawler.settings, method, *args, **kwargs)\n\n    @classmethod\n    def _from_settings(\n        cls,\n        settings: BaseSettings,\n        method: int = SSL.SSLv23_METHOD,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Self:\n        tls_verbose_logging: bool = settings.getbool(\n            \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n        )\n        tls_ciphers: str | None = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n        return cls(  # type: ignore[misc]\n            method=method,\n            tls_verbose_logging=tls_verbose_logging,\n            tls_ciphers=tls_ciphers,\n            *args,\n            **kwargs,\n        )\n\n    def getCertificateOptions(self) -> CertificateOptions:\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n        return CertificateOptions(\n            verify=False,\n            method=self._ssl_method,\n            fixBrokenPeers=True,\n            acceptableCiphers=self.tls_ciphers,\n        )\n\n    # kept for old-style HTTP/1.0 downloader context twisted calls,\n    # e.g. connectSSL()\n    def getContext(self, hostname: Any = None, port: Any = None) -> SSL.Context:\n        ctx: SSL.Context = self.getCertificateOptions().getContext()\n        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n        return ctx\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        return ScrapyClientTLSOptions(\n            hostname.decode(\"ascii\"),\n            self.getContext(),\n            verbose_logging=self.tls_verbose_logging,\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass BrowserLikeContextFactory(ScrapyClientContextFactory):\n    \"\"\"\n    Twisted-recommended context factory for web clients.\n\n    Quoting the documentation of the :class:`~twisted.web.client.Agent` class:\n\n        The default is to use a\n        :class:`~twisted.web.client.BrowserLikePolicyForHTTPS`, so unless you\n        have special requirements you can leave this as-is.\n\n    :meth:`creatorForNetloc` is the same as\n    :class:`~twisted.web.client.BrowserLikePolicyForHTTPS` except this context\n    factory allows setting the TLS/SSL method to use.\n\n    The default OpenSSL method is ``TLS_METHOD`` (also called\n    ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n    \"\"\"\n", "n_tokens": 1207, "byte_len": 5373, "file_sha1": "906ab09fe4ca395a5c41b20a378e1ed6d3073b5d", "start_line": 1, "end_line": 162}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/contextfactory.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/contextfactory.py", "rel_path": "scrapy/core/downloader/contextfactory.py", "module": "scrapy.core.downloader.contextfactory", "ext": "py", "chunk_number": 2, "symbols": ["creatorForNetloc", "__init__", "load_context_factory_from_settings", "AcceptableProtocolsContextFactory", "method", "does", "your", "context", "factory", "doing", "port", "https", "acceptable", "protocols", "upgrade", "seldom", "settings", "creator", "for", "them", "ascii", "shipped", "rejected", "type", "since", "default", "downloade", "clientcontextfactory", "argument", "sslv23", "from_settings", "from_crawler", "_from_settings", "getCertificateOptions", "getContext", "ScrapyClientContextFactory", "BrowserLikeContextFactory", "protocol", "bool", "agent", "future", "python", "declarations", "connection", "browser", "like", "ciphers", "deprecated", "removed", "typ"], "ast_kind": "class_or_type", "text": "    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        # trustRoot set to platformTrust() will use the platform's root CAs.\n        #\n        # This means that a website like https://www.cacert.org will be rejected\n        # by default, since CAcert.org CA certificate is seldom shipped.\n        return optionsForClientTLS(\n            hostname=hostname.decode(\"ascii\"),\n            trustRoot=platformTrust(),\n            extraCertificateOptions={\"method\": self._ssl_method},\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass AcceptableProtocolsContextFactory:\n    \"\"\"Context factory to used to override the acceptable protocols\n    to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN\n    negotiation.\n    \"\"\"\n\n    def __init__(self, context_factory: Any, acceptable_protocols: list[bytes]):\n        verifyObject(IPolicyForHTTPS, context_factory)\n        self._wrapped_context_factory: Any = context_factory\n        self._acceptable_protocols: list[bytes] = acceptable_protocols\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        options: ClientTLSOptions = self._wrapped_context_factory.creatorForNetloc(\n            hostname, port\n        )\n        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n        return options\n\n\ndef load_context_factory_from_settings(\n    settings: BaseSettings, crawler: Crawler\n) -> IPolicyForHTTPS:\n    ssl_method = openssl_methods[settings.get(\"DOWNLOADER_CLIENT_TLS_METHOD\")]\n    context_factory_cls = load_object(settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"])\n    # try method-aware context factory\n    try:\n        context_factory = build_from_crawler(\n            context_factory_cls,\n            crawler,\n            method=ssl_method,\n        )\n    except TypeError:\n        # use context factory defaults\n        context_factory = build_from_crawler(\n            context_factory_cls,\n            crawler,\n        )\n        msg = (\n            f\"{settings['DOWNLOADER_CLIENTCONTEXTFACTORY']} does not accept \"\n            \"a `method` argument (type OpenSSL.SSL method, e.g. \"\n            \"OpenSSL.SSL.SSLv23_METHOD) and/or a `tls_verbose_logging` \"\n            \"argument and/or a `tls_ciphers` argument. Please, upgrade your \"\n            \"context factory class to handle them or ignore them.\"\n        )\n        warnings.warn(msg)\n\n    return context_factory\n", "n_tokens": 517, "byte_len": 2393, "file_sha1": "906ab09fe4ca395a5c41b20a378e1ed6d3073b5d", "start_line": 163, "end_line": 223}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/middleware.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/middleware.py", "rel_path": "scrapy/core/downloader/middleware.py", "module": "scrapy.core.downloader.middleware", "ext": "py", "chunk_number": 1, "symbols": ["_get_mwlist_from_settings", "_add_middleware", "download", "process_request", "process_response", "process_exception", "DownloaderMiddlewareManager", "method", "qualname", "append", "middleware", "future", "spider", "deprecated", "warn", "need", "invalid", "output", "typ", "checking", "passed", "settings", "isinstance", "methods", "requiring", "none", "docs", "argument", "required", "type", "http", "either", "callable", "response", "manager", "deprecate", "getwithbase", "process", "exception", "func", "internet", "typing", "return", "annotations", "class", "downloader", "get", "mwlist", "ignore", "set"], "ast_kind": "class_or_type", "text": "\"\"\"\nDownloader Middleware manager\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning, _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import _defer_sleep, deferred_from_coro\nfrom scrapy.utils.deprecate import argument_is_required\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Generator\n\n    from scrapy import Spider\n    from scrapy.settings import BaseSettings\n\n\nclass DownloaderMiddlewareManager(MiddlewareManager):\n    component_name = \"downloader middleware\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> list[Any]:\n        return build_component_list(settings.getwithbase(\"DOWNLOADER_MIDDLEWARES\"))\n\n    def _add_middleware(self, mw: Any) -> None:\n        if hasattr(mw, \"process_request\"):\n            self.methods[\"process_request\"].append(mw.process_request)\n            self._check_mw_method_spider_arg(mw.process_request)\n        if hasattr(mw, \"process_response\"):\n            self.methods[\"process_response\"].appendleft(mw.process_response)\n            self._check_mw_method_spider_arg(mw.process_response)\n        if hasattr(mw, \"process_exception\"):\n            self.methods[\"process_exception\"].appendleft(mw.process_exception)\n            self._check_mw_method_spider_arg(mw.process_exception)\n\n    @inlineCallbacks\n    def download(\n        self,\n        download_func: Callable[[Request], Deferred[Response]],\n        request: Request,\n        spider: Spider | None = None,\n    ) -> Generator[Deferred[Any], Any, Response | Request]:\n        if argument_is_required(download_func, \"spider\"):\n            warnings.warn(\n                \"The spider argument of download_func is deprecated\"\n                \" and will not be passed in future Scrapy versions.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            need_spider_arg = True\n        else:\n            need_spider_arg = False\n\n        @inlineCallbacks\n        def process_request(\n            request: Request,\n        ) -> Generator[Deferred[Any], Any, Response | Request]:\n            for method in self.methods[\"process_request\"]:\n                method = cast(\"Callable\", method)\n                if method in self._mw_methods_requiring_spider:\n                    response = yield deferred_from_coro(\n                        method(request=request, spider=self._spider)\n                    )\n                else:\n                    response = yield deferred_from_coro(method(request=request))\n                if response is not None and not isinstance(\n                    response, (Response, Request)\n                ):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            if need_spider_arg:\n                return (yield download_func(request, self._spider))  # type: ignore[call-arg]\n            return (yield download_func(request))\n\n        @inlineCallbacks\n        def process_response(\n            response: Response | Request,\n        ) -> Generator[Deferred[Any], Any, Response | Request]:\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            if isinstance(response, Request):\n                return response\n\n            for method in self.methods[\"process_response\"]:\n                method = cast(\"Callable\", method)\n                if method in self._mw_methods_requiring_spider:\n                    response = yield deferred_from_coro(\n                        method(request=request, response=response, spider=self._spider)\n                    )\n                else:\n                    response = yield deferred_from_coro(\n                        method(request=request, response=response)\n                    )\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response\n\n        @inlineCallbacks\n        def process_exception(\n            exception: Exception,\n        ) -> Generator[Deferred[Any], Any, Response | Request]:\n            for method in self.methods[\"process_exception\"]:\n                method = cast(\"Callable\", method)\n                if method in self._mw_methods_requiring_spider:\n                    response = yield deferred_from_coro(\n                        method(\n                            request=request, exception=exception, spider=self._spider\n                        )\n                    )\n                else:\n                    response = yield deferred_from_coro(\n                        method(request=request, exception=exception)\n                    )\n                if response is not None and not isinstance(\n                    response, (Response, Request)\n                ):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            raise exception\n\n        if spider:\n            self._warn_spider_arg(\"download\")\n            self._set_compat_spider(spider)\n        try:\n            result: Response | Request = yield process_request(request)\n        except Exception as ex:\n            yield _defer_sleep()\n            # either returns a request or response (which we pass to process_response())\n            # or reraises the exception\n            result = yield process_exception(ex)\n        return (yield process_response(result))\n", "n_tokens": 1134, "byte_len": 6309, "file_sha1": "5ca52603fedc5521c6fcd8e859c3f67fedff9dae", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/__init__.py", "rel_path": "scrapy/core/downloader/handlers/__init__.py", "module": "scrapy.core.downloader.handlers.__init__", "ext": "py", "chunk_number": 1, "symbols": ["download_request", "__init__", "_get_handler", "_load_handler", "_close", "DownloadHandlerProtocol", "DownloadHandlers", "bool", "unsupported", "spider", "load", "warn", "future", "typ", "checking", "urlparse", "cached", "lazy", "failed", "acceptable", "settings", "instancing", "items", "notconfigured", "handler", "downloa", "handlers", "none", "dhcls", "type", "misc", "close", "http", "exc", "info", "callable", "response", "values", "not", "supported", "getwithbase", "internet", "engine", "stopped", "typing", "return", "annotations", "class", "downloadhandler", "configured"], "ast_kind": "class_or_type", "text": "\"\"\"Download handlers for different schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Protocol, cast\n\nfrom twisted.internet import defer\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import NotConfigured, NotSupported\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Generator\n\n    from twisted.internet.defer import Deferred\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadHandlerProtocol(Protocol):\n    def download_request(\n        self, request: Request, spider: Spider\n    ) -> Deferred[Response]: ...\n\n\nclass DownloadHandlers:\n    def __init__(self, crawler: Crawler):\n        self._crawler: Crawler = crawler\n        # stores acceptable schemes on instancing\n        self._schemes: dict[str, str | Callable[..., Any]] = {}\n        # stores instanced handlers for schemes\n        self._handlers: dict[str, DownloadHandlerProtocol] = {}\n        # remembers failed handlers\n        self._notconfigured: dict[str, str] = {}\n        handlers: dict[str, str | Callable[..., Any]] = without_none_values(\n            cast(\n                \"dict[str, str | Callable[..., Any]]\",\n                crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n            )\n        )\n        for scheme, clspath in handlers.items():\n            self._schemes[scheme] = clspath\n            self._load_handler(scheme, skip_lazy=True)\n\n        crawler.signals.connect(self._close, signals.engine_stopped)\n\n    def _get_handler(self, scheme: str) -> DownloadHandlerProtocol | None:\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = \"no handler available for that scheme\"\n            return None\n\n        return self._load_handler(scheme)\n\n    def _load_handler(\n        self, scheme: str, skip_lazy: bool = False\n    ) -> DownloadHandlerProtocol | None:\n        path = self._schemes[scheme]\n        try:\n            dhcls: type[DownloadHandlerProtocol] = load_object(path)\n            if skip_lazy and getattr(dhcls, \"lazy\", True):\n                return None\n            dh = build_from_crawler(\n                dhcls,\n                self._crawler,\n            )\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error(\n                'Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                {\"clspath\": path, \"scheme\": scheme},\n                exc_info=True,\n                extra={\"crawler\": self._crawler},\n            )\n            self._notconfigured[scheme] = str(ex)\n            return None\n        self._handlers[scheme] = dh\n        return dh\n\n    @_warn_spider_arg\n    def download_request(\n        self, request: Request, spider: Spider | None = None\n    ) -> Deferred[Response]:\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(\n                f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\"\n            )\n        assert self._crawler.spider\n        return handler.download_request(request, self._crawler.spider)\n\n    @defer.inlineCallbacks\n    def _close(self, *_a: Any, **_kw: Any) -> Generator[Deferred[Any], Any, None]:\n        for dh in self._handlers.values():\n            if hasattr(dh, \"close\"):\n                yield dh.close()\n", "n_tokens": 838, "byte_len": 3950, "file_sha1": "fb5c93c03c1fc15bfe85f1e5dae2a6e8aea61349", "start_line": 1, "end_line": 115}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http2.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http2.py", "rel_path": "scrapy/core/downloader/handlers/http2.py", "module": "scrapy.core.downloader.handlers.http2", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "download_request", "close", "_get_agent", "_cb_latency", "_cb_timeout", "H2DownloadHandler", "ScrapyH2Agent", "encoding", "contextfactory", "method", "python", "spider", "get", "agent", "spiders", "future", "typ", "checking", "https", "delayed", "call", "timeout", "urlparse", "cached", "tunneling", "settings", "connections", "than", "scrapy", "proxy", "uri", "none", "ascii", "error", "seconds", "latency", "parse", "took", "http", "bytes", "bind", "address", "response", "internet", "typing", "extensions", "add", "both"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom time import time\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urldefrag\n\nfrom twisted.internet.error import TimeoutError as TxTimeoutError\nfrom twisted.web.client import URI\n\nfrom scrapy.core.downloader.contextfactory import load_context_factory_from_settings\nfrom scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    from twisted.internet.base import DelayedCall\n    from twisted.internet.defer import Deferred\n    from twisted.web.iweb import IPolicyForHTTPS\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Request, Response\n    from scrapy.settings import Settings\n    from scrapy.spiders import Spider\n\n\nclass H2DownloadHandler:\n    def __init__(self, settings: Settings, crawler: Crawler):\n        self._crawler = crawler\n\n        from twisted.internet import reactor\n\n        self._pool = H2ConnectionPool(reactor, settings)\n        self._context_factory = load_context_factory_from_settings(settings, crawler)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        agent = ScrapyH2Agent(\n            context_factory=self._context_factory,\n            pool=self._pool,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request, spider)\n\n    def close(self) -> None:\n        self._pool.close_connections()\n\n\nclass ScrapyH2Agent:\n    _Agent = H2Agent\n    _ProxyAgent = ScrapyProxyH2Agent\n\n    def __init__(\n        self,\n        context_factory: IPolicyForHTTPS,\n        pool: H2ConnectionPool,\n        connect_timeout: int = 10,\n        bind_address: bytes | None = None,\n        crawler: Crawler | None = None,\n    ) -> None:\n        self._context_factory = context_factory\n        self._connect_timeout = connect_timeout\n        self._bind_address = bind_address\n        self._pool = pool\n        self._crawler = crawler\n\n    def _get_agent(self, request: Request, timeout: float | None) -> H2Agent:\n        from twisted.internet import reactor\n\n        bind_address = request.meta.get(\"bindaddress\") or self._bind_address\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            if urlparse_cached(request).scheme == \"https\":\n                # ToDo\n                raise NotImplementedError(\n                    \"Tunneling via CONNECT method using HTTP/2.0 is not yet supported\"\n                )\n            return self._ProxyAgent(\n                reactor=reactor,\n                context_factory=self._context_factory,\n                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding=\"ascii\")),\n                connect_timeout=timeout,\n                bind_address=bind_address,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            context_factory=self._context_factory,\n            connect_timeout=timeout,\n            bind_address=bind_address,\n            pool=self._pool,\n        )\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        timeout = request.meta.get(\"download_timeout\") or self._connect_timeout\n        agent = self._get_agent(request, timeout)\n\n        start_time = time()\n        d = agent.request(request, spider)\n        d.addCallback(self._cb_latency, request, start_time)\n\n        timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, timeout, timeout_cl)\n        return d\n\n    @staticmethod\n    def _cb_latency(\n        response: Response, request: Request, start_time: float\n    ) -> Response:\n        request.meta[\"download_latency\"] = time() - start_time\n        return response\n\n    @staticmethod\n    def _cb_timeout(\n        response: Response, request: Request, timeout: float, timeout_cl: DelayedCall\n    ) -> Response:\n        if timeout_cl.active():\n            timeout_cl.cancel()\n            return response\n\n        url = urldefrag(request.url)[0]\n        raise TxTimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n", "n_tokens": 920, "byte_len": 4369, "file_sha1": "e71d944a28280f6f2f91834dd58f117f00e25d19", "start_line": 1, "end_line": 131}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py", "rel_path": "scrapy/core/downloader/handlers/http11.py", "module": "scrapy.core.downloader.handlers.http11", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "download_request", "close", "cancel_delayed_call", "_ResultT", "HTTP11DownloadHandler", "TunnelError", "TunnelingTCP4ClientEndpoint", "failure", "contextfactory", "disconnect", "timeout", "protocol", "issued", "bool", "ticket", "tunnel", "agent", "python", "connection", "after", "spider", "response", "answer", "doesn", "https", "interfaces", "future", "typ", "requestTunnel", "processProxyResponse", "connectFailed", "connect", "tunnel_request_data", "_getEndpoint", "_requestWithEndpoint", "request", "_get_agent", "_cb_timeout", "_cb_latency", "_headers_from_twisted_response", "_cb_bodyready", "_cancel", "_cb_bodydone", "startProducing", "pauseProducing", "stopProducing", "_finish_response", "connectionMade"], "ast_kind": "class_or_type", "text": "\"\"\"Download handlers for http and https schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport ipaddress\nimport logging\nimport re\nfrom contextlib import suppress\nfrom io import BytesIO\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, TypedDict, TypeVar, cast\nfrom urllib.parse import urldefrag, urlparse\n\nfrom twisted.internet import ssl\nfrom twisted.internet.defer import CancelledError, Deferred, succeed\nfrom twisted.internet.endpoints import TCP4ClientEndpoint\nfrom twisted.internet.error import TimeoutError as TxTimeoutError\nfrom twisted.internet.protocol import Factory, Protocol, connectionDone\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import (\n    URI,\n    Agent,\n    HTTPConnectionPool,\n    ResponseDone,\n    ResponseFailed,\n)\nfrom twisted.web.client import Response as TxResponse\nfrom twisted.web.http import PotentialDataLoss, _DataLoss\nfrom twisted.web.http_headers import Headers as TxHeaders\nfrom twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS, IResponse\nfrom zope.interface import implementer\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader.contextfactory import load_context_factory_from_settings\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes, to_unicode\nfrom scrapy.utils.url import add_http_if_no_scheme\n\nif TYPE_CHECKING:\n    from twisted.internet.base import ReactorBase\n    from twisted.internet.interfaces import IConsumer\n\n    # typing.NotRequired and typing.Self require Python 3.11\n    from typing_extensions import NotRequired, Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass _ResultT(TypedDict):\n    txresponse: TxResponse\n    body: bytes\n    flags: list[str] | None\n    certificate: ssl.Certificate | None\n    ip_address: ipaddress.IPv4Address | ipaddress.IPv6Address | None\n    failure: NotRequired[Failure | None]\n\n\nclass HTTP11DownloadHandler:\n    lazy = False\n\n    def __init__(self, settings: BaseSettings, crawler: Crawler):\n        self._crawler = crawler\n\n        from twisted.internet import reactor\n\n        self._pool: HTTPConnectionPool = HTTPConnectionPool(reactor, persistent=True)\n        self._pool.maxPersistentPerHost = settings.getint(\n            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n        )\n        self._pool._factory.noisy = False\n\n        self._contextFactory: IPolicyForHTTPS = load_context_factory_from_settings(\n            settings, crawler\n        )\n        self._default_maxsize: int = settings.getint(\"DOWNLOAD_MAXSIZE\")\n        self._default_warnsize: int = settings.getint(\"DOWNLOAD_WARNSIZE\")\n        self._fail_on_dataloss: bool = settings.getbool(\"DOWNLOAD_FAIL_ON_DATALOSS\")\n        self._disconnect_timeout: int = 1\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(\n            contextFactory=self._contextFactory,\n            pool=self._pool,\n            maxsize=getattr(spider, \"download_maxsize\", self._default_maxsize),\n            warnsize=getattr(spider, \"download_warnsize\", self._default_warnsize),\n            fail_on_dataloss=self._fail_on_dataloss,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request)\n\n    def close(self) -> Deferred[None]:\n        from twisted.internet import reactor\n\n        d: Deferred[None] = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result: _T) -> _T:\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d\n\n\nclass TunnelError(Exception):\n    \"\"\"An HTTP CONNECT tunnel could not be established by the proxy.\"\"\"\n\n\nclass TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n    \"\"\"An endpoint that tunnels through proxies to allow HTTPS downloads. To\n    accomplish that, this endpoint sends an HTTP CONNECT to the proxy.\n    The HTTP CONNECT is always sent when using this endpoint, I think this could\n    be improved as the CONNECT will be redundant if the connection associated\n    with this endpoint comes from the pool and a CONNECT has already been issued\n    for it.\n    \"\"\"\n\n    _truncatedLength = 1000\n    _responseAnswer = (\n        r\"HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,\" + str(_truncatedLength) + r\"})\"\n    )\n    _responseMatcher = re.compile(_responseAnswer.encode())\n", "n_tokens": 1141, "byte_len": 5248, "file_sha1": "d7f2b4cf2695612b4b5d0c14925731dcfb757b90", "start_line": 1, "end_line": 146}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py", "rel_path": "scrapy/core/downloader/handlers/http11.py", "module": "scrapy.core.downloader.handlers.http11", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "requestTunnel", "processProxyResponse", "connectFailed", "connect", "tunnel_request_data", "TunnelingAgent", "encoding", "failure", "method", "protocol", "those", "tunnel", "agent", "subclass", "after", "buffer", "data", "received", "host", "value", "https", "strange", "port", "behave", "make", "proxy", "bytearray", "unicode", "reactor", "from_crawler", "download_request", "close", "cancel_delayed_call", "_getEndpoint", "_requestWithEndpoint", "request", "_get_agent", "_cb_timeout", "_cb_latency", "_headers_from_twisted_response", "_cb_bodyready", "_cancel", "_cb_bodydone", "startProducing", "pauseProducing", "stopProducing", "_finish_response", "connectionMade", "dataReceived"], "ast_kind": "class_or_type", "text": "    def __init__(\n        self,\n        reactor: ReactorBase,\n        host: str,\n        port: int,\n        proxyConf: tuple[str, int, bytes | None],\n        contextFactory: IPolicyForHTTPS,\n        timeout: float = 30,\n        bindAddress: tuple[str, int] | None = None,\n    ):\n        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n        super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n        self._tunnelReadyDeferred: Deferred[Protocol] = Deferred()\n        self._tunneledHost: str = host\n        self._tunneledPort: int = port\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n        self._connectBuffer: bytearray = bytearray()\n\n    def requestTunnel(self, protocol: Protocol) -> Protocol:\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        assert protocol.transport\n        tunnelReq = tunnel_request_data(\n            self._tunneledHost, self._tunneledPort, self._proxyAuthHeader\n        )\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse  # type: ignore[method-assign]\n        self._protocol = protocol\n        return protocol\n\n    def processProxyResponse(self, data: bytes) -> None:\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        assert self._protocol.transport\n        self._connectBuffer += data\n        # make sure that enough (all) bytes are consumed\n        # and that we've got all HTTP headers (ending with a blank line)\n        # from the proxy so that we don't send those bytes to the TLS layer\n        #\n        # see https://github.com/scrapy/scrapy/issues/2491\n        if b\"\\r\\n\\r\\n\" not in self._connectBuffer:\n            return\n        self._protocol.dataReceived = self._protocolDataReceived  # type: ignore[method-assign]\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n        if respm and int(respm.group(\"status\")) == 200:\n            # set proper Server Name Indication extension\n            sslOptions = self._contextFactory.creatorForNetloc(  # type: ignore[call-arg,misc]\n                self._tunneledHost, self._tunneledPort\n            )\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            extra: Any\n            if respm:\n                extra = {\n                    \"status\": int(respm.group(\"status\")),\n                    \"reason\": respm.group(\"reason\").strip(),\n                }\n            else:\n                extra = data[: self._truncatedLength]\n            self._tunnelReadyDeferred.errback(\n                TunnelError(\n                    \"Could not open CONNECT tunnel with proxy \"\n                    f\"{self._host}:{self._port} [{extra!r}]\"\n                )\n            )\n\n    def connectFailed(self, reason: Failure) -> None:\n        \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n        self._tunnelReadyDeferred.errback(reason)\n\n    def connect(self, protocolFactory: Factory) -> Deferred[Protocol]:\n        self._protocolFactory = protocolFactory\n        connectDeferred = super().connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred\n\n\ndef tunnel_request_data(\n    host: str, port: int, proxy_auth_header: bytes | None = None\n) -> bytes:\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_unicode as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding=\"ascii\") + b\":\" + to_bytes(str(port))\n    tunnel_req = b\"CONNECT \" + host_value + b\" HTTP/1.1\\r\\n\"\n    tunnel_req += b\"Host: \" + host_value + b\"\\r\\n\"\n    if proxy_auth_header:\n        tunnel_req += b\"Proxy-Authorization: \" + proxy_auth_header + b\"\\r\\n\"\n    tunnel_req += b\"\\r\\n\"\n    return tunnel_req\n\n\nclass TunnelingAgent(Agent):\n    \"\"\"An agent that uses a L{TunnelingTCP4ClientEndpoint} to make HTTPS\n    downloads. It may look strange that we have chosen to subclass Agent and not\n    ProxyAgent but consider that after the tunnel is opened the proxy is\n    transparent to the client; thus the agent should behave like there is no\n    proxy involved.\n    \"\"\"\n", "n_tokens": 1153, "byte_len": 4864, "file_sha1": "d7f2b4cf2695612b4b5d0c14925731dcfb757b90", "start_line": 147, "end_line": 258}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py", "rel_path": "scrapy/core/downloader/handlers/http11.py", "module": "scrapy.core.downloader.handlers.http11", "ext": "py", "chunk_number": 3, "symbols": ["__init__", "_getEndpoint", "_requestWithEndpoint", "request", "_get_agent", "ScrapyProxyAgent", "ScrapyAgent", "encoding", "method", "bool", "body", "producer", "connections", "agent", "connect", "timeout", "connection", "get", "maxsize", "bind", "address", "tunneling", "port", "https", "client", "endpoint", "urlparse", "cached", "proxy", "auth", "from_crawler", "download_request", "close", "cancel_delayed_call", "requestTunnel", "processProxyResponse", "connectFailed", "tunnel_request_data", "_cb_timeout", "_cb_latency", "_headers_from_twisted_response", "_cb_bodyready", "_cancel", "_cb_bodydone", "startProducing", "pauseProducing", "stopProducing", "_finish_response", "connectionMade", "dataReceived"], "ast_kind": "class_or_type", "text": "    def __init__(\n        self,\n        *,\n        reactor: ReactorBase,\n        proxyConf: tuple[str, int, bytes | None],\n        contextFactory: IPolicyForHTTPS,\n        connectTimeout: float | None = None,\n        bindAddress: bytes | None = None,\n        pool: HTTPConnectionPool | None = None,\n    ):\n        super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n        self._proxyConf: tuple[str, int, bytes | None] = proxyConf\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n\n    def _getEndpoint(self, uri: URI) -> TunnelingTCP4ClientEndpoint:\n        return TunnelingTCP4ClientEndpoint(\n            reactor=self._reactor,\n            host=uri.host,\n            port=uri.port,\n            proxyConf=self._proxyConf,\n            contextFactory=self._contextFactory,\n            timeout=self._endpointFactory._connectTimeout,\n            bindAddress=self._endpointFactory._bindAddress,\n        )\n\n    def _requestWithEndpoint(\n        self,\n        key: Any,\n        endpoint: TCP4ClientEndpoint,\n        method: bytes,\n        parsedURI: URI,\n        headers: TxHeaders | None,\n        bodyProducer: IBodyProducer | None,\n        requestPath: bytes,\n    ) -> Deferred[IResponse]:\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key += self._proxyConf\n        return super()._requestWithEndpoint(\n            key=key,\n            endpoint=endpoint,\n            method=method,\n            parsedURI=parsedURI,\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=requestPath,\n        )\n\n\nclass ScrapyProxyAgent(Agent):\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        proxyURI: bytes,\n        connectTimeout: float | None = None,\n        bindAddress: bytes | None = None,\n        pool: HTTPConnectionPool | None = None,\n    ):\n        super().__init__(\n            reactor=reactor,\n            connectTimeout=connectTimeout,\n            bindAddress=bindAddress,\n            pool=pool,\n        )\n        self._proxyURI: URI = URI.fromBytes(proxyURI)\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: TxHeaders | None = None,\n        bodyProducer: IBodyProducer | None = None,\n    ) -> Deferred[IResponse]:\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        return self._requestWithEndpoint(\n            key=(b\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n            endpoint=self._getEndpoint(self._proxyURI),\n            method=method,\n            parsedURI=URI.fromBytes(uri),\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=uri,\n        )\n\n\nclass ScrapyAgent:\n    _Agent = Agent\n    _ProxyAgent = ScrapyProxyAgent\n    _TunnelingAgent = TunnelingAgent\n\n    def __init__(\n        self,\n        *,\n        contextFactory: IPolicyForHTTPS,\n        connectTimeout: float = 10,\n        bindAddress: bytes | None = None,\n        pool: HTTPConnectionPool | None = None,\n        maxsize: int = 0,\n        warnsize: int = 0,\n        fail_on_dataloss: bool = True,\n        crawler: Crawler,\n    ):\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n        self._connectTimeout: float = connectTimeout\n        self._bindAddress: bytes | None = bindAddress\n        self._pool: HTTPConnectionPool | None = pool\n        self._maxsize: int = maxsize\n        self._warnsize: int = warnsize\n        self._fail_on_dataloss: bool = fail_on_dataloss\n        self._txresponse: TxResponse | None = None\n        self._crawler: Crawler = crawler\n\n    def _get_agent(self, request: Request, timeout: float) -> Agent:\n        from twisted.internet import reactor\n\n        bindaddress = request.meta.get(\"bindaddress\") or self._bindAddress\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            proxy = add_http_if_no_scheme(proxy)\n            proxy_parsed = urlparse(proxy)\n            proxy_host = proxy_parsed.hostname\n            proxy_port = proxy_parsed.port\n            if not proxy_port:\n                proxy_port = 443 if proxy_parsed.scheme == \"https\" else 80\n            if urlparse_cached(request).scheme == \"https\":\n                assert proxy_host is not None\n                proxyAuth = request.headers.get(b\"Proxy-Authorization\", None)\n                proxyConf = (proxy_host, proxy_port, proxyAuth)\n                return self._TunnelingAgent(\n                    reactor=reactor,\n                    proxyConf=proxyConf,\n                    contextFactory=self._contextFactory,\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n            return self._ProxyAgent(\n                reactor=reactor,\n                proxyURI=to_bytes(proxy, encoding=\"ascii\"),\n                connectTimeout=timeout,\n                bindAddress=bindaddress,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            contextFactory=self._contextFactory,\n            connectTimeout=timeout,\n            bindAddress=bindaddress,\n            pool=self._pool,\n        )\n", "n_tokens": 1167, "byte_len": 5441, "file_sha1": "d7f2b4cf2695612b4b5d0c14925731dcfb757b90", "start_line": 259, "end_line": 415}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py", "rel_path": "scrapy/core/downloader/handlers/http11.py", "module": "scrapy.core.downloader.handlers.http11", "ext": "py", "chunk_number": 4, "symbols": ["download_request", "_cb_timeout", "_cb_latency", "_headers_from_twisted_response", "_cb_bodyready", "_cancel", "encoding", "expected", "size", "failure", "method", "warning", "msg", "immediately", "qualname", "get", "all", "signal", "connection", "latency", "cancelling", "responses", "doesn", "agent", "https", "maxsize", "tunneling", "length", "body", "producer", "__init__", "from_crawler", "close", "cancel_delayed_call", "requestTunnel", "processProxyResponse", "connectFailed", "connect", "tunnel_request_data", "_getEndpoint", "_requestWithEndpoint", "request", "_get_agent", "_cb_bodydone", "startProducing", "pauseProducing", "stopProducing", "_finish_response", "connectionMade", "dataReceived"], "ast_kind": "function_or_method", "text": "    def download_request(self, request: Request) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        timeout = request.meta.get(\"download_timeout\") or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b\"Proxy-Authorization\")\n        bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n        start_time = time()\n        d: Deferred[IResponse] = agent.request(\n            method,\n            to_bytes(url, encoding=\"ascii\"),\n            headers,\n            cast(\"IBodyProducer\", bodyproducer),\n        )\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d2: Deferred[_ResultT] = d.addCallback(self._cb_bodyready, request)\n        d3: Deferred[Response] = d2.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d3.cancel)\n        d3.addBoth(self._cb_timeout, request, url, timeout)\n        return d3\n\n    def _cb_timeout(self, result: _T, request: Request, url: str, timeout: float) -> _T:\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TxTimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n\n    def _cb_latency(self, result: _T, request: Request, start_time: float) -> _T:\n        request.meta[\"download_latency\"] = time() - start_time\n        return result\n\n    @staticmethod\n    def _headers_from_twisted_response(response: TxResponse) -> Headers:\n        headers = Headers()\n        if response.length != UNKNOWN_LENGTH:\n            headers[b\"Content-Length\"] = str(response.length).encode()\n        headers.update(response.headers.getAllRawHeaders())\n        return headers\n\n    def _cb_bodyready(\n        self, txresponse: TxResponse, request: Request\n    ) -> _ResultT | Deferred[_ResultT]:\n        headers_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.headers_received,\n            headers=self._headers_from_twisted_response(txresponse),\n            body_length=txresponse.length,\n            request=request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in headers_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\n                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n                    {\"request\": request, \"handler\": handler.__qualname__},\n                )\n                txresponse._transport.stopProducing()\n                txresponse._transport.loseConnection()\n                return {\n                    \"txresponse\": txresponse,\n                    \"body\": b\"\",\n                    \"flags\": [\"download_stopped\"],\n                    \"certificate\": None,\n                    \"ip_address\": None,\n                    \"failure\": result if result.value.fail else None,\n                }\n\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return {\n                \"txresponse\": txresponse,\n                \"body\": b\"\",\n                \"flags\": None,\n                \"certificate\": None,\n                \"ip_address\": None,\n            }\n\n        maxsize = request.meta.get(\"download_maxsize\", self._maxsize)\n        warnsize = request.meta.get(\"download_warnsize\", self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n        fail_on_dataloss = request.meta.get(\n            \"download_fail_on_dataloss\", self._fail_on_dataloss\n        )\n\n        if maxsize and expected_size > maxsize:\n            warning_msg = (\n                \"Cancelling download of %(url)s: expected response \"\n                \"size (%(size)s) larger than download max size (%(maxsize)s).\"\n            )\n            warning_args = {\n                \"url\": request.url,\n                \"size\": expected_size,\n                \"maxsize\": maxsize,\n            }\n\n            logger.warning(warning_msg, warning_args)\n\n            txresponse._transport.loseConnection()\n            raise CancelledError(warning_msg % warning_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\n                \"Expected response size (%(size)s) larger than \"\n                \"download warn size (%(warnsize)s) in request %(request)s.\",\n                {\"size\": expected_size, \"warnsize\": warnsize, \"request\": request},\n            )\n\n        def _cancel(_: Any) -> None:\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()\n\n        d: Deferred[_ResultT] = Deferred(_cancel)\n        txresponse.deliverBody(\n            _ResponseReader(\n                finished=d,\n                txresponse=txresponse,\n                request=request,\n                maxsize=maxsize,\n                warnsize=warnsize,\n                fail_on_dataloss=fail_on_dataloss,\n                crawler=self._crawler,\n            )\n        )\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d\n", "n_tokens": 1145, "byte_len": 5622, "file_sha1": "d7f2b4cf2695612b4b5d0c14925731dcfb757b90", "start_line": 416, "end_line": 557}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py", "rel_path": "scrapy/core/downloader/handlers/http11.py", "module": "scrapy.core.downloader.handlers.http11", "ext": "py", "chunk_number": 5, "symbols": ["_cb_bodydone", "__init__", "startProducing", "pauseProducing", "stopProducing", "_finish_response", "connectionMade", "dataReceived", "_RequestBodyProducer", "_ResponseReader", "failure", "protocol", "bool", "qualname", "signal", "buffered", "body", "bytes", "after", "data", "received", "reached", "warnsize", "maxsize", "more", "length", "producer", "succeed", "send", "catch", "from_crawler", "download_request", "close", "cancel_delayed_call", "requestTunnel", "processProxyResponse", "connectFailed", "connect", "tunnel_request_data", "_getEndpoint", "_requestWithEndpoint", "request", "_get_agent", "_cb_timeout", "_cb_latency", "_headers_from_twisted_response", "_cb_bodyready", "_cancel", "connectionLost", "_ResultT"], "ast_kind": "class_or_type", "text": "    def _cb_bodydone(\n        self, result: _ResultT, request: Request, url: str\n    ) -> Response | Failure:\n        headers = self._headers_from_twisted_response(result[\"txresponse\"])\n        respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n        try:\n            version = result[\"txresponse\"].version\n            protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n        except (AttributeError, TypeError, IndexError):\n            protocol = None\n        response = respcls(\n            url=url,\n            status=int(result[\"txresponse\"].code),\n            headers=headers,\n            body=result[\"body\"],\n            flags=result[\"flags\"],\n            certificate=result[\"certificate\"],\n            ip_address=result[\"ip_address\"],\n            protocol=protocol,\n        )\n        if result.get(\"failure\"):\n            assert result[\"failure\"]\n            result[\"failure\"].value.response = response\n            return result[\"failure\"]\n        return response\n\n\n@implementer(IBodyProducer)\nclass _RequestBodyProducer:\n    def __init__(self, body: bytes):\n        self.body = body\n        self.length = len(body)\n\n    def startProducing(self, consumer: IConsumer) -> Deferred[None]:\n        consumer.write(self.body)\n        return succeed(None)\n\n    def pauseProducing(self) -> None:\n        pass\n\n    def stopProducing(self) -> None:\n        pass\n\n\nclass _ResponseReader(Protocol):\n    def __init__(\n        self,\n        finished: Deferred[_ResultT],\n        txresponse: TxResponse,\n        request: Request,\n        maxsize: int,\n        warnsize: int,\n        fail_on_dataloss: bool,\n        crawler: Crawler,\n    ):\n        self._finished: Deferred[_ResultT] = finished\n        self._txresponse: TxResponse = txresponse\n        self._request: Request = request\n        self._bodybuf: BytesIO = BytesIO()\n        self._maxsize: int = maxsize\n        self._warnsize: int = warnsize\n        self._fail_on_dataloss: bool = fail_on_dataloss\n        self._fail_on_dataloss_warned: bool = False\n        self._reached_warnsize: bool = False\n        self._bytes_received: int = 0\n        self._certificate: ssl.Certificate | None = None\n        self._ip_address: ipaddress.IPv4Address | ipaddress.IPv6Address | None = None\n        self._crawler: Crawler = crawler\n\n    def _finish_response(\n        self, flags: list[str] | None = None, failure: Failure | None = None\n    ) -> None:\n        self._finished.callback(\n            {\n                \"txresponse\": self._txresponse,\n                \"body\": self._bodybuf.getvalue(),\n                \"flags\": flags,\n                \"certificate\": self._certificate,\n                \"ip_address\": self._ip_address,\n                \"failure\": failure,\n            }\n        )\n\n    def connectionMade(self) -> None:\n        assert self.transport\n        if self._certificate is None:\n            with suppress(AttributeError):\n                self._certificate = ssl.Certificate(\n                    self.transport._producer.getPeerCertificate()\n                )\n\n        if self._ip_address is None:\n            self._ip_address = ipaddress.ip_address(\n                self.transport._producer.getPeer().host\n            )\n\n    def dataReceived(self, bodyBytes: bytes) -> None:\n        # This maybe called several times after cancel was called with buffered data.\n        if self._finished.called:\n            return\n\n        assert self.transport\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        bytes_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.bytes_received,\n            data=bodyBytes,\n            request=self._request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in bytes_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\n                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n                    {\"request\": self._request, \"handler\": handler.__qualname__},\n                )\n                self.transport.stopProducing()\n                self.transport.loseConnection()\n                failure = result if result.value.fail else None\n                self._finish_response(flags=[\"download_stopped\"], failure=failure)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.warning(\n                \"Received (%(bytes)s) bytes larger than download \"\n                \"max size (%(maxsize)s) in request %(request)s.\",\n                {\n                    \"bytes\": self._bytes_received,\n                    \"maxsize\": self._maxsize,\n                    \"request\": self._request,\n                },\n            )\n            # Clear buffer earlier to avoid keeping data in memory for a long time.\n            self._bodybuf.truncate(0)\n            self._finished.cancel()\n\n        if (\n            self._warnsize\n            and self._bytes_received > self._warnsize\n            and not self._reached_warnsize\n        ):\n            self._reached_warnsize = True\n            logger.warning(\n                \"Received more bytes than download \"\n                \"warn size (%(warnsize)s) in request %(request)s.\",\n                {\"warnsize\": self._warnsize, \"request\": self._request},\n            )\n", "n_tokens": 1118, "byte_len": 5366, "file_sha1": "d7f2b4cf2695612b4b5d0c14925731dcfb757b90", "start_line": 558, "end_line": 705}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http11.py", "rel_path": "scrapy/core/downloader/handlers/http11.py", "module": "scrapy.core.downloader.handlers.http11", "ext": "py", "chunk_number": 6, "symbols": ["connectionLost", "failure", "data", "loss", "requests", "reasons", "false", "connection", "lost", "further", "partial", "fail", "dataloss", "response", "failed", "return", "check", "responses", "warning", "logger", "value", "errback", "finish", "broken", "decode", "true", "downloa", "fai", "done", "process", "__init__", "from_crawler", "download_request", "close", "cancel_delayed_call", "requestTunnel", "processProxyResponse", "connectFailed", "connect", "tunnel_request_data", "_getEndpoint", "_requestWithEndpoint", "request", "_get_agent", "_cb_timeout", "_cb_latency", "_headers_from_twisted_response", "_cb_bodyready", "_cancel", "_cb_bodydone"], "ast_kind": "function_or_method", "text": "    def connectionLost(self, reason: Failure = connectionDone) -> None:\n        if self._finished.called:\n            return\n\n        if reason.check(ResponseDone):\n            self._finish_response()\n            return\n\n        if reason.check(PotentialDataLoss):\n            self._finish_response(flags=[\"partial\"])\n            return\n\n        if reason.check(ResponseFailed) and any(\n            r.check(_DataLoss) for r in reason.value.reasons\n        ):\n            if not self._fail_on_dataloss:\n                self._finish_response(flags=[\"dataloss\"])\n                return\n\n            if not self._fail_on_dataloss_warned:\n                logger.warning(\n                    \"Got data loss in %s. If you want to process broken \"\n                    \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                    \" -- This message won't be shown in further requests\",\n                    self._txresponse.request.absoluteURI.decode(),\n                )\n                self._fail_on_dataloss_warned = True\n\n        self._finished.errback(reason)\n", "n_tokens": 210, "byte_len": 1075, "file_sha1": "d7f2b4cf2695612b4b5d0c14925731dcfb757b90", "start_line": 706, "end_line": 735}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/file.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/file.py", "rel_path": "scrapy/core/downloader/handlers/file.py", "module": "scrapy.core.downloader.handlers.file", "ext": "py", "chunk_number": 1, "symbols": ["download_request", "FileDownloadHandler", "defers", "from", "args", "false", "spider", "typing", "lib", "w3lib", "return", "annotations", "file", "download", "class", "scrapy", "future", "typ", "checking", "pathlib", "path", "body", "decorators", "request", "filepath", "uri", "responsetypes", "utils", "import", "lazy", "filename", "http", "self", "respcls", "read", "bytes", "response"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom w3lib.url import file_uri_to_path\n\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.decorators import defers\n\nif TYPE_CHECKING:\n    from scrapy import Request, Spider\n    from scrapy.http import Response\n\n\nclass FileDownloadHandler:\n    lazy = False\n\n    @defers\n    def download_request(self, request: Request, spider: Spider) -> Response:\n        filepath = file_uri_to_path(request.url)\n        body = Path(filepath).read_bytes()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)\n", "n_tokens": 151, "byte_len": 672, "file_sha1": "4775d89794c8222904c50b9e91d1cf5e957f3718", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http.py", "rel_path": "scrapy/core/downloader/handlers/http.py", "module": "scrapy.core.downloader.handlers.http", "ext": "py", "chunk_number": 1, "symbols": ["http", "download", "downloader", "from", "core", "import", "http10", "htt", "scrapy", "http11", "all", "handlers"], "ast_kind": "imports", "text": "from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\nfrom scrapy.core.downloader.handlers.http11 import (\n    HTTP11DownloadHandler as HTTPDownloadHandler,\n)\n\n__all__ = [\n    \"HTTP10DownloadHandler\",\n    \"HTTPDownloadHandler\",\n]\n", "n_tokens": 54, "byte_len": 249, "file_sha1": "0ad22fbb17f41331514ae15164e6701cdeeb8ab7", "start_line": 1, "end_line": 10}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http10.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/http10.py", "rel_path": "scrapy/core/downloader/handlers/http10.py", "module": "scrapy.core.downloader.handlers.http10", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "download_request", "_connect", "HTTP10DownloadHandler", "contextfactory", "connector", "iconnector", "connect", "future", "python", "spider", "deprecated", "interfaces", "removed", "https", "typ", "checking", "port", "webclient", "http", "client", "settings", "unicode", "handlers", "type", "misc", "lazy", "downloade", "clientcontextfactory", "response", "internet", "typing", "extensions", "return", "annotations", "class", "download", "httpclientfactory", "tcp", "classmethod", "warnings", "from", "crawler", "exceptions", "self", "load", "object", "request", "scrapy"], "ast_kind": "class_or_type", "text": "\"\"\"Download handlers for http and https schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from twisted.internet.defer import Deferred\n    from twisted.internet.interfaces import IConnector\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n    from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.settings import BaseSettings\n\n\nclass HTTP10DownloadHandler:\n    lazy = False\n\n    def __init__(self, settings: BaseSettings, crawler: Crawler):\n        warnings.warn(\n            \"HTTP10DownloadHandler is deprecated and will be removed in a future Scrapy version.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        self.HTTPClientFactory: type[ScrapyHTTPClientFactory] = load_object(\n            settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"]\n        )\n        self.ClientContextFactory: type[ScrapyClientContextFactory] = load_object(\n            settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n        )\n        self._settings: BaseSettings = settings\n        self._crawler: Crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred\n\n    def _connect(self, factory: ScrapyHTTPClientFactory) -> IConnector:\n        from twisted.internet import reactor\n\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b\"https\":\n            client_context_factory = build_from_crawler(\n                self.ClientContextFactory,\n                self._crawler,\n            )\n            return reactor.connectSSL(host, port, factory, client_context_factory)\n        return reactor.connectTCP(host, port, factory)\n", "n_tokens": 485, "byte_len": 2386, "file_sha1": "108de8d849bd2f391bddd9470c3156fb7ff87246", "start_line": 1, "end_line": 66}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/s3.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/s3.py", "rel_path": "scrapy/core/downloader/handlers/s3.py", "module": "scrapy.core.downloader.handlers.s3", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "download_request", "S3DownloadHandler", "signer", "cls", "method", "http", "handler", "library", "subclasses", "boto", "python", "connection", "credentials", "amazonaws", "spider", "httpdownloadhandler", "doesn", "unicode", "dict", "future", "typ", "checking", "https", "download", "unexpected", "urlparse", "cached", "missing", "anonymous", "settings", "items", "auth", "none", "handlers", "type", "misc", "default", "found", "could", "response", "add", "secure", "botocore", "available", "internet", "typing", "extensions", "return"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.core.downloader.handlers.http import HTTPDownloadHandler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import build_from_crawler\n\nif TYPE_CHECKING:\n    from twisted.internet.defer import Deferred\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.settings import BaseSettings\n\n\nclass S3DownloadHandler:\n    def __init__(\n        self,\n        settings: BaseSettings,\n        *,\n        crawler: Crawler,\n        aws_access_key_id: str | None = None,\n        aws_secret_access_key: str | None = None,\n        aws_session_token: str | None = None,\n        httpdownloadhandler: type[HTTPDownloadHandler] = HTTPDownloadHandler,\n        **kw: Any,\n    ):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n\n        if not aws_access_key_id:\n            aws_access_key_id = settings[\"AWS_ACCESS_KEY_ID\"]\n        if not aws_secret_access_key:\n            aws_secret_access_key = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        if not aws_session_token:\n            aws_session_token = settings[\"AWS_SESSION_TOKEN\"]\n\n        # If no credentials could be found anywhere,\n        # consider this an anonymous connection request by default;\n        # unless 'anon' was set explicitly (True/False).\n        anon = kw.get(\"anon\")\n        if anon is None and not aws_access_key_id and not aws_secret_access_key:\n            kw[\"anon\"] = True\n        self.anon = kw.get(\"anon\")\n\n        self._signer = None\n        import botocore.auth  # noqa: PLC0415\n        import botocore.credentials  # noqa: PLC0415\n\n        kw.pop(\"anon\", None)\n        if kw:\n            raise TypeError(f\"Unexpected keyword arguments: {kw}\")\n        if not self.anon:\n            assert aws_access_key_id is not None\n            assert aws_secret_access_key is not None\n            SignerCls = botocore.auth.AUTH_TYPE_MAPS[\"s3\"]\n            # botocore.auth.BaseSigner doesn't have an __init__() with args, only subclasses do\n            self._signer = SignerCls(  # type: ignore[call-arg]\n                botocore.credentials.Credentials(\n                    aws_access_key_id, aws_secret_access_key, aws_session_token\n                )\n            )\n\n        _http_handler = build_from_crawler(\n            httpdownloadhandler,\n            crawler,\n        )\n        self._download_http = _http_handler.download_request\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, **kwargs: Any) -> Self:\n        return cls(crawler.settings, crawler=crawler, **kwargs)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        p = urlparse_cached(request)\n        scheme = \"https\" if request.meta.get(\"is_secure\") else \"http\"\n        bucket = p.hostname\n        path = p.path + \"?\" + p.query if p.query else p.path\n        url = f\"{scheme}://{bucket}.s3.amazonaws.com{path}\"\n        if self.anon:\n            request = request.replace(url=url)\n        else:\n            import botocore.awsrequest  # noqa: PLC0415\n\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url=f\"{scheme}://s3.amazonaws.com/{bucket}{path}\",\n                headers=request.headers.to_unicode_dict(),\n                data=request.body,\n            )\n            assert self._signer\n            self._signer.add_auth(awsrequest)\n            request = request.replace(url=url, headers=awsrequest.headers.items())\n        return self._download_http(request, spider)\n", "n_tokens": 829, "byte_len": 3813, "file_sha1": "73f56ecdd3b9d34533cbc2b1d4827077f627a7e5", "start_line": 1, "end_line": 102}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/ftp.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/ftp.py", "rel_path": "scrapy/core/downloader/handlers/ftp.py", "module": "scrapy.core.downloader.handlers.ftp", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "dataReceived", "filename", "close", "from_crawler", "download_request", "gotClient", "_build_response", "_failed", "ReceivedDataProtocol", "FTPDownloadHandler", "build", "response", "failure", "protocol", "bool", "entire", "ftp", "passive", "case", "command", "failed", "python", "connection", "spider", "name", "data", "received", "related", "enabled", "passed", "future", "emulates", "typ", "checking", "port", "path", "urlparse", "cached", "convenience", "settings", "handler", "httpcode", "than", "default", "password", "issues", "none", "user", "server"], "ast_kind": "class_or_type", "text": "\"\"\"\nAn asynchronous FTP file download handler for scrapy which somehow emulates an http response.\n\nFTP connection parameters are passed using the request meta field:\n- ftp_user (required)\n- ftp_password (required)\n- ftp_passive (by default, enabled) sets FTP connection passive mode\n- ftp_local_filename\n        - If not given, file data will come in the response.body, as a normal scrapy Response,\n        which will imply that the entire file will be on memory.\n        - if given, file data will be saved in a local file with the given name\n        This helps when downloading very big files to avoid memory issues. In addition, for\n        convenience the local file name will also be given in the response body.\n\nThe status of the built html response will be, by default\n- 200 in case of success\n- 404 in case specified file was not found in the server (ftp code 550)\n\nor raise corresponding ftp exception otherwise\n\nThe matching from server ftp command return codes to html response codes is defined in the\nCODE_MAPPING attribute of the handler class. The key 'default' is used for any code\nthat is not explicitly present among the map keys. You may need to overwrite this\nmapping if want a different behaviour than default.\n\nIn case of status 200 request, response.headers will come with two keys:\n    'Local Filename' - with the value of the local filename if given\n    'Size' - with size of the downloaded data\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, BinaryIO\nfrom urllib.parse import unquote\n\nfrom twisted.internet.protocol import ClientCreator, Protocol\n\nfrom scrapy.http import Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    from twisted.internet.defer import Deferred\n    from twisted.protocols.ftp import FTPClient\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request, Spider\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nclass ReceivedDataProtocol(Protocol):\n    def __init__(self, filename: bytes | None = None):\n        self.__filename: bytes | None = filename\n        self.body: BinaryIO = (\n            Path(filename.decode()).open(\"wb\") if filename else BytesIO()\n        )\n        self.size: int = 0\n\n    def dataReceived(self, data: bytes) -> None:\n        self.body.write(data)\n        self.size += len(data)\n\n    @property\n    def filename(self) -> bytes | None:\n        return self.__filename\n\n    def close(self) -> None:\n        if self.filename:\n            self.body.close()\n        else:\n            self.body.seek(0)\n\n\n_CODE_RE = re.compile(r\"\\d+\")\n\n\nclass FTPDownloadHandler:\n    lazy = False\n\n    CODE_MAPPING: dict[str, int] = {\n        \"550\": 404,\n        \"default\": 503,\n    }\n\n    def __init__(self, settings: BaseSettings):\n        self.default_user = settings[\"FTP_USER\"]\n        self.default_password = settings[\"FTP_PASSWORD\"]\n        self.passive_mode = settings[\"FTP_PASSIVE_MODE\"]\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        from twisted.internet import reactor\n        from twisted.protocols.ftp import FTPClient\n\n        parsed_url = urlparse_cached(request)\n        user = request.meta.get(\"ftp_user\", self.default_user)\n        password = request.meta.get(\"ftp_password\", self.default_password)\n        passive_mode = (\n            1 if bool(request.meta.get(\"ftp_passive\", self.passive_mode)) else 0\n        )\n        creator = ClientCreator(\n            reactor, FTPClient, user, password, passive=passive_mode\n        )\n        dfd: Deferred[FTPClient] = creator.connectTCP(\n            parsed_url.hostname, parsed_url.port or 21\n        )\n        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n\n    def gotClient(\n        self, client: FTPClient, request: Request, filepath: str\n    ) -> Deferred[Response]:\n        self.client = client\n        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n        d = client.retrieveFile(filepath, protocol)\n        d.addCallback(self._build_response, request, protocol)\n        d.addErrback(self._failed, request)\n        return d\n\n    def _build_response(\n        self, result: Any, request: Request, protocol: ReceivedDataProtocol\n    ) -> Response:\n        self.result = result\n        protocol.close()\n        headers = {\"local filename\": protocol.filename or b\"\", \"size\": protocol.size}\n        body = protocol.filename or protocol.body.read()\n        respcls = responsetypes.from_args(url=request.url, body=body)\n        # hints for Headers-related types may need to be fixed to not use AnyStr\n        return respcls(url=request.url, status=200, body=body, headers=headers)  # type: ignore[arg-type]\n\n    def _failed(self, result: Failure, request: Request) -> Response:\n        from twisted.protocols.ftp import CommandFailed\n\n        message = result.getErrorMessage()\n        if result.type == CommandFailed:\n            m = _CODE_RE.search(message)\n            if m:\n                ftpcode = m.group()\n                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                return Response(\n                    url=request.url, status=httpcode, body=to_bytes(message)\n                )\n        assert result.type\n        raise result.type(result.value)\n", "n_tokens": 1223, "byte_len": 5663, "file_sha1": "f8ac471ea7a1258cbbd7307b857b03f61081c4e7", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/datauri.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/downloader/handlers/datauri.py", "rel_path": "scrapy/core/downloader/handlers/datauri.py", "module": "scrapy.core.downloader.handlers.datauri", "ext": "py", "chunk_number": 1, "symbols": ["download_request", "DataURIDownloadHandler", "encoding", "text", "response", "defers", "data", "uri", "from", "mimetype", "false", "media", "type", "charset", "resp", "kwargs", "issubclass", "spider", "typing", "lib", "w3lib", "return", "dict", "annotations", "class", "scrapy", "future", "typ", "checking", "parse", "body", "decorators", "download", "request", "split", "responsetypes", "utils", "import", "lazy", "http", "self", "respcls"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom w3lib.url import parse_data_uri\n\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.decorators import defers\n\nif TYPE_CHECKING:\n    from scrapy import Request, Spider\n\n\nclass DataURIDownloadHandler:\n    lazy = False\n\n    @defers\n    def download_request(self, request: Request, spider: Spider) -> Response:\n        uri = parse_data_uri(request.url)\n        respcls = responsetypes.from_mimetype(uri.media_type)\n\n        resp_kwargs: dict[str, Any] = {}\n        if issubclass(respcls, TextResponse) and uri.media_type.split(\"/\")[0] == \"text\":\n            charset = uri.media_type_parameters.get(\"charset\")\n            resp_kwargs[\"encoding\"] = charset\n\n        return respcls(url=request.url, body=uri.data, **resp_kwargs)\n", "n_tokens": 197, "byte_len": 863, "file_sha1": "dd2a57507c4cf023e9461df77c5dece6ac544d49", "start_line": 1, "end_line": 29}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/protocol.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/protocol.py", "rel_path": "scrapy/core/http2/protocol.py", "module": "scrapy.core.http2.protocol", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "__str__", "h2_connected", "InvalidNegotiatedProtocol", "RemoteTerminatedConnection", "MethodNotAllowed405", "H2ClientProtocol", "failure", "convention", "address", "protocol", "while", "bool", "boolean", "variables", "active", "streams", "instance", "were", "made", "connected", "connection", "stream", "spider", "goaway", "interfaces", "spiders", "more", "future", "typ", "allowed_max_concurrent_streams", "_send_pending_requests", "pop_stream", "_new_stream", "_write_to_transport", "request", "connectionMade", "_lose_connection_with_error", "handshakeCompleted", "_check_received_data", "dataReceived", "timeoutConnection", "connectionLost", "_handle_events", "connection_terminated", "data_received", "response_received", "settings_acknowledged", "stream_ended", "stream_reset"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport ipaddress\nimport itertools\nimport logging\nfrom collections import deque\nfrom typing import TYPE_CHECKING, Any\n\nfrom h2.config import H2Configuration\nfrom h2.connection import H2Connection\nfrom h2.errors import ErrorCodes\nfrom h2.events import (\n    ConnectionTerminated,\n    DataReceived,\n    Event,\n    ResponseReceived,\n    SettingsAcknowledged,\n    StreamEnded,\n    StreamReset,\n    UnknownFrameReceived,\n    WindowUpdated,\n)\nfrom h2.exceptions import FrameTooLargeError, H2Error\nfrom twisted.internet.error import TimeoutError as TxTimeoutError\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHandshakeListener,\n    IProtocolNegotiationFactory,\n)\nfrom twisted.internet.protocol import Factory, Protocol, connectionDone\nfrom twisted.internet.ssl import Certificate\nfrom twisted.protocols.policies import TimeoutMixin\nfrom zope.interface import implementer\n\nfrom scrapy.core.http2.stream import Stream, StreamCloseReason\nfrom scrapy.http import Request, Response\n\nif TYPE_CHECKING:\n    from ipaddress import IPv4Address, IPv6Address\n\n    from twisted.internet.defer import Deferred\n    from twisted.python.failure import Failure\n    from twisted.web.client import URI\n\n    from scrapy.settings import Settings\n    from scrapy.spiders import Spider\n\n\nlogger = logging.getLogger(__name__)\n\n\nPROTOCOL_NAME = b\"h2\"\n\n\nclass InvalidNegotiatedProtocol(H2Error):\n    def __init__(self, negotiated_protocol: bytes) -> None:\n        self.negotiated_protocol = negotiated_protocol\n\n    def __str__(self) -> str:\n        return f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\"\n\n\nclass RemoteTerminatedConnection(H2Error):\n    def __init__(\n        self,\n        remote_ip_address: IPv4Address | IPv6Address | None,\n        event: ConnectionTerminated,\n    ) -> None:\n        self.remote_ip_address = remote_ip_address\n        self.terminate_event = event\n\n    def __str__(self) -> str:\n        return f\"Received GOAWAY frame from {self.remote_ip_address!r}\"\n\n\nclass MethodNotAllowed405(H2Error):\n    def __init__(self, remote_ip_address: IPv4Address | IPv6Address | None) -> None:\n        self.remote_ip_address = remote_ip_address\n\n    def __str__(self) -> str:\n        return f\"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"\n\n\n@implementer(IHandshakeListener)\nclass H2ClientProtocol(Protocol, TimeoutMixin):\n    IDLE_TIMEOUT = 240\n\n    def __init__(\n        self,\n        uri: URI,\n        settings: Settings,\n        conn_lost_deferred: Deferred[list[BaseException]],\n    ) -> None:\n        \"\"\"\n        Arguments:\n            uri -- URI of the base url to which HTTP/2 Connection will be made.\n                uri is used to verify that incoming client requests have correct\n                base URL.\n            settings -- Scrapy project settings\n            conn_lost_deferred -- Deferred fires with the reason: Failure to notify\n                that connection was lost\n        \"\"\"\n        self._conn_lost_deferred: Deferred[list[BaseException]] = conn_lost_deferred\n\n        config = H2Configuration(client_side=True, header_encoding=\"utf-8\")\n        self.conn = H2Connection(config=config)\n\n        # ID of the next request stream\n        # Following the convention - 'Streams initiated by a client MUST\n        # use odd-numbered stream identifiers' (RFC 7540 - Section 5.1.1)\n        self._stream_id_generator = itertools.count(start=1, step=2)\n\n        # Streams are stored in a dictionary keyed off their stream IDs\n        self.streams: dict[int, Stream] = {}\n\n        # If requests are received before connection is made we keep\n        # all requests in a pool and send them as the connection is made\n        self._pending_request_stream_pool: deque[Stream] = deque()\n\n        # Save an instance of errors raised which lead to losing the connection\n        # We pass these instances to the streams ResponseFailed() failure\n        self._conn_lost_errors: list[BaseException] = []\n\n        # Some meta data of this connection\n        # initialized when connection is successfully made\n        self.metadata: dict[str, Any] = {\n            # Peer certificate instance\n            \"certificate\": None,\n            # Address of the server we are connected to which\n            # is updated when HTTP/2 connection is  made successfully\n            \"ip_address\": None,\n            # URI of the peer HTTP/2 connection is made\n            \"uri\": uri,\n            # Both ip_address and uri are used by the Stream before\n            # initiating the request to verify that the base address\n            # Variables taken from Project Settings\n            \"default_download_maxsize\": settings.getint(\"DOWNLOAD_MAXSIZE\"),\n            \"default_download_warnsize\": settings.getint(\"DOWNLOAD_WARNSIZE\"),\n            # Counter to keep track of opened streams. This counter\n            # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n            # streams are opened which leads to ProtocolError\n            # We use simple FIFO policy to handle pending requests\n            \"active_streams\": 0,\n            # Flag to keep track if settings were acknowledged by the remote\n            # This ensures that we have established a HTTP/2 connection\n            \"settings_acknowledged\": False,\n        }\n\n    @property\n    def h2_connected(self) -> bool:\n        \"\"\"Boolean to keep track of the connection status.\n        This is used while initiating pending streams to make sure\n        that we initiate stream only during active HTTP/2 Connection\n        \"\"\"\n        assert self.transport is not None  # typing\n        return bool(self.transport.connected) and self.metadata[\"settings_acknowledged\"]\n", "n_tokens": 1219, "byte_len": 5708, "file_sha1": "53617294573b9eabfbb174d30b5a986349da8bc0", "start_line": 1, "end_line": 157}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/protocol.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/protocol.py", "rel_path": "scrapy/core/http2/protocol.py", "module": "scrapy.core.http2.protocol", "ext": "py", "chunk_number": 2, "symbols": ["allowed_max_concurrent_streams", "_send_pending_requests", "pop_stream", "_new_stream", "_write_to_transport", "request", "connectionMade", "_lose_connection_with_error", "handshakeCompleted", "_check_received_data", "dataReceived", "does", "initialize", "pop", "stream", "perform", "protocol", "edge", "qualname", "reset", "timeout", "append", "active", "streams", "instance", "made", "connected", "connection", "spider", "data", "__init__", "__str__", "h2_connected", "timeoutConnection", "connectionLost", "_handle_events", "connection_terminated", "data_received", "response_received", "settings_acknowledged", "stream_ended", "stream_reset", "window_updated", "buildProtocol", "acceptableProtocols", "InvalidNegotiatedProtocol", "RemoteTerminatedConnection", "MethodNotAllowed405", "H2ClientProtocol", "H2ClientFactory"], "ast_kind": "function_or_method", "text": "    @property\n    def allowed_max_concurrent_streams(self) -> int:\n        \"\"\"We keep total two streams for client (sending data) and\n        server side (receiving data) for a single request. To be safe\n        we choose the minimum. Since this value can change in event\n        RemoteSettingsChanged we make variable a property.\n        \"\"\"\n        return min(\n            self.conn.local_settings.max_concurrent_streams,\n            self.conn.remote_settings.max_concurrent_streams,\n        )\n\n    def _send_pending_requests(self) -> None:\n        \"\"\"Initiate all pending requests from the deque following FIFO\n        We make sure that at any time {allowed_max_concurrent_streams}\n        streams are active.\n        \"\"\"\n        while (\n            self._pending_request_stream_pool\n            and self.metadata[\"active_streams\"] < self.allowed_max_concurrent_streams\n            and self.h2_connected\n        ):\n            self.metadata[\"active_streams\"] += 1\n            stream = self._pending_request_stream_pool.popleft()\n            stream.initiate_request()\n            self._write_to_transport()\n\n    def pop_stream(self, stream_id: int) -> Stream:\n        \"\"\"Perform cleanup when a stream is closed\"\"\"\n        stream = self.streams.pop(stream_id)\n        self.metadata[\"active_streams\"] -= 1\n        self._send_pending_requests()\n        return stream\n\n    def _new_stream(self, request: Request, spider: Spider) -> Stream:\n        \"\"\"Instantiates a new Stream object\"\"\"\n        stream = Stream(\n            stream_id=next(self._stream_id_generator),\n            request=request,\n            protocol=self,\n            download_maxsize=getattr(\n                spider, \"download_maxsize\", self.metadata[\"default_download_maxsize\"]\n            ),\n            download_warnsize=getattr(\n                spider, \"download_warnsize\", self.metadata[\"default_download_warnsize\"]\n            ),\n        )\n        self.streams[stream.stream_id] = stream\n        return stream\n\n    def _write_to_transport(self) -> None:\n        \"\"\"Write data to the underlying transport connection\n        from the HTTP2 connection instance if any\n        \"\"\"\n        assert self.transport is not None  # typing\n        # Reset the idle timeout as connection is still actively sending data\n        self.resetTimeout()\n\n        data = self.conn.data_to_send()\n        self.transport.write(data)\n\n    def request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        if not isinstance(request, Request):\n            raise TypeError(\n                f\"Expected scrapy.http.Request, received {request.__class__.__qualname__}\"\n            )\n\n        stream = self._new_stream(request, spider)\n        d: Deferred[Response] = stream.get_response()\n\n        # Add the stream to the request pool\n        self._pending_request_stream_pool.append(stream)\n\n        # If we receive a request when connection is idle\n        # We need to initiate pending requests\n        self._send_pending_requests()\n        return d\n\n    def connectionMade(self) -> None:\n        \"\"\"Called by Twisted when the connection is established. We can start\n        sending some data now: we should open with the connection preamble.\n        \"\"\"\n        # Initialize the timeout\n        self.setTimeout(self.IDLE_TIMEOUT)\n\n        assert self.transport is not None  # typing\n        destination = self.transport.getPeer()\n        self.metadata[\"ip_address\"] = ipaddress.ip_address(destination.host)\n\n        # Initiate H2 Connection\n        self.conn.initiate_connection()\n        self._write_to_transport()\n\n    def _lose_connection_with_error(self, errors: list[BaseException]) -> None:\n        \"\"\"Helper function to lose the connection with the error sent as a\n        reason\"\"\"\n        self._conn_lost_errors += errors\n        assert self.transport is not None  # typing\n        self.transport.loseConnection()\n\n    def handshakeCompleted(self) -> None:\n        \"\"\"\n        Close the connection if it's not made via the expected protocol\n        \"\"\"\n        assert self.transport is not None  # typing\n        if (\n            self.transport.negotiatedProtocol is not None\n            and self.transport.negotiatedProtocol != PROTOCOL_NAME\n        ):\n            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n            self._lose_connection_with_error(\n                [InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)]\n            )\n\n    def _check_received_data(self, data: bytes) -> None:\n        \"\"\"Checks for edge cases where the connection to remote fails\n        without raising an appropriate H2Error\n\n        Arguments:\n            data -- Data received from the remote\n        \"\"\"\n        if data.startswith(b\"HTTP/2.0 405 Method Not Allowed\"):\n            raise MethodNotAllowed405(self.metadata[\"ip_address\"])\n\n    def dataReceived(self, data: bytes) -> None:\n        # Reset the idle timeout as connection is still actively receiving data\n        self.resetTimeout()\n\n        try:\n            self._check_received_data(data)\n            events = self.conn.receive_data(data)\n            self._handle_events(events)\n        except H2Error as e:\n            if isinstance(e, FrameTooLargeError):\n                # hyper-h2 does not drop the connection in this scenario, we\n                # need to abort the connection manually.\n                self._conn_lost_errors += [e]\n                assert self.transport is not None  # typing\n                self.transport.abortConnection()\n                return\n\n            # Save this error as ultimately the connection will be dropped\n            # internally by hyper-h2. Saved error will be passed to all the streams\n            # closed with the connection.\n            self._lose_connection_with_error([e])\n        finally:\n            self._write_to_transport()\n", "n_tokens": 1155, "byte_len": 5864, "file_sha1": "53617294573b9eabfbb174d30b5a986349da8bc0", "start_line": 158, "end_line": 305}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/protocol.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/protocol.py", "rel_path": "scrapy/core/http2/protocol.py", "module": "scrapy.core.http2.protocol", "ext": "py", "chunk_number": 3, "symbols": ["timeoutConnection", "connectionLost", "_handle_events", "connection_terminated", "data_received", "response_received", "settings_acknowledged", "stream_ended", "stream_reset", "window_updated", "__init__", "buildProtocol", "acceptableProtocols", "H2ClientFactory", "client", "factory", "failure", "method", "pop", "stream", "cancel", "send", "request", "sent", "receive", "window", "append", "active", "streams", "instance", "__str__", "h2_connected", "allowed_max_concurrent_streams", "_send_pending_requests", "pop_stream", "_new_stream", "_write_to_transport", "connectionMade", "_lose_connection_with_error", "handshakeCompleted", "_check_received_data", "dataReceived", "InvalidNegotiatedProtocol", "RemoteTerminatedConnection", "MethodNotAllowed405", "H2ClientProtocol", "bool", "reset", "timeout", "variables"], "ast_kind": "class_or_type", "text": "    def timeoutConnection(self) -> None:\n        \"\"\"Called when the connection times out.\n        We lose the connection with TimeoutError\"\"\"\n\n        # Check whether there are open streams. If there are, we're going to\n        # want to use the error code PROTOCOL_ERROR. If there aren't, use\n        # NO_ERROR.\n        if (\n            self.conn.open_outbound_streams > 0\n            or self.conn.open_inbound_streams > 0\n            or self.metadata[\"active_streams\"] > 0\n        ):\n            error_code = ErrorCodes.PROTOCOL_ERROR\n        else:\n            error_code = ErrorCodes.NO_ERROR\n        self.conn.close_connection(error_code=error_code)\n        self._write_to_transport()\n\n        self._lose_connection_with_error(\n            [TxTimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")]\n        )\n\n    def connectionLost(self, reason: Failure = connectionDone) -> None:\n        \"\"\"Called by Twisted when the transport connection is lost.\n        No need to write anything to transport here.\n        \"\"\"\n        # Cancel the timeout if not done yet\n        self.setTimeout(None)\n\n        # Notify the connection pool instance such that no new requests are\n        # sent over current connection\n        if not reason.check(connectionDone):\n            self._conn_lost_errors.append(reason)\n\n        self._conn_lost_deferred.callback(self._conn_lost_errors)\n\n        for stream in self.streams.values():\n            if stream.metadata[\"request_sent\"]:\n                close_reason = StreamCloseReason.CONNECTION_LOST\n            else:\n                close_reason = StreamCloseReason.INACTIVE\n            stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n\n        self.metadata[\"active_streams\"] -= len(self.streams)\n        self.streams.clear()\n        self._pending_request_stream_pool.clear()\n        self.conn.close_connection()\n\n    def _handle_events(self, events: list[Event]) -> None:\n        \"\"\"Private method which acts as a bridge between the events\n        received from the HTTP/2 data and IH2EventsHandler\n\n        Arguments:\n            events -- A list of events that the remote peer triggered by sending data\n        \"\"\"\n        for event in events:\n            if isinstance(event, ConnectionTerminated):\n                self.connection_terminated(event)\n            elif isinstance(event, DataReceived):\n                self.data_received(event)\n            elif isinstance(event, ResponseReceived):\n                self.response_received(event)\n            elif isinstance(event, StreamEnded):\n                self.stream_ended(event)\n            elif isinstance(event, StreamReset):\n                self.stream_reset(event)\n            elif isinstance(event, WindowUpdated):\n                self.window_updated(event)\n            elif isinstance(event, SettingsAcknowledged):\n                self.settings_acknowledged(event)\n            elif isinstance(event, UnknownFrameReceived):\n                logger.warning(\"Unknown frame received: %s\", event.frame)\n\n    # Event handler functions starts here\n    def connection_terminated(self, event: ConnectionTerminated) -> None:\n        self._lose_connection_with_error(\n            [RemoteTerminatedConnection(self.metadata[\"ip_address\"], event)]\n        )\n\n    def data_received(self, event: DataReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_data(event.data, event.flow_controlled_length)\n\n    def response_received(self, event: ResponseReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_headers(event.headers)\n\n    def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n        self.metadata[\"settings_acknowledged\"] = True\n\n        # Send off all the pending requests as now we have\n        # established a proper HTTP/2 connection\n        self._send_pending_requests()\n\n        # Update certificate when our HTTP/2 connection is established\n        assert self.transport is not None  # typing\n        self.metadata[\"certificate\"] = Certificate(self.transport.getPeerCertificate())\n\n    def stream_ended(self, event: StreamEnded) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.ENDED, from_protocol=True)\n\n    def stream_reset(self, event: StreamReset) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.RESET, from_protocol=True)\n\n    def window_updated(self, event: WindowUpdated) -> None:\n        if event.stream_id != 0:\n            self.streams[event.stream_id].receive_window_update()\n        else:\n            # Send leftover data for all the streams\n            for stream in self.streams.values():\n                stream.receive_window_update()\n\n\n@implementer(IProtocolNegotiationFactory)\nclass H2ClientFactory(Factory):\n    def __init__(\n        self,\n        uri: URI,\n        settings: Settings,\n        conn_lost_deferred: Deferred[list[BaseException]],\n    ) -> None:\n        self.uri = uri\n        self.settings = settings\n        self.conn_lost_deferred = conn_lost_deferred\n\n    def buildProtocol(self, addr: IAddress) -> H2ClientProtocol:\n        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n\n    def acceptableProtocols(self) -> list[bytes]:\n        return [PROTOCOL_NAME]\n", "n_tokens": 1147, "byte_len": 5794, "file_sha1": "53617294573b9eabfbb174d30b5a986349da8bc0", "start_line": 306, "end_line": 454}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py", "rel_path": "scrapy/core/http2/stream.py", "module": "scrapy.core.http2.stream", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "__str__", "_cancel", "__repr__", "InactiveStreamClosed", "InvalidHostname", "StreamCloseReason", "Stream", "failure", "method", "protocol", "request", "sent", "bidirectional", "instance", "window", "expected", "netloc", "connection", "transport", "stream", "closed", "after", "possible", "about", "identifier", "connectio", "lost", "sending", "private", "_log_warnsize", "get_response", "check_request_url", "_get_request_headers", "initiate_request", "send_data", "receive_window_update", "receive_data", "receive_headers", "reset_stream", "close", "_fire_response_deferred", "content", "length", "bool", "send", "receive", "case", "make", "max"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom enum import Enum\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, Any\n\nfrom h2.errors import ErrorCodes\nfrom h2.exceptions import H2Error, ProtocolError, StreamClosedError\nfrom twisted.internet.defer import CancelledError, Deferred\nfrom twisted.internet.error import ConnectionClosed\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    from hpack import HeaderTuple\n\n    from scrapy.core.http2.protocol import H2ClientProtocol\n    from scrapy.http import Request, Response\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass InactiveStreamClosed(ConnectionClosed):\n    \"\"\"Connection was closed without sending request headers\n    of the stream. This happens when a stream is waiting for other\n    streams to close and connection is lost.\"\"\"\n\n    def __init__(self, request: Request) -> None:\n        self.request = request\n\n    def __str__(self) -> str:\n        return f\"InactiveStreamClosed: Connection was closed without sending the request {self.request!r}\"\n\n\nclass InvalidHostname(H2Error):\n    def __init__(\n        self, request: Request, expected_hostname: str, expected_netloc: str\n    ) -> None:\n        self.request = request\n        self.expected_hostname = expected_hostname\n        self.expected_netloc = expected_netloc\n\n    def __str__(self) -> str:\n        return f\"InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}\"\n\n\nclass StreamCloseReason(Enum):\n    # Received a StreamEnded event from the remote\n    ENDED = 1\n\n    # Received a StreamReset event -- ended abruptly\n    RESET = 2\n\n    # Transport connection was lost\n    CONNECTION_LOST = 3\n\n    # Expected response body size is more than allowed limit\n    MAXSIZE_EXCEEDED = 4\n\n    # Response deferred is cancelled by the client\n    # (happens when client called response_deferred.cancel())\n    CANCELLED = 5\n\n    # Connection lost and the stream was not initiated\n    INACTIVE = 6\n\n    # The hostname of the request is not same as of connected peer hostname\n    # As a result sending this request will the end the connection\n    INVALID_HOSTNAME = 7\n\n\nclass Stream:\n    \"\"\"Represents a single HTTP/2 Stream.\n\n    Stream is a bidirectional flow of bytes within an established connection,\n    which may carry one or more messages. Handles the transfer of HTTP Headers\n    and Data frames.\n\n    Role of this class is to\n    1. Combine all the data frames\n    \"\"\"\n\n    def __init__(\n        self,\n        stream_id: int,\n        request: Request,\n        protocol: H2ClientProtocol,\n        download_maxsize: int = 0,\n        download_warnsize: int = 0,\n    ) -> None:\n        \"\"\"\n        Arguments:\n            stream_id -- Unique identifier for the stream within a single HTTP/2 connection\n            request -- The HTTP request associated to the stream\n            protocol -- Parent H2ClientProtocol instance\n        \"\"\"\n        self.stream_id: int = stream_id\n        self._request: Request = request\n        self._protocol: H2ClientProtocol = protocol\n\n        self._download_maxsize = self._request.meta.get(\n            \"download_maxsize\", download_maxsize\n        )\n        self._download_warnsize = self._request.meta.get(\n            \"download_warnsize\", download_warnsize\n        )\n\n        # Metadata of an HTTP/2 connection stream\n        # initialized when stream is instantiated\n        self.metadata: dict[str, Any] = {\n            \"request_content_length\": (\n                0 if self._request.body is None else len(self._request.body)\n            ),\n            # Flag to keep track whether the stream has initiated the request\n            \"request_sent\": False,\n            # Flag to track whether we have logged about exceeding download warnsize\n            \"reached_warnsize\": False,\n            # Each time we send a data frame, we will decrease value by the amount send.\n            \"remaining_content_length\": (\n                0 if self._request.body is None else len(self._request.body)\n            ),\n            # Flag to keep track whether client (self) have closed this stream\n            \"stream_closed_local\": False,\n            # Flag to keep track whether the server has closed the stream\n            \"stream_closed_server\": False,\n        }\n\n        # Private variable used to build the response\n        # this response is then converted to appropriate Response class\n        # passed to the response deferred callback\n        self._response: dict[str, Any] = {\n            # Data received frame by frame from the server is appended\n            # and passed to the response Deferred when completely received.\n            \"body\": BytesIO(),\n            # The amount of data received that counts against the\n            # flow control window\n            \"flow_controlled_size\": 0,\n            # Headers received after sending the request\n            \"headers\": Headers({}),\n        }\n\n        def _cancel(_: Any) -> None:\n            # Close this stream as gracefully as possible\n            # If the associated request is initiated we reset this stream\n            # else we directly call close() method\n            if self.metadata[\"request_sent\"]:\n                self.reset_stream(StreamCloseReason.CANCELLED)\n            else:\n                self.close(StreamCloseReason.CANCELLED)\n\n        self._deferred_response: Deferred[Response] = Deferred(_cancel)\n\n    def __repr__(self) -> str:\n        return f\"Stream(id={self.stream_id!r})\"\n", "n_tokens": 1189, "byte_len": 5644, "file_sha1": "340027614d2d4fc9a50892bed2dd3ad8579755d3", "start_line": 1, "end_line": 161}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py", "rel_path": "scrapy/core/http2/stream.py", "module": "scrapy.core.http2.stream", "ext": "py", "chunk_number": 2, "symbols": ["_log_warnsize", "get_response", "check_request_url", "_get_request_headers", "initiate_request", "does", "content", "length", "method", "bool", "request", "sent", "append", "netloc", "stream", "name", "about", "make", "sending", "false", "port", "https", "urlparse", "cached", "correct", "value", "bytes", "close", "hold", "items", "__init__", "__str__", "_cancel", "__repr__", "send_data", "receive_window_update", "receive_data", "receive_headers", "reset_stream", "_fire_response_deferred", "InactiveStreamClosed", "InvalidHostname", "StreamCloseReason", "Stream", "failure", "closed", "send", "receive", "window", "case"], "ast_kind": "function_or_method", "text": "    @property\n    def _log_warnsize(self) -> bool:\n        \"\"\"Checks if we have received data which exceeds the download warnsize\n        and whether we have not already logged about it.\n\n        Returns:\n            True if both the above conditions hold true\n            False if any of the conditions is false\n        \"\"\"\n        content_length_header = int(\n            self._response[\"headers\"].get(b\"Content-Length\", -1)\n        )\n        return (\n            self._download_warnsize\n            and (\n                self._response[\"flow_controlled_size\"] > self._download_warnsize\n                or content_length_header > self._download_warnsize\n            )\n            and not self.metadata[\"reached_warnsize\"]\n        )\n\n    def get_response(self) -> Deferred[Response]:\n        \"\"\"Simply return a Deferred which fires when response\n        from the asynchronous request is available\n        \"\"\"\n        return self._deferred_response\n\n    def check_request_url(self) -> bool:\n        # Make sure that we are sending the request to the correct URL\n        url = urlparse_cached(self._request)\n        return (\n            url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n            or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n            or url.netloc\n            == f\"{self._protocol.metadata['ip_address']}:{self._protocol.metadata['uri'].port}\"\n        )\n\n    def _get_request_headers(self) -> list[tuple[str, str]]:\n        url = urlparse_cached(self._request)\n\n        path = url.path\n        if url.query:\n            path += \"?\" + url.query\n\n        # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n        # URIs; \"http\" or \"https\" URIs that do not contain a path component\n        # MUST include a value of '/'. The exception to this rule is an\n        # OPTIONS request for an \"http\" or \"https\" URI that does not include\n        # a path component; these MUST include a \":path\" pseudo-header field\n        # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n        if not path:\n            path = \"*\" if self._request.method == \"OPTIONS\" else \"/\"\n\n        # Make sure pseudo-headers comes before all the other headers\n        headers = [\n            (\":method\", self._request.method),\n            (\":authority\", url.netloc),\n        ]\n\n        # The \":scheme\" and \":path\" pseudo-header fields MUST\n        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n        if self._request.method != \"CONNECT\":\n            headers += [\n                (\":scheme\", self._protocol.metadata[\"uri\"].scheme),\n                (\":path\", path),\n            ]\n\n        content_length = str(len(self._request.body))\n        headers.append((\"Content-Length\", content_length))\n\n        content_length_name = self._request.headers.normkey(b\"Content-Length\")\n        for name, values in self._request.headers.items():\n            for value_bytes in values:\n                value = str(value_bytes, \"utf-8\")\n                if name == content_length_name:\n                    if value != content_length:\n                        logger.warning(\n                            \"Ignoring bad Content-Length header %r of request %r, \"\n                            \"sending %r instead\",\n                            value,\n                            self._request,\n                            content_length,\n                        )\n                    continue\n                headers.append((str(name, \"utf-8\"), value))\n\n        return headers\n\n    def initiate_request(self) -> None:\n        if self.check_request_url():\n            headers = self._get_request_headers()\n            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n            self.metadata[\"request_sent\"] = True\n            self.send_data()\n        else:\n            # Close this stream calling the response errback\n            # Note that we have not sent any headers\n            self.close(StreamCloseReason.INVALID_HOSTNAME)\n", "n_tokens": 826, "byte_len": 3998, "file_sha1": "340027614d2d4fc9a50892bed2dd3ad8579755d3", "start_line": 162, "end_line": 260}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py", "rel_path": "scrapy/core/http2/stream.py", "module": "scrapy.core.http2.stream", "ext": "py", "chunk_number": 3, "symbols": ["send_data", "receive_window_update", "receive_data", "receive_headers", "reset_stream", "window", "update", "expected", "size", "method", "warning", "msg", "send", "immediately", "request", "sent", "receive", "were", "case", "after", "stream", "closed", "name", "about", "sending", "max", "frame", "more", "length", "download", "__init__", "__str__", "_cancel", "__repr__", "_log_warnsize", "get_response", "check_request_url", "_get_request_headers", "initiate_request", "close", "_fire_response_deferred", "InactiveStreamClosed", "InvalidHostname", "StreamCloseReason", "Stream", "failure", "content", "bool", "netloc", "connection"], "ast_kind": "function_or_method", "text": "    def send_data(self) -> None:\n        \"\"\"Called immediately after the headers are sent. Here we send all the\n        data as part of the request.\n\n        If the content length is 0 initially then we end the stream immediately and\n        wait for response data.\n\n        Warning: Only call this method when stream not closed from client side\n           and has initiated request already by sending HEADER frame. If not then\n           stream will raise ProtocolError (raise by h2 state machine).\n        \"\"\"\n        if self.metadata[\"stream_closed_local\"]:\n            raise StreamClosedError(self.stream_id)\n\n        # Firstly, check what the flow control window is for current stream.\n        window_size = self._protocol.conn.local_flow_control_window(\n            stream_id=self.stream_id\n        )\n\n        # Next, check what the maximum frame size is.\n        max_frame_size = self._protocol.conn.max_outbound_frame_size\n\n        # We will send no more than the window size or the remaining file size\n        # of data in this call, whichever is smaller.\n        bytes_to_send_size = min(window_size, self.metadata[\"remaining_content_length\"])\n\n        # We now need to send a number of data frames.\n        while bytes_to_send_size > 0:\n            chunk_size = min(bytes_to_send_size, max_frame_size)\n\n            data_chunk_start_id = (\n                self.metadata[\"request_content_length\"]\n                - self.metadata[\"remaining_content_length\"]\n            )\n            data_chunk = self._request.body[\n                data_chunk_start_id : data_chunk_start_id + chunk_size\n            ]\n\n            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n\n            bytes_to_send_size -= chunk_size\n            self.metadata[\"remaining_content_length\"] -= chunk_size\n\n        self.metadata[\"remaining_content_length\"] = max(\n            0, self.metadata[\"remaining_content_length\"]\n        )\n\n        # End the stream if no more data needs to be send\n        if self.metadata[\"remaining_content_length\"] == 0:\n            self._protocol.conn.end_stream(self.stream_id)\n\n        # Q. What about the rest of the data?\n        # Ans: Remaining Data frames will be sent when we get a WindowUpdate frame\n\n    def receive_window_update(self) -> None:\n        \"\"\"Flow control window size was changed.\n        Send data that earlier could not be sent as we were\n        blocked behind the flow control.\n        \"\"\"\n        if (\n            self.metadata[\"remaining_content_length\"]\n            and not self.metadata[\"stream_closed_server\"]\n            and self.metadata[\"request_sent\"]\n        ):\n            self.send_data()\n\n    def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n        self._response[\"body\"].write(data)\n        self._response[\"flow_controlled_size\"] += flow_controlled_length\n\n        # We check maxsize here in case the Content-Length header was not received\n        if (\n            self._download_maxsize\n            and self._response[\"flow_controlled_size\"] > self._download_maxsize\n        ):\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata[\"reached_warnsize\"] = True\n            warning_msg = (\n                f\"Received more ({self._response['flow_controlled_size']}) bytes than download \"\n                f\"warn size ({self._download_warnsize}) in request {self._request}\"\n            )\n            logger.warning(warning_msg)\n\n        # Acknowledge the data received\n        self._protocol.conn.acknowledge_received_data(\n            self._response[\"flow_controlled_size\"], self.stream_id\n        )\n\n    def receive_headers(self, headers: list[HeaderTuple]) -> None:\n        for name, value in headers:\n            self._response[\"headers\"].appendlist(name, value)\n\n        # Check if we exceed the allowed max data size which can be received\n        expected_size = int(self._response[\"headers\"].get(b\"Content-Length\", -1))\n        if self._download_maxsize and expected_size > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata[\"reached_warnsize\"] = True\n            warning_msg = (\n                f\"Expected response size ({expected_size}) larger than \"\n                f\"download warn size ({self._download_warnsize}) in request {self._request}\"\n            )\n            logger.warning(warning_msg)\n\n    def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n        \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n        if self.metadata[\"stream_closed_local\"]:\n            raise StreamClosedError(self.stream_id)\n\n        # Clear buffer earlier to avoid keeping data in memory for a long time\n        self._response[\"body\"].truncate(0)\n\n        self.metadata[\"stream_closed_local\"] = True\n        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n        self.close(reason)\n", "n_tokens": 1029, "byte_len": 5062, "file_sha1": "340027614d2d4fc9a50892bed2dd3ad8579755d3", "start_line": 261, "end_line": 381}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/stream.py", "rel_path": "scrapy/core/http2/stream.py", "module": "scrapy.core.http2.stream", "ext": "py", "chunk_number": 4, "symbols": ["close", "_fire_response_deferred", "expected", "size", "failure", "pop", "stream", "closed", "protocol", "fire", "response", "bool", "immediately", "qualname", "instance", "case", "each", "payload", "includes", "cancelling", "connectio", "lost", "cancelled", "elif", "length", "download", "maxsize", "port", "make", "sent", "__init__", "__str__", "_cancel", "__repr__", "_log_warnsize", "get_response", "check_request_url", "_get_request_headers", "initiate_request", "send_data", "receive_window_update", "receive_data", "receive_headers", "reset_stream", "InactiveStreamClosed", "InvalidHostname", "StreamCloseReason", "Stream", "method", "content"], "ast_kind": "function_or_method", "text": "    def close(\n        self,\n        reason: StreamCloseReason,\n        errors: list[BaseException] | None = None,\n        from_protocol: bool = False,\n    ) -> None:\n        \"\"\"Based on the reason sent we will handle each case.\"\"\"\n        if self.metadata[\"stream_closed_server\"]:\n            raise StreamClosedError(self.stream_id)\n\n        if not isinstance(reason, StreamCloseReason):\n            raise TypeError(\n                f\"Expected StreamCloseReason, received {reason.__class__.__qualname__}\"\n            )\n\n        # Have default value of errors as an empty list as\n        # some cases can add a list of exceptions\n        errors = errors or []\n\n        if not from_protocol:\n            self._protocol.pop_stream(self.stream_id)\n\n        self.metadata[\"stream_closed_server\"] = True\n\n        # We do not check for Content-Length or Transfer-Encoding in response headers\n        # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n        # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)\n\n        # NOTE: Order of handling the events is important here\n        # As we immediately cancel the request when maxsize is exceeded while\n        # receiving DATA_FRAME's when we have received the headers (not\n        # having Content-Length)\n        if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n            expected_size = int(\n                self._response[\"headers\"].get(\n                    b\"Content-Length\", self._response[\"flow_controlled_size\"]\n                )\n            )\n            error_msg = (\n                f\"Cancelling download of {self._request.url}: received response \"\n                f\"size ({expected_size}) larger than download max size ({self._download_maxsize})\"\n            )\n            logger.error(error_msg)\n            self._deferred_response.errback(CancelledError(error_msg))\n\n        elif reason is StreamCloseReason.ENDED:\n            self._fire_response_deferred()\n\n        # Stream was abruptly ended here\n        elif reason is StreamCloseReason.CANCELLED:\n            # Client has cancelled the request. Remove all the data\n            # received and fire the response deferred with no flags set\n\n            # NOTE: The data is already flushed in Stream.reset_stream() called\n            # immediately when the stream needs to be cancelled\n\n            # There maybe no :status in headers, we make\n            # HTTP Status Code: 499 - Client Closed Request\n            self._response[\"headers\"][\":status\"] = \"499\"\n            self._fire_response_deferred()\n\n        elif reason is StreamCloseReason.RESET:\n            self._deferred_response.errback(\n                ResponseFailed(\n                    [\n                        Failure(\n                            f\"Remote peer {self._protocol.metadata['ip_address']} sent RST_STREAM\",\n                            ProtocolError,\n                        )\n                    ]\n                )\n            )\n\n        elif reason is StreamCloseReason.CONNECTION_LOST:\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        elif reason is StreamCloseReason.INACTIVE:\n            errors.insert(0, InactiveStreamClosed(self._request))\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        else:\n            assert reason is StreamCloseReason.INVALID_HOSTNAME\n            self._deferred_response.errback(\n                InvalidHostname(\n                    self._request,\n                    str(self._protocol.metadata[\"uri\"].host, \"utf-8\"),\n                    f\"{self._protocol.metadata['ip_address']}:{self._protocol.metadata['uri'].port}\",\n                )\n            )\n\n    def _fire_response_deferred(self) -> None:\n        \"\"\"Builds response from the self._response dict\n        and fires the response deferred callback with the\n        generated response instance\"\"\"\n\n        body = self._response[\"body\"].getvalue()\n        response_cls = responsetypes.from_args(\n            headers=self._response[\"headers\"],\n            url=self._request.url,\n            body=body,\n        )\n\n        response = response_cls(\n            url=self._request.url,\n            status=int(self._response[\"headers\"][\":status\"]),\n            headers=self._response[\"headers\"],\n            body=body,\n            request=self._request,\n            certificate=self._protocol.metadata[\"certificate\"],\n            ip_address=self._protocol.metadata[\"ip_address\"],\n            protocol=\"h2\",\n        )\n\n        self._deferred_response.callback(response)\n", "n_tokens": 887, "byte_len": 4561, "file_sha1": "340027614d2d4fc9a50892bed2dd3ad8579755d3", "start_line": 382, "end_line": 496}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/agent.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/agent.py", "rel_path": "scrapy/core/http2/agent.py", "module": "scrapy.core.http2.agent", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "get_connection", "_new_connection", "put_connection", "_remove_connection", "close_connections", "get_endpoint", "get_key", "request", "H2ConnectionPool", "H2Agent", "ScrapyProxyH2Agent", "encoding", "client", "factory", "failure", "contextfactory", "protocol", "hostname", "endpoint", "append", "connections", "instance", "connection", "spider", "pending", "requests", "browser", "like", "popleft", "spiders", "future", "typ", "checking", "port", "acceptable", "protocols", "succeed", "get", "key", "for", "settings", "close", "connecting", "arguments", "reactor", "base", "them", "scrapy", "proxy"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom collections import deque\nfrom typing import TYPE_CHECKING\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import (\n    URI,\n    BrowserLikePolicyForHTTPS,\n    ResponseFailed,\n    _StandardEndpointFactory,\n)\nfrom twisted.web.error import SchemeNotSupported\n\nfrom scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\nfrom scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\n\nif TYPE_CHECKING:\n    from twisted.internet.base import ReactorBase\n    from twisted.internet.endpoints import HostnameEndpoint\n\n    from scrapy.http import Request, Response\n    from scrapy.settings import Settings\n    from scrapy.spiders import Spider\n\n\nConnectionKeyT = tuple[bytes, bytes, int]\n\n\nclass H2ConnectionPool:\n    def __init__(self, reactor: ReactorBase, settings: Settings) -> None:\n        self._reactor = reactor\n        self.settings = settings\n\n        # Store a dictionary which is used to get the respective\n        # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)\n        self._connections: dict[ConnectionKeyT, H2ClientProtocol] = {}\n\n        # Save all requests that arrive before the connection is established\n        self._pending_requests: dict[\n            ConnectionKeyT, deque[Deferred[H2ClientProtocol]]\n        ] = {}\n\n    def get_connection(\n        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n    ) -> Deferred[H2ClientProtocol]:\n        if key in self._pending_requests:\n            # Received a request while connecting to remote\n            # Create a deferred which will fire with the H2ClientProtocol\n            # instance\n            d: Deferred[H2ClientProtocol] = Deferred()\n            self._pending_requests[key].append(d)\n            return d\n\n        # Check if we already have a connection to the remote\n        conn = self._connections.get(key, None)\n        if conn:\n            # Return this connection instance wrapped inside a deferred\n            return defer.succeed(conn)\n\n        # No connection is established for the given URI\n        return self._new_connection(key, uri, endpoint)\n\n    def _new_connection(\n        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n    ) -> Deferred[H2ClientProtocol]:\n        self._pending_requests[key] = deque()\n\n        conn_lost_deferred: Deferred[list[BaseException]] = Deferred()\n        conn_lost_deferred.addCallback(self._remove_connection, key)\n\n        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n        conn_d = endpoint.connect(factory)\n        conn_d.addCallback(self.put_connection, key)\n\n        d: Deferred[H2ClientProtocol] = Deferred()\n        self._pending_requests[key].append(d)\n        return d\n\n    def put_connection(\n        self, conn: H2ClientProtocol, key: ConnectionKeyT\n    ) -> H2ClientProtocol:\n        self._connections[key] = conn\n\n        # Now as we have established a proper HTTP/2 connection\n        # we fire all the deferred's with the connection instance\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.callback(conn)\n\n        return conn\n\n    def _remove_connection(\n        self, errors: list[BaseException], key: ConnectionKeyT\n    ) -> None:\n        self._connections.pop(key)\n\n        # Call the errback of all the pending requests for this connection\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.errback(ResponseFailed(errors))\n\n    def close_connections(self) -> None:\n        \"\"\"Close all the HTTP/2 connections and remove them from pool\n\n        Returns:\n            Deferred that fires when all connections have been closed\n        \"\"\"\n        for conn in self._connections.values():\n            assert conn.transport is not None  # typing\n            conn.transport.abortConnection()\n\n\nclass H2Agent:\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        pool: H2ConnectionPool,\n        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n        connect_timeout: float | None = None,\n        bind_address: bytes | None = None,\n    ) -> None:\n        self._reactor = reactor\n        self._pool = pool\n        self._context_factory = AcceptableProtocolsContextFactory(\n            context_factory, acceptable_protocols=[b\"h2\"]\n        )\n        self.endpoint_factory = _StandardEndpointFactory(\n            self._reactor, self._context_factory, connect_timeout, bind_address\n        )\n\n    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n        return self.endpoint_factory.endpointForURI(uri)\n\n    def get_key(self, uri: URI) -> ConnectionKeyT:\n        \"\"\"\n        Arguments:\n            uri - URI obtained directly from request URL\n        \"\"\"\n        return uri.scheme, uri.host, uri.port\n\n    def request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        uri = URI.fromBytes(bytes(request.url, encoding=\"utf-8\"))\n        try:\n            endpoint = self.get_endpoint(uri)\n        except SchemeNotSupported:\n            return defer.fail(Failure())\n\n        key = self.get_key(uri)\n        d: Deferred[H2ClientProtocol] = self._pool.get_connection(key, uri, endpoint)\n        d2: Deferred[Response] = d.addCallback(\n            lambda conn: conn.request(request, spider)\n        )\n        return d2\n\n\nclass ScrapyProxyH2Agent(H2Agent):", "n_tokens": 1187, "byte_len": 5604, "file_sha1": "5337e573066369ac1f529afa91096ffe783929ac", "start_line": 1, "end_line": 161}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/agent.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/core/http2/agent.py", "rel_path": "scrapy/core/http2/agent.py", "module": "scrapy.core.http2.agent", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "get_endpoint", "get_key", "hostname", "endpoint", "get", "context", "factory", "connect", "timeout", "proxy", "uri", "return", "browser", "like", "instead", "bind", "address", "port", "init", "host", "key", "for", "connection", "from", "pool", "reactor", "base", "super", "none", "get_connection", "_new_connection", "put_connection", "_remove_connection", "close_connections", "request", "H2ConnectionPool", "H2Agent", "ScrapyProxyH2Agent", "encoding", "client", "failure", "contextfactory", "protocol", "append", "connections", "instance", "spider", "pending", "requests"], "ast_kind": "function_or_method", "text": "    def __init__(\n        self,\n        reactor: ReactorBase,\n        proxy_uri: URI,\n        pool: H2ConnectionPool,\n        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n        connect_timeout: float | None = None,\n        bind_address: bytes | None = None,\n    ) -> None:\n        super().__init__(\n            reactor=reactor,\n            pool=pool,\n            context_factory=context_factory,\n            connect_timeout=connect_timeout,\n            bind_address=bind_address,\n        )\n        self._proxy_uri = proxy_uri\n\n    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n        return self.endpoint_factory.endpointForURI(self._proxy_uri)\n\n    def get_key(self, uri: URI) -> ConnectionKeyT:\n        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n        return b\"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n", "n_tokens": 193, "byte_len": 886, "file_sha1": "5337e573066369ac1f529afa91096ffe783929ac", "start_line": 162, "end_line": 186}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/contracts/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/contracts/__init__.py", "rel_path": "scrapy/contracts/__init__.py", "module": "scrapy.contracts.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "add_pre_hook", "wrapper", "add_post_hook", "adjust_request_args", "tested_methods_from_spidercls", "extract_contracts", "from_spider", "from_method", "Contract", "ContractsManager", "failure", "method", "async", "spidercls", "append", "multiline", "case", "add", "from", "spider", "name", "dont", "filter", "future", "typ", "checking", "post", "order", "testcase", "_clean_req", "cb_wrapper", "eb_wrapper", "_create_testcase", "__str__", "ContractTestCase", "self", "desc", "isinstance", "results", "extract", "contracts", "none", "stop", "type", "line", "methods", "getattribute", "http", "exc"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport re\nimport sys\nfrom collections.abc import AsyncGenerator, Iterable\nfrom functools import wraps\nfrom inspect import getmembers\nfrom types import CoroutineType\nfrom typing import TYPE_CHECKING, Any, cast\nfrom unittest import TestCase, TestResult\n\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.python import get_spec\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n    from twisted.python.failure import Failure\n\n    from scrapy import Spider\n\n\nclass Contract:\n    \"\"\"Abstract class for contracts\"\"\"\n\n    request_cls: type[Request] | None = None\n    name: str\n\n    def __init__(self, method: Callable, *args: Any):\n        self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n        self.testcase_post = _create_testcase(method, f\"@{self.name} post-hook\")\n        self.args: tuple[Any, ...] = args\n\n    def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n        if hasattr(self, \"pre_process\"):\n            cb = request.callback\n            assert cb is not None\n\n            @wraps(cb)\n            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                cb_result = cb(response, **cb_kwargs)\n                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                    raise TypeError(\"Contracts don't support async callbacks\")\n                return list(cast(\"Iterable[Any]\", iterate_spider_output(cb_result)))\n\n            request.callback = wrapper\n\n        return request\n\n    def add_post_hook(self, request: Request, results: TestResult) -> Request:\n        if hasattr(self, \"post_process\"):\n            cb = request.callback\n            assert cb is not None\n\n            @wraps(cb)\n            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                cb_result = cb(response, **cb_kwargs)\n                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                    raise TypeError(\"Contracts don't support async callbacks\")\n                output = list(cast(\"Iterable[Any]\", iterate_spider_output(cb_result)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                return output\n\n            request.callback = wrapper\n\n        return request\n\n    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n        return args\n\n\nclass ContractsManager:\n    contracts: dict[str, type[Contract]] = {}\n\n    def __init__(self, contracts: Iterable[type[Contract]]):\n        for contract in contracts:\n            self.contracts[contract.name] = contract\n\n    def tested_methods_from_spidercls(self, spidercls: type[Spider]) -> list[str]:\n        is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n        methods = []\n        for key, value in getmembers(spidercls):\n            if callable(value) and value.__doc__ and is_method(value.__doc__):\n                methods.append(key)\n\n        return methods\n\n    def extract_contracts(self, method: Callable) -> list[Contract]:\n        contracts: list[Contract] = []\n        assert method.__doc__ is not None\n        for line in method.__doc__.split(\"\\n\"):\n            line = line.strip()\n\n            if line.startswith(\"@\"):\n                m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n                if m is None:\n                    continue\n                name, args = m.groups()\n                args = re.split(r\"\\s+\", args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts\n\n    def from_spider(self, spider: Spider, results: TestResult) -> list[Request | None]:\n        requests: list[Request | None] = []\n        for method in self.tested_methods_from_spidercls(type(spider)):\n            bound_method = spider.__getattribute__(method)\n            try:\n                requests.append(self.from_method(bound_method, results))\n            except Exception:\n                case = _create_testcase(bound_method, \"contract\")\n                results.addError(case, sys.exc_info())\n\n        return requests\n\n    def from_method(self, method: Callable, results: TestResult) -> Request | None:\n        contracts = self.extract_contracts(method)\n        if contracts:\n            request_cls = Request\n            for contract in contracts:\n                if contract.request_cls is not None:\n                    request_cls = contract.request_cls\n\n            # calculate request args\n            args, kwargs = get_spec(request_cls.__init__)\n\n            # Don't filter requests to allow\n            # testing different callbacks on the same URL.\n            kwargs[\"dont_filter\"] = True\n            kwargs[\"callback\"] = method\n\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            args.remove(\"self\")\n\n            # check if all positional arguments are defined in kwargs\n            if set(args).issubset(set(kwargs)):\n                request = request_cls(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request\n        return None\n", "n_tokens": 1227, "byte_len": 6318, "file_sha1": "60df9e87071d235eed85645bc04541ef4cd4218c", "start_line": 1, "end_line": 171}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/contracts/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/contracts/__init__.py", "rel_path": "scrapy/contracts/__init__.py", "module": "scrapy.contracts.__init__", "ext": "py", "chunk_number": 2, "symbols": ["_clean_req", "cb_wrapper", "eb_wrapper", "_create_testcase", "__str__", "ContractTestCase", "failure", "method", "wraps", "except", "kwargs", "create", "testcase", "argument", "cast", "errors", "spider", "case", "self", "test", "result", "return", "objects", "name", "lambda", "class", "str", "returning", "output", "clean", "__init__", "add_pre_hook", "wrapper", "add_post_hook", "adjust_request_args", "tested_methods_from_spidercls", "extract_contracts", "from_spider", "from_method", "Contract", "ContractsManager", "async", "spidercls", "append", "multiline", "add", "from", "dont", "filter", "future"], "ast_kind": "class_or_type", "text": "    def _clean_req(\n        self, request: Request, method: Callable, results: TestResult\n    ) -> None:\n        \"\"\"stop the request from returning objects and records any errors\"\"\"\n\n        cb = request.callback\n        assert cb is not None\n\n        @wraps(cb)\n        def cb_wrapper(response: Response, **cb_kwargs: Any) -> None:\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(cast(\"Iterable[Any]\", iterate_spider_output(output)))\n            except Exception:\n                case = _create_testcase(method, \"callback\")\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure: Failure) -> None:\n            case = _create_testcase(method, \"errback\")\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper\n\n\ndef _create_testcase(method: Callable, desc: str) -> TestCase:\n    spider = method.__self__.name  # type: ignore[attr-defined]\n\n    class ContractTestCase(TestCase):\n        def __str__(_self) -> str:  # pylint: disable=no-self-argument\n            return f\"[{spider}] {method.__name__} ({desc})\"\n\n    name = f\"{spider}_{method.__name__}\"\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)\n", "n_tokens": 305, "byte_len": 1362, "file_sha1": "60df9e87071d235eed85645bc04541ef4cd4218c", "start_line": 172, "end_line": 208}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/contracts/default.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/contracts/default.py", "rel_path": "scrapy/contracts/default.py", "module": "scrapy.contracts.default", "ext": "py", "chunk_number": 1, "symbols": ["adjust_request_args", "__init__", "post_process", "UrlContract", "CallbackKeywordArgumentsContract", "MetadataContract", "ReturnsContract", "ScrapesContract", "presence", "loads", "bool", "returns", "contract", "name", "future", "arg", "arg1", "missing", "encoded", "isinstance", "items", "index", "error", "none", "join", "json", "item", "adapter", "http", "min", "bound", "callable", "post", "process", "max", "metadata", "mandatory", "quantity", "requests", "expected", "general", "callback", "keyword", "argument", "assertion", "typing", "contracts", "return", "page", "body"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport json\nfrom typing import Any, Callable\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.contracts import Contract\nfrom scrapy.exceptions import ContractFail\nfrom scrapy.http import Request\n\n\n# contracts\nclass UrlContract(Contract):\n    \"\"\"Contract to set the url of the request (mandatory)\n    @url http://scrapy.org\n    \"\"\"\n\n    name = \"url\"\n\n    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n        args[\"url\"] = self.args[0]\n        return args\n\n\nclass CallbackKeywordArgumentsContract(Contract):\n    \"\"\"Contract to set the keyword arguments for the request.\n    The value should be a JSON-encoded dictionary, e.g.:\n\n    @cb_kwargs {\"arg1\": \"some value\"}\n    \"\"\"\n\n    name = \"cb_kwargs\"\n\n    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n        args[\"cb_kwargs\"] = json.loads(\" \".join(self.args))\n        return args\n\n\nclass MetadataContract(Contract):\n    \"\"\"Contract to set metadata arguments for the request.\n    The value should be JSON-encoded dictionary, e.g.:\n\n    @meta {\"arg1\": \"some value\"}\n    \"\"\"\n\n    name = \"meta\"\n\n    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n        args[\"meta\"] = json.loads(\" \".join(self.args))\n        return args\n\n\nclass ReturnsContract(Contract):\n    \"\"\"Contract to check the output of a callback\n\n    general form:\n    @returns request(s)/item(s) [min=1 [max]]\n\n    e.g.:\n    @returns request\n    @returns request 2\n    @returns request 2 10\n    @returns request 0 10\n    \"\"\"\n\n    name = \"returns\"\n    object_type_verifiers: dict[str | None, Callable[[Any], bool]] = {\n        \"request\": lambda x: isinstance(x, Request),\n        \"requests\": lambda x: isinstance(x, Request),\n        \"item\": is_item,\n        \"items\": is_item,\n    }\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n\n        if len(self.args) not in [1, 2, 3]:\n            raise ValueError(\n                f\"Incorrect argument quantity: expected 1, 2 or 3, got {len(self.args)}\"\n            )\n        self.obj_name = self.args[0] or None\n        self.obj_type_verifier = self.object_type_verifiers[self.obj_name]\n\n        try:\n            self.min_bound: float = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound: float = int(self.args[2])\n        except IndexError:\n            self.max_bound = float(\"inf\")\n\n    def post_process(self, output: list[Any]) -> None:\n        occurrences = 0\n        for x in output:\n            if self.obj_type_verifier(x):\n                occurrences += 1\n\n        assertion = self.min_bound <= occurrences <= self.max_bound\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = str(self.min_bound)\n            else:\n                expected = f\"{self.min_bound}..{self.max_bound}\"\n\n            raise ContractFail(\n                f\"Returned {occurrences} {self.obj_name}, expected {expected}\"\n            )\n\n\nclass ScrapesContract(Contract):\n    \"\"\"Contract to check presence of fields in scraped items\n    @scrapes page_name page_body\n    \"\"\"\n\n    name = \"scrapes\"\n\n    def post_process(self, output: list[Any]) -> None:\n        for x in output:\n            if is_item(x):\n                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                if missing:\n                    missing_fields = \", \".join(missing)\n                    raise ContractFail(f\"Missing fields: {missing_fields}\")\n", "n_tokens": 829, "byte_len": 3553, "file_sha1": "daedf81c6b15cd15cf4a70573a463944bcadb8fa", "start_line": 1, "end_line": 128}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/crawl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/crawl.py", "rel_path": "scrapy/spiders/crawl.py", "module": "scrapy.spiders.crawl", "ext": "py", "chunk_number": 1, "symbols": ["_identity", "_identity_process_request", "_get_method", "__init__", "_compile", "_parse", "parse_start_url", "process_results", "_build_request", "_requests_to_follow", "_callback", "Rule", "CrawlSpider", "failure", "method", "bool", "default", "link", "parse", "with", "seen", "future", "python", "spider", "pages", "rules", "deprecated", "spiders", "compile", "replaces", "_errback", "_parse_response", "_handle_failure", "_compile_rules", "from_crawler", "async", "append", "typ", "checking", "removed", "elif", "requests", "follow", "errback", "process", "links", "settings", "isinstance", "extractor", "results"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis modules implements the CrawlSpider which is the recommended spider to use\nfor scraping typical websites that requires crawling pages.\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport warnings\nfrom collections.abc import AsyncIterator, Awaitable, Callable\nfrom typing import TYPE_CHECKING, Any, Optional, TypeVar, cast\n\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.link import Link\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.deprecate import method_is_overridden\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http.request import CallbackT\n\n\n_T = TypeVar(\"_T\")\nProcessLinksT = Callable[[list[Link]], list[Link]]\nProcessRequestT = Callable[[Request, Response], Optional[Request]]\n\n\ndef _identity(x: _T) -> _T:\n    return x\n\n\ndef _identity_process_request(request: Request, response: Response) -> Request | None:\n    return request\n\n\ndef _get_method(method: Callable | str | None, spider: Spider) -> Callable | None:\n    if callable(method):\n        return method\n    if isinstance(method, str):\n        return getattr(spider, method, None)\n    return None\n\n\n_default_link_extractor = LinkExtractor()\n\n\nclass Rule:\n    def __init__(\n        self,\n        link_extractor: LinkExtractor | None = None,\n        callback: CallbackT | str | None = None,\n        cb_kwargs: dict[str, Any] | None = None,\n        follow: bool | None = None,\n        process_links: ProcessLinksT | str | None = None,\n        process_request: ProcessRequestT | str | None = None,\n        errback: Callable[[Failure], Any] | str | None = None,\n    ):\n        self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n        self.callback: CallbackT | str | None = callback\n        self.errback: Callable[[Failure], Any] | str | None = errback\n        self.cb_kwargs: dict[str, Any] = cb_kwargs or {}\n        self.process_links: ProcessLinksT | str = process_links or _identity\n        self.process_request: ProcessRequestT | str = (\n            process_request or _identity_process_request\n        )\n        self.follow: bool = follow if follow is not None else not callback\n\n    def _compile(self, spider: Spider) -> None:\n        # this replaces method names with methods and we can't express this in type hints\n        self.callback = cast(\"CallbackT\", _get_method(self.callback, spider))\n        self.errback = cast(\n            \"Callable[[Failure], Any]\", _get_method(self.errback, spider)\n        )\n        self.process_links = cast(\n            \"ProcessLinksT\", _get_method(self.process_links, spider)\n        )\n        self.process_request = cast(\n            \"ProcessRequestT\", _get_method(self.process_request, spider)\n        )\n\n\nclass CrawlSpider(Spider):\n    rules: Sequence[Rule] = ()\n    _rules: list[Rule]\n    _follow_links: bool\n\n    def __init__(self, *a: Any, **kw: Any):\n        super().__init__(*a, **kw)\n        self._compile_rules()\n        if method_is_overridden(self.__class__, CrawlSpider, \"_parse_response\"):\n            warnings.warn(\n                f\"The CrawlSpider._parse_response method, which the \"\n                f\"{global_object_name(self.__class__)} class overrides, is \"\n                f\"deprecated: it will be removed in future Scrapy releases. \"\n                f\"Please override the CrawlSpider.parse_with_rules method \"\n                f\"instead.\"\n            )\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        return self.parse_with_rules(\n            response=response,\n            callback=self.parse_start_url,\n            cb_kwargs=kwargs,\n            follow=True,\n        )\n\n    def parse_start_url(self, response: Response, **kwargs: Any) -> Any:\n        return []\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        return results\n\n    def _build_request(self, rule_index: int, link: Link) -> Request:\n        return Request(\n            url=link.url,\n            callback=self._callback,\n            errback=self._errback,\n            meta={\"rule\": rule_index, \"link_text\": link.text},\n        )\n\n    def _requests_to_follow(self, response: Response) -> Iterable[Request | None]:\n        if not isinstance(response, HtmlResponse):\n            return\n        seen: set[Link] = set()\n        for rule_index, rule in enumerate(self._rules):\n            links: list[Link] = [\n                lnk\n                for lnk in rule.link_extractor.extract_links(response)\n                if lnk not in seen\n            ]\n            for link in cast(\"ProcessLinksT\", rule.process_links)(links):\n                seen.add(link)\n                request = self._build_request(rule_index, link)\n                yield cast(\"ProcessRequestT\", rule.process_request)(request, response)\n\n    def _callback(self, response: Response, **cb_kwargs: Any) -> Any:\n        rule = self._rules[cast(\"int\", response.meta[\"rule\"])]\n        return self.parse_with_rules(\n            response,\n            cast(\"CallbackT\", rule.callback),\n            {**rule.cb_kwargs, **cb_kwargs},\n            rule.follow,\n        )\n", "n_tokens": 1210, "byte_len": 5529, "file_sha1": "283f4d27e461791eb7ebdbbd0142a6d56f9fd3d5", "start_line": 1, "end_line": 159}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/crawl.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/crawl.py", "rel_path": "scrapy/spiders/crawl.py", "module": "scrapy.spiders.crawl", "ext": "py", "chunk_number": 2, "symbols": ["_errback", "_parse_response", "_handle_failure", "_compile_rules", "from_crawler", "failure", "method", "async", "will", "bool", "await", "getbool", "parse", "with", "kwargs", "append", "cast", "spider", "future", "please", "res", "return", "dict", "args", "meta", "instead", "crawl", "attr", "ignore", "defined", "_identity", "_identity_process_request", "_get_method", "__init__", "_compile", "_parse", "parse_start_url", "process_results", "_build_request", "_requests_to_follow", "_callback", "Rule", "CrawlSpider", "default", "link", "seen", "python", "pages", "rules", "deprecated"], "ast_kind": "function_or_method", "text": "    def _errback(self, failure: Failure) -> Iterable[Any]:\n        rule = self._rules[cast(\"int\", failure.request.meta[\"rule\"])]  # type: ignore[attr-defined]\n        return self._handle_failure(\n            failure, cast(\"Callable[[Failure], Any]\", rule.errback)\n        )\n\n    async def parse_with_rules(\n        self,\n        response: Response,\n        callback: CallbackT | None,\n        cb_kwargs: dict[str, Any],\n        follow: bool = True,\n    ) -> AsyncIterator[Any]:\n        if callback:\n            cb_res = callback(response, **cb_kwargs) or ()\n            if isinstance(cb_res, AsyncIterator):\n                cb_res = await collect_asyncgen(cb_res)\n            elif isinstance(cb_res, Awaitable):\n                cb_res = await cb_res\n            cb_res = self.process_results(response, cb_res)\n            for request_or_item in iterate_spider_output(cb_res):\n                yield request_or_item\n\n        if follow and self._follow_links:\n            for request_or_item in self._requests_to_follow(response):\n                yield request_or_item\n\n    def _parse_response(\n        self,\n        response: Response,\n        callback: CallbackT | None,\n        cb_kwargs: dict[str, Any],\n        follow: bool = True,\n    ) -> AsyncIterator[Any]:\n        warnings.warn(\n            \"The CrawlSpider._parse_response method is deprecated: \"\n            \"it will be removed in future Scrapy releases. \"\n            \"Please use the CrawlSpider.parse_with_rules method instead.\",\n            stacklevel=2,\n        )\n        return self.parse_with_rules(response, callback, cb_kwargs, follow)\n\n    def _handle_failure(\n        self, failure: Failure, errback: Callable[[Failure], Any] | None\n    ) -> Iterable[Any]:\n        if errback:\n            results = errback(failure) or ()\n            yield from iterate_spider_output(results)\n\n    def _compile_rules(self) -> None:\n        self._rules = []\n        for rule in self.rules:\n            self._rules.append(copy.copy(rule))\n            self._rules[-1]._compile(self)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool(\"CRAWLSPIDER_FOLLOW_LINKS\")\n        return spider\n", "n_tokens": 504, "byte_len": 2300, "file_sha1": "283f4d27e461791eb7ebdbbd0142a6d56f9fd3d5", "start_line": 160, "end_line": 220}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/sitemap.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/sitemap.py", "rel_path": "scrapy/spiders/sitemap.py", "module": "scrapy.spiders.sitemap", "ext": "py", "chunk_number": 1, "symbols": ["from_crawler", "__init__", "start_requests", "sitemap_filter", "_parse_sitemap", "_get_sitemap_body", "regex", "SitemapSpider", "sitemap", "spider", "method", "async", "bool", "append", "python", "robots", "uncompressed", "size", "after", "decompression", "max", "spiders", "future", "typ", "checking", "elif", "rules", "date", "sent", "start", "iterloc", "alternate", "requests", "settings", "annotation", "isinstance", "warn", "here", "than", "encoding", "none", "word", "base", "url", "docs", "type", "gzipped", "without", "parse", "http"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport re\n\n# Iterable is needed at the run time for the SitemapSpider._parse_sitemap() annotation\nfrom collections.abc import AsyncIterator, Iterable, Sequence  # noqa: TC003\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom scrapy.http import Request, Response, XmlResponse\nfrom scrapy.spiders import Spider\nfrom scrapy.utils._compression import _DecompressionMaxSizeExceeded\nfrom scrapy.utils.gz import gunzip, gzip_magic_number\nfrom scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http.request import CallbackT\n\nlogger = logging.getLogger(__name__)\n\n\nclass SitemapSpider(Spider):\n    sitemap_urls: Sequence[str] = ()\n    sitemap_rules: Sequence[tuple[re.Pattern[str] | str, str | CallbackT]] = [\n        (\"\", \"parse\")\n    ]\n    sitemap_follow: Sequence[re.Pattern[str] | str] = [\"\"]\n    sitemap_alternate_links: bool = False\n    _max_size: int\n    _warn_size: int\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._max_size = getattr(\n            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\n        )\n        spider._warn_size = getattr(\n            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\n        )\n        return spider\n\n    def __init__(self, *a: Any, **kw: Any):\n        super().__init__(*a, **kw)\n        self._cbs: list[tuple[re.Pattern[str], CallbackT]] = []\n        for r, c in self.sitemap_rules:\n            if isinstance(c, str):\n                c = cast(\"CallbackT\", getattr(self, c))\n            self._cbs.append((regex(r), c))\n        self._follow: list[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n\n    async def start(self) -> AsyncIterator[Any]:\n        for item_or_request in self.start_requests():\n            yield item_or_request\n\n    def start_requests(self) -> Iterable[Request]:\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)\n\n    def sitemap_filter(\n        self, entries: Iterable[dict[str, Any]]\n    ) -> Iterable[dict[str, Any]]:\n        \"\"\"This method can be used to filter sitemap entries by their\n        attributes, for example, you can filter locs with lastmod greater\n        than a given date (see docs).\n        \"\"\"\n        yield from entries\n\n    def _parse_sitemap(self, response: Response) -> Iterable[Request]:\n        if response.url.endswith(\"/robots.txt\"):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\n                    \"Ignoring invalid sitemap: %(response)s\",\n                    {\"response\": response},\n                    extra={\"spider\": self},\n                )\n                return\n\n            s = Sitemap(body)\n            it = self.sitemap_filter(s)\n\n            if s.type == \"sitemapindex\":\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == \"urlset\":\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break\n\n    def _get_sitemap_body(self, response: Response) -> bytes | None:\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        if gzip_magic_number(response):\n            uncompressed_size = len(response.body)\n            max_size = response.meta.get(\"download_maxsize\", self._max_size)\n            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\n            try:\n                body = gunzip(response.body, max_size=max_size)\n            except _DecompressionMaxSizeExceeded:\n                return None\n            if uncompressed_size < warn_size <= len(body):\n                logger.warning(\n                    f\"{response} body size after decompression ({len(body)} B) \"\n                    f\"is larger than the download warning size ({warn_size} B).\"\n                )\n            return body\n        # actual gzipped sitemap files are decompressed above ;\n        # if we are here (response body is not gzipped)\n        # and have a response for .xml.gz,\n        # it usually means that it was already gunzipped\n        # by HttpCompression middleware,\n        # the HTTP response being sent with \"Content-Encoding: gzip\"\n        # without actually being a .xml.gz file in the first place,\n        # merely XML gzip-compressed on the fly,\n        # in other word, here, we have plain XML\n        if response.url.endswith(\".xml\") or response.url.endswith(\".xml.gz\"):\n            return response.body\n        return None\n\n\ndef regex(x: re.Pattern[str] | str) -> re.Pattern[str]:\n    if isinstance(x, str):\n        return re.compile(x)\n    return x\n\n", "n_tokens": 1205, "byte_len": 5477, "file_sha1": "f8558e00b0d634d97f6bb438fc17ed915b31862b", "start_line": 1, "end_line": 140}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/sitemap.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/sitemap.py", "rel_path": "scrapy/spiders/sitemap.py", "module": "scrapy.spiders.sitemap", "ext": "py", "chunk_number": 2, "symbols": ["iterloc", "consider", "yield", "from", "bool", "false", "link", "iterable", "xhtml", "alternate", "also", "urls", "dict", "from_crawler", "__init__", "start_requests", "sitemap_filter", "_parse_sitemap", "_get_sitemap_body", "regex", "SitemapSpider", "sitemap", "spider", "method", "async", "append", "python", "robots", "uncompressed", "size", "after", "decompression", "max", "spiders", "future", "typ", "checking", "elif", "rules", "date", "sent", "start", "requests", "settings", "annotation", "isinstance", "warn", "here", "than", "encoding"], "ast_kind": "function_or_method", "text": "def iterloc(it: Iterable[dict[str, Any]], alt: bool = False) -> Iterable[str]:\n    for d in it:\n        yield d[\"loc\"]\n\n        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n        if alt and \"alternate\" in d:\n            yield from d[\"alternate\"]\n", "n_tokens": 66, "byte_len": 263, "file_sha1": "f8558e00b0d634d97f6bb438fc17ed915b31862b", "start_line": 141, "end_line": 148}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/__init__.py", "rel_path": "scrapy/spiders/__init__.py", "module": "scrapy.spiders.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "logger", "log", "from_crawler", "_set_crawler", "start_requests", "_parse", "parse", "update_settings", "handles_request", "Spider", "MySpider", "method", "async", "bool", "handles", "request", "subclass", "each", "python", "miss", "spider", "name", "iterate", "initial", "enabled", "dont", "filter", "url", "from", "close", "__repr__", "sitemap", "deprecated", "spiders", "future", "typ", "checking", "elif", "https", "start", "requests", "settings", "imports", "lower", "items", "than", "none", "docs", "type"], "ast_kind": "class_or_type", "text": "\"\"\"\nBase class for Scrapy spiders\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport warnings\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom scrapy import signals\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import url_is_from_spider\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Iterable\n\n    from twisted.internet.defer import Deferred\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http.request import CallbackT\n    from scrapy.settings import BaseSettings, _SettingsKeyT\n    from scrapy.utils.log import SpiderLoggerAdapter\n\n\nclass Spider(object_ref):\n    \"\"\"Base class that any spider must subclass.\n\n    It provides a default :meth:`start` implementation that sends\n    requests based on the :attr:`start_urls` class attribute and calls the\n    :meth:`parse` method for each response.\n    \"\"\"\n\n    name: str\n    custom_settings: dict[_SettingsKeyT, Any] | None = None\n\n    #: Start URLs. See :meth:`start`.\n    start_urls: list[str]\n\n    def __init__(self, name: str | None = None, **kwargs: Any):\n        if name is not None:\n            self.name: str = name\n        elif not getattr(self, \"name\", None):\n            raise ValueError(f\"{type(self).__name__} must have a name\")\n        self.__dict__.update(kwargs)\n        if not hasattr(self, \"start_urls\"):\n            self.start_urls: list[str] = []\n\n    @property\n    def logger(self) -> SpiderLoggerAdapter:\n        # circular import\n        from scrapy.utils.log import SpiderLoggerAdapter  # noqa: PLC0415\n\n        logger = logging.getLogger(self.name)\n        return SpiderLoggerAdapter(logger, {\"spider\": self})\n\n    def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider\n\n    def _set_crawler(self, crawler: Crawler) -> None:\n        self.crawler: Crawler = crawler\n        self.settings: BaseSettings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)\n\n    async def start(self) -> AsyncIterator[Any]:\n        \"\"\"Yield the initial :class:`~scrapy.Request` objects to send.\n\n        .. versionadded:: 2.13\n\n        For example:\n\n        .. code-block:: python\n\n            from scrapy import Request, Spider\n\n\n            class MySpider(Spider):\n                name = \"myspider\"\n\n                async def start(self):\n                    yield Request(\"https://toscrape.com/\")\n\n        The default implementation reads URLs from :attr:`start_urls` and\n        yields a request for each with :attr:`~scrapy.Request.dont_filter`\n        enabled. It is functionally equivalent to:\n\n        .. code-block:: python\n\n            async def start(self):\n                for url in self.start_urls:\n                    yield Request(url, dont_filter=True)\n\n        You can also yield :ref:`items <topics-items>`. For example:\n\n        .. code-block:: python\n\n            async def start(self):\n                yield {\"foo\": \"bar\"}\n\n        To write spiders that work on Scrapy versions lower than 2.13,\n        define also a synchronous ``start_requests()`` method that returns an\n        iterable. For example:\n\n        .. code-block:: python\n\n            def start_requests(self):\n                yield Request(\"https://toscrape.com/\")\n\n        .. seealso:: :ref:`start-requests`\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", category=ScrapyDeprecationWarning, module=r\"^scrapy\\.spiders$\"\n            )\n            for item_or_request in self.start_requests():\n                yield item_or_request\n\n    def start_requests(self) -> Iterable[Any]:\n        warnings.warn(\n            (\n                \"The Spider.start_requests() method is deprecated, use \"\n                \"Spider.start() instead. If you are calling \"\n                \"super().start_requests() from a Spider.start() override, \"\n                \"iterate super().start() instead.\"\n            ),\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        if not self.start_urls and hasattr(self, \"start_url\"):\n            raise AttributeError(\n                \"Crawling could not start: 'start_urls' not found \"\n                \"or empty (but found 'start_url' attribute instead, \"\n                \"did you miss an 's'?)\"\n            )\n        for url in self.start_urls:\n            yield Request(url, dont_filter=True)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        return self.parse(response, **kwargs)\n\n    if TYPE_CHECKING:\n        parse: CallbackT\n    else:\n\n        def parse(self, response: Response, **kwargs: Any) -> Any:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__}.parse callback is not defined\"\n            )\n\n    @classmethod\n    def update_settings(cls, settings: BaseSettings) -> None:\n        settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n\n    @classmethod\n    def handles_request(cls, request: Request) -> bool:\n        return url_is_from_spider(request.url, cls)\n", "n_tokens": 1242, "byte_len": 5719, "file_sha1": "aa6cec0b81f9a41fd934b4beaf2f41307678d157", "start_line": 1, "end_line": 176}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/__init__.py", "rel_path": "scrapy/spiders/__init__.py", "module": "scrapy.spiders.__init__", "ext": "py", "chunk_number": 2, "symbols": ["close", "__repr__", "sitemap", "spider", "cast", "return", "name", "crawl", "spiders", "scrapy", "staticmethod", "all", "repr", "from", "closed", "imports", "getattr", "none", "csv", "feed", "type", "level", "reason", "import", "rule", "self", "xml", "deferred", "callable", "__init__", "logger", "log", "from_crawler", "_set_crawler", "start_requests", "_parse", "parse", "update_settings", "handles_request", "Spider", "MySpider", "method", "async", "bool", "handles", "request", "subclass", "each", "python", "miss"], "ast_kind": "function_or_method", "text": "    @staticmethod\n    def close(spider: Spider, reason: str) -> Deferred[None] | None:\n        closed = getattr(spider, \"closed\", None)\n        if callable(closed):\n            return cast(\"Deferred[None] | None\", closed(reason))\n        return None\n\n    def __repr__(self) -> str:\n        return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"\n\n\n# Top-level imports\nfrom scrapy.spiders.crawl import CrawlSpider, Rule\nfrom scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\nfrom scrapy.spiders.sitemap import SitemapSpider\n\n__all__ = [\n    \"CSVFeedSpider\",\n    \"CrawlSpider\",\n    \"Rule\",\n    \"SitemapSpider\",\n    \"Spider\",\n    \"XMLFeedSpider\",\n]\n", "n_tokens": 180, "byte_len": 663, "file_sha1": "aa6cec0b81f9a41fd934b4beaf2f41307678d157", "start_line": 177, "end_line": 201}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/feed.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/feed.py", "rel_path": "scrapy/spiders/feed.py", "module": "scrapy.spiders.feed", "ext": "py", "chunk_number": 1, "symbols": ["process_results", "adapt_response", "parse_node", "parse_nodes", "_parse", "_iternodes", "_register_namespaces", "parse_row", "XMLFeedSpider", "CSVFeedSpider", "originated", "method", "parse", "row", "your", "containing", "register", "namespaces", "xpath", "unsupported", "each", "spider", "selector", "name", "iternodes", "spiders", "future", "typ", "checking", "faster", "parse_rows", "elif", "regarding", "make", "changes", "adapt", "response", "scrape", "order", "provided", "items", "isinstance", "them", "results", "none", "docs", "html", "type", "since", "methods"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the XMLFeedSpider which is the recommended spider to use\nfor scraping from an XML feed.\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.exceptions import NotConfigured, NotSupported\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.selector import Selector\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.iterators import csviter, xmliter_lxml\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n\n\nclass XMLFeedSpider(Spider):\n    \"\"\"\n    This class intends to be the base class for spiders that scrape\n    from XML feeds.\n\n    You can choose whether to parse the file using the 'iternodes' iterator, an\n    'xml' selector, or an 'html' selector.  In most cases, it's convenient to\n    use iternodes, since it's a faster and cleaner.\n    \"\"\"\n\n    iterator: str = \"iternodes\"\n    itertag: str = \"item\"\n    namespaces: Sequence[tuple[str, str]] = ()\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (items or requests).\n        \"\"\"\n        return results\n\n    def adapt_response(self, response: Response) -> Response:\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response\n\n    def parse_node(self, response: Response, selector: Selector) -> Any:\n        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n        if hasattr(self, \"parse_item\"):  # backward compatibility\n            return self.parse_item(response, selector)\n        raise NotImplementedError\n\n    def parse_nodes(self, response: Response, nodes: Iterable[Selector]) -> Any:\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either an item, a request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            yield from self.process_results(response, ret)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        if not hasattr(self, \"parse_node\"):\n            raise NotConfigured(\n                \"You must define parse_node method in order to scrape this XML feed\"\n            )\n\n        response = self.adapt_response(response)\n        nodes: Iterable[Selector]\n        if self.iterator == \"iternodes\":\n            nodes = self._iternodes(response)\n        elif self.iterator == \"xml\":\n            if not isinstance(response, TextResponse):\n                raise ValueError(\"Response content isn't text\")\n            selector = Selector(response, type=\"xml\")\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f\"//{self.itertag}\")\n        elif self.iterator == \"html\":\n            if not isinstance(response, TextResponse):\n                raise ValueError(\"Response content isn't text\")\n            selector = Selector(response, type=\"html\")\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f\"//{self.itertag}\")\n        else:\n            raise NotSupported(\"Unsupported node iterator\")\n\n        return self.parse_nodes(response, nodes)\n\n    def _iternodes(self, response: Response) -> Iterable[Selector]:\n        for node in xmliter_lxml(response, self.itertag):\n            self._register_namespaces(node)\n            yield node\n\n    def _register_namespaces(self, selector: Selector) -> None:\n        for prefix, uri in self.namespaces:\n            selector.register_namespace(prefix, uri)\n\n\nclass CSVFeedSpider(Spider):\n    \"\"\"Spider for parsing CSV feeds.\n    It receives a CSV file in a response; iterates through each of its rows,\n    and calls parse_row with a dict containing each field's data.\n\n    You can set some options regarding the CSV file, such as the delimiter, quotechar\n    and the file's headers.\n    \"\"\"\n\n    delimiter: str | None = (\n        None  # When this is None, python's csv module's default delimiter is used\n    )\n    quotechar: str | None = (\n        None  # When this is None, python's csv module's default quotechar is used\n    )\n    headers: list[str] | None = None\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return results\n\n    def adapt_response(self, response: Response) -> Response:\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return response\n\n    def parse_row(self, response: Response, row: dict[str, str]) -> Any:\n        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n        raise NotImplementedError\n", "n_tokens": 1151, "byte_len": 5481, "file_sha1": "4718174df914cb3d3aeed550965c76d80be00b39", "start_line": 1, "end_line": 141}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/feed.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/feed.py", "rel_path": "scrapy/spiders/feed.py", "module": "scrapy.spiders.feed", "ext": "py", "chunk_number": 2, "symbols": ["parse_rows", "_parse", "method", "parse", "row", "hasattr", "representing", "purposes", "processing", "spider", "opportunity", "each", "define", "override", "dict", "return", "delimiter", "quotechar", "with", "rows", "csviter", "headers", "gives", "not", "configured", "also", "adapt", "response", "yield", "scrape", "process_results", "adapt_response", "parse_node", "parse_nodes", "_iternodes", "_register_namespaces", "parse_row", "XMLFeedSpider", "CSVFeedSpider", "originated", "your", "containing", "register", "namespaces", "xpath", "unsupported", "selector", "name", "iternodes", "spiders"], "ast_kind": "function_or_method", "text": "    def parse_rows(self, response: Response) -> Any:\n        \"\"\"Receives a response and a dict (representing each row) with a key for\n        each provided (or detected) header of the CSV file.  This spider also\n        gives the opportunity to override adapt_response and\n        process_results methods for pre and post-processing purposes.\n        \"\"\"\n\n        for row in csviter(\n            response, self.delimiter, self.headers, quotechar=self.quotechar\n        ):\n            ret = iterate_spider_output(self.parse_row(response, row))\n            yield from self.process_results(response, ret)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        if not hasattr(self, \"parse_row\"):\n            raise NotConfigured(\n                \"You must define parse_row method in order to scrape this CSV feed\"\n            )\n        response = self.adapt_response(response)\n        return self.parse_rows(response)\n", "n_tokens": 192, "byte_len": 931, "file_sha1": "4718174df914cb3d3aeed550965c76d80be00b39", "start_line": 142, "end_line": 162}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/init.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spiders/init.py", "rel_path": "scrapy/spiders/init.py", "module": "scrapy.spiders.init", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "start_requests", "initialized", "init_request", "InitSpider", "method", "async", "immediately", "your", "initialize", "future", "spider", "deprecated", "spiders", "removed", "typ", "checking", "more", "start", "requests", "docstring", "none", "code", "http", "default", "facilities", "response", "initializing", "overridden", "typing", "return", "annotations", "class", "when", "ignore", "several", "warnings", "function", "filterwarnings", "kwargs", "will", "exceptions", "super", "catch", "calls", "iterate", "this", "self", "item", "request"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Iterable\n\n    from scrapy import Request\n    from scrapy.http import Response\n\n\nclass InitSpider(Spider):\n    \"\"\"Base Spider with initialization facilities\n\n    .. warning:: This class is deprecated. Copy its code into your project if needed.\n    It will be removed in a future Scrapy version.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        warnings.warn(\n            \"InitSpider is deprecated. Copy its code from Scrapy's source if needed. \"\n            \"Will be removed in a future version.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n    async def start(self) -> AsyncIterator[Any]:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", category=ScrapyDeprecationWarning, module=r\"^scrapy\\.spiders$\"\n            )\n            for item_or_request in self.start_requests():\n                yield item_or_request\n\n    def start_requests(self) -> Iterable[Request]:\n        self._postinit_reqs: Iterable[Request] = super().start_requests()\n        return cast(\"Iterable[Request]\", iterate_spider_output(self.init_request()))\n\n    def initialized(self, response: Response | None = None) -> Any:\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop(\"_postinit_reqs\")\n\n    def init_request(self) -> Any:\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()\n", "n_tokens": 509, "byte_len": 2522, "file_sha1": "ed985c3730c861eb2e2c8599b1e3323d58d53738", "start_line": 1, "end_line": 65}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py", "rel_path": "scrapy/pipelines/files.py", "module": "scrapy.pipelines.files", "ext": "py", "chunk_number": 1, "symbols": ["_to_string", "_md5sum", "__init__", "persist_file", "stat_file", "_get_filesystem_path", "_mkdir", "FileException", "StatInfo", "FilesStoreProtocol", "FSFilesStore", "S3FilesStore", "failure", "library", "files", "stat", "seen", "boto", "headers", "python", "mimetypes", "spider", "callback", "domain", "error", "oserror", "future", "typ", "checking", "string", "_onsuccess", "_get_boto_key", "_headers_to_botocore_kwargs", "_get_content_type", "_get_blob_path", "_stat_file", "from_settings", "from_crawler", "_from_settings", "_update_stores", "_get_store", "media_to_download", "media_failed", "media_downloaded", "inc_stats", "get_media_requests", "file_downloaded", "item_completed", "file_path", "GCSFilesStore"], "ast_kind": "class_or_type", "text": "\"\"\"\nFiles Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport functools\nimport hashlib\nimport logging\nimport mimetypes\nimport time\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom ftplib import FTP\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import IO, TYPE_CHECKING, Any, NoReturn, Protocol, TypedDict, cast\nfrom urllib.parse import urlparse\n\nfrom itemadapter import ItemAdapter\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom twisted.internet.threads import deferToThread\n\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.datatypes import CaseInsensitiveDict\nfrom scrapy.utils.deprecate import method_is_overridden\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.python import get_func_args, global_object_name, to_bytes\nfrom scrapy.utils.request import referer_str\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n    from os import PathLike\n\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _to_string(path: str | PathLike[str]) -> str:\n    return str(path)  # convert a Path object to string\n\n\ndef _md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> _md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()  # noqa: S324\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\nclass FileException(Exception):\n    \"\"\"General media error exception\"\"\"\n\n\nclass StatInfo(TypedDict, total=False):\n    checksum: str\n    last_modified: float\n\n\nclass FilesStoreProtocol(Protocol):\n    def __init__(self, basedir: str): ...\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: dict[str, Any] | None = None,\n        headers: dict[str, str] | None = None,\n    ) -> Deferred[Any] | None: ...\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> StatInfo | Deferred[StatInfo]: ...\n\n\nclass FSFilesStore:\n    def __init__(self, basedir: str | PathLike[str]):\n        basedir = _to_string(basedir)\n        if \"://\" in basedir:\n            basedir = basedir.split(\"://\", 1)[1]\n        self.basedir: str = basedir\n        self._mkdir(Path(self.basedir))\n        self.created_directories: defaultdict[MediaPipeline.SpiderInfo, set[str]] = (\n            defaultdict(set)\n        )\n\n    def persist_file(\n        self,\n        path: str | PathLike[str],\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: dict[str, Any] | None = None,\n        headers: dict[str, str] | None = None,\n    ) -> None:\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(absolute_path.parent, info)\n        absolute_path.write_bytes(buf.getvalue())\n\n    def stat_file(\n        self, path: str | PathLike[str], info: MediaPipeline.SpiderInfo\n    ) -> StatInfo:\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = absolute_path.stat().st_mtime\n        except OSError:\n            return {}\n\n        with absolute_path.open(\"rb\") as f:\n            checksum = _md5sum(f)\n\n        return {\"last_modified\": last_modified, \"checksum\": checksum}\n\n    def _get_filesystem_path(self, path: str | PathLike[str]) -> Path:\n        path_comps = _to_string(path).split(\"/\")\n        return Path(self.basedir, *path_comps)\n\n    def _mkdir(\n        self, dirname: Path, domain: MediaPipeline.SpiderInfo | None = None\n    ) -> None:\n        seen: set[str] = self.created_directories[domain] if domain else set()\n        if str(dirname) not in seen:\n            if not dirname.exists():\n                dirname.mkdir(parents=True)\n            seen.add(str(dirname))\n\n\nclass S3FilesStore:\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n    AWS_SESSION_TOKEN = None\n    AWS_ENDPOINT_URL = None\n    AWS_REGION_NAME = None\n    AWS_USE_SSL = None\n    AWS_VERIFY = None\n\n    POLICY = \"private\"  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n    HEADERS = {\n        \"Cache-Control\": \"max-age=172800\",\n    }\n\n    def __init__(self, uri: str):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")", "n_tokens": 1184, "byte_len": 5001, "file_sha1": "1603438b3ed3de7fa12ea60cf1f152c2f57b93b8", "start_line": 1, "end_line": 171}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py", "rel_path": "scrapy/pipelines/files.py", "module": "scrapy.pipelines.files", "ext": "py", "chunk_number": 2, "symbols": ["stat_file", "_onsuccess", "_get_boto_key", "persist_file", "_headers_to_botocore_kwargs", "GCSFilesStore", "read", "lock", "create", "client", "boto", "headers", "grant", "full", "mode", "algorithm", "cache", "control", "hold", "body", "regio", "name", "put", "object", "until", "settings", "ssl", "aws", "mktime", "items", "_to_string", "_md5sum", "__init__", "_get_filesystem_path", "_mkdir", "_get_content_type", "_get_blob_path", "_stat_file", "from_settings", "from_crawler", "_from_settings", "_update_stores", "_get_store", "media_to_download", "media_failed", "media_downloaded", "inc_stats", "get_media_requests", "file_downloaded", "item_completed"], "ast_kind": "class_or_type", "text": "        import botocore.session  # noqa: PLC0415\n\n        session = botocore.session.get_session()\n        self.s3_client = session.create_client(\n            \"s3\",\n            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n            aws_session_token=self.AWS_SESSION_TOKEN,\n            endpoint_url=self.AWS_ENDPOINT_URL,\n            region_name=self.AWS_REGION_NAME,\n            use_ssl=self.AWS_USE_SSL,\n            verify=self.AWS_VERIFY,\n        )\n        if not uri.startswith(\"s3://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n        self.bucket, self.prefix = uri[5:].split(\"/\", 1)\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _onsuccess(boto_key: dict[str, Any]) -> StatInfo:\n            checksum = boto_key[\"ETag\"].strip('\"')\n            last_modified = boto_key[\"LastModified\"]\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {\"checksum\": checksum, \"last_modified\": modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)\n\n    def _get_boto_key(self, path: str) -> Deferred[dict[str, Any]]:\n        key_name = f\"{self.prefix}{path}\"\n        return cast(\n            \"Deferred[dict[str, Any]]\",\n            deferToThread(\n                self.s3_client.head_object,  # type: ignore[attr-defined]\n                Bucket=self.bucket,\n                Key=key_name,\n            ),\n        )\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: dict[str, Any] | None = None,\n        headers: dict[str, str] | None = None,\n    ) -> Deferred[Any]:\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = f\"{self.prefix}{path}\"\n        buf.seek(0)\n        extra = self._headers_to_botocore_kwargs(self.HEADERS)\n        if headers:\n            extra.update(self._headers_to_botocore_kwargs(headers))\n        return deferToThread(\n            self.s3_client.put_object,  # type: ignore[attr-defined]\n            Bucket=self.bucket,\n            Key=key_name,\n            Body=buf,\n            Metadata={k: str(v) for k, v in (meta or {}).items()},\n            ACL=self.POLICY,\n            **extra,\n        )\n\n    def _headers_to_botocore_kwargs(self, headers: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaseInsensitiveDict(\n            {\n                \"Content-Type\": \"ContentType\",\n                \"Cache-Control\": \"CacheControl\",\n                \"Content-Disposition\": \"ContentDisposition\",\n                \"Content-Encoding\": \"ContentEncoding\",\n                \"Content-Language\": \"ContentLanguage\",\n                \"Content-Length\": \"ContentLength\",\n                \"Content-MD5\": \"ContentMD5\",\n                \"Expires\": \"Expires\",\n                \"X-Amz-Grant-Full-Control\": \"GrantFullControl\",\n                \"X-Amz-Grant-Read\": \"GrantRead\",\n                \"X-Amz-Grant-Read-ACP\": \"GrantReadACP\",\n                \"X-Amz-Grant-Write-ACP\": \"GrantWriteACP\",\n                \"X-Amz-Object-Lock-Legal-Hold\": \"ObjectLockLegalHoldStatus\",\n                \"X-Amz-Object-Lock-Mode\": \"ObjectLockMode\",\n                \"X-Amz-Object-Lock-Retain-Until-Date\": \"ObjectLockRetainUntilDate\",\n                \"X-Amz-Request-Payer\": \"RequestPayer\",\n                \"X-Amz-Server-Side-Encryption\": \"ServerSideEncryption\",\n                \"X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id\": \"SSEKMSKeyId\",\n                \"X-Amz-Server-Side-Encryption-Context\": \"SSEKMSEncryptionContext\",\n                \"X-Amz-Server-Side-Encryption-Customer-Algorithm\": \"SSECustomerAlgorithm\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key\": \"SSECustomerKey\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key-Md5\": \"SSECustomerKeyMD5\",\n                \"X-Amz-Storage-Class\": \"StorageClass\",\n                \"X-Amz-Tagging\": \"Tagging\",\n                \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n            }\n        )\n        extra: dict[str, Any] = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n            extra[kwarg] = value\n        return extra\n\n\nclass GCSFilesStore:\n    GCS_PROJECT_ID = None\n\n    CACHE_CONTROL = \"max-age=172800\"\n\n    # The bucket's default object ACL will be applied to the object.\n    # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n    POLICY = None\n", "n_tokens": 1108, "byte_len": 4747, "file_sha1": "1603438b3ed3de7fa12ea60cf1f152c2f57b93b8", "start_line": 172, "end_line": 285}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py", "rel_path": "scrapy/pipelines/files.py", "module": "scrapy.pipelines.files", "ext": "py", "chunk_number": 3, "symbols": ["__init__", "stat_file", "_onsuccess", "_get_content_type", "_get_blob_path", "persist_file", "_stat_file", "FTPFilesStore", "files", "bool", "stream", "doesn", "port", "upload", "from", "date", "username", "saving", "google", "mktime", "items", "defer", "thread", "exist", "none", "projec", "gcs", "content", "type", "predefined", "_to_string", "_md5sum", "_get_filesystem_path", "_mkdir", "_get_boto_key", "_headers_to_botocore_kwargs", "from_settings", "from_crawler", "_from_settings", "_update_stores", "_get_store", "media_to_download", "media_failed", "media_downloaded", "inc_stats", "get_media_requests", "file_downloaded", "item_completed", "file_path", "FileException"], "ast_kind": "class_or_type", "text": "    def __init__(self, uri: str):\n        from google.cloud import storage  # noqa: PLC0415\n\n        client = storage.Client(project=self.GCS_PROJECT_ID)\n        bucket, prefix = uri[5:].split(\"/\", 1)\n        self.bucket = client.bucket(bucket)\n        self.prefix: str = prefix\n        permissions = self.bucket.test_iam_permissions(\n            [\"storage.objects.get\", \"storage.objects.create\"]\n        )\n        if \"storage.objects.get\" not in permissions:\n            logger.warning(\n                \"No 'storage.objects.get' permission for GSC bucket %(bucket)s. \"\n                \"Checking if files are up to date will be impossible. Files will be downloaded every time.\",\n                {\"bucket\": bucket},\n            )\n        if \"storage.objects.create\" not in permissions:\n            logger.error(\n                \"No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!\",\n                {\"bucket\": bucket},\n            )\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _onsuccess(blob) -> StatInfo:\n            if blob:\n                checksum = base64.b64decode(blob.md5_hash).hex()\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {\"checksum\": checksum, \"last_modified\": last_modified}\n            return {}\n\n        blob_path = self._get_blob_path(path)\n        return cast(\n            \"Deferred[StatInfo]\",\n            deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess),\n        )\n\n    def _get_content_type(self, headers: dict[str, str] | None) -> str:\n        if headers and \"Content-Type\" in headers:\n            return headers[\"Content-Type\"]\n        return \"application/octet-stream\"\n\n    def _get_blob_path(self, path: str) -> str:\n        return self.prefix + path\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: dict[str, Any] | None = None,\n        headers: dict[str, str] | None = None,\n    ) -> Deferred[Any]:\n        blob_path = self._get_blob_path(path)\n        blob = self.bucket.blob(blob_path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY,\n        )\n\n\nclass FTPFilesStore:\n    FTP_USERNAME: str | None = None\n    FTP_PASSWORD: str | None = None\n    USE_ACTIVE_MODE: bool | None = None\n\n    def __init__(self, uri: str):\n        if not uri.startswith(\"ftp://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 'ftp'\")\n        u = urlparse(uri)\n        assert u.port\n        assert u.hostname\n        self.port: int = u.port\n        self.host: str = u.hostname\n        self.port = int(u.port or 21)\n        assert self.FTP_USERNAME\n        assert self.FTP_PASSWORD\n        self.username: str = u.username or self.FTP_USERNAME\n        self.password: str = u.password or self.FTP_PASSWORD\n        self.basedir: str = u.path.rstrip(\"/\")\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: dict[str, Any] | None = None,\n        headers: dict[str, str] | None = None,\n    ) -> Deferred[Any]:\n        path = f\"{self.basedir}/{path}\"\n        return deferToThread(\n            ftp_store_file,\n            path=path,\n            file=buf,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.USE_ACTIVE_MODE,\n        )\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _stat_file(path: str) -> StatInfo:\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()  # noqa: S324\n                ftp.retrbinary(f\"RETR {file_path}\", m.update)\n                return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n\n        return cast(\"Deferred[StatInfo]\", deferToThread(_stat_file, path))\n\n", "n_tokens": 1037, "byte_len": 4669, "file_sha1": "1603438b3ed3de7fa12ea60cf1f152c2f57b93b8", "start_line": 286, "end_line": 413}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py", "rel_path": "scrapy/pipelines/files.py", "module": "scrapy.pipelines.files", "ext": "py", "chunk_number": 4, "symbols": ["__init__", "from_settings", "from_crawler", "_from_settings", "FilesPipeline", "finish", "init", "processed", "method", "those", "site", "stat", "your", "instance", "case", "made", "future", "spider", "doesn", "files", "result", "deprecated", "removed", "doing", "elif", "expires", "cls", "name", "date", "settings", "_to_string", "_md5sum", "persist_file", "stat_file", "_get_filesystem_path", "_mkdir", "_onsuccess", "_get_boto_key", "_headers_to_botocore_kwargs", "_get_content_type", "_get_blob_path", "_stat_file", "_update_stores", "_get_store", "media_to_download", "media_failed", "media_downloaded", "inc_stats", "get_media_requests", "file_downloaded"], "ast_kind": "class_or_type", "text": "class FilesPipeline(MediaPipeline):\n    \"\"\"Abstract pipeline that implement the file downloading\n\n    This pipeline tries to minimize network transfers and file processing,\n    doing stat of the files and determining if file is new, up-to-date or\n    expired.\n\n    ``new`` files are those that pipeline never processed and needs to be\n        downloaded from supplier site the first time.\n\n    ``uptodate`` files are the ones that the pipeline processed and are still\n        valid files.\n\n    ``expired`` files are those that pipeline already processed but the last\n        modification was made long time ago, so a reprocessing is recommended to\n        refresh it in case of change.\n\n    \"\"\"\n\n    MEDIA_NAME: str = \"file\"\n    EXPIRES: int = 90\n    STORE_SCHEMES: dict[str, type[FilesStoreProtocol]] = {\n        \"\": FSFilesStore,\n        \"file\": FSFilesStore,\n        \"s3\": S3FilesStore,\n        \"gs\": GCSFilesStore,\n        \"ftp\": FTPFilesStore,\n    }\n    DEFAULT_FILES_URLS_FIELD: str = \"file_urls\"\n    DEFAULT_FILES_RESULT_FIELD: str = \"files\"\n\n    def __init__(\n        self,\n        store_uri: str | PathLike[str],\n        download_func: Callable[[Request, Spider], Response] | None = None,\n        settings: Settings | dict[str, Any] | None = None,\n        *,\n        crawler: Crawler | None = None,\n    ):\n        if not (store_uri and (store_uri := _to_string(store_uri))):\n            from scrapy.pipelines.images import ImagesPipeline  # noqa: PLC0415\n\n            setting_name = (\n                \"IMAGES_STORE\" if isinstance(self, ImagesPipeline) else \"FILES_STORE\"\n            )\n            raise NotConfigured(\n                f\"{setting_name} setting must be set to a valid path (not empty) \"\n                f\"to enable {self.__class__.__name__}.\"\n            )\n\n        if crawler is not None:\n            if settings is not None:\n                warnings.warn(\n                    f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n                    category=ScrapyDeprecationWarning,\n                    stacklevel=2,\n                )\n            settings = crawler.settings\n        elif isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        cls_name = \"FilesPipeline\"\n        self.store: FilesStoreProtocol = self._get_store(store_uri)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=cls_name, settings=settings\n        )\n        self.expires: int = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field: str = settings.get(\n            resolve(\"FILES_URLS_FIELD\"), self.FILES_URLS_FIELD\n        )\n        self.files_result_field: str = settings.get(\n            resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n        )\n\n        super().__init__(\n            download_func=download_func,\n            settings=settings if not crawler else None,\n            crawler=crawler,\n        )\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -> Self:\n        warnings.warn(\n            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return cls._from_settings(settings, None)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if method_is_overridden(cls, FilesPipeline, \"from_settings\"):\n            warnings.warn(\n                f\"{global_object_name(cls)} overrides FilesPipeline.from_settings().\"\n                f\" This method is deprecated and won't be called in future Scrapy versions,\"\n                f\" please update your code so that it overrides from_crawler() instead.\",\n                category=ScrapyDeprecationWarning,\n            )\n            o = cls.from_settings(crawler.settings)\n            o._finish_init(crawler)\n            return o\n        return cls._from_settings(crawler.settings, crawler)\n\n    @classmethod\n    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n        cls._update_stores(settings)\n        store_uri = settings[\"FILES_STORE\"]\n        if \"crawler\" in get_func_args(cls.__init__):\n            o = cls(store_uri, crawler=crawler)\n        else:\n            o = cls(store_uri, settings=settings)\n            if crawler:\n                o._finish_init(crawler)\n            warnings.warn(\n                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n                category=ScrapyDeprecationWarning,\n            )\n        return o\n", "n_tokens": 1075, "byte_len": 5143, "file_sha1": "1603438b3ed3de7fa12ea60cf1f152c2f57b93b8", "start_line": 414, "end_line": 538}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py", "rel_path": "scrapy/pipelines/files.py", "module": "scrapy.pipelines.files", "ext": "py", "chunk_number": 5, "symbols": ["_update_stores", "_get_store", "media_to_download", "_onsuccess", "media_failed", "failure", "referer", "error", "inc", "stats", "regio", "name", "path", "overloads", "unknown", "debug", "settings", "ssl", "aws", "maybe", "deferred", "ftp", "store", "isinstance", "medi", "verify", "none", "user", "dfd", "dfd2", "_to_string", "_md5sum", "__init__", "persist_file", "stat_file", "_get_filesystem_path", "_mkdir", "_get_boto_key", "_headers_to_botocore_kwargs", "_get_content_type", "_get_blob_path", "_stat_file", "from_settings", "from_crawler", "_from_settings", "media_downloaded", "inc_stats", "get_media_requests", "file_downloaded", "item_completed"], "ast_kind": "function_or_method", "text": "    @classmethod\n    def _update_stores(cls, settings: BaseSettings) -> None:\n        s3store: type[S3FilesStore] = cast(\n            \"type[S3FilesStore]\", cls.STORE_SCHEMES[\"s3\"]\n        )\n        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n        s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n\n        gcs_store: type[GCSFilesStore] = cast(\n            \"type[GCSFilesStore]\", cls.STORE_SCHEMES[\"gs\"]\n        )\n        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n        gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n\n        ftp_store: type[FTPFilesStore] = cast(\n            \"type[FTPFilesStore]\", cls.STORE_SCHEMES[\"ftp\"]\n        )\n        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n    def _get_store(self, uri: str) -> FilesStoreProtocol:\n        # to support win32 paths like: C:\\\\some\\dir\n        scheme = \"file\" if Path(uri).is_absolute() else urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)\n\n    def media_to_download(\n        self, request: Request, info: MediaPipeline.SpiderInfo, *, item: Any = None\n    ) -> Deferred[FileInfo | None] | None:\n        def _onsuccess(result: StatInfo) -> FileInfo | None:\n            if not result:\n                return None  # returning None force download\n\n            last_modified = result.get(\"last_modified\", None)\n            if not last_modified:\n                return None  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return None  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                \"File (uptodate): Downloaded %(medianame)s from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"medianame\": self.MEDIA_NAME, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            self.inc_stats(\"uptodate\")\n\n            checksum = result.get(\"checksum\", None)\n            return {\n                \"url\": request.url,\n                \"path\": path,\n                \"checksum\": checksum,\n                \"status\": \"uptodate\",\n            }\n\n        path = self.file_path(request, info=info, item=item)\n        # maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n        dfd: Deferred[StatInfo] = maybeDeferred(self.store.stat_file, path, info)  # type: ignore[call-overload]\n        dfd2: Deferred[FileInfo | None] = dfd.addCallback(_onsuccess)\n        dfd2.addErrback(lambda _: None)\n        dfd2.addErrback(\n            lambda f: logger.error(\n                self.__class__.__name__ + \".store.stat_file\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": info.spider},\n            )\n        )\n        return dfd2\n\n    def media_failed(\n        self, failure: Failure, request: Request, info: MediaPipeline.SpiderInfo\n    ) -> NoReturn:\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                \"File (unknown-error): Error downloading %(medianame)s from \"\n                \"%(request)s referred in <%(referer)s>: %(exception)s\",\n                {\n                    \"medianame\": self.MEDIA_NAME,\n                    \"request\": request,\n                    \"referer\": referer,\n                    \"exception\": failure.value,\n                },\n                extra={\"spider\": info.spider},\n            )\n\n        raise FileException\n", "n_tokens": 942, "byte_len": 4134, "file_sha1": "1603438b3ed3de7fa12ea60cf1f152c2f57b93b8", "start_line": 539, "end_line": 637}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/files.py", "rel_path": "scrapy/pipelines/files.py", "module": "scrapy.pipelines.files", "ext": "py", "chunk_number": 6, "symbols": ["media_downloaded", "inc_stats", "get_media_requests", "file_downloaded", "item_completed", "file_path", "referer", "guess", "type", "media", "ext", "inc", "value", "errormsg", "mimetypes", "callback", "files", "result", "error", "stats", "string", "path", "trying", "unknown", "debug", "interface", "isinstance", "results", "none", "code", "_to_string", "_md5sum", "__init__", "persist_file", "stat_file", "_get_filesystem_path", "_mkdir", "_onsuccess", "_get_boto_key", "_headers_to_botocore_kwargs", "_get_content_type", "_get_blob_path", "_stat_file", "from_settings", "from_crawler", "_from_settings", "_update_stores", "_get_store", "media_to_download", "media_failed"], "ast_kind": "function_or_method", "text": "    def media_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> FileInfo:\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                \"File (code: %(status)s): Error downloading file from \"\n                \"%(request)s referred in <%(referer)s>\",\n                {\"status\": response.status, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"download-error\")\n\n        if not response.body:\n            logger.warning(\n                \"File (empty-content): Empty file from %(request)s referred \"\n                \"in <%(referer)s>: no-content\",\n                {\"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"empty-content\")\n\n        status = \"cached\" if \"cached\" in response.flags else \"downloaded\"\n        logger.debug(\n            \"File (%(status)s): Downloaded file from %(request)s referred in \"\n            \"<%(referer)s>\",\n            {\"status\": status, \"request\": request, \"referer\": referer},\n            extra={\"spider\": info.spider},\n        )\n        self.inc_stats(status)\n\n        try:\n            path = self.file_path(request, response=response, info=info, item=item)\n            checksum = self.file_downloaded(response, request, info, item=item)\n        except FileException as exc:\n            logger.warning(\n                \"File (error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>: %(errormsg)s\",\n                {\"request\": request, \"referer\": referer, \"errormsg\": str(exc)},\n                extra={\"spider\": info.spider},\n                exc_info=True,\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                \"File (unknown-error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"request\": request, \"referer\": referer},\n                exc_info=True,\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(str(exc))\n\n        return {\n            \"url\": request.url,\n            \"path\": path,\n            \"checksum\": checksum,\n            \"status\": status,\n        }\n\n    def inc_stats(self, status: str) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\"file_count\")\n        self.crawler.stats.inc_value(f\"file_status_count/{status}\")\n\n    # Overridable Interface\n    def get_media_requests(\n        self, item: Any, info: MediaPipeline.SpiderInfo\n    ) -> list[Request]:\n        urls = ItemAdapter(item).get(self.files_urls_field, [])\n        if not isinstance(urls, list):\n            raise TypeError(\n                f\"{self.files_urls_field} must be a list of URLs, got {type(urls).__name__}. \"\n            )\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def file_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        path = self.file_path(request, response=response, info=info, item=item)\n        buf = BytesIO(response.body)\n        checksum = _md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum\n\n    def item_completed(\n        self, results: list[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n    ) -> Any:\n        with suppress(KeyError):\n            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(\n        self,\n        request: Request,\n        response: Response | None = None,\n        info: MediaPipeline.SpiderInfo | None = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # noqa: S324\n        media_ext = Path(request.url).suffix\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = \"\"\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = cast(\"str\", mimetypes.guess_extension(media_type))\n        return f\"full/{media_guid}{media_ext}\"\n", "n_tokens": 975, "byte_len": 4524, "file_sha1": "1603438b3ed3de7fa12ea60cf1f152c2f57b93b8", "start_line": 638, "end_line": 760}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/media.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/media.py", "rel_path": "scrapy/pipelines/media.py", "module": "scrapy.pipelines.media", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_finish_init", "_handle_statuses", "_key_for_pipe", "FileInfo", "MediaPipeline", "SpiderInfo", "faile", "results", "finish", "init", "failure", "bool", "call", "later", "instance", "future", "python", "literal", "becomes", "spider", "callback", "warn", "deprecated", "passed", "removed", "typ", "checking", "elif", "settings", "from_crawler", "open_spider", "process_item", "_process_request", "_modify_media_request", "_check_media_to_download", "_cache_result_and_execute_waiters", "media_to_download", "get_media_requests", "media_downloaded", "media_failed", "item_completed", "file_path", "method", "inconsistent", "fingerprint", "misc", "media", "failed", "http"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport functools\nimport logging\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Any, Literal, TypedDict, Union, cast\n\nfrom twisted import version as twisted_version\nfrom twisted.internet.defer import (\n    Deferred,\n    DeferredList,\n    inlineCallbacks,\n    maybeDeferred,\n)\nfrom twisted.python.failure import Failure\nfrom twisted.python.versions import Version\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http.request import NO_CALLBACK, Request\nfrom scrapy.settings import Settings\nfrom scrapy.utils.asyncio import call_later\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.decorators import _warn_spider_arg\nfrom scrapy.utils.defer import _DEFER_DELAY, _defer_sleep, deferred_from_coro\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.python import get_func_args, global_object_name\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Generator\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.utils.request import RequestFingerprinterProtocol\n\n\nclass FileInfo(TypedDict):\n    url: str\n    path: str\n    checksum: str | None\n    status: str\n\n\nFileInfoOrError = Union[tuple[Literal[True], FileInfo], tuple[Literal[False], Failure]]\n\nlogger = logging.getLogger(__name__)\n\n\nclass MediaPipeline(ABC):\n    crawler: Crawler\n    _fingerprinter: RequestFingerprinterProtocol\n    _modern_init = False\n\n    LOG_FAILED_RESULTS: bool = True\n\n    class SpiderInfo:\n        def __init__(self, spider: Spider):\n            self.spider: Spider = spider\n            self.downloading: set[bytes] = set()\n            self.downloaded: dict[bytes, FileInfo | Failure] = {}\n            self.waiting: defaultdict[bytes, list[Deferred[FileInfo]]] = defaultdict(\n                list\n            )\n\n    def __init__(\n        self,\n        download_func: Callable[[Request, Spider], Response] | None = None,\n        settings: Settings | dict[str, Any] | None = None,\n        *,\n        crawler: Crawler | None = None,\n    ):\n        self.download_func = download_func\n\n        if crawler is not None:\n            if settings is not None:\n                warnings.warn(\n                    f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n                    category=ScrapyDeprecationWarning,\n                    stacklevel=2,\n                )\n            settings = crawler.settings\n        elif isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n        )\n        self.allow_redirects: bool = settings.getbool(\n            resolve(\"MEDIA_ALLOW_REDIRECTS\"), False\n        )\n        self._handle_statuses(self.allow_redirects)\n\n        if crawler:\n            self._finish_init(crawler)\n            self._modern_init = True\n        else:\n            warnings.warn(\n                f\"MediaPipeline.__init__() was called without the crawler argument\"\n                f\" when creating {global_object_name(self.__class__)}.\"\n                f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n\n    def _finish_init(self, crawler: Crawler) -> None:\n        # This was done in from_crawler() before 2.12, now it's done in __init__()\n        # if the crawler was passed to it and may be needed to be called in other\n        # deprecated code paths explicitly too. After the crawler argument of __init__()\n        # becomes mandatory this should be inlined there.\n        self.crawler = crawler\n        assert crawler.request_fingerprinter\n        self._fingerprinter = crawler.request_fingerprinter\n\n    def _handle_statuses(self, allow_redirects: bool) -> None:\n        self.handle_httpstatus_list = None\n        if allow_redirects:\n            self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n\n    def _key_for_pipe(\n        self,\n        key: str,\n        base_class_name: str | None = None,\n        settings: Settings | None = None,\n    ) -> str:\n        class_name = self.__class__.__name__\n        formatted_key = f\"{class_name.upper()}_{key}\"\n        if (\n            not base_class_name\n            or class_name == base_class_name\n            or (settings and not settings.get(formatted_key))\n        ):\n            return key\n        return formatted_key\n", "n_tokens": 1042, "byte_len": 4993, "file_sha1": "2c669899aea9a456ce1def5655a406a3b60c5be6", "start_line": 1, "end_line": 141}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/media.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/media.py", "rel_path": "scrapy/pipelines/media.py", "module": "scrapy.pipelines.media", "ext": "py", "chunk_number": 2, "symbols": ["from_crawler", "open_spider", "process_item", "_process_request", "_modify_media_request", "_check_media_to_download", "finish", "init", "failure", "seen", "append", "instance", "future", "spider", "callback", "doesn", "deprecated", "warn", "removed", "elif", "doing", "handle", "httpstatus", "inconsistent", "download", "async", "settings", "maybe", "deferred", "fingerprint", "__init__", "_finish_init", "_handle_statuses", "_key_for_pipe", "_cache_result_and_execute_waiters", "media_to_download", "get_media_requests", "media_downloaded", "media_failed", "item_completed", "file_path", "FileInfo", "MediaPipeline", "SpiderInfo", "faile", "results", "method", "bool", "passed", "misc"], "ast_kind": "function_or_method", "text": "    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        pipe: Self\n        if hasattr(cls, \"from_settings\"):\n            pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n            warnings.warn(\n                f\"{global_object_name(cls)} has from_settings() and either doesn't have\"\n                \" from_crawler() or calls MediaPipeline.from_crawler() from it,\"\n                \" so from_settings() was used to create the instance of it.\"\n                \" This is deprecated and calling from_settings() will be removed\"\n                \" in a future Scrapy version. Please move the initialization code into\"\n                \" from_crawler() or __init__().\",\n                category=ScrapyDeprecationWarning,\n            )\n        elif \"crawler\" in get_func_args(cls.__init__):\n            pipe = cls(crawler=crawler)\n        else:\n            pipe = cls()\n            warnings.warn(\n                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n                category=ScrapyDeprecationWarning,\n            )\n        if not pipe._modern_init:\n            pipe._finish_init(crawler)\n        return pipe\n\n    @_warn_spider_arg\n    def open_spider(self, spider: Spider | None = None) -> None:\n        assert self.crawler.spider\n        self.spiderinfo = self.SpiderInfo(self.crawler.spider)\n\n    @_warn_spider_arg\n    def process_item(\n        self, item: Any, spider: Spider | None = None\n    ) -> Deferred[list[FileInfoOrError]]:\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info, item) for r in requests]\n        dfd = cast(\n            \"Deferred[list[FileInfoOrError]]\", DeferredList(dlist, consumeErrors=True)\n        )\n        return dfd.addCallback(self.item_completed, item, info)\n\n    @inlineCallbacks\n    def _process_request(\n        self, request: Request, info: SpiderInfo, item: Any\n    ) -> Generator[Deferred[Any], Any, FileInfo]:\n        fp = self._fingerprinter.fingerprint(request)\n\n        eb = request.errback\n        request.callback = NO_CALLBACK\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            yield _defer_sleep()\n            cached_result = info.downloaded[fp]\n            if isinstance(cached_result, Failure):\n                if eb:\n                    return eb(cached_result)\n                cached_result.raiseException()\n            return cached_result\n\n        # Otherwise, wait for result\n        wad: Deferred[FileInfo] = Deferred()\n        if eb:\n            wad.addErrback(eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return (yield wad)\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        yield _defer_sleep()\n        result: FileInfo | Failure\n        try:\n            file_info = yield maybeDeferred(\n                self.media_to_download, request, info, item=item\n            )\n            if file_info:\n                # got a result without downloading\n                result = file_info\n            else:\n                # download the result\n                result = yield self._check_media_to_download(request, info, item=item)\n        except Exception:\n            result = Failure()\n            logger.exception(result)\n        self._cache_result_and_execute_waiters(result, fp, info)\n        return (yield wad)  # it must return wad at last\n\n    def _modify_media_request(self, request: Request) -> None:\n        if self.handle_httpstatus_list:\n            request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n        else:\n            request.meta[\"handle_httpstatus_all\"] = True\n\n    @inlineCallbacks\n    def _check_media_to_download(  # pylint: disable=inconsistent-return-statements\n        self, request: Request, info: SpiderInfo, item: Any\n    ) -> Generator[Deferred[Any], Any, FileInfo]:\n        try:\n            if self.download_func:\n                # this ugly code was left only to support tests. TODO: remove\n                response = yield maybeDeferred(self.download_func, request, info.spider)\n            else:\n                self._modify_media_request(request)\n                assert self.crawler.engine\n                response = yield deferred_from_coro(\n                    self.crawler.engine.download_async(request)\n                )\n            return self.media_downloaded(response, request, info, item=item)\n        except Exception:\n            failure = self.media_failed(Failure(), request, info)\n            if isinstance(failure, Failure):\n                warnings.warn(\n                    f\"{global_object_name(self.media_failed)} returned a Failure instance.\"\n                    f\" This is deprecated, please raise an exception instead, e.g. via failure.raiseException().\",\n                    category=ScrapyDeprecationWarning,\n                    stacklevel=2,\n                )\n                failure.raiseException()\n", "n_tokens": 1079, "byte_len": 5271, "file_sha1": "2c669899aea9a456ce1def5655a406a3b60c5be6", "start_line": 142, "end_line": 267}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/media.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/media.py", "rel_path": "scrapy/pipelines/media.py", "module": "scrapy.pipelines.media", "ext": "py", "chunk_number": 3, "symbols": ["_cache_result_and_execute_waiters", "media_to_download", "get_media_requests", "media_downloaded", "media_failed", "item_completed", "file_path", "faile", "results", "failure", "method", "processed", "starting", "call", "later", "instance", "avoiding", "failed", "interface", "isinstance", "none", "returns", "fixes", "type", "code", "media", "leak", "found", "exc", "info", "__init__", "_finish_init", "_handle_statuses", "_key_for_pipe", "from_crawler", "open_spider", "process_item", "_process_request", "_modify_media_request", "_check_media_to_download", "FileInfo", "MediaPipeline", "SpiderInfo", "bool", "spider", "passed", "removed", "inconsistent", "fingerprint", "misc"], "ast_kind": "function_or_method", "text": "    def _cache_result_and_execute_waiters(\n        self, result: FileInfo | Failure, fp: bytes, info: SpiderInfo\n    ) -> None:\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            if twisted_version < Version(\"twisted\", 24, 10, 0):\n                result.stack = []  # type: ignore[method-assign]\n            # This code fixes a memory leak by avoiding to keep references to\n            # the Request and Response objects on the Media Pipeline cache.\n            #\n            # What happens when the media_downloaded callback raises an\n            # exception, for example a FileException('download-error') when\n            # the Response status code is not 200 OK, is that the original\n            # StopIteration exception (which in turn contains the failed\n            # Response and by extension, the original Request) gets encapsulated\n            # within the FileException context.\n            #\n            # Originally, Scrapy was using twisted.internet.defer.returnValue\n            # inside functions decorated with twisted.internet.defer.inlineCallbacks,\n            # encapsulating the returned Response in a _DefGen_Return exception\n            # instead of a StopIteration.\n            #\n            # To avoid keeping references to the Response and therefore Request\n            # objects on the Media Pipeline cache, we should wipe the context of\n            # the encapsulated exception when it is a StopIteration instance\n            context = getattr(result.value, \"__context__\", None)\n            if isinstance(context, StopIteration):\n                result.value.__context__ = None\n\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            if isinstance(result, Failure):\n                call_later(_DEFER_DELAY, wad.errback, result)\n            else:\n                call_later(_DEFER_DELAY, wad.callback, result)\n\n    # Overridable Interface\n    @abstractmethod\n    def media_to_download(\n        self, request: Request, info: SpiderInfo, *, item: Any = None\n    ) -> Deferred[FileInfo | None] | None:\n        \"\"\"Check request before starting download\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_media_requests(self, item: Any, info: SpiderInfo) -> list[Request]:\n        \"\"\"Returns the media requests to download\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def media_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> FileInfo:\n        \"\"\"Handler for success downloads\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def media_failed(\n        self, failure: Failure, request: Request, info: SpiderInfo\n    ) -> Failure:\n        \"\"\"Handler for failed downloads\"\"\"\n        raise NotImplementedError\n\n    def item_completed(\n        self, results: list[FileInfoOrError], item: Any, info: SpiderInfo\n    ) -> Any:\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    assert isinstance(value, Failure)\n                    logger.error(\n                        \"%(class)s found errors processing %(item)s\",\n                        {\"class\": self.__class__.__name__, \"item\": item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={\"spider\": info.spider},\n                    )\n        return item\n\n    @abstractmethod\n    def file_path(\n        self,\n        request: Request,\n        response: Response | None = None,\n        info: SpiderInfo | None = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        \"\"\"Returns the path where downloaded media should be stored\"\"\"\n        raise NotImplementedError\n", "n_tokens": 786, "byte_len": 3982, "file_sha1": "2c669899aea9a456ce1def5655a406a3b60c5be6", "start_line": 268, "end_line": 366}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/__init__.py", "rel_path": "scrapy/pipelines/__init__.py", "module": "scrapy.pipelines.__init__", "ext": "py", "chunk_number": 1, "symbols": ["_get_mwlist_from_settings", "_add_middleware", "process_item", "_process_parallel", "get_dfd", "eb", "open_spider", "close_spider", "ItemPipelineManager", "failure", "method", "async", "bool", "append", "spider", "deprecated", "warn", "future", "typ", "checking", "settings", "methods", "requiring", "item", "none", "docs", "type", "callable", "process", "getwithbase", "await", "internet", "pipeline", "fire", "one", "typing", "return", "middleware", "annotations", "add", "callback", "class", "get", "mwlist", "global", "object", "set", "compat", "maybe", "deferred"], "ast_kind": "class_or_type", "text": "\"\"\"\nItem pipeline\n\nSee documentation in docs/item-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Callable, cast\n\nfrom twisted.internet.defer import Deferred, DeferredList\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import deferred_from_coro, maybeDeferred_coro\nfrom scrapy.utils.python import global_object_name\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from twisted.python.failure import Failure\n\n    from scrapy import Spider\n    from scrapy.settings import Settings\n\n\nclass ItemPipelineManager(MiddlewareManager):\n    component_name = \"item pipeline\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n        return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n\n    def _add_middleware(self, pipe: Any) -> None:\n        if hasattr(pipe, \"open_spider\"):\n            self.methods[\"open_spider\"].append(pipe.open_spider)\n            self._check_mw_method_spider_arg(pipe.open_spider)\n        if hasattr(pipe, \"close_spider\"):\n            self.methods[\"close_spider\"].appendleft(pipe.close_spider)\n            self._check_mw_method_spider_arg(pipe.close_spider)\n        if hasattr(pipe, \"process_item\"):\n            self.methods[\"process_item\"].append(pipe.process_item)\n            self._check_mw_method_spider_arg(pipe.process_item)\n\n    def process_item(self, item: Any, spider: Spider | None = None) -> Deferred[Any]:\n        if spider:\n            self._set_compat_spider(spider)\n        warnings.warn(\n            f\"{global_object_name(type(self))}.process_item() is deprecated, use process_item_async() instead.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return deferred_from_coro(self.process_item_async(item))\n\n    async def process_item_async(self, item: Any) -> Any:\n        return await self._process_chain(\"process_item\", item, add_spider=True)\n\n    def _process_parallel(self, methodname: str) -> Deferred[list[None]]:\n        methods = cast(\"Iterable[Callable[..., None]]\", self.methods[methodname])\n\n        def get_dfd(method: Callable[..., None]) -> Deferred[None]:\n            if method in self._mw_methods_requiring_spider:\n                return maybeDeferred_coro(method, self._spider)\n            return maybeDeferred_coro(method)\n\n        dfds = [get_dfd(m) for m in methods]\n        d: Deferred[list[tuple[bool, None]]] = DeferredList(\n            dfds, fireOnOneErrback=True, consumeErrors=True\n        )\n        d2: Deferred[list[None]] = d.addCallback(lambda r: [x[1] for x in r])\n\n        def eb(failure: Failure) -> Failure:\n            return failure.value.subFailure\n\n        d2.addErrback(eb)\n        return d2\n\n    def open_spider(self, spider: Spider | None = None) -> Deferred[list[None]]:\n        if spider:\n            self._warn_spider_arg(\"open_spider\")\n            self._set_compat_spider(spider)\n        return self._process_parallel(\"open_spider\")\n\n    def close_spider(self, spider: Spider | None = None) -> Deferred[list[None]]:\n        if spider:\n            self._warn_spider_arg(\"close_spider\")\n            self._set_compat_spider(spider)\n        return self._process_parallel(\"close_spider\")\n", "n_tokens": 750, "byte_len": 3367, "file_sha1": "9ee4cac2a26bd3c72a618f17a673b656a6dc210e", "start_line": 1, "end_line": 91}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/images.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/images.py", "rel_path": "scrapy/pipelines/images.py", "module": "scrapy.pipelines.images", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_from_settings", "file_downloaded", "image_downloaded", "ImageException", "ImagesPipeline", "image", "exception", "finish", "init", "subclasses", "get", "images", "instance", "future", "python", "min", "height", "jpeg", "spider", "callback", "installing", "doesn", "expires", "deprecated", "removed", "typ", "checking", "defaul", "elif", "get_images", "convert_image", "get_media_requests", "item_completed", "file_path", "thumb_path", "webp", "updating", "small", "convert", "contextlib", "save", "thumbs", "paste", "settings", "thumb", "path", "rgba", "result", "isinstance"], "ast_kind": "class_or_type", "text": "\"\"\"\nImages Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport hashlib\nimport warnings\nfrom contextlib import suppress\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, Any\n\nfrom itemadapter import ItemAdapter\n\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import get_func_args, global_object_name, to_bytes\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable\n    from os import PathLike\n\n    from PIL import Image\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n\n\nclass ImageException(FileException):\n    \"\"\"General image error exception\"\"\"\n\n\nclass ImagesPipeline(FilesPipeline):\n    \"\"\"Abstract pipeline that implement the image thumbnail generation logic\"\"\"\n\n    MEDIA_NAME: str = \"image\"\n\n    # Uppercase attributes kept for backward compatibility with code that subclasses\n    # ImagesPipeline. They may be overridden by settings.\n    MIN_WIDTH: int = 0\n    MIN_HEIGHT: int = 0\n    EXPIRES: int = 90\n    THUMBS: dict[str, tuple[int, int]] = {}\n    DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n    DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n\n    def __init__(\n        self,\n        store_uri: str | PathLike[str],\n        download_func: Callable[[Request, Spider], Response] | None = None,\n        settings: Settings | dict[str, Any] | None = None,\n        *,\n        crawler: Crawler | None = None,\n    ):\n        try:\n            from PIL import Image  # noqa: PLC0415\n\n            self._Image = Image\n        except ImportError:\n            raise NotConfigured(\n                \"ImagesPipeline requires installing Pillow 8.0.0 or later\"\n            )\n\n        super().__init__(\n            store_uri,\n            settings=settings if not crawler else None,\n            download_func=download_func,\n            crawler=crawler,\n        )\n\n        if crawler is not None:\n            if settings is not None:\n                warnings.warn(\n                    f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n                    category=ScrapyDeprecationWarning,\n                    stacklevel=2,\n                )\n            settings = crawler.settings\n        elif isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        resolve = functools.partial(\n            self._key_for_pipe,\n            base_class_name=\"ImagesPipeline\",\n            settings=settings,\n        )\n        self.expires: int = settings.getint(resolve(\"IMAGES_EXPIRES\"), self.EXPIRES)\n\n        if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n            self.IMAGES_RESULT_FIELD: str = self.DEFAULT_IMAGES_RESULT_FIELD\n        if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n            self.IMAGES_URLS_FIELD: str = self.DEFAULT_IMAGES_URLS_FIELD\n\n        self.images_urls_field: str = settings.get(\n            resolve(\"IMAGES_URLS_FIELD\"), self.IMAGES_URLS_FIELD\n        )\n        self.images_result_field: str = settings.get(\n            resolve(\"IMAGES_RESULT_FIELD\"), self.IMAGES_RESULT_FIELD\n        )\n        self.min_width: int = settings.getint(\n            resolve(\"IMAGES_MIN_WIDTH\"), self.MIN_WIDTH\n        )\n        self.min_height: int = settings.getint(\n            resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT\n        )\n        self.thumbs: dict[str, tuple[int, int]] = settings.get(\n            resolve(\"IMAGES_THUMBS\"), self.THUMBS\n        )\n\n    @classmethod\n    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n        cls._update_stores(settings)\n        store_uri = settings[\"IMAGES_STORE\"]\n        if \"crawler\" in get_func_args(cls.__init__):\n            o = cls(store_uri, crawler=crawler)\n        else:\n            o = cls(store_uri, settings=settings)\n            if crawler:\n                o._finish_init(crawler)\n            warnings.warn(\n                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n                category=ScrapyDeprecationWarning,\n            )\n        return o\n\n    def file_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        return self.image_downloaded(response, request, info, item=item)\n\n    def image_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        checksum: str | None = None\n        for path, image, buf in self.get_images(response, request, info, item=item):\n            if checksum is None:\n                buf.seek(0)\n                checksum = _md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path,\n                buf,\n                info,\n                meta={\"width\": width, \"height\": height},\n                headers={\"Content-Type\": \"image/jpeg\"},\n            )\n        assert checksum is not None\n        return checksum\n", "n_tokens": 1206, "byte_len": 5760, "file_sha1": "4042bc7ef72042efc813900e6ddb77ca7e8c0d79", "start_line": 1, "end_line": 172}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/images.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/pipelines/images.py", "rel_path": "scrapy/pipelines/images.py", "module": "scrapy.pipelines.images", "ext": "py", "chunk_number": 2, "symbols": ["get_images", "convert_image", "get_media_requests", "item_completed", "file_path", "thumb_path", "image", "exception", "get", "images", "webp", "updating", "min", "height", "small", "convert", "callback", "elif", "save", "paste", "thumb", "path", "rgba", "result", "items", "isinstance", "results", "none", "type", "item", "__init__", "_from_settings", "file_downloaded", "image_downloaded", "ImageException", "ImagesPipeline", "finish", "init", "subclasses", "instance", "future", "python", "jpeg", "spider", "installing", "doesn", "expires", "deprecated", "removed", "typ"], "ast_kind": "function_or_method", "text": "    def get_images(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> Iterable[tuple[str, Image.Image, BytesIO]]:\n        path = self.file_path(request, response=response, info=info, item=item)\n        orig_image = self._Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\n                \"Image too small \"\n                f\"({width}x{height} < \"\n                f\"{self.min_width}x{self.min_height})\"\n            )\n\n        image, buf = self.convert_image(\n            orig_image, response_body=BytesIO(response.body)\n        )\n        yield path, image, buf\n\n        for thumb_id, size in self.thumbs.items():\n            thumb_path = self.thumb_path(\n                request, thumb_id, response=response, info=info, item=item\n            )\n            thumb_image, thumb_buf = self.convert_image(image, size, response_body=buf)\n            yield thumb_path, thumb_image, thumb_buf\n\n    def convert_image(\n        self,\n        image: Image.Image,\n        size: tuple[int, int] | None = None,\n        *,\n        response_body: BytesIO,\n    ) -> tuple[Image.Image, BytesIO]:\n        if image.format in (\"PNG\", \"WEBP\") and image.mode == \"RGBA\":\n            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert(\"RGB\")\n        elif image.mode == \"P\":\n            image = image.convert(\"RGBA\")\n            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert(\"RGB\")\n        elif image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        if size:\n            image = image.copy()\n            try:\n                # Image.Resampling.LANCZOS was added in Pillow 9.1.0\n                # remove this try except block,\n                # when updating the minimum requirements for Pillow.\n                resampling_filter = self._Image.Resampling.LANCZOS\n            except AttributeError:\n                resampling_filter = self._Image.ANTIALIAS  # type: ignore[attr-defined]\n            image.thumbnail(size, resampling_filter)\n        elif image.format == \"JPEG\":\n            return image, response_body\n\n        buf = BytesIO()\n        image.save(buf, \"JPEG\")\n        return image, buf\n\n    def get_media_requests(\n        self, item: Any, info: MediaPipeline.SpiderInfo\n    ) -> list[Request]:\n        urls = ItemAdapter(item).get(self.images_urls_field, [])\n        if not isinstance(urls, list):\n            raise TypeError(\n                f\"{self.images_urls_field} must be a list of URLs, got {type(urls).__name__}. \"\n            )\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def item_completed(\n        self, results: list[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n    ) -> Any:\n        with suppress(KeyError):\n            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(\n        self,\n        request: Request,\n        response: Response | None = None,\n        info: MediaPipeline.SpiderInfo | None = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # noqa: S324\n        return f\"full/{image_guid}.jpg\"\n\n    def thumb_path(\n        self,\n        request: Request,\n        thumb_id: str,\n        response: Response | None = None,\n        info: MediaPipeline.SpiderInfo | None = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # noqa: S324\n        return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n", "n_tokens": 881, "byte_len": 3893, "file_sha1": "4042bc7ef72042efc813900e6ddb77ca7e8c0d79", "start_line": 173, "end_line": 279}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/serialize.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/serialize.py", "rel_path": "scrapy/utils/serialize.py", "module": "scrapy.utils.serialize", "ext": "py", "chunk_number": 1, "symbols": ["default", "__init__", "ScrapyJSONEncoder", "ScrapyJSONDecoder", "method", "will", "internet", "scrapy", "json", "deprecation", "future", "typing", "twisted", "return", "tim", "format", "name", "args", "class", "decoder", "encoder", "strftime", "time", "deprecated", "decimal", "defer", "removed", "category", "date", "init", "warnings", "version", "from", "list", "kwargs", "stacklevel", "request", "isinstance", "exceptions", "super", "type", "item", "adapter", "dat", "import", "datetime", "asdict", "http", "self", "deferred"], "ast_kind": "class_or_type", "text": "import datetime\nimport decimal\nimport json\nimport warnings\nfrom typing import Any\n\nfrom itemadapter import ItemAdapter, is_item\nfrom twisted.internet import defer\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\n\n\nclass ScrapyJSONEncoder(json.JSONEncoder):\n    DATE_FORMAT = \"%Y-%m-%d\"\n    TIME_FORMAT = \"%H:%M:%S\"\n\n    def default(self, o: Any) -> Any:\n        if isinstance(o, set):\n            return list(o)\n        if isinstance(o, datetime.datetime):\n            return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n        if isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        if isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        if isinstance(o, decimal.Decimal):\n            return str(o)\n        if isinstance(o, defer.Deferred):\n            return str(o)\n        if isinstance(o, Request):\n            return f\"<{type(o).__name__} {o.method} {o.url}>\"\n        if isinstance(o, Response):\n            return f\"<{type(o).__name__} {o.status} {o.url}>\"\n        if is_item(o):\n            return ItemAdapter(o).asdict()\n        return super().default(o)\n\n\nclass ScrapyJSONDecoder(json.JSONDecoder):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\n            \"The ScrapyJSONDecoder class is deprecated and will be removed in a future version of Scrapy.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        super().__init__(*args, **kwargs)\n", "n_tokens": 341, "byte_len": 1530, "file_sha1": "922562a3a987e2b7deead2d2b2c83dc8806afb3b", "start_line": 1, "end_line": 48}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/deprecate.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/deprecate.py", "rel_path": "scrapy/utils/deprecate.py", "module": "scrapy.utils.deprecate", "ext": "py", "chunk_number": 1, "symbols": ["attribute", "create_deprecated_class", "__new__", "__init__", "__instancecheck__", "__subclasscheck__", "OldName", "NewName", "DeprecatedClass", "method", "issued", "library", "bool", "subclasses", "subclass", "warn", "instance", "inst", "magic", "name", "deprecated", "mro", "helpers", "future", "typ", "checking", "new", "class", "https", "clsdict", "__call__", "_clspath", "update_classpath", "method_is_overridden", "argument_is_required", "doesn", "error", "deprecatio", "rules", "base", "following", "isinstance", "rename", "replacement", "object", "sometimes", "issues", "isclass", "none", "docs"], "ast_kind": "class_or_type", "text": "\"\"\"Some helpers for deprecation messages\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport warnings\nfrom typing import TYPE_CHECKING, Any, overload\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.python import get_func_args_dict\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n\ndef attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> None:\n    cname = obj.__class__.__name__\n    warnings.warn(\n        f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n        f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n        ScrapyDeprecationWarning,\n        stacklevel=3,\n    )\n\n\ndef create_deprecated_class(\n    name: str,\n    new_class: type,\n    clsdict: dict[str, Any] | None = None,\n    warn_category: type[Warning] = ScrapyDeprecationWarning,\n    warn_once: bool = True,\n    old_class_path: str | None = None,\n    new_class_path: str | None = None,\n    subclass_warn_message: str = \"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n    instance_warn_message: str = \"{cls} is deprecated, instantiate {new} instead.\",\n) -> type:\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    # https://github.com/python/mypy/issues/4177\n    class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]\n        # pylint: disable=no-self-argument\n        deprecated_class: type | None = None\n        warned_on_subclass: bool = False\n\n        def __new__(  # pylint: disable=bad-classmethod-argument\n            metacls, name: str, bases: tuple[type, ...], clsdict_: dict[str, Any]\n        ) -> type:\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name: str, bases: tuple[type, ...], clsdict_: dict[str, Any]):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(\n                    cls=_clspath(cls),\n                    old=_clspath(old, old_class_path),\n                    new=_clspath(new_class, new_class_path),\n                )\n                if warn_once:\n                    msg += \" (warning only on first subclass, there may be others)\"\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super().__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst: Any) -> bool:\n            return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))\n\n        def __subclasscheck__(cls, sub: type) -> bool:\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, \"__mro__\", ())\n            return any(c in {cls, new_class} for c in mro)\n", "n_tokens": 1016, "byte_len": 4395, "file_sha1": "bbd73ae92fe7c4a77a40647b1dfc91ac8a1179ae", "start_line": 1, "end_line": 110}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/deprecate.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/deprecate.py", "rel_path": "scrapy/utils/deprecate.py", "module": "scrapy.utils.deprecate", "ext": "py", "chunk_number": 2, "symbols": ["__call__", "_clspath", "update_classpath", "method_is_overridden", "argument_is_required", "method", "bool", "subclass", "name", "doesn", "error", "deprecated", "new", "class", "base", "deprecatio", "rules", "clsdict", "instance", "warn", "isinstance", "replacement", "object", "sometimes", "none", "argument", "required", "type", "default", "get", "attribute", "create_deprecated_class", "__new__", "__init__", "__instancecheck__", "__subclasscheck__", "OldName", "NewName", "DeprecatedClass", "issued", "library", "subclasses", "inst", "magic", "mro", "helpers", "future", "typ", "checking", "https"], "ast_kind": "class_or_type", "text": "        def __call__(cls, *args: Any, **kwargs: Any) -> Any:\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(\n                    cls=_clspath(cls, old_class_path),\n                    new=_clspath(new_class, new_class_path),\n                )\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(f\"Error detecting parent module: {e!r}\")\n\n    return deprecated_cls\n\n\ndef _clspath(cls: type, forced: str | None = None) -> str:\n    if forced is not None:\n        return forced\n    return f\"{cls.__module__}.{cls.__name__}\"\n\n\nDEPRECATION_RULES: list[tuple[str, str]] = []\n\n\n@overload\ndef update_classpath(path: str) -> str: ...\n\n\n@overload\ndef update_classpath(path: Any) -> Any: ...\n\n\ndef update_classpath(path: Any) -> Any:\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if isinstance(path, str) and path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\n                f\"`{path}` class is deprecated, use `{new_path}` instead\",\n                ScrapyDeprecationWarning,\n            )\n            return new_path\n    return path\n\n\ndef method_is_overridden(subclass: type, base_class: type, method_name: str) -> bool:\n    \"\"\"\n    Return True if a method named ``method_name`` of a ``base_class``\n    is overridden in a ``subclass``.\n\n    >>> class Base:\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub1(Base):\n    ...     pass\n    >>> class Sub2(Base):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub3(Sub1):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub4(Sub2):\n    ...     pass\n    >>> method_is_overridden(Base, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub1, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub2, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub3, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub4, Base, 'foo')\n    True\n    \"\"\"\n    base_method = getattr(base_class, method_name)\n    sub_method = getattr(subclass, method_name)\n    return base_method.__code__ is not sub_method.__code__\n\n\ndef argument_is_required(func: Callable[..., Any], arg_name: str) -> bool:\n    \"\"\"\n    Check if a function argument is required (exists and doesn't have a default value).\n\n    .. versionadded:: VERSION\n\n    >>> def func(a, b=1, c=None):\n    ...     pass\n    >>> argument_is_required(func, 'a')\n    True\n    >>> argument_is_required(func, 'b')\n    False\n    >>> argument_is_required(func, 'c')\n    False\n    >>> argument_is_required(func, 'd')\n    False\n    \"\"\"\n    args = get_func_args_dict(func)\n    param = args.get(arg_name)\n    return param is not None and param.default is inspect.Parameter.empty\n", "n_tokens": 835, "byte_len": 3458, "file_sha1": "bbd73ae92fe7c4a77a40647b1dfc91ac8a1179ae", "start_line": 111, "end_line": 222}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/gz.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/gz.py", "rel_path": "scrapy/utils/gz.py", "module": "scrapy.utils.gz", "ext": "py", "chunk_number": 1, "symbols": ["gunzip", "gzip_magic_number", "while", "number", "seek", "gzip", "bool", "except", "decompressed", "errors", "read", "read1", "chun", "size", "typing", "return", "much", "decompression", "max", "possible", "resilient", "small", "annotations", "about", "nbytes", "break", "complete", "catching", "pages", "scrapy", "error", "oserror", "future", "typ", "checking", "eof", "some", "body", "given", "getbuffer", "exceed", "file", "struct", "from", "compression", "otherwise", "chunk", "write", "there", "magic"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport struct\nfrom gzip import GzipFile\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING\n\nfrom ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\n\nif TYPE_CHECKING:\n    from scrapy.http import Response\n\n\ndef gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output_stream = BytesIO()\n    chunk = b\".\"\n    decompressed_size = 0\n    while chunk:\n        try:\n            chunk = f.read1(_CHUNK_SIZE)\n        except (OSError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output_stream is empty\n            if output_stream.getbuffer().nbytes > 0:\n                break\n            raise\n        decompressed_size += len(chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef gzip_magic_number(response: Response) -> bool:\n    return response.body[:3] == b\"\\x1f\\x8b\\x08\"\n", "n_tokens": 345, "byte_len": 1475, "file_sha1": "f4e6c5248960edebf6f8274d0eb5266e1e37575a", "start_line": 1, "end_line": 47}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/misc.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/misc.py", "rel_path": "scrapy/utils/misc.py", "module": "scrapy.utils.misc", "ext": "py", "chunk_number": 1, "symbols": ["arg_to_iter", "load_object", "walk_modules", "md5sum", "rel_has_nofollow", "importing", "while", "bool", "append", "thrown", "instance", "future", "loads", "spider", "name", "doesn", "import", "module", "rindex", "error", "deprecated", "removed", "typ", "checking", "string", "contextlib", "unexpected", "variable", "back", "isinstance", "create_instance", "build_from_crawler", "set_environ", "walk_callable", "is_generator_with_return_value", "returns_none", "warn_on_generator_with_return_value", "method", "qualname", "your", "tracker", "raises", "determine", "potential", "generator", "with", "includes", "about", "popleft", "return"], "ast_kind": "class_or_type", "text": "\"\"\"Helper functions which don't fit anywhere else\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport hashlib\nimport inspect\nimport os\nimport re\nimport warnings\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom importlib import import_module\nfrom pkgutil import iter_modules\nfrom typing import IO, TYPE_CHECKING, Any, TypeVar, cast\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.item import Item\nfrom scrapy.utils.datatypes import LocalWeakReferencedCache\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable, Iterator\n    from types import ModuleType\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\n_ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\nT = TypeVar(\"T\")\n\n\ndef arg_to_iter(arg: Any) -> Iterable[Any]:\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, \"__iter__\"):\n        return cast(\"Iterable[Any]\", arg)\n    return [arg]\n\n\ndef load_object(path: str | Callable[..., Any]) -> Any:\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    The object can be the import path of a class, function, variable or an\n    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n\n    If ``path`` is not a string, but is a callable object, such as a class or\n    a function, then return it as is.\n    \"\"\"\n\n    if not isinstance(path, str):\n        if callable(path):\n            return path\n        raise TypeError(\n            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n        )\n\n    try:\n        dot = path.rindex(\".\")\n    except ValueError:\n        raise ValueError(f\"Error loading object '{path}': not a full path\")\n\n    module, name = path[:dot], path[dot + 1 :]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n\n    return obj\n\n\ndef walk_modules(path: str) -> list[ModuleType]:\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods: list[ModuleType] = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, \"__path__\"):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + \".\" + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods\n\n\ndef md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    warnings.warn(\n        (\n            \"The scrapy.utils.misc.md5sum function is deprecated and will be \"\n            \"removed in a future version of Scrapy.\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    m = hashlib.md5()  # noqa: S324\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\ndef rel_has_nofollow(rel: str | None) -> bool:\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n\n", "n_tokens": 882, "byte_len": 3740, "file_sha1": "597e5133ceddde52835cd4e05956a5e137c8195e", "start_line": 1, "end_line": 131}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/misc.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/misc.py", "rel_path": "scrapy/utils/misc.py", "module": "scrapy.utils.misc", "ext": "py", "chunk_number": 2, "symbols": ["create_instance", "build_from_crawler", "set_environ", "walk_callable", "is_generator_with_return_value", "returns_none", "while", "bool", "qualname", "raises", "generator", "instance", "future", "with", "includes", "popleft", "return", "node", "deprecated", "removed", "elif", "settings", "create", "items", "isinstance", "versionchanged", "than", "callbacks", "function", "def", "arg_to_iter", "load_object", "walk_modules", "md5sum", "rel_has_nofollow", "warn_on_generator_with_return_value", "importing", "method", "your", "tracker", "determine", "append", "potential", "thrown", "loads", "spider", "name", "about", "doesn", "semantics"], "ast_kind": "class_or_type", "text": "def create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n\n    .. versionchanged:: 2.2\n       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n       extension has not been implemented correctly).\n    \"\"\"\n    warnings.warn(\n        \"The create_instance() function is deprecated. \"\n        \"Please use build_from_crawler() instead.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specify at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return instance\n\n\ndef build_from_crawler(\n    objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n) -> T:\n    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n\n    .. versionadded:: 2.12\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n\n    Raises ``TypeError`` if the resulting instance is ``None``.\n    \"\"\"\n    if hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        warnings.warn(\n            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n            \" This is deprecated and calling from_settings() will be removed in a future\"\n            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n            \" from_settings() with crawler.settings.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return cast(\"T\", instance)\n\n\n@contextmanager\ndef set_environ(**kwargs: str) -> Iterator[None]:\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v\n\n\ndef walk_callable(node: ast.AST) -> Iterable[ast.AST]:\n    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n    functions defined within the node.\n    \"\"\"\n    todo: deque[ast.AST] = deque([node])\n    walked_func_def = False\n    while todo:\n        node = todo.popleft()\n        if isinstance(node, ast.FunctionDef):\n            if walked_func_def:\n                continue\n            walked_func_def = True\n        todo.extend(ast.iter_child_nodes(node))\n        yield node\n\n\n_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n\n\ndef is_generator_with_return_value(callable: Callable[..., Any]) -> bool:  # noqa: A002\n    \"\"\"\n    Returns True if a callable is a generator function which includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n    if callable in _generator_callbacks_cache:\n        return bool(_generator_callbacks_cache[callable])\n\n    def returns_none(return_node: ast.Return) -> bool:\n        value = return_node.value\n        return value is None or (\n            isinstance(value, ast.Constant) and value.value is None\n        )\n\n    if inspect.isgeneratorfunction(callable):\n        func = callable\n        while isinstance(func, partial):\n            func = func.func\n\n        src = inspect.getsource(func)\n        pattern = re.compile(r\"(^[\\t ]+)\")\n        code = pattern.sub(\"\", src)\n\n        match = pattern.match(src)  # finds indentation\n        if match:\n            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n\n        tree = ast.parse(code)\n        for node in walk_callable(tree):\n            if isinstance(node, ast.Return) and not returns_none(node):\n                _generator_callbacks_cache[callable] = True\n                return bool(_generator_callbacks_cache[callable])\n\n    _generator_callbacks_cache[callable] = False\n    return bool(_generator_callbacks_cache[callable])\n\n", "n_tokens": 1215, "byte_len": 5374, "file_sha1": "597e5133ceddde52835cd4e05956a5e137c8195e", "start_line": 132, "end_line": 280}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/misc.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/misc.py", "rel_path": "scrapy/utils/misc.py", "module": "scrapy.utils.misc", "ext": "py", "chunk_number": 3, "symbols": ["warn_on_generator_with_return_value", "implementation", "generators", "method", "unable", "will", "war", "generato", "your", "getbool", "except", "tracker", "determine", "potential", "python", "logs", "spider", "generator", "working", "with", "please", "github", "return", "includes", "name", "reference", "about", "semantics", "unexpected", "warning", "arg_to_iter", "load_object", "walk_modules", "md5sum", "rel_has_nofollow", "create_instance", "build_from_crawler", "set_environ", "walk_callable", "is_generator_with_return_value", "returns_none", "importing", "while", "bool", "qualname", "raises", "append", "thrown", "instance", "future"], "ast_kind": "function_or_method", "text": "def warn_on_generator_with_return_value(\n    spider: Spider,\n    callable: Callable[..., Any],  # noqa: A002\n) -> None:\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    if not spider.settings.getbool(\"WARN_ON_GENERATOR_RETURN_VALUE\"):\n        return\n    try:\n        if is_generator_with_return_value(callable):\n            warnings.warn(\n                f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                'a generator and includes a \"return\" statement with a value '\n                \"different than None. This could lead to unexpected behaviour. Please see \"\n                \"https://docs.python.org/3/reference/simple_stmts.html#the-return-statement \"\n                'for details about the semantics of the \"return\" statement within generators',\n                stacklevel=2,\n            )\n    except IndentationError:\n        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n        warnings.warn(\n            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n            \"This will not prevent your code from working, but it prevents Scrapy from detecting \"\n            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n            \"Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), \"\n            f'including the code of \"{callable_name}\"',\n            stacklevel=2,\n        )\n", "n_tokens": 320, "byte_len": 1533, "file_sha1": "597e5133ceddde52835cd4e05956a5e137c8195e", "start_line": 281, "end_line": 311}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/console.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/console.py", "rel_path": "scrapy/utils/console.py", "module": "scrapy.utils.console", "ext": "py", "chunk_number": 1, "symbols": ["_embed_ipython_shell", "wrapper", "_embed_bpython_shell", "_embed_ptpython_shell", "_embed_standard_shell", "get_shell_embed_func", "start_python_console", "instance", "python", "load", "default", "interactive", "shell", "embeddable", "future", "typ", "checking", "console", "acceptable", "order", "imports", "rlcompleter", "repl", "banner", "none", "parents", "type", "code", "clear", "config", "always", "redef", "callable", "locals", "readline", "unix", "typing", "breaks", "return", "annotations", "ignore", "defaul", "pytho", "terminal", "print", "embed", "ipython", "standard", "setup", "repeated"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport code\nfrom collections.abc import Callable\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\nEmbedFuncT = Callable[..., None]\nKnownShellsT = dict[str, Callable[..., EmbedFuncT]]\n\n\ndef _embed_ipython_shell(\n    namespace: dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start an IPython Shell\"\"\"\n    try:\n        from IPython.terminal.embed import InteractiveShellEmbed  # noqa: T100,PLC0415\n        from IPython.terminal.ipapp import load_default_config  # noqa: PLC0415\n    except ImportError:\n        from IPython.frontend.terminal.embed import (  # type: ignore[no-redef]  # noqa: T100,PLC0415\n            InteractiveShellEmbed,\n        )\n        from IPython.frontend.terminal.ipapp import (  # type: ignore[no-redef]  # noqa: PLC0415\n            load_default_config,\n        )\n\n    @wraps(_embed_ipython_shell)\n    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n        config = load_default_config()\n        # Always use .instance() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config\n        )\n        shell()\n\n    return wrapper\n\n\ndef _embed_bpython_shell(\n    namespace: dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a bpython shell\"\"\"\n    import bpython  # noqa: PLC0415\n\n    @wraps(_embed_bpython_shell)\n    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n        bpython.embed(locals_=namespace, banner=banner)\n\n    return wrapper\n\n\ndef _embed_ptpython_shell(\n    namespace: dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a ptpython shell\"\"\"\n    import ptpython.repl  # noqa: PLC0415  # pylint: disable=import-error\n\n    @wraps(_embed_ptpython_shell)\n    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n        print(banner)\n        ptpython.repl.embed(locals=namespace)\n\n    return wrapper\n\n\ndef _embed_standard_shell(\n    namespace: dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a standard python shell\"\"\"\n    try:  # readline module is only available on unix systems\n        import readline  # noqa: PLC0415\n    except ImportError:\n        pass\n    else:\n        import rlcompleter  # noqa: F401,PLC0415\n\n        readline.parse_and_bind(\"tab:complete\")  # type: ignore[attr-defined]\n\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n        code.interact(banner=banner, local=namespace)\n\n    return wrapper\n\n\nDEFAULT_PYTHON_SHELLS: KnownShellsT = {\n    \"ptpython\": _embed_ptpython_shell,\n    \"ipython\": _embed_ipython_shell,\n    \"bpython\": _embed_bpython_shell,\n    \"python\": _embed_standard_shell,\n}\n\n\ndef get_shell_embed_func(\n    shells: Iterable[str] | None = None, known_shells: KnownShellsT | None = None\n) -> EmbedFuncT | None:\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None:  # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None:  # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue\n    return None\n\n\ndef start_python_console(\n    namespace: dict[str, Any] | None = None,\n    banner: str = \"\",\n    shells: Iterable[str] | None = None,\n) -> None:\n    \"\"\"Start Python console bound to the given namespace.\n    Readline support and tab completion will be used on Unix, if available.\n    \"\"\"\n    if namespace is None:\n        namespace = {}\n\n    try:\n        shell = get_shell_embed_func(shells)\n        if shell is not None:\n            shell(namespace=namespace, banner=banner)\n    except SystemExit:  # raised when using exit() in python code.interact\n        pass\n", "n_tokens": 1079, "byte_len": 4438, "file_sha1": "9d159a12ea4e43fa0ff66bcd1ee1fc5c47a17050", "start_line": 1, "end_line": 139}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/testsite.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/testsite.py", "rel_path": "scrapy/utils/testsite.py", "module": "scrapy.utils.testsite", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "tearDown", "url", "render", "test_site", "SiteTest", "NoMetaRefreshRedirect", "encoding", "site", "text", "gb18030", "baseurl", "tear", "down", "charset", "internet", "module", "scrapy", "deprecation", "works", "refresh", "plain", "twisted", "test", "listen", "tcp", "return", "resource", "replace", "name", "class", "urljoin", "put", "child", "meta", "main", "deprecated", "path", "print", "port", "pylint", "body", "redirected", "redirect", "interface", "stop", "listening", "warnings", "data", "from"], "ast_kind": "class_or_type", "text": "import warnings\nfrom urllib.parse import urljoin\n\nfrom twisted.web import resource, server, static, util\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nwarnings.warn(\n    \"The scrapy.utils.testsite module is deprecated.\",\n    ScrapyDeprecationWarning,\n)\n\n\nclass SiteTest:\n    def setUp(self):\n        from twisted.internet import reactor\n\n        super().setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"\n\n    def tearDown(self):\n        super().tearDown()\n        self.site.stopListening()\n\n    def url(self, path: str) -> str:\n        return urljoin(self.baseurl, path)\n\n\nclass NoMetaRefreshRedirect(util.Redirect):\n    def render(self, request: server.Request) -> bytes:\n        content = util.Redirect.render(self, request)\n        return content.replace(\n            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n        )\n\n\ndef test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(\n        b\"html\",\n        static.Data(\n            b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n            \"text/html\",\n        ),\n    )\n    r.putChild(\n        b\"enc-gb18030\",\n        static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n    )\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor  # pylint: disable=ungrouped-imports\n\n    port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n    print(f\"http://localhost:{port.getHost().port}/\")\n    reactor.run()\n", "n_tokens": 452, "byte_len": 1860, "file_sha1": "aaa2c7225e8bd7bea863a5370fced1607381bb52", "start_line": 1, "end_line": 64}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/reactor.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/reactor.py", "rel_path": "scrapy/utils/reactor.py", "module": "scrapy.utils.reactor", "ext": "py", "chunk_number": 1, "symbols": ["listen_tcp", "__init__", "schedule", "cancel", "__call__", "set_asyncio_event_loop_policy", "install_reactor", "_get_asyncio_event_loop", "CallLaterOnce", "set", "event", "protocol", "async", "call", "later", "scheduled", "append", "asyncio", "deferreds", "case", "python", "enabled", "restrict", "like", "future", "typ", "checking", "behave", "contextlib", "inconsistent", "set_asyncio_event_loop", "verify_installed_reactor", "verify_installed_asyncio_event_loop", "is_reactor_installed", "is_asyncio_reactor_installed", "does", "problems", "bool", "qualname", "eventloop", "deprecation", "warning", "emitted", "possible", "about", "doesn", "silently", "https", "issuecomment", "interface"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport asyncio\nimport sys\nfrom contextlib import suppress\nfrom typing import TYPE_CHECKING, Any, Generic, TypeVar\nfrom warnings import catch_warnings, filterwarnings\n\nfrom twisted.internet import asyncioreactor, error\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import global_object_name\n\nif TYPE_CHECKING:\n    from asyncio import AbstractEventLoop\n    from collections.abc import Callable\n\n    from twisted.internet.protocol import ServerFactory\n    from twisted.internet.tcp import Port\n\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    from scrapy.utils.asyncio import CallLaterResult\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n\n\ndef listen_tcp(portrange: list[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]  # pylint: disable=inconsistent-return-statements  # noqa: RET503\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    from twisted.internet import reactor\n\n    if len(portrange) > 2:\n        raise ValueError(f\"invalid portrange: {portrange}\")\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1] + 1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise\n\n\nclass CallLaterOnce(Generic[_T]):\n    \"\"\"Schedule a function to be called in the next reactor loop, but only if\n    it hasn't been already scheduled since the last time it ran.\n    \"\"\"\n\n    def __init__(self, func: Callable[_P, _T], *a: _P.args, **kw: _P.kwargs):\n        self._func: Callable[_P, _T] = func\n        self._a: tuple[Any, ...] = a\n        self._kw: dict[str, Any] = kw\n        self._call: CallLaterResult | None = None\n        self._deferreds: list[Deferred] = []\n\n    def schedule(self, delay: float = 0) -> None:\n        # circular import\n        from scrapy.utils.asyncio import call_later  # noqa: PLC0415\n\n        if self._call is None:\n            self._call = call_later(delay, self)\n\n    def cancel(self) -> None:\n        if self._call:\n            self._call.cancel()\n\n    def __call__(self) -> _T:\n        # circular import\n        from scrapy.utils.asyncio import call_later  # noqa: PLC0415\n\n        self._call = None\n        result = self._func(*self._a, **self._kw)\n\n        for d in self._deferreds:\n            call_later(0, d.callback, None)\n        self._deferreds = []\n\n        return result\n\n    async def wait(self):\n        # circular import\n        from scrapy.utils.defer import maybe_deferred_to_future  # noqa: PLC0415\n\n        d = Deferred()\n        self._deferreds.append(d)\n        await maybe_deferred_to_future(d)\n\n\n_asyncio_reactor_path = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n\n\ndef set_asyncio_event_loop_policy() -> None:\n    \"\"\"The policy functions from asyncio often behave unexpectedly,\n    so we restrict their use to the absolutely essential case.\n    This should only be used to install the reactor.\n    \"\"\"\n    policy = asyncio.get_event_loop_policy()\n    if sys.platform == \"win32\" and not isinstance(\n        policy, asyncio.WindowsSelectorEventLoopPolicy\n    ):\n        policy = asyncio.WindowsSelectorEventLoopPolicy()\n        asyncio.set_event_loop_policy(policy)\n\n\ndef install_reactor(reactor_path: str, event_loop_path: str | None = None) -> None:\n    \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n    import path. Also installs the asyncio event loop with the specified import\n    path if the asyncio reactor is enabled\"\"\"\n    reactor_class = load_object(reactor_path)\n    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n        set_asyncio_event_loop_policy()\n        with suppress(error.ReactorAlreadyInstalledError):\n            event_loop = set_asyncio_event_loop(event_loop_path)\n            asyncioreactor.install(eventloop=event_loop)\n    else:\n        *module, _ = reactor_path.split(\".\")\n        installer_path = [*module, \"install\"]\n        installer = load_object(\".\".join(installer_path))\n        with suppress(error.ReactorAlreadyInstalledError):\n            installer()\n\n\ndef _get_asyncio_event_loop() -> AbstractEventLoop:\n    return set_asyncio_event_loop(None)\n\n", "n_tokens": 1033, "byte_len": 4460, "file_sha1": "7382b18b9a54fc6a20f946d11c707567356212f3", "start_line": 1, "end_line": 132}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/reactor.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/reactor.py", "rel_path": "scrapy/utils/reactor.py", "module": "scrapy.utils.reactor", "ext": "py", "chunk_number": 2, "symbols": ["set_asyncio_event_loop", "verify_installed_reactor", "verify_installed_asyncio_event_loop", "is_reactor_installed", "is_asyncio_reactor_installed", "does", "problems", "bool", "qualname", "asyncio", "eventloop", "set", "emitted", "deprecation", "warning", "future", "case", "python", "possible", "about", "doesn", "silently", "https", "issuecomment", "reactor", "path", "loop", "isinstance", "following", "because", "listen_tcp", "__init__", "schedule", "cancel", "__call__", "set_asyncio_event_loop_policy", "install_reactor", "_get_asyncio_event_loop", "CallLaterOnce", "event", "protocol", "async", "call", "later", "scheduled", "append", "deferreds", "enabled", "restrict", "like"], "ast_kind": "class_or_type", "text": "def set_asyncio_event_loop(event_loop_path: str | None) -> AbstractEventLoop:\n    \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n    if event_loop_path is not None:\n        event_loop_class: type[AbstractEventLoop] = load_object(event_loop_path)\n        event_loop = _get_asyncio_event_loop()\n        if not isinstance(event_loop, event_loop_class):\n            event_loop = event_loop_class()\n            asyncio.set_event_loop(event_loop)\n    else:\n        try:\n            with catch_warnings():\n                # In Python 3.10.9, 3.11.1, 3.12 and 3.13, a DeprecationWarning\n                # is emitted about the lack of a current event loop, because in\n                # Python 3.14 and later `get_event_loop` will raise a\n                # RuntimeError in that event. Because our code is already\n                # prepared for that future behavior, we ignore the deprecation\n                # warning.\n                filterwarnings(\n                    \"ignore\",\n                    message=\"There is no current event loop\",\n                    category=DeprecationWarning,\n                )\n                event_loop = asyncio.get_event_loop()\n        except RuntimeError:\n            # `get_event_loop` raises RuntimeError when called with no asyncio\n            # event loop yet installed in the following scenarios:\n            # - Previsibly on Python 3.14 and later.\n            #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902\n            event_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(event_loop)\n    return event_loop\n\n\ndef verify_installed_reactor(reactor_path: str) -> None:\n    \"\"\"Raise :exc:`RuntimeError` if the installed\n    :mod:`~twisted.internet.reactor` does not match the specified import\n    path or if no reactor is installed.\"\"\"\n    if not is_reactor_installed():\n        raise RuntimeError(\n            \"verify_installed_reactor() called without an installed reactor.\"\n        )\n\n    from twisted.internet import reactor\n\n    expected_reactor_type = load_object(reactor_path)\n    reactor_type = type(reactor)\n    if not reactor_type == expected_reactor_type:\n        raise RuntimeError(\n            f\"The installed reactor ({global_object_name(reactor_type)}) \"\n            f\"does not match the requested one ({reactor_path})\"\n        )\n\n\ndef verify_installed_asyncio_event_loop(loop_path: str) -> None:\n    \"\"\"Raise :exc:`RuntimeError` if the even loop of the installed\n    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`\n    does not match the specified import path or if no reactor is installed.\"\"\"\n    if not is_reactor_installed():\n        raise RuntimeError(\n            \"verify_installed_asyncio_event_loop() called without an installed reactor.\"\n        )\n\n    from twisted.internet import reactor\n\n    loop_class = load_object(loop_path)\n    if isinstance(reactor._asyncioEventloop, loop_class):\n        return\n    installed = (\n        f\"{reactor._asyncioEventloop.__class__.__module__}\"\n        f\".{reactor._asyncioEventloop.__class__.__qualname__}\"\n    )\n    raise RuntimeError(\n        \"Scrapy found an asyncio Twisted reactor already \"\n        f\"installed, and its event loop class ({installed}) does \"\n        \"not match the one specified in the ASYNCIO_EVENT_LOOP \"\n        f\"setting ({global_object_name(loop_class)})\"\n    )\n\n\ndef is_reactor_installed() -> bool:\n    \"\"\"Check whether a :mod:`~twisted.internet.reactor` is installed.\"\"\"\n    return \"twisted.internet.reactor\" in sys.modules\n\n\ndef is_asyncio_reactor_installed() -> bool:\n    \"\"\"Check whether the installed reactor is :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n\n    Raise a :exc:`RuntimeError` if no reactor is installed.\n\n    In a future Scrapy version, when Scrapy supports running without a Twisted\n    reactor, this function won't be useful for checking if it's possible to use\n    asyncio features, so the code that that doesn't directly require a Twisted\n    reactor should use :func:`scrapy.utils.asyncio.is_asyncio_available`\n    instead of this function.\n\n    .. versionchanged:: 2.13\n       In earlier Scrapy versions this function silently installed the default\n       reactor if there was no reactor installed. Now it raises an exception to\n       prevent silent problems in this case.\n    \"\"\"\n    if not is_reactor_installed():\n        raise RuntimeError(\n            \"is_asyncio_reactor_installed() called without an installed reactor.\"\n        )\n\n    from twisted.internet import reactor\n\n    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n", "n_tokens": 981, "byte_len": 4599, "file_sha1": "7382b18b9a54fc6a20f946d11c707567356212f3", "start_line": 133, "end_line": 241}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/curl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/curl.py", "rel_path": "scrapy/utils/curl.py", "module": "scrapy.utils.curl", "ext": "py", "chunk_number": 1, "symbols": ["__call__", "error", "_parse_headers_and_cookies", "curl_to_request_kwargs", "DataAction", "CurlParser", "method", "bool", "containing", "parsed", "args", "append", "cookies", "morsel", "syntax", "emitted", "lib", "w3lib", "curl", "request", "name", "enabled", "future", "typ", "checking", "string", "parse", "headers", "missing", "namespace", "unknown", "cookie", "param", "verbose", "items", "known", "here", "auth", "none", "parameter", "join", "dest", "shlex", "http", "default", "either", "sequence", "automatically", "compression", "values"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport argparse\nimport warnings\nfrom http.cookies import SimpleCookie\nfrom shlex import split\nfrom typing import TYPE_CHECKING, Any, NoReturn\nfrom urllib.parse import urlparse\n\nfrom w3lib.http import basic_auth_header\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n\nclass DataAction(argparse.Action):\n    def __call__(\n        self,\n        parser: argparse.ArgumentParser,\n        namespace: argparse.Namespace,\n        values: str | Sequence[Any] | None,\n        option_string: str | None = None,\n    ) -> None:\n        value = str(values)\n        value = value.removeprefix(\"$\")\n        setattr(namespace, self.dest, value)\n\n\nclass CurlParser(argparse.ArgumentParser):\n    def error(self, message: str) -> NoReturn:\n        error_msg = f\"There was an error parsing the curl command: {message}\"\n        raise ValueError(error_msg)\n\n\ncurl_parser = CurlParser()\ncurl_parser.add_argument(\"url\")\ncurl_parser.add_argument(\"-H\", \"--header\", dest=\"headers\", action=\"append\")\ncurl_parser.add_argument(\"-X\", \"--request\", dest=\"method\")\ncurl_parser.add_argument(\"-b\", \"--cookie\", dest=\"cookies\", action=\"append\")\ncurl_parser.add_argument(\"-d\", \"--data\", \"--data-raw\", dest=\"data\", action=DataAction)\ncurl_parser.add_argument(\"-u\", \"--user\", dest=\"auth\")\n\n\nsafe_to_ignore_arguments = [\n    [\"--compressed\"],\n    # `--compressed` argument is not safe to ignore, but it's included here\n    # because the `HttpCompressionMiddleware` is enabled by default\n    [\"-s\", \"--silent\"],\n    [\"-v\", \"--verbose\"],\n    [\"-#\", \"--progress-bar\"],\n]\n\nfor argument in safe_to_ignore_arguments:\n    curl_parser.add_argument(*argument, action=\"store_true\")\n\n\ndef _parse_headers_and_cookies(\n    parsed_args: argparse.Namespace,\n) -> tuple[list[tuple[str, bytes]], dict[str, str]]:\n    headers: list[tuple[str, bytes]] = []\n    cookies: dict[str, str] = {}\n    for header in parsed_args.headers or ():\n        name, val = header.split(\":\", 1)\n        name = name.strip()\n        val = val.strip()\n        if name.title() == \"Cookie\":\n            for name, morsel in SimpleCookie(val).items():\n                cookies[name] = morsel.value\n        else:\n            headers.append((name, val))\n\n    for cookie_param in parsed_args.cookies or ():\n        # curl can treat this parameter as either \"key=value; key2=value2\" pairs, or a filename.\n        # Scrapy will only support key-value pairs.\n        if \"=\" not in cookie_param:\n            continue\n        for name, morsel in SimpleCookie(cookie_param).items():\n            cookies[name] = morsel.value\n\n    if parsed_args.auth:\n        user, password = parsed_args.auth.split(\":\", 1)\n        headers.append((\"Authorization\", basic_auth_header(user, password)))\n\n    return headers, cookies\n\n\ndef curl_to_request_kwargs(\n    curl_command: str, ignore_unknown_options: bool = True\n) -> dict[str, Any]:\n    \"\"\"Convert a cURL command syntax to Request kwargs.\n\n    :param str curl_command: string containing the curl command\n    :param bool ignore_unknown_options: If true, only a warning is emitted when\n                                        cURL options are unknown. Otherwise\n                                        raises an error. (default: True)\n    :return: dictionary of Request kwargs\n    \"\"\"\n\n    curl_args = split(curl_command)\n\n    if curl_args[0] != \"curl\":\n        raise ValueError('A curl command must start with \"curl\"')\n\n    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n\n    if argv:\n        msg = f\"Unrecognized options: {', '.join(argv)}\"\n        if ignore_unknown_options:\n            warnings.warn(msg)\n        else:\n            raise ValueError(msg)\n\n    url = parsed_args.url\n\n    # curl automatically prepends 'http' if the scheme is missing, but Request\n    # needs the scheme to work\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme:\n        url = \"http://\" + url\n\n    method = parsed_args.method or \"GET\"\n\n    result: dict[str, Any] = {\"method\": method.upper(), \"url\": url}\n\n    headers, cookies = _parse_headers_and_cookies(parsed_args)\n\n    if headers:\n        result[\"headers\"] = headers\n    if cookies:\n        result[\"cookies\"] = cookies\n    if parsed_args.data:\n        result[\"body\"] = parsed_args.data\n        if not parsed_args.method:\n            # if the \"data\" is specified but the \"method\" is not specified,\n            # the default method is 'POST'\n            result[\"method\"] = \"POST\"\n\n    return result\n", "n_tokens": 1001, "byte_len": 4449, "file_sha1": "b322984dae87e9c9469b3e513462368f2fd663e9", "start_line": 1, "end_line": 139}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/signal.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/signal.py", "rel_path": "scrapy/utils/signal.py", "module": "scrapy.utils.signal", "ext": "py", "chunk_number": 1, "symbols": ["send_catch_log", "send_catch_log_deferred", "logerror", "disconnect_all", "disconnect", "failure", "async", "logs", "append", "signal", "after", "failures", "responses", "error", "like", "useful", "future", "https", "send", "catch", "deferreds", "loop", "isinstance", "handler", "get", "all", "results", "none", "handlers", "returns", "html", "sequence", "exc", "info", "fired", "await", "internet", "spider", "functions", "asynchronous", "add", "both", "typing", "return", "annotations", "maybe", "deferred", "sender", "noqa", "list"], "ast_kind": "function_or_method", "text": "\"\"\"Helper functions for working with signals\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import Generator, Sequence\nfrom typing import Any as TypingAny\n\nfrom pydispatch.dispatcher import (\n    Anonymous,\n    Any,\n    disconnect,\n    getAllReceivers,\n    liveReceivers,\n)\nfrom pydispatch.robustapply import robustApply\nfrom twisted.internet.defer import Deferred, DeferredList, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.utils.defer import maybe_deferred_to_future, maybeDeferred_coro\nfrom scrapy.utils.log import failure_to_exc_info\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_catch_log(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny,\n) -> list[tuple[TypingAny, TypingAny]]:\n    \"\"\"Like ``pydispatcher.robust.sendRobust()`` but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop(\"dont_log\", ())\n    dont_log = tuple(dont_log) if isinstance(dont_log, Sequence) else (dont_log,)\n    dont_log += (StopDownload,)\n    spider = named.get(\"spider\")\n    responses: list[tuple[TypingAny, TypingAny]] = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        result: TypingAny\n        try:\n            response = robustApply(\n                receiver, signal=signal, sender=sender, *arguments, **named\n            )\n            if isinstance(response, Deferred):\n                logger.error(\n                    \"Cannot return deferreds from signal handler: %(receiver)s\",\n                    {\"receiver\": receiver},\n                    extra={\"spider\": spider},\n                )\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\n                \"Error caught on signal handler: %(receiver)s\",\n                {\"receiver\": receiver},\n                exc_info=True,\n                extra={\"spider\": spider},\n            )\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses\n\n\n@inlineCallbacks\ndef send_catch_log_deferred(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny,\n) -> Generator[Deferred[TypingAny], TypingAny, list[tuple[TypingAny, TypingAny]]]:\n    \"\"\"Like :func:`send_catch_log` but supports :ref:`asynchronous signal handlers\n    <signal-deferred>`.\n\n    Returns a deferred that gets fired once all signal handlers have finished.\n    \"\"\"\n\n    def logerror(failure: Failure, recv: TypingAny) -> Failure:\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\n                \"Error caught on signal handler: %(receiver)s\",\n                {\"receiver\": recv},\n                exc_info=failure_to_exc_info(failure),\n                extra={\"spider\": spider},\n            )\n        return failure\n\n    dont_log = named.pop(\"dont_log\", None)\n    spider = named.get(\"spider\")\n    dfds: list[Deferred[tuple[TypingAny, TypingAny]]] = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d: Deferred[TypingAny] = maybeDeferred_coro(\n            robustApply, receiver, signal=signal, sender=sender, *arguments, **named\n        )\n        d.addErrback(logerror, receiver)\n        # TODO https://pylint.readthedocs.io/en/latest/user_guide/messages/warning/cell-var-from-loop.html\n        d2: Deferred[tuple[TypingAny, TypingAny]] = d.addBoth(\n            lambda result: (\n                receiver,  # pylint: disable=cell-var-from-loop  # noqa: B023\n                result,\n            )\n        )\n        dfds.append(d2)\n\n    results = yield DeferredList(dfds)\n    return [result[1] for result in results]\n\n\nasync def send_catch_log_async(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny,\n) -> list[tuple[TypingAny, TypingAny]]:\n    \"\"\"Like :func:`send_catch_log` but supports :ref:`asynchronous signal handlers\n    <signal-deferred>`.\n\n    Returns a coroutine that completes once all signal handlers have finished.\n\n    .. versionadded:: VERSION\n    \"\"\"\n    return await maybe_deferred_to_future(\n        send_catch_log_deferred(signal, sender, *arguments, **named)\n    )\n\n\ndef disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests.\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)\n", "n_tokens": 1076, "byte_len": 4662, "file_sha1": "2f0403910bb64e6716a8d6783e494e46f646d604", "start_line": 1, "end_line": 137}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/log.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/log.py", "rel_path": "scrapy/utils/log.py", "module": "scrapy.utils.log", "ext": "py", "chunk_number": 1, "symbols": ["failure_to_exc_info", "__init__", "filter", "configure_logging", "install_scrapy_root_handler", "get_scrapy_root_handler", "_get_handler", "TopLevelFormatter", "encoding", "failure", "filelock", "initialize", "does", "log", "record", "bool", "python", "name", "disable", "existing", "going", "future", "typ", "checking", "elif", "install", "scrapy", "null", "handler", "get", "log_scrapy_info", "log_reactor_info", "write", "flush", "emit", "logformatter_adapter", "process", "StreamLogger", "LogCounterHandler", "SpiderLoggerAdapter", "takes", "taken", "asyncio", "eventloop", "formatter", "inc", "value", "instance", "case", "stream"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nimport pprint\nimport sys\nfrom collections.abc import MutableMapping\nfrom logging.config import dictConfig\nfrom typing import TYPE_CHECKING, Any, Optional, cast\n\nfrom twisted.internet import asyncioreactor\nfrom twisted.python import log as twisted_log\nfrom twisted.python.failure import Failure\n\nimport scrapy\nfrom scrapy.settings import Settings, _SettingsKeyT\nfrom scrapy.utils.versions import get_versions\n\nif TYPE_CHECKING:\n    from types import TracebackType\n\n    from scrapy.crawler import Crawler\n    from scrapy.logformatter import LogFormatterResult\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef failure_to_exc_info(\n    failure: Failure,\n) -> tuple[type[BaseException], BaseException, TracebackType | None] | None:\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        assert failure.type\n        assert failure.value\n        return (\n            failure.type,\n            failure.value,\n            cast(\"Optional[TracebackType]\", failure.getTracebackObject()),\n        )\n    return None\n\n\nclass TopLevelFormatter(logging.Filter):\n    \"\"\"Keep only top level loggers' name (direct children from root) from\n    records.\n\n    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics\n    the old Scrapy log behaviour and helps shortening long names.\n\n    Since it can't be set for just one logger (it won't propagate for its\n    children), it's going to be set in the root handler, with a parametrized\n    ``loggers`` list where it should act.\n    \"\"\"\n\n    def __init__(self, loggers: list[str] | None = None):\n        super().__init__()\n        self.loggers: list[str] = loggers or []\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        if any(record.name.startswith(logger + \".\") for logger in self.loggers):\n            record.name = record.name.split(\".\", 1)[0]\n        return True\n\n\nDEFAULT_LOGGING = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"loggers\": {\n        \"filelock\": {\n            \"level\": \"ERROR\",\n        },\n        \"hpack\": {\n            \"level\": \"ERROR\",\n        },\n        \"scrapy\": {\n            \"level\": \"DEBUG\",\n        },\n        \"twisted\": {\n            \"level\": \"ERROR\",\n        },\n    },\n}\n\n\ndef configure_logging(\n    settings: Settings | dict[_SettingsKeyT, Any] | None = None,\n    install_root_handler: bool = True,\n) -> None:\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver(\"twisted\")\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool(\"LOG_STDOUT\"):\n        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))\n\n    if install_root_handler:\n        install_scrapy_root_handler(settings)\n\n\n_scrapy_root_handler: logging.Handler | None = None\n\n\ndef install_scrapy_root_handler(settings: Settings) -> None:\n    global _scrapy_root_handler  # noqa: PLW0603  # pylint: disable=global-statement\n\n    if (\n        _scrapy_root_handler is not None\n        and _scrapy_root_handler in logging.root.handlers\n    ):\n        logging.root.removeHandler(_scrapy_root_handler)\n    logging.root.setLevel(logging.NOTSET)\n    _scrapy_root_handler = _get_handler(settings)\n    logging.root.addHandler(_scrapy_root_handler)\n\n\ndef get_scrapy_root_handler() -> logging.Handler | None:\n    return _scrapy_root_handler\n\n\ndef _get_handler(settings: Settings) -> logging.Handler:\n    \"\"\"Return a log handler object according to settings\"\"\"\n    filename = settings.get(\"LOG_FILE\")\n    handler: logging.Handler\n    if filename:\n        mode = \"a\" if settings.getbool(\"LOG_FILE_APPEND\") else \"w\"\n        encoding = settings.get(\"LOG_ENCODING\")\n        handler = logging.FileHandler(filename, mode=mode, encoding=encoding)\n    elif settings.getbool(\"LOG_ENABLED\"):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get(\"LOG_FORMAT\"), datefmt=settings.get(\"LOG_DATEFORMAT\")\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get(\"LOG_LEVEL\"))\n    if settings.getbool(\"LOG_SHORT_NAMES\"):\n        handler.addFilter(TopLevelFormatter([\"scrapy\"]))\n    return handler\n\n", "n_tokens": 1160, "byte_len": 5306, "file_sha1": "1292c72f93be81924b09d82e1c514f3376178de8", "start_line": 1, "end_line": 173}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/log.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/log.py", "rel_path": "scrapy/utils/log.py", "module": "scrapy.utils.log", "ext": "py", "chunk_number": 2, "symbols": ["log_scrapy_info", "log_reactor_info", "__init__", "write", "flush", "emit", "logformatter_adapter", "process", "StreamLogger", "LogCounterHandler", "SpiderLoggerAdapter", "takes", "log", "record", "taken", "asyncio", "eventloop", "formatter", "inc", "value", "instance", "case", "stream", "levels", "doesn", "https", "play", "scrapy", "get", "versions", "failure_to_exc_info", "filter", "configure_logging", "install_scrapy_root_handler", "get_scrapy_root_handler", "_get_handler", "TopLevelFormatter", "encoding", "failure", "filelock", "initialize", "does", "bool", "python", "name", "disable", "existing", "going", "future", "typ"], "ast_kind": "class_or_type", "text": "def log_scrapy_info(settings: Settings) -> None:\n    logger.info(\n        \"Scrapy %(version)s started (bot: %(bot)s)\",\n        {\"version\": scrapy.__version__, \"bot\": settings[\"BOT_NAME\"]},\n    )\n    software = settings.getlist(\"LOG_VERSIONS\")\n    if not software:\n        return\n    versions = pprint.pformat(dict(get_versions(software)), sort_dicts=False)\n    logger.info(f\"Versions:\\n{versions}\")\n\n\ndef log_reactor_info() -> None:\n    from twisted.internet import reactor\n\n    logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n        logger.debug(\n            \"Using asyncio event loop: %s.%s\",\n            reactor._asyncioEventloop.__module__,\n            reactor._asyncioEventloop.__class__.__name__,\n        )\n\n\nclass StreamLogger:\n    \"\"\"Fake file-like stream object that redirects writes to a logger instance\n\n    Taken from:\n        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n    \"\"\"\n\n    def __init__(self, logger: logging.Logger, log_level: int = logging.INFO):\n        self.logger: logging.Logger = logger\n        self.log_level: int = log_level\n        self.linebuf: str = \"\"\n\n    def write(self, buf: str) -> None:\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())\n\n    def flush(self) -> None:\n        for h in self.logger.handlers:\n            h.flush()\n\n\nclass LogCounterHandler(logging.Handler):\n    \"\"\"Record log levels count into a crawler stats\"\"\"\n\n    def __init__(self, crawler: Crawler, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.crawler: Crawler = crawler\n\n    def emit(self, record: logging.LogRecord) -> None:\n        sname = f\"log_count/{record.levelname}\"\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(sname)\n\n\ndef logformatter_adapter(\n    logkws: LogFormatterResult,\n) -> tuple[int, str, dict[str, Any] | tuple[Any, ...]]:\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n\n    level = logkws.get(\"level\", logging.INFO)\n    message = logkws.get(\"msg\") or \"\"\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = cast(\"dict[str, Any]\", logkws) if not logkws.get(\"args\") else logkws[\"args\"]\n\n    return (level, message, args)\n\n\nclass SpiderLoggerAdapter(logging.LoggerAdapter):\n    def process(\n        self, msg: str, kwargs: MutableMapping[str, Any]\n    ) -> tuple[str, MutableMapping[str, Any]]:\n        \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n        if isinstance(kwargs.get(\"extra\"), MutableMapping):\n            kwargs[\"extra\"].update(self.extra)\n        else:\n            kwargs[\"extra\"] = self.extra\n\n        return msg, kwargs\n", "n_tokens": 707, "byte_len": 2997, "file_sha1": "1292c72f93be81924b09d82e1c514f3376178de8", "start_line": 174, "end_line": 261}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/_compression.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/_compression.py", "rel_path": "scrapy/utils/_compression.py", "module": "scrapy.utils._compression", "ext": "py", "chunk_number": 1, "symbols": ["_brotli_decompress", "_inflate", "_unbrotli", "_unzstd", "_DecompressionMaxSizeExceeded", "decompressobj", "decompress", "future", "zstandard", "decompression", "max", "name", "yaws", "deprecated", "more", "codimi", "contextlib", "uninstall", "sent", "brotlipy", "exceed", "microsoft", "process", "decompressed", "size", "stop", "html", "http", "ugly", "number", "seek", "gzip", "suppress", "faq", "faq38", "zstd", "decompressor", "return", "class", "servers", "deflate", "itself", "hack", "warnings", "input", "chunk", "zlib", "unzstd", "exceptions", "write"], "ast_kind": "class_or_type", "text": "import contextlib\nimport zlib\nfrom io import BytesIO\nfrom warnings import warn\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\ntry:\n    try:\n        import brotli\n    except ImportError:\n        import brotlicffi as brotli\nexcept ImportError:\n    pass\nelse:\n    try:\n        brotli.Decompressor.process\n    except AttributeError:\n        warn(\n            (\n                \"You have brotlipy installed, and Scrapy will use it, but \"\n                \"Scrapy support for brotlipy is deprecated and will stop \"\n                \"working in a future version of Scrapy. brotlipy itself is \"\n                \"deprecated, it has been superseded by brotlicffi. Please, \"\n                \"uninstall brotlipy and install brotli or brotlicffi instead. \"\n                \"brotlipy has the same import name as brotli, so keeping both \"\n                \"installed is strongly discouraged.\"\n            ),\n            ScrapyDeprecationWarning,\n        )\n\n        def _brotli_decompress(decompressor, data):\n            return decompressor.decompress(data)\n\n    else:\n\n        def _brotli_decompress(decompressor, data):\n            return decompressor.process(data)\n\n\nwith contextlib.suppress(ImportError):\n    import zstandard\n\n\n_CHUNK_SIZE = 65536  # 64 KiB\n\n\nclass _DecompressionMaxSizeExceeded(ValueError):\n    pass\n\n\ndef _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = zlib.decompressobj()\n    raw_decompressor = zlib.decompressobj(wbits=-15)\n    input_stream = BytesIO(data)\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        input_chunk = input_stream.read(_CHUNK_SIZE)\n        try:\n            output_chunk = decompressor.decompress(input_chunk)\n        except zlib.error:\n            if decompressor != raw_decompressor:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                decompressor = raw_decompressor\n                output_chunk = decompressor.decompress(input_chunk)\n            else:\n                raise\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = brotli.Decompressor()\n    input_stream = BytesIO(data)\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        input_chunk = input_stream.read(_CHUNK_SIZE)\n        output_chunk = _brotli_decompress(decompressor, input_chunk)\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = zstandard.ZstdDecompressor()\n    stream_reader = decompressor.stream_reader(BytesIO(data))\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        output_chunk = stream_reader.read(_CHUNK_SIZE)\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n", "n_tokens": 978, "byte_len": 4295, "file_sha1": "5aec010ad52ce5d3ed111ee2b0fcfdba7f6fb6bb", "start_line": 1, "end_line": 125}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/job.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/job.py", "rel_path": "scrapy/utils/job.py", "module": "scrapy.utils.job", "ext": "py", "chunk_number": 1, "symbols": ["job_dir", "typing", "return", "annotations", "path", "scrapy", "future", "typ", "checking", "mkdir", "pathlib", "job", "dir", "true", "from", "settings", "base", "jobdir", "exists", "none", "parents", "import"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from scrapy.settings import BaseSettings\n\n\ndef job_dir(settings: BaseSettings) -> str | None:\n    path: str | None = settings[\"JOBDIR\"]\n    if not path:\n        return None\n    if not Path(path).exists():\n        Path(path).mkdir(parents=True)\n    return path\n", "n_tokens": 88, "byte_len": 377, "file_sha1": "6110029a17aa5ad485a753ab5078aa1a93ec48c7", "start_line": 1, "end_line": 17}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/conf.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/conf.py", "rel_path": "scrapy/utils/conf.py", "module": "scrapy.utils.conf", "ext": "py", "chunk_number": 1, "symbols": ["build_component_list", "_check_components", "_map_keys", "_validate_values", "arglist_to_dict", "closest_scrapy_cfg", "init_env", "get_config", "get_sources", "feed_complete_default_values_from_settings", "encoding", "arglist", "initialize", "usage", "error", "bool", "arg", "arg2", "your", "prevpath", "item", "export", "append", "real", "python", "getpriority", "name", "locate", "val", "val2", "feed_process_params_from_cli", "check_valid_format", "after", "future", "typ", "checking", "path", "config", "parser", "arg1", "feed", "process", "numbers", "runspider", "overwrite", "output", "scrap", "setting", "settings", "isinstance"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport numbers\nimport os\nimport sys\nfrom configparser import ConfigParser\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, cast\n\nfrom scrapy.exceptions import UsageError\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.deprecate import update_classpath\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from collections.abc import Collection, Iterable, Mapping, MutableMapping\n\n\ndef build_component_list(\n    compdict: MutableMapping[Any, Any],\n    *,\n    convert: Callable[[Any], Any] = update_classpath,\n) -> list[Any]:\n    \"\"\"Compose a component list from a :ref:`component priority dictionary\n    <component-priority-dictionaries>`.\"\"\"\n\n    def _check_components(complist: Collection[Any]) -> None:\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(\n                f\"Some paths in {complist!r} convert to the same object, \"\n                \"please update your settings\"\n            )\n\n    def _map_keys(compdict: Mapping[Any, Any]) -> BaseSettings | dict[Any, Any]:\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                assert prio is not None\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(\n                        f\"Some paths in {list(compdict.keys())!r} \"\n                        \"convert to the same \"\n                        \"object, please update your settings\"\n                    )\n                compbs.set(convert(k), v, priority=prio)\n            return compbs\n        _check_components(compdict)\n        return {convert(k): v for k, v in compdict.items()}\n\n    def _validate_values(compdict: Mapping[Any, Any]) -> None:\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(\n                    f\"Invalid value {value} for component {name}, \"\n                    \"please provide a real number or None instead\"\n                )\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n\n\ndef arglist_to_dict(arglist: list[str]) -> dict[str, str]:\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split(\"=\", 1) for x in arglist)\n\n\ndef closest_scrapy_cfg(\n    path: str | os.PathLike = \".\",\n    prevpath: str | os.PathLike | None = None,\n) -> str:\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if prevpath is not None and str(path) == str(prevpath):\n        return \"\"\n    path = Path(path).resolve()\n    cfgfile = path / \"scrapy.cfg\"\n    if cfgfile.exists():\n        return str(cfgfile)\n    return closest_scrapy_cfg(path.parent, path)\n\n\ndef init_env(project: str = \"default\", set_syspath: bool = True) -> None:\n    \"\"\"Initialize environment to use command-line tool from inside a project\n    dir. This sets the Scrapy settings module and modifies the Python path to\n    be able to locate the project module.\n    \"\"\"\n    cfg = get_config()\n    if cfg.has_option(\"settings\", project):\n        os.environ[\"SCRAPY_SETTINGS_MODULE\"] = cfg.get(\"settings\", project)\n    closest = closest_scrapy_cfg()\n    if closest:\n        projdir = str(Path(closest).parent)\n        if set_syspath and projdir not in sys.path:\n            sys.path.append(projdir)\n\n\ndef get_config(use_closest: bool = True) -> ConfigParser:\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg\n\n\ndef get_sources(use_closest: bool = True) -> list[str]:\n    xdg_config_home = (\n        os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n    )\n    sources = [\n        \"/etc/scrapy.cfg\",\n        r\"c:\\scrapy\\scrapy.cfg\",\n        str(Path(xdg_config_home) / \"scrapy.cfg\"),\n        str(Path(\"~/.scrapy.cfg\").expanduser()),\n    ]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources\n\n\ndef feed_complete_default_values_from_settings(\n    feed: dict[str, Any], settings: BaseSettings\n) -> dict[str, Any]:\n    out = feed.copy()\n    out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n    out.setdefault(\"item_export_kwargs\", {})\n    if settings[\"FEED_EXPORT_INDENT\"] is None:\n        out.setdefault(\"indent\", None)\n    else:\n        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n    return out\n\n", "n_tokens": 1165, "byte_len": 5120, "file_sha1": "b8148c5d466e678be60063427dfb00f55bfe5a71", "start_line": 1, "end_line": 143}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/conf.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/conf.py", "rel_path": "scrapy/utils/conf.py", "module": "scrapy.utils.conf", "ext": "py", "chunk_number": 2, "symbols": ["feed_process_params_from_cli", "check_valid_format", "used", "getwithbase", "usage", "error", "format", "false", "check", "valid", "result", "except", "feed", "uri", "cast", "return", "feeds", "supported", "please", "getdict", "quantities", "dict", "after", "colon", "value", "replace", "their", "commands", "checks", "output", "build_component_list", "_check_components", "_map_keys", "_validate_values", "arglist_to_dict", "closest_scrapy_cfg", "init_env", "get_config", "get_sources", "feed_complete_default_values_from_settings", "encoding", "arglist", "initialize", "bool", "arg", "arg2", "your", "prevpath", "item", "export"], "ast_kind": "function_or_method", "text": "def feed_process_params_from_cli(\n    settings: BaseSettings,\n    output: list[str],\n    *,\n    overwrite_output: list[str] | None = None,\n) -> dict[str, dict[str, Any]]:\n    \"\"\"\n    Receives feed export params (from the 'crawl' or 'runspider' commands),\n    checks for inconsistencies in their quantities and returns a dictionary\n    suitable to be used as the FEEDS setting.\n    \"\"\"\n    valid_output_formats: Iterable[str] = without_none_values(\n        cast(\"dict[str, str]\", settings.getwithbase(\"FEED_EXPORTERS\"))\n    ).keys()\n\n    def check_valid_format(output_format: str) -> None:\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )\n\n    overwrite = False\n    if overwrite_output:\n        if output:\n            raise UsageError(\n                \"Please use only one of -o/--output and -O/--overwrite-output\"\n            )\n        output = overwrite_output\n        overwrite = True\n\n    result: dict[str, dict[str, Any]] = {}\n    for element in output:\n        try:\n            feed_uri, feed_format = element.rsplit(\":\", 1)\n            check_valid_format(feed_format)\n        except (ValueError, UsageError):\n            feed_uri = element\n            feed_format = Path(element).suffix.replace(\".\", \"\")\n        else:\n            if feed_uri == \"-\":\n                feed_uri = \"stdout:\"\n        check_valid_format(feed_format)\n        result[feed_uri] = {\"format\": feed_format}\n        if overwrite:\n            result[feed_uri][\"overwrite\"] = True\n\n    # FEEDS setting should take precedence over the matching CLI options\n    result.update(settings.getdict(\"FEEDS\"))\n\n    return result\n", "n_tokens": 418, "byte_len": 1912, "file_sha1": "b8148c5d466e678be60063427dfb00f55bfe5a71", "start_line": 144, "end_line": 197}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/asyncio.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/asyncio.py", "rel_path": "scrapy/utils/asyncio.py", "module": "scrapy.utils.asyncio", "ext": "py", "chunk_number": 1, "symbols": ["is_asyncio_available", "__init__", "running", "start", "AsyncioLoopingCall", "while", "async", "type", "var", "bool", "parallel", "immediately", "future", "python", "currently", "coroutine", "related", "possible", "doesn", "sleep", "kwargs", "between", "more", "typ", "checking", "delayed", "call", "callable", "loop", "generator", "_to_sleep", "stop", "_call", "create_looping_call", "call_later", "from_asyncio", "from_twisted", "cancel", "CallLaterResult", "later", "instance", "after", "error", "elif", "delay", "provided", "iterating", "isinstance", "task", "asyncio"], "ast_kind": "class_or_type", "text": "\"\"\"Utilities related to asyncio and its support in Scrapy.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport time\nfrom collections.abc import AsyncIterator, Callable, Coroutine, Iterable\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.task import LoopingCall\n\nfrom scrapy.utils.asyncgen import as_async_generator\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\n\nif TYPE_CHECKING:\n    from twisted.internet.base import DelayedCall\n\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    # typing.Self, typing.TypeVarTuple and typing.Unpack require Python 3.11\n    from typing_extensions import Concatenate, ParamSpec, Self, TypeVarTuple, Unpack\n\n    _P = ParamSpec(\"_P\")\n    _Ts = TypeVarTuple(\"_Ts\")\n\n\n_T = TypeVar(\"_T\")\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef is_asyncio_available() -> bool:\n    \"\"\"Check if it's possible to call asyncio code that relies on the asyncio event loop.\n\n    .. versionadded:: VERSION\n\n    Currently this function is identical to\n    :func:`scrapy.utils.reactor.is_asyncio_reactor_installed`: it returns\n    ``True`` if the Twisted reactor that is installed is\n    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`, returns\n    ``False`` if a different reactor is installed, and raises a\n    :exc:`RuntimeError` if no reactor is installed. In a future Scrapy version,\n    when Scrapy supports running without a Twisted reactor, this function will\n    also return ``True`` when running in that mode, so code that doesn't\n    directly require a Twisted reactor should use this function instead of\n    :func:`~scrapy.utils.reactor.is_asyncio_reactor_installed`.\n\n    When this returns ``True``, an asyncio loop is installed and used by\n    Scrapy. It's possible to call functions that require it, such as\n    :func:`asyncio.sleep`, and await on :class:`asyncio.Future` objects in\n    Scrapy-related code.\n\n    When this returns ``False``, a non-asyncio Twisted reactor is installed.\n    It's not possible to use asyncio features that require an asyncio event\n    loop or await on :class:`asyncio.Future` objects in Scrapy-related code,\n    but it's possible to await on :class:`~twisted.internet.defer.Deferred`\n    objects.\n    \"\"\"\n    if not is_reactor_installed():\n        raise RuntimeError(\n            \"is_asyncio_available() called without an installed reactor.\"\n        )\n\n    return is_asyncio_reactor_installed()\n\n\nasync def _parallel_asyncio(\n    iterable: Iterable[_T] | AsyncIterator[_T],\n    count: int,\n    callable_: Callable[Concatenate[_T, _P], Coroutine[Any, Any, None]],\n    *args: _P.args,\n    **kwargs: _P.kwargs,\n) -> None:\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    This function is only used in\n    :meth:`scrapy.core.scraper.Scraper.handle_spider_output_async` and so it\n    assumes that neither *callable* nor iterating *iterable* will raise an\n    exception.\n    \"\"\"\n    queue: asyncio.Queue[_T | None] = asyncio.Queue()\n\n    async def worker() -> None:\n        while True:\n            item = await queue.get()\n            if item is None:\n                break\n            try:\n                await callable_(item, *args, **kwargs)\n            finally:\n                queue.task_done()\n\n    async def fill_queue() -> None:\n        async for item in as_async_generator(iterable):\n            await queue.put(item)\n        for _ in range(count):\n            await queue.put(None)\n\n    fill_task = asyncio.create_task(fill_queue())\n    work_tasks = [asyncio.create_task(worker()) for _ in range(count)]\n    await asyncio.wait([fill_task, *work_tasks])\n\n\nclass AsyncioLoopingCall:\n    \"\"\"A simple implementation of a periodic call using asyncio, keeping\n    some API and behavior compatibility with the Twisted ``LoopingCall``.\n\n    The function is called every *interval* seconds, independent of the finish\n    time of the previous call. If the function  is still running when it's time\n    to call it again, calls are skipped until the function finishes.\n\n    The function must not return a coroutine or a ``Deferred``.\n    \"\"\"\n\n    def __init__(self, func: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs):\n        self._func: Callable[_P, _T] = func\n        self._args: tuple[Any, ...] = args\n        self._kwargs: dict[str, Any] = kwargs\n        self._task: asyncio.Task | None = None\n        self.interval: float | None = None\n        self._start_time: float | None = None\n\n    @property\n    def running(self) -> bool:\n        return self._start_time is not None\n\n    def start(self, interval: float, now: bool = True) -> None:\n        \"\"\"Start calling the function every *interval* seconds.\n\n        :param interval: The interval in seconds between calls.\n        :type interval: float\n\n        :param now: If ``True``, also call the function immediately.\n        :type now: bool\n        \"\"\"\n        if self.running:\n            raise RuntimeError(\"AsyncioLoopingCall already running\")\n\n        if interval <= 0:\n            raise ValueError(\"Interval must be greater than 0\")\n\n        self.interval = interval\n        self._start_time = time.time()\n        if now:\n            self._call()\n        loop = asyncio.get_event_loop()\n        self._task = loop.create_task(self._loop())\n", "n_tokens": 1235, "byte_len": 5405, "file_sha1": "b2644ed996ccd736452df2ab62a4e402cdfd4c81", "start_line": 1, "end_line": 151}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/asyncio.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/asyncio.py", "rel_path": "scrapy/utils/asyncio.py", "module": "scrapy.utils.asyncio", "ext": "py", "chunk_number": 2, "symbols": ["_to_sleep", "stop", "_call", "create_looping_call", "call_later", "from_asyncio", "from_twisted", "cancel", "CallLaterResult", "while", "async", "call", "later", "instance", "after", "coroutine", "sleep", "error", "kwargs", "elif", "delayed", "delay", "loop", "provided", "isinstance", "asyncio", "looping", "object", "func", "none", "is_asyncio_available", "__init__", "running", "start", "AsyncioLoopingCall", "type", "var", "bool", "parallel", "immediately", "future", "python", "currently", "related", "possible", "doesn", "between", "more", "typ", "checking"], "ast_kind": "class_or_type", "text": "    def _to_sleep(self) -> float:\n        \"\"\"Return the time to sleep until the next call.\"\"\"\n        assert self.interval is not None\n        assert self._start_time is not None\n        now = time.time()\n        running_for = now - self._start_time\n        return self.interval - (running_for % self.interval)\n\n    async def _loop(self) -> None:\n        \"\"\"Run an infinite loop that calls the function periodically.\"\"\"\n        while self.running:\n            await asyncio.sleep(self._to_sleep())\n            self._call()\n\n    def stop(self) -> None:\n        \"\"\"Stop the periodic calls.\"\"\"\n        self.interval = self._start_time = None\n        if self._task is not None:\n            self._task.cancel()\n            self._task = None\n\n    def _call(self) -> None:\n        \"\"\"Execute the function.\"\"\"\n        try:\n            result = self._func(*self._args, **self._kwargs)\n        except Exception:\n            logger.exception(\"Error calling the AsyncioLoopingCall function\")\n            self.stop()\n        else:\n            if isinstance(result, (Coroutine, Deferred)):\n                self.stop()\n                raise TypeError(\n                    \"The AsyncioLoopingCall function must not return a coroutine or a Deferred\"\n                )\n\n\ndef create_looping_call(\n    func: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs\n) -> AsyncioLoopingCall | LoopingCall:\n    \"\"\"Create an instance of a looping call class.\n\n    This creates an instance of :class:`AsyncioLoopingCall` or\n    :class:`LoopingCall`, depending on whether asyncio support is available.\n    \"\"\"\n    if is_asyncio_available():\n        return AsyncioLoopingCall(func, *args, **kwargs)\n    return LoopingCall(func, *args, **kwargs)\n\n\ndef call_later(\n    delay: float, func: Callable[[Unpack[_Ts]], object], *args: Unpack[_Ts]\n) -> CallLaterResult:\n    \"\"\"Schedule a function to be called after a delay.\n\n    This uses either ``loop.call_later()`` or ``reactor.callLater()``, depending\n    on whether asyncio support is available.\n    \"\"\"\n    if is_asyncio_available():\n        loop = asyncio.get_event_loop()\n        return CallLaterResult.from_asyncio(loop.call_later(delay, func, *args))\n\n    from twisted.internet import reactor\n\n    return CallLaterResult.from_twisted(reactor.callLater(delay, func, *args))\n\n\nclass CallLaterResult:\n    \"\"\"An universal result for :func:`call_later`, wrapping either\n    :class:`asyncio.TimerHandle` or :class:`twisted.internet.base.DelayedCall`.\n\n    The provided API is close to the :class:`asyncio.TimerHandle` one: there is\n    no ``active()`` (as there is no such public API in\n    :class:`asyncio.TimerHandle`) but ``cancel()`` can be called on already\n    called or cancelled instances.\n    \"\"\"\n\n    _timer_handle: asyncio.TimerHandle | None = None\n    _delayed_call: DelayedCall | None = None\n\n    @classmethod\n    def from_asyncio(cls, timer_handle: asyncio.TimerHandle) -> Self:\n        \"\"\"Create a CallLaterResult from an asyncio TimerHandle.\"\"\"\n        o = cls()\n        o._timer_handle = timer_handle\n        return o\n\n    @classmethod\n    def from_twisted(cls, delayed_call: DelayedCall) -> Self:\n        \"\"\"Create a CallLaterResult from a Twisted DelayedCall.\"\"\"\n        o = cls()\n        o._delayed_call = delayed_call\n        return o\n\n    def cancel(self) -> None:\n        \"\"\"Cancel the underlying delayed call.\n\n        Does nothing if the delayed call was already called or cancelled.\n        \"\"\"\n        if self._timer_handle:\n            self._timer_handle.cancel()\n            self._timer_handle = None\n        elif self._delayed_call and self._delayed_call.active():\n            self._delayed_call.cancel()\n            self._delayed_call = None\n", "n_tokens": 843, "byte_len": 3689, "file_sha1": "b2644ed996ccd736452df2ab62a4e402cdfd4c81", "start_line": 152, "end_line": 256}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/sitemap.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/sitemap.py", "rel_path": "scrapy/utils/sitemap.py", "module": "scrapy.utils.sitemap", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "__iter__", "sitemap_urls_from_robots", "Sitemap", "sitemap", "spider", "append", "robots", "name", "future", "typ", "checking", "main", "alternate", "href", "lower", "isinstance", "none", "base", "url", "type", "without", "attrib", "line", "parse", "notice", "note", "iterator", "iter", "typing", "splitlines", "over", "annotations", "class", "startswith", "provide", "files", "contained", "urls", "root", "split", "change", "self", "else", "resolve", "entities", "xml", "parser", "index", "text"], "ast_kind": "class_or_type", "text": "\"\"\"\nModule for processing Sitemaps.\n\nNote: The main purpose of this module is to provide support for the\nSitemapSpider, its API is subject to change without notice.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import urljoin\n\nimport lxml.etree\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Iterator\n\n\nclass Sitemap:\n    \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n    (type=sitemapindex) files\"\"\"\n\n    def __init__(self, xmltext: str | bytes):\n        xmlp = lxml.etree.XMLParser(\n            recover=True, remove_comments=True, resolve_entities=False\n        )\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n        rt = self._root.tag\n        assert isinstance(rt, str)\n        self.type = rt.split(\"}\", 1)[1] if \"}\" in rt else rt\n\n    def __iter__(self) -> Iterator[dict[str, Any]]:\n        for elem in self._root.getchildren():\n            d: dict[str, Any] = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                assert isinstance(tag, str)\n                name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n\n                if name == \"link\":\n                    if \"href\" in el.attrib:\n                        d.setdefault(\"alternate\", []).append(el.get(\"href\"))\n                else:\n                    d[name] = el.text.strip() if el.text else \"\"\n\n            if \"loc\" in d:\n                yield d\n\n\ndef sitemap_urls_from_robots(\n    robots_text: str, base_url: str | None = None\n) -> Iterable[str]:\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith(\"sitemap:\"):\n            url = line.split(\":\", 1)[1].strip()\n            yield urljoin(base_url or \"\", url)\n", "n_tokens": 432, "byte_len": 1839, "file_sha1": "6ba654de2963b10ed300f97955d6e0bede160651", "start_line": 1, "end_line": 60}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/boto.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/boto.py", "rel_path": "scrapy/utils/boto.py", "module": "scrapy.utils.boto", "ext": "py", "chunk_number": 1, "symbols": ["is_botocore_available", "plc0415", "true", "noqa", "boto", "import", "bool", "false", "except", "botocore", "available", "error", "helpers", "f401", "return"], "ast_kind": "function_or_method", "text": "\"\"\"Boto/botocore helpers\"\"\"\n\n\ndef is_botocore_available() -> bool:\n    try:\n        import botocore  # noqa: F401,PLC0415\n\n        return True\n    except ImportError:\n        return False\n", "n_tokens": 51, "byte_len": 188, "file_sha1": "34c1ab25b19f5273e993ce9a4c5c5a5d8c314a52", "start_line": 1, "end_line": 11}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/request.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/request.py", "rel_path": "scrapy/utils/request.py", "module": "scrapy.utils.request", "ext": "py", "chunk_number": 1, "symbols": ["fingerprint", "from_crawler", "__init__", "request_authenticate", "RequestFingerprinterProtocol", "RequestFingerprinter", "encoding", "does", "method", "dumps", "those", "takes", "bool", "browser", "reques", "fingerprinte", "cookies", "calculating", "instance", "fragments", "reliably", "python", "lib", "w3lib", "future", "spider", "accessible", "cache", "deprecated", "removed", "request_httprepr", "referer_str", "request_from_dict", "_get_method", "request_to_curl", "containing", "stream", "name", "controlled", "typ", "checking", "https", "string", "elif", "urlparse", "cached", "username", "header", "value", "urlunparse"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module provides some useful functions for working with\nscrapy.Request objects\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Protocol\nfrom urllib.parse import urlunparse\nfrom weakref import WeakKeyDictionary\n\nfrom w3lib.http import basic_auth_header\nfrom w3lib.url import canonicalize_url\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\n_fingerprint_cache: WeakKeyDictionary[\n    Request, dict[tuple[tuple[bytes, ...] | None, bool], bytes]\n] = WeakKeyDictionary()\n\n\ndef fingerprint(\n    request: Request,\n    *,\n    include_headers: Iterable[bytes | str] | None = None,\n    keep_fragments: bool = False,\n) -> bytes:\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n    ``http://www.example.com/query?id=111&cat=222``,\n    ``http://www.example.com/query?cat=222&id=111``.\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (i.e. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accessible to authenticated users:\n    ``http://www.example.com/members/offers.html``.\n\n    Lots of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingerprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    Also, servers usually ignore fragments in urls when handling requests,\n    so they are also ignored by default when calculating the fingerprint.\n    If you want to include them, set the keep_fragments argument to True\n    (for instance when handling requests with a headless browser).\n    \"\"\"\n    processed_include_headers: tuple[bytes, ...] | None = None\n    if include_headers:\n        processed_include_headers = tuple(\n            to_bytes(h.lower()) for h in sorted(include_headers)\n        )\n    cache = _fingerprint_cache.setdefault(request, {})\n    cache_key = (processed_include_headers, keep_fragments)\n    if cache_key not in cache:\n        # To decode bytes reliably (JSON does not support bytes), regardless of\n        # character encoding, we use bytes.hex()\n        headers: dict[str, list[str]] = {}\n        if processed_include_headers:\n            for header in processed_include_headers:\n                if header in request.headers:\n                    headers[header.hex()] = [\n                        header_value.hex()\n                        for header_value in request.headers.getlist(header)\n                    ]\n        fingerprint_data = {\n            \"method\": to_unicode(request.method),\n            \"url\": canonicalize_url(request.url, keep_fragments=keep_fragments),\n            \"body\": (request.body or b\"\").hex(),\n            \"headers\": headers,\n        }\n        fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n        cache[cache_key] = hashlib.sha1(  # noqa: S324\n            fingerprint_json.encode()\n        ).digest()\n    return cache[cache_key]\n\n\nclass RequestFingerprinterProtocol(Protocol):\n    def fingerprint(self, request: Request) -> bytes: ...\n\n\nclass RequestFingerprinter:\n    \"\"\"Default fingerprinter.\n\n    It takes into account a canonical version\n    (:func:`w3lib.url.canonicalize_url`) of :attr:`request.url\n    <scrapy.Request.url>` and the values of :attr:`request.method\n    <scrapy.Request.method>` and :attr:`request.body\n    <scrapy.Request.body>`. It then generates an `SHA1\n    <https://en.wikipedia.org/wiki/SHA-1>`_ hash.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def __init__(self, crawler: Crawler | None = None):\n        if crawler:\n            implementation = crawler.settings.get(\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n            )\n        else:\n            implementation = \"SENTINEL\"\n\n        if implementation != \"SENTINEL\":\n            message = (\n                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n                \"It will be removed in a future version of Scrapy.\"\n            )\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n        self._fingerprint = fingerprint\n\n    def fingerprint(self, request: Request) -> bytes:\n        return self._fingerprint(request)\n\n\ndef request_authenticate(\n    request: Request,\n    username: str,\n    password: str,\n) -> None:\n    \"\"\"Authenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    warnings.warn(\n        \"The request_authenticate function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    request.headers[\"Authorization\"] = basic_auth_header(username, password)\n\n", "n_tokens": 1186, "byte_len": 5554, "file_sha1": "37f78d61b8c9470193f245a20ee2646964b12f6e", "start_line": 1, "end_line": 157}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/request.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/request.py", "rel_path": "scrapy/utils/request.py", "module": "scrapy.utils.request", "ext": "py", "chunk_number": 2, "symbols": ["request_httprepr", "referer_str", "request_from_dict", "_get_method", "request_to_curl", "method", "containing", "cookies", "stream", "spider", "name", "controlled", "elif", "string", "urlparse", "cached", "urlunparse", "request", "from", "unicode", "provided", "items", "isinstance", "referrer", "suitable", "object", "none", "join", "type", "since", "fingerprint", "from_crawler", "__init__", "request_authenticate", "RequestFingerprinterProtocol", "RequestFingerprinter", "encoding", "does", "dumps", "those", "takes", "bool", "browser", "reques", "fingerprinte", "calculating", "instance", "fragments", "reliably", "python"], "ast_kind": "function_or_method", "text": "def request_httprepr(request: Request) -> bytes:\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b\"\") + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s\n\n\ndef referer_str(request: Request) -> str | None:\n    \"\"\"Return Referer HTTP header suitable for logging.\"\"\"\n    referrer = request.headers.get(\"Referer\")\n    if referrer is None:\n        return referrer\n    return to_unicode(referrer, errors=\"replace\")\n\n\ndef request_from_dict(d: dict[str, Any], *, spider: Spider | None = None) -> Request:\n    \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    request_cls: type[Request] = load_object(d[\"_class\"]) if \"_class\" in d else Request\n    kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}\n    if d.get(\"callback\") and spider:\n        kwargs[\"callback\"] = _get_method(spider, d[\"callback\"])\n    if d.get(\"errback\") and spider:\n        kwargs[\"errback\"] = _get_method(spider, d[\"errback\"])\n    return request_cls(**kwargs)\n\n\ndef _get_method(obj: Any, name: Any) -> Any:\n    \"\"\"Helper function for request_from_dict\"\"\"\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(f\"Method {name!r} not found in: {obj}\")\n\n\ndef request_to_curl(request: Request) -> str:\n    \"\"\"\n    Converts a :class:`~scrapy.Request` object to a curl command.\n\n    :param :class:`~scrapy.Request`: Request object to be converted\n    :return: string containing the curl command\n    \"\"\"\n    method = request.method\n\n    data = f\"--data-raw '{request.body.decode('utf-8')}'\" if request.body else \"\"\n\n    headers = \" \".join(\n        f\"-H '{k.decode()}: {v[0].decode()}'\" for k, v in request.headers.items()\n    )\n\n    url = request.url\n    cookies = \"\"\n    if request.cookies:\n        if isinstance(request.cookies, dict):\n            cookie = \"; \".join(f\"{k}={v}\" for k, v in request.cookies.items())\n            cookies = f\"--cookie '{cookie}'\"\n        elif isinstance(request.cookies, list):\n            cookie = \"; \".join(\n                f\"{next(iter(c.keys()))}={next(iter(c.values()))}\"\n                for c in request.cookies\n            )\n            cookies = f\"--cookie '{cookie}'\"\n\n    curl_cmd = f\"curl -X {method} {url} {data} {headers} {cookies}\".strip()\n    return \" \".join(curl_cmd.split())\n", "n_tokens": 741, "byte_len": 2949, "file_sha1": "37f78d61b8c9470193f245a20ee2646964b12f6e", "start_line": 158, "end_line": 237}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/response.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/response.py", "rel_path": "scrapy/utils/response.py", "module": "scrapy.utils.response", "ext": "py", "chunk_number": 1, "symbols": ["get_base_url", "get_meta_refresh", "response_status_message", "_remove_html_comments", "open_in_browser", "parse_details", "encoding", "ignore", "tags", "parse", "browser", "unsupported", "lib", "w3lib", "name", "open", "future", "typ", "checking", "https", "schools", "w3schools", "elif", "displayed", "dirty", "equiv", "imports", "unicode", "href", "isinstance", "unknown", "repl", "none", "parameter", "html", "code", "type", "script", "http", "bytes", "callable", "response", "could", "openfunc", "fname", "functions", "typing", "useful", "return", "plus"], "ast_kind": "function_or_method", "text": "\"\"\"\nThis module provides some useful functions for working with\nscrapy.http.Response objects\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport tempfile\nimport webbrowser\nfrom typing import TYPE_CHECKING, Any\nfrom weakref import WeakKeyDictionary\n\nfrom twisted.web import http\nfrom w3lib import html\n\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable\n\n    from scrapy.http import Response, TextResponse\n\n_baseurl_cache: WeakKeyDictionary[Response, str] = WeakKeyDictionary()\n\n\ndef get_base_url(response: TextResponse) -> str:\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(\n            text, response.url, response.encoding\n        )\n    return _baseurl_cache[response]\n\n\n_metaref_cache: WeakKeyDictionary[Response, tuple[None, None] | tuple[float, str]] = (\n    WeakKeyDictionary()\n)\n\n\ndef get_meta_refresh(\n    response: TextResponse,\n    ignore_tags: Iterable[str] = (\"script\", \"noscript\"),\n) -> tuple[None, None] | tuple[float, str]:\n    \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(\n            text, get_base_url(response), response.encoding, ignore_tags=ignore_tags\n        )\n    return _metaref_cache[response]\n\n\ndef response_status_message(status: bytes | float | str) -> str:\n    \"\"\"Return status code plus status text descriptive message\"\"\"\n    status_int = int(status)\n    message = http.RESPONSES.get(status_int, \"Unknown Status\")\n    return f\"{status_int} {to_unicode(message)}\"\n\n\ndef _remove_html_comments(body: bytes) -> bytes:\n    start = body.find(b\"<!--\")\n    while start != -1:\n        end = body.find(b\"-->\", start + 1)\n        if end == -1:\n            return body[:start]\n        body = body[:start] + body[end + 3 :]\n        start = body.find(b\"<!--\")\n    return body\n\n\ndef open_in_browser(\n    response: TextResponse,\n    _openfunc: Callable[[str], Any] = webbrowser.open,\n) -> Any:\n    \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for\n    external links to work, e.g. so that images and styles are displayed.\n\n    .. _base tag: https://www.w3schools.com/tags/tag_base.asp\n\n    For example:\n\n    .. code-block:: python\n\n        from scrapy.utils.response import open_in_browser\n\n\n        def parse_details(self, response):\n            if \"item name\" not in response.body:\n                open_in_browser(response)\n    \"\"\"\n    # circular imports\n    from scrapy.http import HtmlResponse, TextResponse  # noqa: PLC0415\n\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b\"<base\" not in body:\n            _remove_html_comments(body)\n            repl = rf'\\0<base href=\"{response.url}\">'\n            body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)\n        ext = \".html\"\n    elif isinstance(response, TextResponse):\n        ext = \".txt\"\n    else:\n        raise TypeError(f\"Unsupported response type: {response.__class__.__name__}\")\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(f\"file://{fname}\")\n", "n_tokens": 815, "byte_len": 3422, "file_sha1": "216e43ac3c5d492a970a4de7c9f6204abb5ac2ae", "start_line": 1, "end_line": 113}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/httpobj.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/httpobj.py", "rel_path": "scrapy/utils/httpobj.py", "module": "scrapy.utils.httpobj", "ext": "py", "chunk_number": 1, "symbols": ["urlparse_cached", "weak", "key", "result", "urlparse", "cache", "argument", "functions", "typing", "return", "objects", "annotations", "scrapy", "future", "typ", "checking", "cached", "parse", "weakref", "from", "request", "response", "caching", "object", "urllib", "where", "import", "helper", "http"], "ast_kind": "function_or_method", "text": "\"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import ParseResult, urlparse\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    from scrapy.http import Request, Response\n\n\n_urlparse_cache: WeakKeyDictionary[Request | Response, ParseResult] = (\n    WeakKeyDictionary()\n)\n\n\ndef urlparse_cached(request_or_response: Request | Response) -> ParseResult:\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]\n", "n_tokens": 161, "byte_len": 765, "file_sha1": "7bc41655c23a42ead0d3c1d852b873c8806887df", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/test.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/test.py", "rel_path": "scrapy/utils/test.py", "module": "scrapy.utils.test", "ext": "py", "chunk_number": 1, "symbols": ["assert_gcs_environ", "skip_if_no_boto", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "get_reactor_settings", "get_crawler", "get_pythonpath", "get_testenv", "spidercls", "library", "delete", "loads", "bool", "your", "populate", "boto", "append", "agent", "prevent", "warnings", "future", "ftp", "data", "spider", "import", "module", "deprecated", "removed", "typ", "assert_samelines", "get_from_asyncio_queue", "mock_google_cloud_storage", "get_web_client_agent_req", "between", "checking", "port", "blob", "mock", "path", "text", "text1", "missing", "correct", "username", "get", "crawler", "assert", "gcs", "system"], "ast_kind": "function_or_method", "text": "\"\"\"\nThis module contains some assorted functions used in tests\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport os\nimport warnings\nfrom ftplib import FTP\nfrom importlib import import_module\nfrom pathlib import Path\nfrom posixpath import split\nfrom typing import TYPE_CHECKING, Any, TypeVar, cast\nfrom unittest import TestCase, mock\n\nfrom twisted.trial.unittest import SkipTest\nfrom twisted.web.client import Agent\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.deprecate import create_deprecated_class\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\nfrom scrapy.utils.spider import DefaultSpider\n\nif TYPE_CHECKING:\n    from collections.abc import Awaitable\n\n    from twisted.internet.defer import Deferred\n    from twisted.web.client import Response as TxResponse\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\n_T = TypeVar(\"_T\")\n\n\ndef assert_gcs_environ() -> None:\n    warnings.warn(\n        \"The assert_gcs_environ() function is deprecated and will be removed in a future version of Scrapy.\"\n        \" Check GCS_PROJECT_ID directly.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    if \"GCS_PROJECT_ID\" not in os.environ:\n        raise SkipTest(\"GCS_PROJECT_ID not found\")\n\n\ndef skip_if_no_boto() -> None:\n    warnings.warn(\n        \"The skip_if_no_boto() function is deprecated and will be removed in a future version of Scrapy.\"\n        \" Check scrapy.utils.boto.is_botocore_available() directly.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    if not is_botocore_available():\n        raise SkipTest(\"missing botocore library\")\n\n\ndef get_gcs_content_and_delete(\n    bucket: Any, path: str\n) -> tuple[bytes, list[dict[str, str]], Any]:\n    from google.cloud import storage  # noqa: PLC0415\n\n    warnings.warn(\n        \"The get_gcs_content_and_delete() function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob\n\n\ndef get_ftp_content_and_delete(\n    path: str,\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n) -> bytes:\n    warnings.warn(\n        \"The get_ftp_content_and_delete() function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data: list[bytes] = []\n\n    def buffer_data(data: bytes) -> None:\n        ftp_data.append(data)\n\n    ftp.retrbinary(f\"RETR {path}\", buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return b\"\".join(ftp_data)\n\n\nTestSpider = create_deprecated_class(\"TestSpider\", DefaultSpider)\n\n\ndef get_reactor_settings() -> dict[str, Any]:\n    \"\"\"Return a settings dict that works with the installed reactor.\n\n    ``Crawler._apply_settings()`` checks that the installed reactor matches the\n    settings, so tests that run the crawler in the current process may need to\n    pass a correct ``\"TWISTED_REACTOR\"`` setting value when creating it.\n    \"\"\"\n    if not is_reactor_installed():\n        raise RuntimeError(\n            \"get_reactor_settings() called without an installed reactor,\"\n            \" you may need to install a reactor explicitly when running your tests.\"\n        )\n    settings: dict[str, Any] = {}\n    if not is_asyncio_reactor_installed():\n        settings[\"TWISTED_REACTOR\"] = None\n    return settings\n\n\ndef get_crawler(\n    spidercls: type[Spider] | None = None,\n    settings_dict: dict[str, Any] | None = None,\n    prevent_warnings: bool = True,\n) -> Crawler:\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    # When needed, useful settings can be added here, e.g. ones that prevent\n    # deprecation warnings.\n    settings: dict[str, Any] = {\n        **get_reactor_settings(),\n        **(settings_dict or {}),\n    }\n    runner = CrawlerRunner(settings)\n    crawler = runner.create_crawler(spidercls or DefaultSpider)\n    crawler._apply_settings()\n    return crawler\n\n\ndef get_pythonpath() -> str:\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module(\"scrapy\").__path__[0]\n    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n\n\ndef get_testenv() -> dict[str, str]:\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = get_pythonpath()\n    return env\n\n", "n_tokens": 1229, "byte_len": 5279, "file_sha1": "53788c860862a6b6181566b3022e9bb62a5dd376", "start_line": 1, "end_line": 168}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/test.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/test.py", "rel_path": "scrapy/utils/test.py", "module": "scrapy.utils.test", "ext": "py", "chunk_number": 2, "symbols": ["assert_samelines", "get_from_asyncio_queue", "mock_google_cloud_storage", "get_web_client_agent_req", "agent", "future", "deprecated", "between", "removed", "blob", "mock", "text", "text1", "google", "create", "autospec", "none", "return", "value", "encode", "text2", "line", "testcase", "creates", "get", "from", "values", "assert", "equal", "internet", "assert_gcs_environ", "skip_if_no_boto", "get_gcs_content_and_delete", "get_ftp_content_and_delete", "buffer_data", "get_reactor_settings", "get_crawler", "get_pythonpath", "get_testenv", "spidercls", "library", "delete", "loads", "bool", "your", "populate", "boto", "append", "prevent", "warnings"], "ast_kind": "function_or_method", "text": "def assert_samelines(\n    testcase: TestCase, text1: str, text2: str, msg: str | None = None\n) -> None:\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    warnings.warn(\n        \"The assert_samelines function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)  # noqa: PT009\n\n\ndef get_from_asyncio_queue(value: _T) -> Awaitable[_T]:\n    q: asyncio.Queue[_T] = asyncio.Queue()\n    getter = q.get()\n    q.put_nowait(value)\n    return getter\n\n\ndef mock_google_cloud_storage() -> tuple[Any, Any, Any]:\n    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n    classes and set their proper return values.\n    \"\"\"\n    from google.cloud.storage import Blob, Bucket, Client  # noqa: PLC0415\n\n    warnings.warn(\n        \"The mock_google_cloud_storage() function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    client_mock = mock.create_autospec(Client)\n\n    bucket_mock = mock.create_autospec(Bucket)\n    client_mock.get_bucket.return_value = bucket_mock\n\n    blob_mock = mock.create_autospec(Blob)\n    bucket_mock.blob.return_value = blob_mock\n\n    return (client_mock, bucket_mock, blob_mock)\n\n\ndef get_web_client_agent_req(url: str) -> Deferred[TxResponse]:\n    from twisted.internet import reactor\n\n    agent = Agent(reactor)\n    return cast(\"Deferred[TxResponse]\", agent.request(b\"GET\", url.encode(\"utf-8\")))\n", "n_tokens": 397, "byte_len": 1654, "file_sha1": "53788c860862a6b6181566b3022e9bb62a5dd376", "start_line": 169, "end_line": 218}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/display.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/display.py", "rel_path": "scrapy/utils/display.py", "module": "scrapy.utils.display", "ext": "py", "chunk_number": 1, "symbols": ["_enable_windows_terminal_processing", "_tty_supports_color", "_colorize", "pformat", "pprint", "isatty", "text", "bool", "set", "console", "except", "module", "processing", "packaging", "typing", "windows", "return", "stdout", "name", "support", "enable", "enabled", "with", "attr", "ignore", "defined", "colorize", "terminal", "import", "error", "https", "formatters", "stackoverflow", "parse", "version", "kernel", "kernel32", "pylint", "formatter", "python", "lexer", "print", "plc0415", "true", "noqa", "from", "providing", "kwargs", "platform", "ansi"], "ast_kind": "function_or_method", "text": "\"\"\"\npprint and pformat wrappers with colorization support\n\"\"\"\n\nimport ctypes\nimport platform\nimport sys\nfrom pprint import pformat as pformat_\nfrom typing import Any\n\nfrom packaging.version import Version as parse_version\n\n\ndef _enable_windows_terminal_processing() -> bool:\n    # https://stackoverflow.com/a/36760881\n    kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]\n    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))\n\n\ndef _tty_supports_color() -> bool:\n    if sys.platform != \"win32\":\n        return True\n\n    if parse_version(platform.version()) < parse_version(\"10.0.14393\"):\n        return True\n\n    # Windows >= 10.0.14393 interprets ANSI escape sequences providing terminal\n    # processing is enabled.\n    return _enable_windows_terminal_processing()\n\n\ndef _colorize(text: str, colorize: bool = True) -> str:\n    # pylint: disable=no-name-in-module\n    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n        return text\n    try:\n        from pygments import highlight  # noqa: PLC0415\n    except ImportError:\n        return text\n    from pygments.formatters import TerminalFormatter  # noqa: PLC0415\n    from pygments.lexers import PythonLexer  # noqa: PLC0415\n\n    return highlight(text, PythonLexer(), TerminalFormatter())\n\n\ndef pformat(obj: Any, *args: Any, **kwargs: Any) -> str:\n    return _colorize(pformat_(obj), kwargs.pop(\"colorize\", True))\n\n\ndef pprint(obj: Any, *args: Any, **kwargs: Any) -> None:\n    print(pformat(obj, *args, **kwargs))\n", "n_tokens": 372, "byte_len": 1524, "file_sha1": "463110c33d6461636f88271819284d6504dca9bc", "start_line": 1, "end_line": 52}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/url.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/url.py", "rel_path": "scrapy/utils/url.py", "module": "scrapy.utils.url", "ext": "py", "chunk_number": 1, "symbols": ["__getattr__", "url_is_from_any_domain", "url_is_from_spider", "url_has_any_extension", "escape_ajax", "add_http_if_no_scheme", "_is_posix_path", "_is_windows_path", "_is_filesystem_path", "guess_scheme", "library", "bool", "netloc", "future", "lib", "w3lib", "after", "spider", "name", "import", "module", "deprecated", "url", "from", "removed", "typ", "checking", "string", "any", "uri", "strip_url", "credentials", "strip", "replaces", "https", "port", "missing", "username", "urlunparse", "escaped", "fragment", "public", "lower", "provided", "type", "html", "parse", "http", "default", "either"], "ast_kind": "function_or_method", "text": "\"\"\"\nThis module contains general purpose URL functions not found in the standard\nlibrary.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport warnings\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING, Union\nfrom urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\nfrom warnings import warn\n\nfrom w3lib.url import __all__ as _public_w3lib_objects\nfrom w3lib.url import add_or_replace_parameter as _add_or_replace_parameter\nfrom w3lib.url import any_to_uri as _any_to_uri\nfrom w3lib.url import parse_url as _parse_url\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\n\ndef __getattr__(name: str):\n    if name in (\"_unquotepath\", \"_safe_chars\", \"parse_url\", *_public_w3lib_objects):\n        obj_type = \"attribute\" if name == \"_safe_chars\" else \"function\"\n        warnings.warn(\n            f\"The scrapy.utils.url.{name} {obj_type} is deprecated, use w3lib.url.{name} instead.\",\n            ScrapyDeprecationWarning,\n        )\n        return getattr(import_module(\"w3lib.url\"), name)\n\n    raise AttributeError\n\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from scrapy import Spider\n\nUrlT = Union[str, bytes, ParseResult]\n\n\ndef url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = _parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith(f\".{d}\")) for d in domains)\n\n\ndef url_is_from_spider(url: UrlT, spider: type[Spider]) -> bool:\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(\n        url, [spider.name, *getattr(spider, \"allowed_domains\", [])]\n    )\n\n\ndef url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n    \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n    lowercase_path = _parse_url(url).path.lower()\n    return any(lowercase_path.endswith(ext) for ext in extensions)\n\n\ndef escape_ajax(url: str) -> str:\n    \"\"\"\n    Return the crawlable url\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    warn(\n        \"escape_ajax() is deprecated and will be removed in a future Scrapy version.\",\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    defrag, frag = urldefrag(url)\n    if not frag.startswith(\"!\"):\n        return url\n    return _add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n\n\ndef add_http_if_no_scheme(url: str) -> str:\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.IGNORECASE)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url\n\n\ndef _is_posix_path(string: str) -> bool:\n    return bool(\n        re.match(\n            r\"\"\"\n            ^                   # start with...\n            (\n                \\.              # ...a single dot,\n                (\n                    \\. | [^/\\.]+  # optionally followed by\n                )?                # either a second dot or some characters\n                |\n                ~   # $HOME\n            )?      # optional match of \".\", \"..\" or \".blabla\"\n            /       # at least one \"/\" for a file path,\n            .       # and something after the \"/\"\n            \"\"\",\n            string,\n            flags=re.VERBOSE,\n        )\n    )\n\n\ndef _is_windows_path(string: str) -> bool:\n    return bool(\n        re.match(\n            r\"\"\"\n            ^\n            (\n                [a-z]:\\\\\n                | \\\\\\\\\n            )\n            \"\"\",\n            string,\n            flags=re.IGNORECASE | re.VERBOSE,\n        )\n    )\n\n\ndef _is_filesystem_path(string: str) -> bool:\n    return _is_posix_path(string) or _is_windows_path(string)\n\n\ndef guess_scheme(url: str) -> str:\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n    http:// otherwise.\"\"\"\n    if _is_filesystem_path(url):\n        return _any_to_uri(url)\n    return add_http_if_no_scheme(url)\n\n", "n_tokens": 1186, "byte_len": 4886, "file_sha1": "9b96a61e91fcf8529011af6102061f2b941d2fae", "start_line": 1, "end_line": 158}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/url.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/url.py", "rel_path": "scrapy/utils/url.py", "module": "scrapy.utils.url", "ext": "py", "chunk_number": 2, "symbols": ["strip_url", "fragment", "user", "bool", "false", "drops", "netloc", "dropping", "password", "return", "credentials", "replace", "strip", "else", "url", "with", "path", "resp", "replaces", "string", "https", "component", "port", "some", "urlparse", "also", "username", "urlunparse", "query", "true", "__getattr__", "url_is_from_any_domain", "url_is_from_spider", "url_has_any_extension", "escape_ajax", "add_http_if_no_scheme", "_is_posix_path", "_is_windows_path", "_is_filesystem_path", "guess_scheme", "library", "future", "lib", "w3lib", "after", "spider", "name", "import", "module", "deprecated"], "ast_kind": "function_or_method", "text": "def strip_url(\n    url: str,\n    strip_credentials: bool = True,\n    strip_default_port: bool = True,\n    origin_only: bool = False,\n    strip_fragment: bool = True,\n) -> str:\n    \"\"\"Strip URL string from some of its components:\n\n    - ``strip_credentials`` removes \"user:password@\"\n    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n      from http:// (resp. https://, ftp://) URLs\n    - ``origin_only`` replaces path component with \"/\", also dropping\n      query and fragment components ; it also strips credentials\n    - ``strip_fragment`` drops any #fragment component\n    \"\"\"\n\n    parsed_url = urlparse(url)\n    netloc = parsed_url.netloc\n    if (strip_credentials or origin_only) and (\n        parsed_url.username or parsed_url.password\n    ):\n        netloc = netloc.split(\"@\")[-1]\n\n    if (\n        strip_default_port\n        and parsed_url.port\n        and (parsed_url.scheme, parsed_url.port)\n        in (\n            (\"http\", 80),\n            (\"https\", 443),\n            (\"ftp\", 21),\n        )\n    ):\n        netloc = netloc.replace(f\":{parsed_url.port}\", \"\")\n\n    return urlunparse(\n        (\n            parsed_url.scheme,\n            netloc,\n            \"/\" if origin_only else parsed_url.path,\n            \"\" if origin_only else parsed_url.params,\n            \"\" if origin_only else parsed_url.query,\n            \"\" if strip_fragment else parsed_url.fragment,\n        )\n    )\n", "n_tokens": 322, "byte_len": 1405, "file_sha1": "9b96a61e91fcf8529011af6102061f2b941d2fae", "start_line": 159, "end_line": 205}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/benchserver.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/benchserver.py", "rel_path": "scrapy/utils/benchserver.py", "module": "scrapy.utils.benchserver", "ext": "py", "chunk_number": 1, "symbols": ["getChild", "render", "_getarg", "_print_listening", "Root", "argstr", "root", "factory", "doseq", "internet", "urlencode", "resource", "typing", "twisted", "return", "listen", "tcp", "name", "class", "show", "print", "listening", "main", "total", "port", "call", "when", "head", "body", "http", "host", "type", "s311", "site", "true", "noqa", "from", "getarg", "href", "random", "assert", "copy", "request", "nlist", "write", "bench", "range", "none", "urllib", "encode"], "ast_kind": "class_or_type", "text": "import random\nfrom typing import Any\nfrom urllib.parse import urlencode\n\nfrom twisted.web.resource import Resource\nfrom twisted.web.server import Request, Site\n\n\nclass Root(Resource):\n    isLeaf = True\n\n    def getChild(self, name: str, request: Request) -> Resource:\n        return self\n\n    def render(self, request: Request) -> bytes:\n        total = _getarg(request, b\"total\", 100, int)\n        show = _getarg(request, b\"show\", 10, int)\n        nlist = [random.randint(1, total) for _ in range(show)]  # noqa: S311\n        request.write(b\"<html><head></head><body>\")\n        assert request.args is not None\n        args = request.args.copy()\n        for nl in nlist:\n            args[\"n\"] = nl\n            argstr = urlencode(args, doseq=True)\n            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\".encode())\n        request.write(b\"</body></html>\")\n        return b\"\"\n\n\ndef _getarg(request, name: bytes, default: Any = None, type_=str):\n    return type_(request.args[name][0]) if name in request.args else default\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor\n\n    root = Root()\n    factory = Site(root)\n    httpPort = reactor.listenTCP(8998, Site(root))\n\n    def _print_listening() -> None:\n        httpHost = httpPort.getHost()\n        print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n\n    reactor.callWhenRunning(_print_listening)\n    reactor.run()\n", "n_tokens": 351, "byte_len": 1417, "file_sha1": "13ef0de5781e6d40881ccc16fa20ee42c54b9992", "start_line": 1, "end_line": 47}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/datatypes.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/datatypes.py", "rel_path": "scrapy/utils/datatypes.py", "module": "scrapy.utils.datatypes", "ext": "py", "chunk_number": 1, "symbols": ["__new__", "__init__", "__getitem__", "__setitem__", "__delitem__", "__contains__", "__copy__", "normkey", "normvalue", "get", "setdefault", "update", "fromkeys", "pop", "__repr__", "_normkey", "_normvalue", "CaselessDict", "CaseInsensitiveDict", "LocalCache", "bool", "case", "python", "doesn", "caseless", "dict", "deprecated", "structure", "future", "typ", "LocalWeakReferencedCache", "SequenceExclude", "while", "initial", "avoiding", "useful", "checking", "contextlib", "finite", "ordered", "slots", "lower", "getitem", "items", "isinstance", "insensitive", "caching", "none", "dictionary", "older"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module contains data types used by Scrapy which are not included in the\nPython Standard Library.\n\nThis module must not depend on any module outside the Standard Library.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport contextlib\nimport warnings\nimport weakref\nfrom collections import OrderedDict\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, Any, AnyStr, TypeVar\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\nclass CaselessDict(dict):\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n        # circular import\n        from scrapy.http.headers import Headers  # noqa: PLC0415\n\n        if issubclass(cls, CaselessDict) and not issubclass(cls, Headers):\n            warnings.warn(\n                \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__new__(cls, *args, **kwargs)\n\n    def __init__(\n        self,\n        seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n    ):\n        super().__init__()\n        if seq:\n            self.update(seq)\n\n    def __getitem__(self, key: AnyStr) -> Any:\n        return dict.__getitem__(self, self.normkey(key))\n\n    def __setitem__(self, key: AnyStr, value: Any) -> None:\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))\n\n    def __delitem__(self, key: AnyStr) -> None:\n        dict.__delitem__(self, self.normkey(key))\n\n    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n        return dict.__contains__(self, self.normkey(key))\n\n    has_key = __contains__\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n\n    def normkey(self, key: AnyStr) -> AnyStr:\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()\n\n    def normvalue(self, value: Any) -> Any:\n        \"\"\"Method to normalize values prior to be set\"\"\"\n        return value\n\n    def get(self, key: AnyStr, def_val: Any = None) -> Any:\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))\n\n    def setdefault(self, key: AnyStr, def_val: Any = None) -> Any:\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))  # type: ignore[arg-type]\n\n    # doesn't fully implement MutableMapping.update()\n    def update(self, seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]]) -> None:  # type: ignore[override]\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super().update(iseq)\n\n    @classmethod\n    def fromkeys(cls, keys: Iterable[AnyStr], value: Any = None) -> Self:  # type: ignore[override]\n        return cls((k, value) for k in keys)  # type: ignore[misc]\n\n    def pop(self, key: AnyStr, *args: Any) -> Any:\n        return dict.pop(self, self.normkey(key), *args)\n\n\nclass CaseInsensitiveDict(collections.UserDict):\n    \"\"\"A dict-like structure that accepts strings or bytes\n    as keys and allows case-insensitive lookups.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._keys: dict = {}\n        super().__init__(*args, **kwargs)\n\n    def __getitem__(self, key: AnyStr) -> Any:\n        normalized_key = self._normkey(key)\n        return super().__getitem__(self._keys[normalized_key.lower()])\n\n    def __setitem__(self, key: AnyStr, value: Any) -> None:\n        normalized_key = self._normkey(key)\n        try:\n            lower_key = self._keys[normalized_key.lower()]\n            del self[lower_key]\n        except KeyError:\n            pass\n        super().__setitem__(normalized_key, self._normvalue(value))\n        self._keys[normalized_key.lower()] = normalized_key\n\n    def __delitem__(self, key: AnyStr) -> None:\n        normalized_key = self._normkey(key)\n        stored_key = self._keys.pop(normalized_key.lower())\n        super().__delitem__(stored_key)\n\n    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n        normalized_key = self._normkey(key)\n        return normalized_key.lower() in self._keys\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {super().__repr__()}>\"\n\n    def _normkey(self, key: AnyStr) -> AnyStr:\n        return key\n\n    def _normvalue(self, value: Any) -> Any:\n        return value\n\n\nclass LocalCache(OrderedDict[_KT, _VT]):\n    \"\"\"Dictionary with a finite number of keys.\n\n    Older items expires first.\n    \"\"\"\n\n    def __init__(self, limit: int | None = None):\n        super().__init__()\n        self.limit: int | None = limit\n", "n_tokens": 1222, "byte_len": 4919, "file_sha1": "6a2cf8632e7d489458968d767e22bc3d3bcca792", "start_line": 1, "end_line": 153}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/datatypes.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/datatypes.py", "rel_path": "scrapy/utils/datatypes.py", "module": "scrapy.utils.datatypes", "ext": "py", "chunk_number": 2, "symbols": ["__setitem__", "__init__", "__getitem__", "__contains__", "LocalWeakReferencedCache", "SequenceExclude", "implementation", "underlying", "false", "keeping", "weak", "key", "except", "bool", "instantiated", "local", "cache", "received", "suppress", "referenceable", "override", "return", "cannot", "item", "last", "class", "initial", "limited", "making", "with", "__new__", "__delitem__", "__copy__", "normkey", "normvalue", "get", "setdefault", "update", "fromkeys", "pop", "__repr__", "_normkey", "_normvalue", "CaselessDict", "CaseInsensitiveDict", "LocalCache", "while", "case", "python", "doesn"], "ast_kind": "class_or_type", "text": "    def __setitem__(self, key: _KT, value: _VT) -> None:\n        if self.limit:\n            while len(self) >= self.limit:\n                self.popitem(last=False)\n        super().__setitem__(key, value)\n\n\nclass LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n    \"\"\"\n    A weakref.WeakKeyDictionary implementation that uses LocalCache as its\n    underlying data structure, making it ordered and capable of being size-limited.\n\n    Useful for memoization, while avoiding keeping received\n    arguments in memory only because of the cached references.\n\n    Note: like LocalCache and unlike weakref.WeakKeyDictionary,\n    it cannot be instantiated with an initial dictionary.\n    \"\"\"\n\n    def __init__(self, limit: int | None = None):\n        super().__init__()\n        self.data: LocalCache = LocalCache(limit=limit)\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        # if raised, key is not weak-referenceable, skip caching\n        with contextlib.suppress(TypeError):\n            super().__setitem__(key, value)\n\n    def __getitem__(self, key: _KT) -> _VT | None:  # type: ignore[override]\n        try:\n            return super().__getitem__(key)\n        except (TypeError, KeyError):\n            return None  # key is either not weak-referenceable or not cached\n\n\nclass SequenceExclude:\n    \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n\n    def __init__(self, seq: Sequence[Any]):\n        self.seq: Sequence[Any] = seq\n\n    def __contains__(self, item: Any) -> bool:\n        return item not in self.seq\n", "n_tokens": 358, "byte_len": 1541, "file_sha1": "6a2cf8632e7d489458968d767e22bc3d3bcca792", "start_line": 154, "end_line": 197}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/engine.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/engine.py", "rel_path": "scrapy/utils/engine.py", "module": "scrapy.utils.engine", "ext": "py", "chunk_number": 1, "symbols": ["get_engine_status", "format_engine_status", "print_engine_status", "used", "scraper", "downloader", "some", "slot", "global", "core", "except", "result", "f401", "spider", "functions", "working", "typing", "exception", "return", "name", "annotations", "idle", "scheduler", "checks", "current", "with", "time", "debugging", "get", "engine", "scrapy", "eval", "future", "typ", "checking", "print", "pylint", "inprogress", "itemproc", "size", "test", "noqa", "from", "list", "tuple", "execution", "closing", "active", "format", "needs"], "ast_kind": "function_or_method", "text": "\"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n\nfrom __future__ import annotations\n\n# used in global tests code\nfrom time import time  # noqa: F401\nfrom typing import TYPE_CHECKING, Any\n\nif TYPE_CHECKING:\n    from scrapy.core.engine import ExecutionEngine\n\n\ndef get_engine_status(engine: ExecutionEngine) -> list[tuple[str, Any]]:\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle()\",\n        \"engine._slot.closing\",\n        \"len(engine._slot.inprogress)\",\n        \"len(engine._slot.scheduler.dqs or [])\",\n        \"len(engine._slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks: list[tuple[str, Any]] = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]  # noqa: S307  # pylint: disable=eval-used\n        except Exception as e:\n            checks += [(test, f\"{type(e).__name__} (exception)\")]\n\n    return checks\n\n\ndef format_engine_status(engine: ExecutionEngine) -> str:\n    checks = get_engine_status(engine)\n    s = \"Execution engine status\\n\\n\"\n    for test, result in checks:\n        s += f\"{test:<47} : {result}\\n\"\n    s += \"\\n\"\n\n    return s\n\n\ndef print_engine_status(engine: ExecutionEngine) -> None:\n    print(format_engine_status(engine))\n", "n_tokens": 378, "byte_len": 1602, "file_sha1": "e93c3ea32a443b95d4d03a20e184b5c73d1876e0", "start_line": 1, "end_line": 54}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py", "rel_path": "scrapy/utils/defer.py", "module": "scrapy.utils.defer", "ext": "py", "chunk_number": 1, "symbols": ["defer_fail", "defer_succeed", "_defer_sleep", "defer_result", "mustbe_deferred", "parallel", "failure", "async", "bool", "call", "later", "taken", "livejournal", "coiterate", "python", "coroutine", "sleep", "deprecated", "explicit", "future", "typ", "checking", "more", "https", "succeed", "delay", "deferreds", "loop", "maybe", "deferred", "__init__", "_callback", "_errback", "_call_anext", "__next__", "parallel_async", "process_chain", "process_chain_both", "process_parallel", "eb", "iter_errback", "deferred_from_coro", "deferred_f_from_coro_f", "f", "maybeDeferred_coro", "deferred_to_future", "maybe_deferred_to_future", "_schedule_coro", "ensure_awaitable", "_AsyncCooperatorAdapter"], "ast_kind": "function_or_method", "text": "\"\"\"\nHelper functions for dealing with Twisted deferreds\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport warnings\nfrom asyncio import Future\nfrom collections.abc import Awaitable, Coroutine, Iterable, Iterator\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, Generic, TypeVar, cast, overload\n\nfrom twisted.internet.defer import Deferred, DeferredList, fail, succeed\nfrom twisted.internet.task import Cooperator\nfrom twisted.python import failure\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.asyncio import call_later, is_asyncio_available\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Callable\n\n    from twisted.python.failure import Failure\n\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n\n_T = TypeVar(\"_T\")\n_T2 = TypeVar(\"_T2\")\n\n\n_DEFER_DELAY = 0.1\n\n\ndef defer_fail(_failure: Failure) -> Deferred[Any]:\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    warnings.warn(\n        \"scrapy.utils.defer.defer_fail() is deprecated, use\"\n        \" twisted.internet.defer.fail(), plus an explicit sleep if needed.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    from twisted.internet import reactor\n\n    d: Deferred[Any] = Deferred()\n    reactor.callLater(_DEFER_DELAY, d.errback, _failure)\n    return d\n\n\ndef defer_succeed(result: _T) -> Deferred[_T]:\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    warnings.warn(\n        \"scrapy.utils.defer.defer_succeed() is deprecated, use\"\n        \" twisted.internet.defer.succeed(), plus an explicit sleep if needed.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    from twisted.internet import reactor\n\n    d: Deferred[_T] = Deferred()\n    reactor.callLater(_DEFER_DELAY, d.callback, result)\n    return d\n\n\ndef _defer_sleep() -> Deferred[None]:\n    \"\"\"Delay by _DEFER_DELAY so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d: Deferred[None] = Deferred()\n    call_later(_DEFER_DELAY, d.callback, None)\n    return d\n\n\nasync def _defer_sleep_async() -> None:\n    \"\"\"Delay by _DEFER_DELAY so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    if is_asyncio_available():\n        await asyncio.sleep(_DEFER_DELAY)\n    else:\n        await _defer_sleep()\n\n\ndef defer_result(result: Any) -> Deferred[Any]:\n    warnings.warn(\n        \"scrapy.utils.defer.defer_result() is deprecated, use\"\n        \" twisted.internet.defer.success() and twisted.internet.defer.fail(),\"\n        \" plus an explicit sleep if needed, or explicit reactor.callLater().\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    if isinstance(result, Deferred):\n        return result\n\n    from twisted.internet import reactor\n\n    d: Deferred[Any] = Deferred()\n    if isinstance(result, failure.Failure):\n        reactor.callLater(_DEFER_DELAY, d.errback, result)\n    else:\n        reactor.callLater(_DEFER_DELAY, d.callback, result)\n    return d\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, Deferred[_T]], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[_T]: ...\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, _T], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[_T]: ...\n\n\ndef mustbe_deferred(\n    f: Callable[_P, Deferred[_T] | _T],\n    *args: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[_T]:\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    warnings.warn(\n        \"scrapy.utils.defer.mustbe_deferred() is deprecated, use\"\n        \" twisted.internet.defer.maybeDeferred(), with an explicit sleep if needed.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    result: _T | Deferred[_T] | Failure\n    try:\n        result = f(*args, **kw)\n    except Exception:\n        result = failure.Failure()\n    return defer_result(result)\n\n\ndef parallel(\n    iterable: Iterable[_T],\n    count: int,\n    callable: Callable[Concatenate[_T, _P], _T2],  # noqa: A002\n    *args: _P.args,\n    **named: _P.kwargs,\n) -> Deferred[list[tuple[bool, Iterator[_T2]]]]:\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: https://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = Cooperator()\n    work: Iterator[_T2] = (callable(elem, *args, **named) for elem in iterable)\n    return DeferredList([coop.coiterate(work) for _ in range(count)])\n\n", "n_tokens": 1233, "byte_len": 5126, "file_sha1": "68e7333df8d7b866cb2f32d7bede6577c8cd3bbd", "start_line": 1, "end_line": 173}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py", "rel_path": "scrapy/utils/defer.py", "module": "scrapy.utils.defer", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "_callback", "_errback", "_call_anext", "__next__", "_AsyncCooperatorAdapter", "failure", "does", "takes", "async", "those", "bool", "parallel", "aiterable", "cooperative", "task", "append", "coiterate", "awaiting", "python", "serialize", "after", "possible", "stopping", "callable", "args", "oldest", "makes", "anext", "passing", "defer_fail", "defer_succeed", "_defer_sleep", "defer_result", "mustbe_deferred", "parallel_async", "process_chain", "process_chain_both", "process_parallel", "eb", "iter_errback", "deferred_from_coro", "deferred_f_from_coro_f", "f", "maybeDeferred_coro", "deferred_to_future", "maybe_deferred_to_future", "_schedule_coro", "ensure_awaitable", "MySpider"], "ast_kind": "class_or_type", "text": "class _AsyncCooperatorAdapter(Iterator, Generic[_T]):\n    \"\"\"A class that wraps an async iterable into a normal iterator suitable\n    for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n    it calls the callable directly in the callback, instead of providing a more\n    generic interface.\n\n    On the outside, this class behaves as an iterator that yields Deferreds.\n    Each Deferred is fired with the result of the callable which was called on\n    the next result from aiterator. It raises StopIteration when aiterator is\n    exhausted, as expected.\n\n    Cooperator calls __next__() multiple times and waits on the Deferreds\n    returned from it. As async generators (since Python 3.8) don't support\n    awaiting on __anext__() several times in parallel, we need to serialize\n    this. It's done by storing the Deferreds returned from __next__() and\n    firing the oldest one when a result from __anext__() is available.\n\n    The workflow:\n    1. When __next__() is called for the first time, it creates a Deferred, stores it\n    in self.waiting_deferreds and returns it. It also makes a Deferred that will wait\n    for self.aiterator.__anext__() and puts it into self.anext_deferred.\n    2. If __next__() is called again before self.anext_deferred fires, more Deferreds\n    are added to self.waiting_deferreds.\n    3. When self.anext_deferred fires, it either calls _callback() or _errback(). Both\n    clear self.anext_deferred.\n    3.1. _callback() calls the callable passing the result value that it takes, pops a\n    Deferred from self.waiting_deferreds, and if the callable result was a Deferred, it\n    chains those Deferreds so that the waiting Deferred will fire when the result\n    Deferred does, otherwise it fires it directly. This causes one awaiting task to\n    receive a result. If self.waiting_deferreds is still not empty, new __anext__() is\n    called and self.anext_deferred is populated.\n    3.2. _errback() checks the exception class. If it's StopAsyncIteration it means\n    self.aiterator is exhausted and so it sets self.finished and fires all\n    self.waiting_deferreds. Other exceptions are propagated.\n    4. If __next__() is called after __anext__() was handled, then if self.finished is\n    True, it raises StopIteration, otherwise it acts like in step 2, but if\n    self.anext_deferred is now empty is also populates it with a new __anext__().\n\n    Note that CooperativeTask ignores the value returned from the Deferred that it waits\n    for, so we fire them with None when needed.\n\n    It may be possible to write an async iterator-aware replacement for\n    Cooperator/CooperativeTask and use it instead of this adapter to achieve the same\n    goal.\n    \"\"\"\n\n    def __init__(\n        self,\n        aiterable: AsyncIterator[_T],\n        callable_: Callable[Concatenate[_T, _P], Deferred[Any] | None],\n        *callable_args: _P.args,\n        **callable_kwargs: _P.kwargs,\n    ):\n        self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n        self.callable: Callable[Concatenate[_T, _P], Deferred[Any] | None] = callable_\n        self.callable_args: tuple[Any, ...] = callable_args\n        self.callable_kwargs: dict[str, Any] = callable_kwargs\n        self.finished: bool = False\n        self.waiting_deferreds: list[Deferred[Any]] = []\n        self.anext_deferred: Deferred[_T] | None = None\n\n    def _callback(self, result: _T) -> None:\n        # This gets called when the result from aiterator.__anext__() is available.\n        # It calls the callable on it and sends the result to the oldest waiting Deferred\n        # (by chaining if the result is a Deferred too or by firing if not).\n        self.anext_deferred = None\n        callable_result = self.callable(\n            result, *self.callable_args, **self.callable_kwargs\n        )\n        d = self.waiting_deferreds.pop(0)\n        if isinstance(callable_result, Deferred):\n            callable_result.chainDeferred(d)\n        else:\n            d.callback(None)\n        if self.waiting_deferreds:\n            self._call_anext()\n\n    def _errback(self, failure: Failure) -> None:\n        # This gets called on any exceptions in aiterator.__anext__().\n        # It handles StopAsyncIteration by stopping the iteration and reraises all others.\n        self.anext_deferred = None\n        failure.trap(StopAsyncIteration)\n        self.finished = True\n        for d in self.waiting_deferreds:\n            d.callback(None)\n\n    def _call_anext(self) -> None:\n        # This starts waiting for the next result from aiterator.\n        # If aiterator is exhausted, _errback will be called.\n        self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n        self.anext_deferred.addCallbacks(self._callback, self._errback)\n\n    def __next__(self) -> Deferred[Any]:\n        # This puts a new Deferred into self.waiting_deferreds and returns it.\n        # It also calls __anext__() if needed.\n        if self.finished:\n            raise StopIteration\n        d: Deferred[Any] = Deferred()\n        self.waiting_deferreds.append(d)\n        if not self.anext_deferred:\n            self._call_anext()\n        return d\n\n", "n_tokens": 1231, "byte_len": 5146, "file_sha1": "68e7333df8d7b866cb2f32d7bede6577c8cd3bbd", "start_line": 174, "end_line": 277}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py", "rel_path": "scrapy/utils/defer.py", "module": "scrapy.utils.defer", "ext": "py", "chunk_number": 3, "symbols": ["parallel_async", "process_chain", "process_chain_both", "process_parallel", "eb", "iter_errback", "deferred_from_coro", "failure", "while", "async", "bool", "parallel", "aiterable", "coiterate", "future", "from", "coroutine", "coroutines", "doesn", "sleep", "like", "deprecated", "anext", "removed", "succeed", "iterating", "isinstance", "object", "none", "wrapping", "defer_fail", "defer_succeed", "_defer_sleep", "defer_result", "mustbe_deferred", "__init__", "_callback", "_errback", "_call_anext", "__next__", "deferred_f_from_coro_f", "f", "maybeDeferred_coro", "deferred_to_future", "maybe_deferred_to_future", "_schedule_coro", "ensure_awaitable", "_AsyncCooperatorAdapter", "MySpider", "those"], "ast_kind": "function_or_method", "text": "def parallel_async(\n    async_iterable: AsyncIterator[_T],\n    count: int,\n    callable: Callable[Concatenate[_T, _P], Deferred[Any] | None],  # noqa: A002\n    *args: _P.args,\n    **named: _P.kwargs,\n) -> Deferred[list[tuple[bool, Iterator[Deferred[Any]]]]]:\n    \"\"\"Like ``parallel`` but for async iterators\"\"\"\n    coop = Cooperator()\n    work: Iterator[Deferred[Any]] = _AsyncCooperatorAdapter(\n        async_iterable, callable, *args, **named\n    )\n    dl: Deferred[list[tuple[bool, Iterator[Deferred[Any]]]]] = DeferredList(\n        [coop.coiterate(work) for _ in range(count)]\n    )\n    return dl\n\n\ndef process_chain(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], _T]],\n    input: _T,  # noqa: A002\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[_T]:\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    warnings.warn(\n        \"process_chain() is deprecated.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    d: Deferred[_T] = Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d\n\n\ndef process_chain_both(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    errbacks: Iterable[Callable[Concatenate[Failure, _P], Any]],\n    input: Any,  # noqa: A002\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    warnings.warn(\n        \"process_chain_both() is deprecated and will be removed in a future\"\n        \" Scrapy version.\",\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    d: Deferred = Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallback(cb, *a, **kw)\n        d.addErrback(eb, *a, **kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d\n\n\ndef process_parallel(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], _T2]],\n    input: _T,  # noqa: A002\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[list[_T2]]:  # pragma: no cover\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    warnings.warn(\n        \"process_parallel() is deprecated.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    dfds = [succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d: Deferred[list[tuple[bool, _T2]]] = DeferredList(\n        dfds, fireOnOneErrback=True, consumeErrors=True\n    )\n    d2: Deferred[list[_T2]] = d.addCallback(lambda r: [x[1] for x in r])\n\n    def eb(failure: Failure) -> Failure:\n        return failure.value.subFailure\n\n    d2.addErrback(eb)\n    return d2\n\n\ndef iter_errback(\n    iterable: Iterable[_T],\n    errback: Callable[Concatenate[Failure, _P], Any],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Iterable[_T]:\n    \"\"\"Wrap an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\nasync def aiter_errback(\n    aiterable: AsyncIterator[_T],\n    errback: Callable[Concatenate[Failure, _P], Any],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> AsyncIterator[_T]:\n    \"\"\"Wrap an async iterable calling an errback if an error is caught while\n    iterating it. Similar to :func:`scrapy.utils.defer.iter_errback`.\n    \"\"\"\n    it = aiterable.__aiter__()\n    while True:\n        try:\n            yield await it.__anext__()\n        except StopAsyncIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\n@overload\ndef deferred_from_coro(o: Awaitable[_T]) -> Deferred[_T]: ...\n\n\n@overload\ndef deferred_from_coro(o: _T2) -> _T2: ...\n\n\ndef deferred_from_coro(o: Awaitable[_T] | _T2) -> Deferred[_T] | _T2:\n    \"\"\"Convert a coroutine or other awaitable object into a Deferred,\n    or return the object as is if it isn't a coroutine.\"\"\"\n    if isinstance(o, Deferred):\n        return o\n    if inspect.isawaitable(o):\n        if not is_asyncio_available():\n            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n            return Deferred.fromCoroutine(cast(\"Coroutine[Deferred[Any], Any, _T]\", o))\n        # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n        return Deferred.fromFuture(asyncio.ensure_future(o))\n    return o\n\n", "n_tokens": 1217, "byte_len": 4578, "file_sha1": "68e7333df8d7b866cb2f32d7bede6577c8cd3bbd", "start_line": 278, "end_line": 427}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/defer.py", "rel_path": "scrapy/utils/defer.py", "module": "scrapy.utils.defer", "ext": "py", "chunk_number": 4, "symbols": ["deferred_f_from_coro_f", "f", "maybeDeferred_coro", "deferred_to_future", "maybe_deferred_to_future", "_schedule_coro", "ensure_awaitable", "MySpider", "coro", "kwargs", "wrapping", "failure", "price", "deferred", "future", "async", "callables", "wrap", "case", "ruf006", "coroutine", "spider", "coroutines", "from", "doesn", "passed", "https", "succeed", "debug", "loop", "defer_fail", "defer_succeed", "_defer_sleep", "defer_result", "mustbe_deferred", "parallel", "__init__", "_callback", "_errback", "_call_anext", "__next__", "parallel_async", "process_chain", "process_chain_both", "process_parallel", "eb", "iter_errback", "deferred_from_coro", "_AsyncCooperatorAdapter", "those"], "ast_kind": "class_or_type", "text": "def deferred_f_from_coro_f(\n    coro_f: Callable[_P, Awaitable[_T]],\n) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Convert a coroutine function into a function that returns a Deferred.\n\n    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n    This is useful for callback chains, as callback functions are called with the previous callback result.\n    \"\"\"\n\n    @wraps(coro_f)\n    def f(*coro_args: _P.args, **coro_kwargs: _P.kwargs) -> Deferred[_T]:\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n\n    return f\n\n\ndef maybeDeferred_coro(\n    f: Callable[_P, Any], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[Any]:\n    \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n    try:\n        result = f(*args, **kw)\n    except:  # noqa: E722  # pylint: disable=bare-except\n        return fail(failure.Failure(captureVars=Deferred.debug))\n\n    if isinstance(result, Deferred):\n        return result\n    if asyncio.isfuture(result) or inspect.isawaitable(result):\n        return deferred_from_coro(result)\n    if isinstance(result, failure.Failure):\n        return fail(result)\n    return succeed(result)\n\n\ndef deferred_to_future(d: Deferred[_T]) -> Future[_T]:\n    \"\"\"Return an :class:`asyncio.Future` object that wraps *d*.\n\n    This function requires\n    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor` to be\n    installed.\n\n    When :ref:`using the asyncio reactor <install-asyncio>`, you cannot await\n    on :class:`~twisted.internet.defer.Deferred` objects from :ref:`Scrapy\n    callables defined as coroutines <coroutine-support>`, you can only await on\n    ``Future`` objects. Wrapping ``Deferred`` objects into ``Future`` objects\n    allows you to wait on them::\n\n        class MySpider(Spider):\n            ...\n            async def parse(self, response):\n                additional_request = scrapy.Request('https://example.org/price')\n                deferred = self.crawler.engine.download(additional_request)\n                additional_response = await deferred_to_future(deferred)\n\n    .. versionadded:: 2.6.0\n\n    .. versionchanged:: VERSION\n        This function no longer installs an asyncio loop if called before the\n        Twisted asyncio reactor is installed. A :exc:`RuntimeError` is raised\n        in this case.\n    \"\"\"\n    if not is_asyncio_available():\n        raise RuntimeError(\"deferred_to_future() requires AsyncioSelectorReactor.\")\n    return d.asFuture(asyncio.get_event_loop())\n\n\ndef maybe_deferred_to_future(d: Deferred[_T]) -> Deferred[_T] | Future[_T]:\n    \"\"\"Return *d* as an object that can be awaited from a :ref:`Scrapy callable\n    defined as a coroutine <coroutine-support>`.\n\n    What you can await in Scrapy callables defined as coroutines depends on the\n    value of :setting:`TWISTED_REACTOR`:\n\n    -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only\n        await on :class:`asyncio.Future` objects.\n\n    -   When not using the asyncio reactor, you can only await on\n        :class:`~twisted.internet.defer.Deferred` objects.\n\n    If you want to write code that uses ``Deferred`` objects but works with any\n    reactor, use this function on all ``Deferred`` objects::\n\n        class MySpider(Spider):\n            ...\n            async def parse(self, response):\n                additional_request = scrapy.Request('https://example.org/price')\n                deferred = self.crawler.engine.download(additional_request)\n                additional_response = await maybe_deferred_to_future(deferred)\n\n    .. versionadded:: 2.6.0\n    \"\"\"\n    if not is_asyncio_available():\n        return d\n    return deferred_to_future(d)\n\n\ndef _schedule_coro(coro: Coroutine[Any, Any, Any]) -> None:\n    \"\"\"Schedule the coroutine as a task or a Deferred.\n\n    This doesn't store the reference to the task/Deferred, so a better\n    alternative is calling :func:`scrapy.utils.defer.deferred_from_coro`,\n    keeping the result, and adding proper exception handling (e.g. errbacks) to\n    it.\n    \"\"\"\n    if not is_asyncio_available():\n        Deferred.fromCoroutine(coro)\n        return\n    loop = asyncio.get_event_loop()\n    loop.create_task(coro)  # noqa: RUF006\n\n\n@overload\ndef ensure_awaitable(o: Awaitable[_T]) -> Awaitable[_T]: ...\n\n\n@overload\ndef ensure_awaitable(o: _T) -> Awaitable[_T]: ...\n\n\ndef ensure_awaitable(o: _T | Awaitable[_T]) -> Awaitable[_T]:\n    \"\"\"Convert any value to an awaitable object.\n\n    For a :class:`~twisted.internet.defer.Deferred` object, use\n    :func:`maybe_deferred_to_future` to wrap it into a suitable object. For an\n    awaitable object of a different type, return it as is. For any other\n    value, return a coroutine that completes with that value.\n\n    .. versionadded:: VERSION\n    \"\"\"\n    if isinstance(o, Deferred):\n        return maybe_deferred_to_future(o)\n    if inspect.isawaitable(o):\n        return o\n\n    async def coro() -> _T:\n        return o\n\n    return coro()\n", "n_tokens": 1196, "byte_len": 4984, "file_sha1": "68e7333df8d7b866cb2f32d7bede6577c8cd3bbd", "start_line": 428, "end_line": 566}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/iterators.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/iterators.py", "rel_path": "scrapy/utils/iterators.py", "module": "scrapy.utils.iterators", "ext": "py", "chunk_number": 1, "symbols": ["xmliter", "xmliter_lxml", "__init__", "read", "_StreamReader", "encoding", "method", "header", "end", "text", "bool", "xpath", "iterparse", "literal", "tagname", "iterate", "selector", "name", "namespace", "deprecated", "dotall", "useful", "future", "typ", "checking", "string", "https", "elif", "document", "first", "_read_string", "_read_unicode", "csviter", "_body_or_str", "separate", "removed", "length", "provided", "expected", "types", "encoded", "isinstance", "object", "character", "none", "encode", "returns", "nodetext", "join", "nodename"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport csv\nimport logging\nimport re\nfrom io import StringIO\nfrom typing import TYPE_CHECKING, Any, Literal, cast, overload\nfrom warnings import warn\n\nfrom lxml import etree\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import re_rsearch\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterator\n\nlogger = logging.getLogger(__name__)\n\n\ndef xmliter(obj: Response | str | bytes, nodename: str) -> Iterator[Selector]:\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    warn(\n        (\n            \"xmliter is deprecated and its use strongly discouraged because \"\n            \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"\n            \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    nodename_patt = re.escape(nodename)\n\n    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.DOTALL)\n    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.DOTALL)\n    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.DOTALL)\n    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.DOTALL)\n    text = _body_or_str(obj)\n\n    document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n    document_header = (\n        document_header_match.group().strip() if document_header_match else \"\"\n    )\n    header_end_idx = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"\n    namespaces: dict[str, str] = {}\n    if header_end:\n        for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n            assert header_end_idx\n            tag = re.search(\n                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\",\n                text[: header_end_idx[1]],\n                re.DOTALL,\n            )\n            if tag:\n                for x in re.findall(NAMESPACE_RE, tag.group()):\n                    namespaces[x[1]] = x[0]\n\n    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = (\n            document_header\n            + match.group().replace(\n                nodename, f\"{nodename} {' '.join(namespaces.values())}\", 1\n            )\n            + header_end\n        )\n        yield Selector(text=nodetext, type=\"xml\")\n\n\ndef xmliter_lxml(\n    obj: Response | str | bytes,\n    nodename: str,\n    namespace: str | None = None,\n    prefix: str = \"x\",\n) -> Iterator[Selector]:\n    reader = _StreamReader(obj)\n    tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n    iterable = etree.iterparse(\n        reader,\n        encoding=reader.encoding,\n        events=(\"end\", \"start-ns\"),\n        resolve_entities=False,\n        huge_tree=True,\n    )\n    selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n    needs_namespace_resolution = not namespace and \":\" in nodename\n    if needs_namespace_resolution:\n        prefix, nodename = nodename.split(\":\", maxsplit=1)\n    for event, data in iterable:\n        if event == \"start-ns\":\n            assert isinstance(data, tuple)\n            if needs_namespace_resolution:\n                _prefix, _namespace = data\n                if _prefix != prefix:\n                    continue\n                namespace = _namespace\n                needs_namespace_resolution = False\n                selxpath = f\"//{prefix}:{nodename}\"\n                tag = f\"{{{namespace}}}{nodename}\"\n            continue\n        assert isinstance(data, etree._Element)\n        node = data\n        if node.tag != tag:\n            continue\n        nodetext = etree.tostring(node, encoding=\"unicode\")\n        node.clear()\n        xs = Selector(text=nodetext, type=\"xml\")\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]\n\n\nclass _StreamReader:\n    def __init__(self, obj: Response | str | bytes):\n        self._ptr: int = 0\n        self._text: str | bytes\n        if isinstance(obj, TextResponse):\n            self._text, self.encoding = obj.body, obj.encoding\n        elif isinstance(obj, Response):\n            self._text, self.encoding = obj.body, \"utf-8\"\n        else:\n            self._text, self.encoding = obj, \"utf-8\"\n        self._is_unicode: bool = isinstance(self._text, str)\n        self._is_first_read: bool = True\n\n    def read(self, n: int = 65535) -> bytes:\n        method: Callable[[int], bytes] = (\n            self._read_unicode if self._is_unicode else self._read_string\n        )\n        result = method(n)\n        if self._is_first_read:\n            self._is_first_read = False\n            result = result.lstrip()\n        return result\n", "n_tokens": 1208, "byte_len": 4985, "file_sha1": "080ed13a9fa90c93383e6115c00b74c9a7cf5237", "start_line": 1, "end_line": 146}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/iterators.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/iterators.py", "rel_path": "scrapy/utils/iterators.py", "module": "scrapy.utils.iterators", "ext": "py", "chunk_number": 2, "symbols": ["_read_string", "_read_unicode", "csviter", "_body_or_str", "encoding", "text", "bool", "separate", "future", "literal", "removed", "string", "length", "read", "provided", "expected", "types", "encoded", "isinstance", "object", "character", "none", "encode", "returns", "join", "type", "iterator", "response", "unicode", "ptr", "xmliter", "xmliter_lxml", "__init__", "_StreamReader", "method", "header", "end", "xpath", "iterparse", "tagname", "iterate", "selector", "name", "namespace", "deprecated", "dotall", "useful", "typ", "checking", "https"], "ast_kind": "function_or_method", "text": "    def _read_string(self, n: int = 65535) -> bytes:\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return cast(\"bytes\", self._text)[s:e]\n\n    def _read_unicode(self, n: int = 65535) -> bytes:\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return cast(\"str\", self._text)[s:e].encode(\"utf-8\")\n\n\ndef csviter(\n    obj: Response | str | bytes,\n    delimiter: str | None = None,\n    headers: list[str] | None = None,\n    encoding: str | None = None,\n    quotechar: str | None = None,\n) -> Iterator[dict[str, str]]:\n    \"\"\"Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    if encoding is not None:\n        warn(\n            \"The encoding argument of csviter() is ignored and will be removed\"\n            \" in a future Scrapy version.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n    lines = StringIO(_body_or_str(obj, unicode=True))\n\n    kwargs: dict[str, Any] = {}\n    if delimiter:\n        kwargs[\"delimiter\"] = delimiter\n    if quotechar:\n        kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            headers = next(csv_r)\n        except StopIteration:\n            return\n\n    for row in csv_r:\n        if len(row) != len(headers):\n            logger.warning(\n                \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                \"should be: %(csvheader)d)\",\n                {\n                    \"csvlnum\": csv_r.line_num,\n                    \"csvrow\": len(row),\n                    \"csvheader\": len(headers),\n                },\n            )\n            continue\n        yield dict(zip(headers, row))\n\n\n@overload\ndef _body_or_str(obj: Response | str | bytes) -> str: ...\n\n\n@overload\ndef _body_or_str(obj: Response | str | bytes, unicode: Literal[True]) -> str: ...\n\n\n@overload\ndef _body_or_str(obj: Response | str | bytes, unicode: Literal[False]) -> bytes: ...\n\n\ndef _body_or_str(obj: Response | str | bytes, unicode: bool = True) -> str | bytes:\n    expected_types = (Response, str, bytes)\n    if not isinstance(obj, expected_types):\n        expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n        raise TypeError(\n            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"\n        )\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        if isinstance(obj, TextResponse):\n            return obj.text\n        return obj.body.decode(\"utf-8\")\n    if isinstance(obj, str):\n        return obj if unicode else obj.encode(\"utf-8\")\n    return obj.decode(\"utf-8\") if unicode else obj\n", "n_tokens": 745, "byte_len": 3023, "file_sha1": "080ed13a9fa90c93383e6115c00b74c9a7cf5237", "start_line": 147, "end_line": 246}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/python.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/python.py", "rel_path": "scrapy/utils/python.py", "module": "scrapy.utils.python", "ext": "py", "chunk_number": 1, "symbols": ["flatten", "iflatten", "is_listlike", "unique", "to_unicode", "to_bytes", "encoding", "bool", "seen", "append", "retrieved", "future", "python", "strict", "deprecated", "removed", "typ", "checking", "order", "unicode", "async", "generator", "isinstance", "object", "sequences", "none", "returns", "encode", "type", "bytes", "re_rsearch", "_chunk_iter", "memoizemethod_noargs", "new_method", "binary_is_text", "get_func_args_dict", "get_func_args", "get_spec", "equal_attributes", "without_none_values", "global_object_name", "garbage_collect", "__init__", "extend", "__iter__", "__next__", "__aiter__", "MutableChain", "MutableAsyncChain", "while"], "ast_kind": "function_or_method", "text": "\"\"\"\nThis module contains essential stuff that should've come with Python itself ;)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport gc\nimport inspect\nimport re\nimport sys\nimport warnings\nimport weakref\nfrom collections.abc import AsyncIterator, Iterable, Mapping\nfrom functools import partial, wraps\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, TypeVar, overload\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.asyncgen import as_async_generator\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterator\n    from re import Pattern\n\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    # typing.Self requires Python 3.11\n    from typing_extensions import Concatenate, ParamSpec, Self\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\ndef flatten(x: Iterable[Any]) -> list[Any]:\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    warnings.warn(\n        \"The flatten function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    return list(iflatten(x))\n\n\ndef iflatten(x: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    warnings.warn(\n        \"The iflatten function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    for el in x:\n        if is_listlike(el):\n            yield from iflatten(el)\n        else:\n            yield el\n\n\ndef is_listlike(x: Any) -> bool:\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n\n\ndef unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> list[_T]:\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result: list[_T] = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result\n\n\ndef to_unicode(\n    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n) -> str:\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError(\n            f\"to_unicode must receive a bytes or str object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.decode(encoding, errors)\n\n\ndef to_bytes(\n    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n) -> bytes:\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError(\n            f\"to_bytes must receive a str or bytes object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.encode(encoding, errors)\n\n", "n_tokens": 1050, "byte_len": 3976, "file_sha1": "d54355ecbc1c6f6c672fadda048c19060487d320", "start_line": 1, "end_line": 145}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/python.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/python.py", "rel_path": "scrapy/utils/python.py", "module": "scrapy.utils.python", "ext": "py", "chunk_number": 2, "symbols": ["re_rsearch", "_chunk_iter", "memoizemethod_noargs", "new_method", "binary_is_text", "get_func_args_dict", "get_func_args", "get_spec", "does", "method", "bool", "containing", "ismethod", "entire", "unprintable", "expression", "case", "spec", "name", "elif", "string", "regarding", "getfullargspec", "kilobytes", "continues", "chunk", "size", "isinstance", "process", "items", "flatten", "iflatten", "is_listlike", "unique", "to_unicode", "to_bytes", "equal_attributes", "without_none_values", "global_object_name", "garbage_collect", "__init__", "extend", "__iter__", "__next__", "__aiter__", "MutableChain", "MutableAsyncChain", "encoding", "while", "async"], "ast_kind": "class_or_type", "text": "def re_rsearch(\n    pattern: str | Pattern[str], text: str, chunk_size: int = 1024\n) -> tuple[int, int] | None:\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter() -> Iterable[tuple[str, int]]:\n        offset = len(text)\n        while True:\n            offset -= chunk_size * 1024\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = list(pattern.finditer(chunk))\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None\n\n\n_SelfT = TypeVar(\"_SelfT\")\n\n\ndef memoizemethod_noargs(\n    method: Callable[Concatenate[_SelfT, _P], _T],\n) -> Callable[Concatenate[_SelfT, _P], _T]:\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache: weakref.WeakKeyDictionary[_SelfT, _T] = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self: _SelfT, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method\n\n\n_BINARYCHARS = {\n    i for i in range(32) if to_bytes(chr(i)) not in {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n}\n\n\ndef binary_is_text(data: bytes) -> bool:\n    \"\"\"Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)\n\n\ndef get_func_args_dict(\n    func: Callable[..., Any], stripself: bool = False\n) -> Mapping[str, inspect.Parameter]:\n    \"\"\"Return the argument dict of a callable object.\n\n    .. versionadded:: VERSION\n    \"\"\"\n    if not callable(func):\n        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n\n    args: Mapping[str, inspect.Parameter]\n    try:\n        sig = inspect.signature(func)\n    except ValueError:\n        return {}\n\n    if isinstance(func, partial):\n        partial_args = func.args\n        partial_kw = func.keywords\n\n        args = {}\n        for name, param in sig.parameters.items():\n            if name in partial_args:\n                continue\n            if partial_kw and name in partial_kw:\n                continue\n            args[name] = param\n    else:\n        args = sig.parameters\n\n    if stripself and args and \"self\" in args:\n        args = {k: v for k, v in args.items() if k != \"self\"}\n    return args\n\n\ndef get_func_args(func: Callable[..., Any], stripself: bool = False) -> list[str]:\n    \"\"\"Return the argument name list of a callable object\"\"\"\n    return list(get_func_args_dict(func, stripself=stripself))\n\n\ndef get_spec(func: Callable[..., Any]) -> tuple[list[str], dict[str, Any]]:\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getfullargspec(func)\n    elif hasattr(func, \"__call__\"):  # noqa: B004\n        spec = inspect.getfullargspec(func.__call__)\n    else:\n        raise TypeError(f\"{type(func)} is not callable\")\n\n    defaults: tuple[Any, ...] = spec.defaults or ()\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs\n\n", "n_tokens": 1151, "byte_len": 4668, "file_sha1": "d54355ecbc1c6f6c672fadda048c19060487d320", "start_line": 146, "end_line": 292}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/python.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/python.py", "rel_path": "scrapy/utils/python.py", "module": "scrapy.utils.python", "ext": "py", "chunk_number": 3, "symbols": ["equal_attributes", "without_none_values", "global_object_name", "garbage_collect", "__init__", "extend", "__iter__", "__next__", "__aiter__", "MutableChain", "MutableAsyncChain", "async", "callables", "bool", "qualname", "future", "deprecated", "anext", "removed", "elif", "temp", "temp1", "generator", "isinstance", "items", "object", "none", "type", "from", "iterable", "flatten", "iflatten", "is_listlike", "unique", "to_unicode", "to_bytes", "re_rsearch", "_chunk_iter", "memoizemethod_noargs", "new_method", "binary_is_text", "get_func_args_dict", "get_func_args", "get_spec", "encoding", "while", "does", "method", "containing", "seen"], "ast_kind": "class_or_type", "text": "def equal_attributes(\n    obj1: Any, obj2: Any, attributes: list[str | Callable[[Any], Any]] | None\n) -> bool:\n    \"\"\"Compare two objects attributes\"\"\"\n    warnings.warn(\n        \"The equal_attributes function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True\n\n\n@overload\ndef without_none_values(iterable: Mapping[_KT, _VT]) -> dict[_KT, _VT]: ...\n\n\n@overload\ndef without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n\n\ndef without_none_values(\n    iterable: Mapping[_KT, _VT] | Iterable[_KT],\n) -> dict[_KT, _VT] | Iterable[_KT]:\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    if isinstance(iterable, Mapping):\n        return {k: v for k, v in iterable.items() if v is not None}\n    # the iterable __init__ must take another iterable\n    return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n\n\ndef global_object_name(obj: Any) -> str:\n    \"\"\"Return the full import path of the given object.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    >>> global_object_name(Request.replace)\n    'scrapy.http.request.Request.replace'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__qualname__}\"\n\n\nif hasattr(sys, \"pypy_version_info\"):\n\n    def garbage_collect() -> None:\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()\n\nelse:\n\n    def garbage_collect() -> None:\n        gc.collect()\n\n\nclass MutableChain(Iterable[_T]):\n    \"\"\"\n    Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T]):\n        self.data: Iterator[_T] = chain.from_iterable(args)\n\n    def extend(self, *iterables: Iterable[_T]) -> None:\n        self.data = chain(self.data, chain.from_iterable(iterables))\n\n    def __iter__(self) -> Iterator[_T]:\n        return self\n\n    def __next__(self) -> _T:\n        return next(self.data)\n\n\nasync def _async_chain(\n    *iterables: Iterable[_T] | AsyncIterator[_T],\n) -> AsyncIterator[_T]:\n    for it in iterables:\n        async for o in as_async_generator(it):\n            yield o\n\n\nclass MutableAsyncChain(AsyncIterator[_T]):\n    \"\"\"\n    Similar to MutableChain but for async iterables\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T] | AsyncIterator[_T]):\n        self.data: AsyncIterator[_T] = _async_chain(*args)\n\n    def extend(self, *iterables: Iterable[_T] | AsyncIterator[_T]) -> None:\n        self.data = _async_chain(self.data, _async_chain(*iterables))\n\n    def __aiter__(self) -> Self:\n        return self\n\n    async def __anext__(self) -> _T:\n        return await self.data.__anext__()\n", "n_tokens": 823, "byte_len": 3323, "file_sha1": "d54355ecbc1c6f6c672fadda048c19060487d320", "start_line": 293, "end_line": 407}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/testproc.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/testproc.py", "rel_path": "scrapy/utils/testproc.py", "module": "scrapy.utils.testproc", "ext": "py", "chunk_number": 1, "symbols": ["execute", "_process_finished", "__init__", "outReceived", "errReceived", "processEnded", "ProcessTest", "TestProcessProtocol", "failure", "protocol", "bool", "deprecated", "future", "typ", "checking", "scrap", "setting", "chdirs", "settings", "spawn", "process", "test", "none", "code", "runtime", "error", "testproc", "internet", "command", "typing", "return", "stdout", "annotations", "add", "callback", "class", "err", "received", "ended", "check", "warnings", "noqa", "list", "exceptions", "trial", "self", "raise", "terminated", "python", "module"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport os\nimport sys\nimport warnings\nfrom typing import TYPE_CHECKING, cast\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.protocol import ProcessProtocol\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from twisted.internet.error import ProcessTerminated\n    from twisted.python.failure import Failure\n\n\nwarnings.warn(\n    \"The scrapy.utils.testproc module is deprecated.\",\n    ScrapyDeprecationWarning,\n)\n\n\nclass ProcessTest:\n    command: str | None = None\n    prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n    cwd = os.getcwd()  # trial chdirs to temp dir  # noqa: PTH109\n\n    def execute(\n        self,\n        args: Iterable[str],\n        check_code: bool = True,\n        settings: str | None = None,\n    ) -> Deferred[TestProcessProtocol]:\n        from twisted.internet import reactor\n\n        env = os.environ.copy()\n        if settings is not None:\n            env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n        assert self.command\n        cmd = [*self.prefix, self.command, *args]\n        pp = TestProcessProtocol()\n        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred\n\n    def _process_finished(\n        self, pp: TestProcessProtocol, cmd: list[str], check_code: bool\n    ) -> tuple[int, bytes, bytes]:\n        if pp.exitcode and check_code:\n            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n            msg += f\"\\n>>> stdout <<<\\n{pp.out.decode()}\"\n            msg += \"\\n\"\n            msg += f\"\\n>>> stderr <<<\\n{pp.err.decode()}\"\n            raise RuntimeError(msg)\n        return cast(\"int\", pp.exitcode), pp.out, pp.err\n\n\nclass TestProcessProtocol(ProcessProtocol):\n    def __init__(self) -> None:\n        self.deferred: Deferred[TestProcessProtocol] = Deferred()\n        self.out: bytes = b\"\"\n        self.err: bytes = b\"\"\n        self.exitcode: int | None = None\n\n    def outReceived(self, data: bytes) -> None:\n        self.out += data\n\n    def errReceived(self, data: bytes) -> None:\n        self.err += data\n\n    def processEnded(self, status: Failure) -> None:\n        self.exitcode = cast(\"ProcessTerminated\", status.value).exitCode\n        self.deferred.callback(self)\n", "n_tokens": 549, "byte_len": 2353, "file_sha1": "3bffdbb944fca033f47fefcdac12205abac95e97", "start_line": 1, "end_line": 77}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/template.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/template.py", "rel_path": "scrapy/utils/template.py", "module": "scrapy.utils.template", "ext": "py", "chunk_number": 1, "symbols": ["render_templatefile", "string_camelcase", "utf", "utf8", "pound", "compile", "string", "camelcase", "functions", "working", "typing", "render", "templatefile", "return", "camel", "case", "annotations", "template", "camelcas", "invali", "with", "path", "future", "typ", "checking", "obj", "pathlib", "suffix", "convert", "remove", "invalid", "chars", "missing", "images", "templates", "version", "from", "lost", "tmpl", "kwargs", "rename", "content", "substitute", "none", "word", "import", "helper", "read", "text", "like"], "ast_kind": "function_or_method", "text": "\"\"\"Helper functions for working with templates\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport string\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nif TYPE_CHECKING:\n    from os import PathLike\n\n\ndef render_templatefile(path: str | PathLike, **kwargs: Any) -> None:\n    path_obj = Path(path)\n    raw = path_obj.read_text(\"utf8\")\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path_obj.with_suffix(\"\") if path_obj.suffix == \".tmpl\" else path_obj\n\n    if path_obj.suffix == \".tmpl\":\n        path_obj.rename(render_path)\n\n    render_path.write_text(content, \"utf8\")\n\n\nCAMELCASE_INVALID_CHARS = re.compile(r\"[^a-zA-Z\\d]\")\n\n\ndef string_camelcase(string: str) -> str:\n    \"\"\"Convert a word  to its CamelCase version and remove invalid chars\n\n    >>> string_camelcase('lost-pound')\n    'LostPound'\n\n    >>> string_camelcase('missing_images')\n    'MissingImages'\n\n    \"\"\"\n    return CAMELCASE_INVALID_CHARS.sub(\"\", string.title())\n", "n_tokens": 238, "byte_len": 979, "file_sha1": "d8b2ef3133b762819f601895669831e9f65c382b", "start_line": 1, "end_line": 42}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/versions.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/versions.py", "rel_path": "scrapy/utils/versions.py", "module": "scrapy.utils.versions", "ext": "py", "chunk_number": 1, "symbols": ["_version", "get_versions", "scrapy_components_versions", "pyopenssl", "defaul", "software", "python", "scrapy", "deprecation", "versions", "log", "return", "replace", "item", "annotations", "instead", "libxml", "libxml2", "deprecated", "future", "etree", "lxml", "lowercase", "get", "warnings", "version", "default", "settings", "from", "list", "importlib", "lower", "libxm", "tuple", "stacklevel", "platform", "exceptions", "none", "join", "utils", "import", "components", "openssl", "warn", "metadata"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport platform\nimport sys\nfrom importlib.metadata import version\nfrom warnings import warn\n\nimport lxml.etree\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.settings.default_settings import LOG_VERSIONS\nfrom scrapy.utils.ssl import get_openssl_version\n\n_DEFAULT_SOFTWARE = [\"Scrapy\", *LOG_VERSIONS]\n\n\ndef _version(item):\n    lowercase_item = item.lower()\n    if lowercase_item == \"libxml2\":\n        return \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n    if lowercase_item == \"platform\":\n        return platform.platform()\n    if lowercase_item == \"pyopenssl\":\n        return get_openssl_version()\n    if lowercase_item == \"python\":\n        return sys.version.replace(\"\\n\", \"- \")\n    return version(item)\n\n\ndef get_versions(\n    software: list | None = None,\n) -> list[tuple[str, str]]:\n    software = software or _DEFAULT_SOFTWARE\n    return [(item, _version(item)) for item in software]\n\n\ndef scrapy_components_versions() -> list[tuple[str, str]]:\n    warn(\n        (\n            \"scrapy.utils.versions.scrapy_components_versions() is deprecated, \"\n            \"use scrapy.utils.versions.get_versions() instead.\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    return get_versions()\n", "n_tokens": 281, "byte_len": 1273, "file_sha1": "035caa9084e708c9e7b28bb062fb434cedd5d841", "start_line": 1, "end_line": 47}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/ossignal.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/ossignal.py", "rel_path": "scrapy/utils/ossignal.py", "module": "scrapy.utils.ossignal", "ext": "py", "chunk_number": 1, "symbols": ["install_shutdown_handlers", "stdlib", "types", "break", "bool", "shutdown", "install", "false", "hasattr", "default", "int", "sig", "signal", "override", "sigint", "sigterm", "typing", "dict", "installed", "annotations", "startswith", "typeshed", "with", "place", "frame", "type", "future", "collections", "ctrl", "pylint", "signame", "given", "callable", "comparison", "true", "getsignal", "from", "function", "handlers", "handler", "copy", "getattr", "signals", "isinstance", "already", "windows", "none", "there", "names", "common"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport signal\nfrom collections.abc import Callable\nfrom types import FrameType\nfrom typing import Any, Optional, Union\n\n# copy of _HANDLER from typeshed/stdlib/signal.pyi\nSignalHandlerT = Union[\n    Callable[[int, Optional[FrameType]], Any], int, signal.Handlers, None\n]\n\nsignal_names: dict[int, str] = {}\nfor signame in dir(signal):\n    if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n        signum = getattr(signal, signame)\n        if isinstance(signum, int):\n            signal_names[signum] = signame\n\n\ndef install_shutdown_handlers(\n    function: SignalHandlerT, override_sigint: bool = True\n) -> None:\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n    SIGINT handler won't be installed if there is already a handler in place\n    (e.g. Pdb)\n    \"\"\"\n    signal.signal(signal.SIGTERM, function)\n    if (\n        signal.getsignal(signal.SIGINT)  # pylint: disable=comparison-with-callable\n        == signal.default_int_handler\n        or override_sigint\n    ):\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, \"SIGBREAK\"):\n        signal.signal(signal.SIGBREAK, function)\n", "n_tokens": 311, "byte_len": 1300, "file_sha1": "6e246f89879510321a4fcad1b1dae0bb02218dd0", "start_line": 1, "end_line": 39}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/ssl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/ssl.py", "rel_path": "scrapy/utils/ssl.py", "module": "scrapy.utils.ssl", "ext": "py", "chunk_number": 1, "symbols": ["ffi_buf_to_string", "x509name_to_string", "get_temp_key_info", "get_openssl_version", "cname", "key", "hasattr", "pke", "rsa", "crypto", "name", "python", "append", "null", "ecdh", "apps", "errors", "typing", "return", "type", "get", "group", "nid", "obj", "replace", "annotations", "pkey", "evp", "cryptography", "scrapy", "removed", "future", "typ", "checking", "open", "ssl", "string", "elif", "free", "adapted", "system", "openssl", "repr", "decode", "info", "x509name", "leay", "version", "from", "grou"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nimport OpenSSL._util as pyOpenSSLutil\nimport OpenSSL.SSL\nimport OpenSSL.version\n\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from OpenSSL.crypto import X509Name\n\n\ndef ffi_buf_to_string(buf: Any) -> str:\n    return to_unicode(pyOpenSSLutil.ffi.string(buf))\n\n\ndef x509name_to_string(x509name: X509Name) -> str:\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer: Any = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(\n        x509name._name, result_buffer, len(result_buffer)\n    )\n\n    return ffi_buf_to_string(result_buffer)\n\n\ndef get_temp_key_info(ssl_object: Any) -> str | None:\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    if not hasattr(pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"):\n        # removed in cryptography 40.0.0\n        return None\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append(\"RSA\")\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append(\"DH\")\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append(\"ECDH\")\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(\n            pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key)\n        )\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append(f\"{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits\")\n    return \", \".join(key_info)\n\n\ndef get_openssl_version() -> str:\n    system_openssl_bytes = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n    system_openssl = system_openssl_bytes.decode(\"ascii\", errors=\"replace\")\n    return f\"{OpenSSL.version.__version__} ({system_openssl})\"\n", "n_tokens": 683, "byte_len": 2472, "file_sha1": "4b21a0c5704635baa95e3f1027c54c8f72d8c85b", "start_line": 1, "end_line": 68}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/trackref.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/trackref.py", "rel_path": "scrapy/utils/trackref.py", "module": "scrapy.utils.trackref", "ext": "py", "chunk_number": 1, "symbols": ["__new__", "format_live_refs", "print_live_refs", "get_oldest", "iter_all", "object_ref", "library", "subclass", "case", "python", "becomes", "name", "tabular", "enabled", "oldest", "future", "typ", "checking", "slots", "particular", "items", "object", "inherit", "none", "type", "print", "live", "sorted", "defaultdict", "values", "performance", "representation", "typing", "extensions", "functions", "keep", "ref", "return", "disabled", "break", "over", "hard", "annotations", "format", "class", "ignore", "record", "some", "tracked", "kwargs"], "ast_kind": "class_or_type", "text": "\"\"\"This module provides some functions and classes to record and report\nreferences to live object instances.\n\nIf you want live objects for a particular class to be tracked, you only have to\nsubclass from object_ref (instead of object).\n\nAbout performance: This library has a minimal performance impact when enabled,\nand no performance penalty at all when disabled (as object_ref becomes just an\nalias to object in that case).\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom time import time\nfrom typing import TYPE_CHECKING, Any\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nNoneType = type(None)\nlive_refs: defaultdict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n\n\nclass object_ref:\n    \"\"\"Inherit from this class to a keep a record of live instances\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj\n\n\n# using Any as it's hard to type type(None)\ndef format_live_refs(ignore: Any = NoneType) -> str:\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(wdict.values())\n        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n    return s\n\n\ndef print_live_refs(*a: Any, **kw: Any) -> None:\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))\n\n\ndef get_oldest(class_name: str) -> Any:\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(wdict.items(), key=itemgetter(1))[0]\n    return None\n\n\ndef iter_all(class_name: str) -> Iterable[Any]:\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            return wdict.keys()\n    return []\n", "n_tokens": 554, "byte_len": 2330, "file_sha1": "36f7e469075c1325561ad006e9a4c96d05589b67", "start_line": 1, "end_line": 78}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/ftp.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/ftp.py", "rel_path": "scrapy/utils/ftp.py", "module": "scrapy.utils.ftp", "ext": "py", "chunk_number": 1, "symbols": ["ftp_makedirs_cwd", "ftp_store_file", "ftp", "makedirs", "seek", "bool", "false", "uploads", "except", "argument", "first", "call", "command", "appe", "typing", "password", "connection", "credentials", "stor", "else", "current", "error", "perm", "path", "with", "passed", "ftplib", "port", "dirname", "given", "host", "username", "directories", "connect", "set", "pasv", "true", "creating", "store", "from", "they", "logged", "storbinary", "close", "parent", "already", "split", "object", "exist", "none"], "ast_kind": "function_or_method", "text": "import posixpath\nfrom ftplib import FTP, error_perm\nfrom posixpath import dirname\nfrom typing import IO\n\n\ndef ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool = True) -> None:\n    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n    argument (as a ftplib.FTP object), creating all parent directories if they\n    don't exist. The ftplib.FTP object must be already connected and logged in.\n    \"\"\"\n    try:\n        ftp.cwd(path)\n    except error_perm:\n        ftp_makedirs_cwd(ftp, dirname(path), False)\n        ftp.mkd(path)\n        if first_call:\n            ftp.cwd(path)\n\n\ndef ftp_store_file(\n    *,\n    path: str,\n    file: IO[bytes],\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n    overwrite: bool = True,\n) -> None:\n    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n    to the directory extracted from given path, then uploads the file to server\n    \"\"\"\n    with FTP() as ftp:\n        ftp.connect(host, port)\n        ftp.login(username, password)\n        if use_active_mode:\n            ftp.set_pasv(False)\n        file.seek(0)\n        dirname, filename = posixpath.split(path)\n        ftp_makedirs_cwd(ftp, dirname)\n        command = \"STOR\" if overwrite else \"APPE\"\n        ftp.storbinary(f\"{command} {filename}\", file)\n        file.close()\n", "n_tokens": 331, "byte_len": 1362, "file_sha1": "4cc411f6a6e1305f20725cfc1b3db44018b2aecf", "start_line": 1, "end_line": 46}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/spider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/spider.py", "rel_path": "scrapy/utils/spider.py", "module": "scrapy.utils.spider", "ext": "py", "chunk_number": 1, "symbols": ["iterate_spider_output", "iter_spider_classes", "spidercls_for_request", "DefaultSpider", "bool", "spider", "loader", "literal", "name", "load", "spiders", "passed", "future", "typ", "checking", "https", "isasyncgen", "iscoroutine", "spiderloader", "than", "isclass", "none", "join", "type", "misc", "log", "default", "found", "spidercls", "values", "optionally", "internet", "able", "typing", "return", "over", "annotations", "add", "callback", "class", "ignore", "iterator", "more", "stackoverflow", "multiple", "questions", "find", "request", "iterate", "this"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport inspect\nimport logging\nfrom typing import TYPE_CHECKING, Any, Literal, TypeVar, overload\n\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import deferred_from_coro\nfrom scrapy.utils.misc import arg_to_iter\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Iterable\n    from types import CoroutineType, ModuleType\n\n    from twisted.internet.defer import Deferred\n\n    from scrapy import Request\n    from scrapy.spiderloader import SpiderLoaderProtocol\n\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\n# https://stackoverflow.com/questions/60222982\n@overload\ndef iterate_spider_output(result: AsyncGenerator[_T]) -> AsyncGenerator[_T]: ...  # type: ignore[overload-overlap]\n\n\n@overload\ndef iterate_spider_output(result: CoroutineType[Any, Any, _T]) -> Deferred[_T]: ...\n\n\n@overload\ndef iterate_spider_output(result: _T) -> Iterable[Any]: ...\n\n\ndef iterate_spider_output(\n    result: Any,\n) -> Iterable[Any] | AsyncGenerator[_T] | Deferred[_T]:\n    if inspect.isasyncgen(result):\n        return result\n    if inspect.iscoroutine(result):\n        d = deferred_from_coro(result)\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(deferred_from_coro(result))\n\n\ndef iter_spider_classes(module: ModuleType) -> Iterable[type[Spider]]:\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (i.e. which have name)\n    \"\"\"\n    for obj in vars(module).values():\n        if (\n            inspect.isclass(obj)\n            and issubclass(obj, Spider)\n            and obj.__module__ == module.__name__\n            and getattr(obj, \"name\", None)\n        ):\n            yield obj\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoaderProtocol,\n    request: Request,\n    default_spidercls: type[Spider],\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> type[Spider]: ...\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoaderProtocol,\n    request: Request,\n    default_spidercls: Literal[None],\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> type[Spider] | None: ...\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoaderProtocol,\n    request: Request,\n    *,\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> type[Spider] | None: ...\n\n\ndef spidercls_for_request(\n    spider_loader: SpiderLoaderProtocol,\n    request: Request,\n    default_spidercls: type[Spider] | None = None,\n    log_none: bool = False,\n    log_multiple: bool = False,\n) -> type[Spider] | None:\n    \"\"\"Return a spider class that handles the given Request.\n\n    This will look for the spiders that can handle the given request (using\n    the spider loader) and return a Spider class if (and only if) there is\n    only one Spider able to handle the Request.\n\n    If multiple spiders (or no spider) are found, it will return the\n    default_spidercls passed. It can optionally log if multiple or no spiders\n    are found.\n    \"\"\"\n    snames = spider_loader.find_by_request(request)\n    if len(snames) == 1:\n        return spider_loader.load(snames[0])\n\n    if len(snames) > 1 and log_multiple:\n        logger.error(\n            \"More than one spider can handle: %(request)s - %(snames)s\",\n            {\"request\": request, \"snames\": \", \".join(snames)},\n        )\n\n    if len(snames) == 0 and log_none:\n        logger.error(\n            \"Unable to find spider that handles: %(request)s\", {\"request\": request}\n        )\n\n    return default_spidercls\n\n\nclass DefaultSpider(Spider):\n    name = \"default\"\n", "n_tokens": 860, "byte_len": 3599, "file_sha1": "13c0b807b305857000380d3b8c90e0328d3ac990", "start_line": 1, "end_line": 132}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/asyncgen.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/asyncgen.py", "rel_path": "scrapy/utils/asyncgen.py", "module": "scrapy.utils.asyncgen", "ext": "py", "chunk_number": 1, "symbols": ["async", "generator", "result", "type", "var", "sync", "typing", "return", "annotations", "collect", "asyncgen", "future", "collections", "yield", "from", "list", "iterator", "iterable", "isinstance", "into", "import", "else", "wraps"], "ast_kind": "imports", "text": "from __future__ import annotations\n\nfrom collections.abc import AsyncGenerator, AsyncIterator, Iterable\nfrom typing import TypeVar\n\n_T = TypeVar(\"_T\")\n\n\nasync def collect_asyncgen(result: AsyncIterator[_T]) -> list[_T]:\n    return [x async for x in result]\n\n\nasync def as_async_generator(\n    it: Iterable[_T] | AsyncIterator[_T],\n) -> AsyncGenerator[_T]:\n    \"\"\"Wraps an iterable (sync or async) into an async generator.\"\"\"\n    if isinstance(it, AsyncIterator):\n        async for r in it:\n            yield r\n    else:\n        for r in it:\n            yield r\n", "n_tokens": 134, "byte_len": 561, "file_sha1": "3bfeb0d729e7c63f6d9da3bc20acedb9e18c4100", "start_line": 1, "end_line": 23}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/project.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/project.py", "rel_path": "scrapy/utils/project.py", "module": "scrapy.utils.project", "ext": "py", "chunk_number": 1, "symbols": ["inside_project", "project_data_dir", "data_path", "get_project_settings", "bool", "unmodified", "doesn", "import", "module", "future", "path", "obj", "scrap", "setting", "settings", "datadir", "project", "data", "items", "exist", "parents", "datadi", "section", "scrapy", "envvars", "init", "env", "default", "like", "return", "has", "option", "annotations", "startswith", "not", "configured", "get", "warnings", "valid", "exists", "exceptions", "setdict", "raise", "else", "absolute", "priority", "infer", "replace", "environ", "mkdir"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport os\nimport warnings\nfrom importlib import import_module\nfrom pathlib import Path\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n\nENVVAR = \"SCRAPY_SETTINGS_MODULE\"\nDATADIR_CFG_SECTION = \"datadir\"\n\n\ndef inside_project() -> bool:\n    scrapy_module = os.environ.get(ENVVAR)\n    if scrapy_module:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(\n                f\"Cannot import scrapy settings module {scrapy_module}: {exc}\"\n            )\n        else:\n            return True\n    return bool(closest_scrapy_cfg())\n\n\ndef project_data_dir(project: str = \"default\") -> str:\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = Path(cfg.get(DATADIR_CFG_SECTION, project))\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\n                \"Unable to find scrapy.cfg file to infer project data dir\"\n            )\n        d = (Path(scrapy_cfg).parent / \".scrapy\").resolve()\n    if not d.exists():\n        d.mkdir(parents=True)\n    return str(d)\n\n\ndef data_path(path: str | os.PathLike[str], createdir: bool = False) -> str:\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    path_obj = Path(path)\n    if not path_obj.is_absolute():\n        if inside_project():\n            path_obj = Path(project_data_dir(), path)\n        else:\n            path_obj = Path(\".scrapy\", path)\n    if createdir and not path_obj.exists():\n        path_obj.mkdir(parents=True)\n    return str(path_obj)\n\n\ndef get_project_settings() -> Settings:\n    if ENVVAR not in os.environ:\n        project = os.environ.get(\"SCRAPY_PROJECT\", \"default\")\n        init_env(project)\n\n    settings = Settings()\n    settings_module_path = os.environ.get(ENVVAR)\n    if settings_module_path:\n        settings.setmodule(settings_module_path, priority=\"project\")\n\n    valid_envvars = {\n        \"CHECK\",\n        \"PROJECT\",\n        \"PYTHON_SHELL\",\n        \"SETTINGS_MODULE\",\n    }\n\n    scrapy_envvars = {\n        k[7:]: v\n        for k, v in os.environ.items()\n        if k.startswith(\"SCRAPY_\") and k.replace(\"SCRAPY_\", \"\") in valid_envvars\n    }\n\n    settings.setdict(scrapy_envvars, priority=\"project\")\n\n    return settings\n", "n_tokens": 597, "byte_len": 2629, "file_sha1": "4c536b9ccdf6c3e6dab89c4d0674ad9a797e8aa4", "start_line": 1, "end_line": 91}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/decorators.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/utils/decorators.py", "rel_path": "scrapy/utils/decorators.py", "module": "scrapy.utils.decorators", "ext": "py", "chunk_number": 1, "symbols": ["deprecated", "deco", "wrapped", "defers", "inthread", "_warn_spider_arg", "check_args", "sync_inner", "async", "qualname", "emitted", "future", "python", "coroutine", "warn", "spider", "thread", "passed", "removed", "typ", "checking", "make", "passing", "maybe", "deferred", "defer", "none", "callable", "await", "internet", "argument", "typing", "extensions", "functions", "check", "args", "return", "item", "annotations", "bind", "warnings", "function", "kwargs", "being", "sure", "exceptions", "decorator", "this", "inner", "generator"], "ast_kind": "function_or_method", "text": "from __future__ import annotations\n\nimport inspect\nimport warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, TypeVar, overload\n\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom twisted.internet.threads import deferToThread\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Callable, Coroutine\n\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n\n_T = TypeVar(\"_T\")\n\n\ndef deprecated(\n    use_instead: Any = None,\n) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    def deco(func: Callable[_P, _T]) -> Callable[_P, _T]:\n        @wraps(func)\n        def wrapped(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return wrapped\n\n    if callable(use_instead):\n        deco = deco(use_instead)\n        use_instead = None\n    return deco\n\n\ndef defers(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n\n    @wraps(func)\n    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n        return maybeDeferred(func, *a, **kw)\n\n    return wrapped\n\n\ndef inthread(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Decorator to call a function in a thread and return a deferred with the\n    result\n    \"\"\"\n\n    @wraps(func)\n    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n        return deferToThread(func, *a, **kw)\n\n    return wrapped\n\n\n@overload\ndef _warn_spider_arg(\n    func: Callable[_P, Coroutine[Any, Any, _T]],\n) -> Callable[_P, Coroutine[Any, Any, _T]]: ...\n\n\n@overload\ndef _warn_spider_arg(\n    func: Callable[_P, AsyncGenerator[_T]],\n) -> Callable[_P, AsyncGenerator[_T]]: ...\n\n\n@overload\ndef _warn_spider_arg(func: Callable[_P, _T]) -> Callable[_P, _T]: ...\n\n\ndef _warn_spider_arg(\n    func: Callable[_P, _T],\n) -> (\n    Callable[_P, _T]\n    | Callable[_P, Coroutine[Any, Any, _T]]\n    | Callable[_P, AsyncGenerator[_T]]\n):\n    \"\"\"Decorator to warn if a ``spider`` argument is passed to a function.\"\"\"\n\n    def check_args(*args: _P.args, **kwargs: _P.kwargs) -> None:\n        bound = inspect.signature(func).bind(*args, **kwargs)\n        if \"spider\" in bound.arguments:\n            warnings.warn(\n                f\"Passing a 'spider' argument to {func.__qualname__}() is deprecated and \"\n                \"the argument will be removed in a future Scrapy version.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=3,\n            )\n\n    if inspect.iscoroutinefunction(func):\n\n        @wraps(func)\n        async def async_inner(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n            check_args(*args, **kwargs)\n            return await func(*args, **kwargs)\n\n        return async_inner\n\n    if inspect.isasyncgenfunction(func):\n\n        @wraps(func)\n        async def asyncgen_inner(\n            *args: _P.args, **kwargs: _P.kwargs\n        ) -> AsyncGenerator[_T]:\n            check_args(*args, **kwargs)\n            async for item in func(*args, **kwargs):\n                yield item\n\n        return asyncgen_inner\n\n    @wraps(func)\n    def sync_inner(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n        check_args(*args, **kwargs)\n        return func(*args, **kwargs)\n\n    return sync_inner\n", "n_tokens": 962, "byte_len": 3750, "file_sha1": "ef47be0fee25be3f863da2211c64f8acca7e1ac9", "start_line": 1, "end_line": 133}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/statsmailer.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/statsmailer.py", "rel_path": "scrapy/extensions/statsmailer.py", "module": "scrapy.extensions.statsmailer", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_closed", "StatsMailer", "address", "statscollectors", "finishes", "recipients", "internet", "requires", "spider", "typing", "extensions", "signal", "twisted", "python", "return", "email", "name", "annotations", "class", "getlist", "scrapy", "not", "configured", "future", "mail", "typ", "checking", "defer", "classmethod", "closed", "extension", "body", "scraping", "init", "connect", "from", "crawler", "enable", "stats", "collector", "list", "settings", "statsmaile", "rcpts", "assert", "signals", "mailer", "sender"], "ast_kind": "class_or_type", "text": "\"\"\"\nStatsMailer extension sends an email when a spider finishes scraping.\n\nUse STATSMAILER_RCPTS setting to enable and give the recipient mail address\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.mail import MailSender\n\nif TYPE_CHECKING:\n    from twisted.internet.defer import Deferred\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nclass StatsMailer:\n    def __init__(self, stats: StatsCollector, recipients: list[str], mail: MailSender):\n        self.stats: StatsCollector = stats\n        self.recipients: list[str] = recipients\n        self.mail: MailSender = mail\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n        if not recipients:\n            raise NotConfigured\n        mail: MailSender = MailSender.from_crawler(crawler)\n        assert crawler.stats\n        o = cls(crawler.stats, recipients, mail)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_closed(self, spider: Spider) -> Deferred[None] | None:\n        spider_stats = self.stats.get_stats()\n        body = \"Global stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n        body += f\"\\n\\n{spider.name} stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in spider_stats.items())\n        return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)\n", "n_tokens": 402, "byte_len": 1710, "file_sha1": "8844702731480ffb3b25b6256e82e246cc24a89d", "start_line": 1, "end_line": 49}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/telnet.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/telnet.py", "rel_path": "scrapy/extensions/telnet.py", "module": "scrapy.extensions.telnet", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "start_listening", "stop_listening", "protocol", "login", "_get_telnet_vars", "TelnetConsole", "Portal", "bool", "signal", "python", "get", "telnet", "credentials", "interfaces", "more", "future", "typ", "checking", "port", "server", "https", "console", "stop", "listening", "username", "update", "telnetconsol", "enabled", "send", "catch", "settings", "here", "object", "self", "none", "encode", "docs", "html", "print", "live", "note", "transport", "utf", "utf8", "internet", "argument", "manhole", "spider"], "ast_kind": "class_or_type", "text": "\"\"\"\nScrapy Telnet Console extension\n\nSee documentation in docs/topics/telnetconsole.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport binascii\nimport logging\nimport os\nimport pprint\nfrom typing import TYPE_CHECKING, Any\n\nfrom twisted.conch import telnet\nfrom twisted.conch.insults import insults\nfrom twisted.internet import protocol\n\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.decorators import defers\nfrom scrapy.utils.engine import print_engine_status\nfrom scrapy.utils.reactor import listen_tcp\nfrom scrapy.utils.trackref import print_live_refs\n\nif TYPE_CHECKING:\n    from twisted.internet.tcp import Port\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n# signal to update telnet variables\n# args: telnet_vars\nupdate_telnet_vars = object()\n\n\nclass TelnetConsole(protocol.ServerFactory):\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n            raise NotConfigured\n\n        self.crawler: Crawler = crawler\n        self.noisy: bool = False\n        self.portrange: list[int] = [\n            int(x) for x in crawler.settings.getlist(\"TELNETCONSOLE_PORT\")\n        ]\n        self.host: str = crawler.settings[\"TELNETCONSOLE_HOST\"]\n        self.username: str = crawler.settings[\"TELNETCONSOLE_USERNAME\"]\n        self.password: str = crawler.settings[\"TELNETCONSOLE_PASSWORD\"]\n\n        if not self.password:\n            self.password = binascii.hexlify(os.urandom(8)).decode(\"utf8\")\n            logger.info(\"Telnet Password: %s\", self.password)\n\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def start_listening(self) -> None:\n        self.port: Port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\n            \"Telnet console listening on %(host)s:%(port)d\",\n            {\"host\": h.host, \"port\": h.port},\n            extra={\"crawler\": self.crawler},\n        )\n\n    def stop_listening(self) -> None:\n        self.port.stopListening()\n\n    def protocol(self) -> telnet.TelnetTransport:\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n\n            @defers\n            def login(self_, credentials, mind, *interfaces):  # pylint: disable=no-self-argument\n                if not (\n                    credentials.username == self.username.encode(\"utf8\")\n                    and credentials.checkPassword(self.password.encode(\"utf8\"))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                from twisted.conch import manhole\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol, manhole.Manhole, self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())\n\n    def _get_telnet_vars(self) -> dict[str, Any]:\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        assert self.crawler.engine\n        telnet_vars: dict[str, Any] = {\n            \"engine\": self.crawler.engine,\n            \"spider\": self.crawler.engine.spider,\n            \"crawler\": self.crawler,\n            \"extensions\": self.crawler.extensions,\n            \"stats\": self.crawler.stats,\n            \"settings\": self.crawler.settings,\n            \"est\": lambda: print_engine_status(self.crawler.engine),\n            \"p\": pprint.pprint,\n            \"prefs\": print_live_refs,\n            \"help\": \"This is Scrapy telnet console. For more info see: \"\n            \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars\n", "n_tokens": 881, "byte_len": 4070, "file_sha1": "db145ed4f489278276bedc442e6a3eb6bfbfe57f", "start_line": 1, "end_line": 118}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/memusage.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/memusage.py", "rel_path": "scrapy/extensions/memusage.py", "module": "scrapy.extensions.memusage", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "get_virtual_size", "engine_started", "engine_stopped", "update", "_check_limit", "_check_warning", "MemoryUsage", "bool", "darwin", "append", "signal", "python", "memusag", "warnin", "mac", "macos", "import", "module", "linux", "get", "engine", "future", "typ", "checking", "settings", "mail", "sender", "check", "_send_report", "limit", "asyncio", "looping", "none", "stop", "subj", "docs", "reason", "value", "reached", "name", "bot", "memory", "stdlib", "socket", "exceeded", "tasks", "internet", "spider"], "ast_kind": "class_or_type", "text": "\"\"\"\nMemoryUsage extension\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport socket\nimport sys\nfrom importlib import import_module\nfrom pprint import pformat\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.mail import MailSender\nfrom scrapy.utils.asyncio import AsyncioLoopingCall, create_looping_call\nfrom scrapy.utils.defer import _schedule_coro\nfrom scrapy.utils.engine import get_engine_status\n\nif TYPE_CHECKING:\n    from twisted.internet.task import LoopingCall\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MemoryUsage:\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"MEMUSAGE_ENABLED\"):\n            raise NotConfigured\n        try:\n            # stdlib's resource module is only available on unix platforms.\n            self.resource = import_module(\"resource\")\n        except ImportError:\n            raise NotConfigured\n\n        self.crawler: Crawler = crawler\n        self.warned: bool = False\n        self.notify_mails: list[str] = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n        self.limit: int = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n        self.warning: int = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n        self.check_interval: float = crawler.settings.getfloat(\n            \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n        )\n        self.mail: MailSender = MailSender.from_crawler(crawler)\n        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def get_virtual_size(self) -> int:\n        size: int = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != \"darwin\":\n            # on macOS ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size\n\n    def engine_started(self) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n        self.tasks: list[AsyncioLoopingCall | LoopingCall] = []\n        tsk = create_looping_call(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = create_looping_call(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = create_looping_call(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n\n    def engine_stopped(self) -> None:\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()\n\n    def update(self) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.max_value(\"memusage/max\", self.get_virtual_size())\n\n    def _check_limit(self) -> None:\n        assert self.crawler.engine\n        assert self.crawler.stats\n        peak_mem_usage = self.get_virtual_size()\n        if peak_mem_usage > self.limit:\n            self.crawler.stats.set_value(\"memusage/limit_reached\", 1)\n            mem = self.limit / 1024 / 1024\n            logger.error(\n                \"Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...\",\n                {\"memusage\": mem},\n                extra={\"crawler\": self.crawler},\n            )\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n                    f\"memory usage exceeded {mem}MiB at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value(\"memusage/limit_notified\", 1)\n\n            if self.crawler.engine.spider is not None:\n                _schedule_coro(\n                    self.crawler.engine.close_spider_async(reason=\"memusage_exceeded\")\n                )\n            else:\n                _schedule_coro(self.crawler.stop_async())\n        else:\n            logger.info(\n                \"Peak memory usage is %(virtualsize)dMiB\",\n                {\"virtualsize\": peak_mem_usage / 1024 / 1024},\n            )\n\n    def _check_warning(self) -> None:\n        if self.warned:  # warn only once\n            return\n        assert self.crawler.stats\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value(\"memusage/warning_reached\", 1)\n            mem = self.warning / 1024 / 1024\n            logger.warning(\n                \"Memory usage reached %(memusage)dMiB\",\n                {\"memusage\": mem},\n                extra={\"crawler\": self.crawler},\n            )\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} warning: \"\n                    f\"memory usage reached {mem}MiB at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value(\"memusage/warning_notified\", 1)\n            self.warned = True\n", "n_tokens": 1159, "byte_len": 5324, "file_sha1": "cb4b59481adcd3250f39602a45116b2fb3ce441c", "start_line": 1, "end_line": 145}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/memusage.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/memusage.py", "rel_path": "scrapy/extensions/memusage.py", "module": "scrapy.extensions.memusage", "ext": "py", "chunk_number": 2, "symbols": ["_send_report", "memory", "status", "get", "virtual", "useful", "engine", "subject", "startup", "with", "mail", "usage", "some", "maximum", "info", "pformat", "rcpts", "notification", "send", "report", "list", "assert", "memusage", "additional", "none", "current", "crawler", "self", "value", "stats", "__init__", "from_crawler", "get_virtual_size", "engine_started", "engine_stopped", "update", "_check_limit", "_check_warning", "MemoryUsage", "bool", "darwin", "append", "signal", "python", "memusag", "warnin", "mac", "macos", "import", "module"], "ast_kind": "function_or_method", "text": "    def _send_report(self, rcpts: list[str], subject: str) -> None:\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        assert self.crawler.engine\n        assert self.crawler.stats\n        stats = self.crawler.stats\n        s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup') / 1024 / 1024}M\\r\\n\"\n        s += f\"Maximum memory usage          : {stats.get_value('memusage/max') / 1024 / 1024}M\\r\\n\"\n        s += f\"Current memory usage          : {self.get_virtual_size() / 1024 / 1024}M\\r\\n\"\n\n        s += (\n            \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        )\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)\n", "n_tokens": 202, "byte_len": 799, "file_sha1": "cb4b59481adcd3250f39602a45116b2fb3ce441c", "start_line": 146, "end_line": 162}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py", "rel_path": "scrapy/extensions/feedexport.py", "module": "scrapy.extensions.feedexport", "ext": "py", "chunk_number": 1, "symbols": ["build_storage", "__init__", "accepts", "open", "store", "_store_in_thread", "ItemFilter", "IFeedStorage", "FeedStorageProtocol", "BlockingFeedStorage", "StdoutFeedStorage", "failure", "initialize", "bool", "item", "filter", "exported", "python", "lib", "w3lib", "stream", "spider", "storage", "storaget", "deprecated", "passed", "error", "oserror", "future", "typ", "from_crawler", "start_exporting", "_get_exporter", "finish_exporting", "open_spider", "_close_slot", "get_file", "_handle_store_error", "_handle_store_success", "_start_new_batch", "item_scraped", "_load_components", "_exporter_supported", "_settings_are_valid", "_storage_supported", "_get_storage", "_get_uri_params", "_load_filter", "FileFeedStorage", "S3FeedStorage"], "ast_kind": "class_or_type", "text": "\"\"\"\nFeed Exports extension\n\nSee documentation in docs/topics/feed-exports.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport logging\nimport re\nimport sys\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Callable\nfrom datetime import datetime, timezone\nfrom pathlib import Path, PureWindowsPath\nfrom tempfile import NamedTemporaryFile\nfrom typing import IO, TYPE_CHECKING, Any, Optional, Protocol, TypeVar, cast\nfrom urllib.parse import unquote, urlparse\n\nfrom twisted.internet.defer import Deferred, DeferredList, maybeDeferred\nfrom twisted.internet.threads import deferToThread\nfrom w3lib.url import file_uri_to_path\nfrom zope.interface import Interface, implementer\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.extensions.postprocessing import PostProcessingManager\nfrom scrapy.utils.conf import feed_complete_default_values_from_settings\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from _typeshed import OpenBinaryMode\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.exporters import BaseItemExporter\n    from scrapy.settings import BaseSettings, Settings\n\n\nlogger = logging.getLogger(__name__)\n\nUriParamsCallableT = Callable[[dict[str, Any], Spider], Optional[dict[str, Any]]]\n\n_StorageT = TypeVar(\"_StorageT\", bound=\"FeedStorageProtocol\")\n\n\ndef build_storage(\n    builder: Callable[..., _StorageT],\n    uri: str,\n    *args: Any,\n    feed_options: dict[str, Any] | None = None,\n    preargs: Iterable[Any] = (),\n    **kwargs: Any,\n) -> _StorageT:\n    warnings.warn(\n        \"scrapy.extensions.feedexport.build_storage() is deprecated, call the builder directly.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    kwargs[\"feed_options\"] = feed_options\n    return builder(*preargs, uri, *args, **kwargs)\n\n\nclass ItemFilter:\n    \"\"\"\n    This will be used by FeedExporter to decide if an item should be allowed\n    to be exported to a particular feed.\n\n    :param feed_options: feed specific options passed from FeedExporter\n    :type feed_options: dict\n    \"\"\"\n\n    feed_options: dict[str, Any] | None\n    item_classes: tuple[type, ...]\n\n    def __init__(self, feed_options: dict[str, Any] | None) -> None:\n        self.feed_options = feed_options\n        if feed_options is not None:\n            self.item_classes = tuple(\n                load_object(item_class)\n                for item_class in feed_options.get(\"item_classes\") or ()\n            )\n        else:\n            self.item_classes = ()\n\n    def accepts(self, item: Any) -> bool:\n        \"\"\"\n        Return ``True`` if `item` should be exported or ``False`` otherwise.\n\n        :param item: scraped item which user wants to check if is acceptable\n        :type item: :ref:`Scrapy items <topics-items>`\n        :return: `True` if accepted, `False` otherwise\n        :rtype: bool\n        \"\"\"\n        if self.item_classes:\n            return isinstance(item, self.item_classes)\n        return True  # accept all items by default\n\n\nclass IFeedStorage(Interface):\n    \"\"\"Interface that all Feed Storages must implement\"\"\"\n\n    # pylint: disable=no-self-argument\n\n    def __init__(uri, *, feed_options=None):  # pylint: disable=super-init-not-called\n        \"\"\"Initialize the storage with the parameters given in the URI and the\n        feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n    def open(spider):\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(file):\n        \"\"\"Store the given file stream\"\"\"\n\n\nclass FeedStorageProtocol(Protocol):\n    \"\"\"Reimplementation of ``IFeedStorage`` that can be used in type hints.\"\"\"\n\n    def __init__(self, uri: str, *, feed_options: dict[str, Any] | None = None):\n        \"\"\"Initialize the storage with the parameters given in the URI and the\n        feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n        \"\"\"Store the given file stream\"\"\"\n\n\n@implementer(IFeedStorage)\nclass BlockingFeedStorage(ABC):\n    def open(self, spider: Spider) -> IO[bytes]:\n        path = spider.crawler.settings[\"FEED_TEMPDIR\"]\n        if path and not Path(path).is_dir():\n            raise OSError(\"Not a Directory: \" + str(path))\n\n        return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n\n    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n        return deferToThread(self._store_in_thread, file)\n\n    @abstractmethod\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        raise NotImplementedError\n\n\n@implementer(IFeedStorage)\nclass StdoutFeedStorage:", "n_tokens": 1183, "byte_len": 5266, "file_sha1": "30926a3be6391008c5bc61bde30a035db585097e", "start_line": 1, "end_line": 161}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py", "rel_path": "scrapy/extensions/feedexport.py", "module": "scrapy.extensions.feedexport", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "open", "store", "from_crawler", "_store_in_thread", "FileFeedStorage", "S3FeedStorage", "GCSFeedStorage", "does", "library", "your", "fileobj", "spider", "session", "regio", "name", "path", "missing", "username", "settings", "file", "uri", "region", "none", "parents", "bucketname", "extra", "args", "projec", "gcs", "build_storage", "accepts", "start_exporting", "_get_exporter", "finish_exporting", "open_spider", "_close_slot", "get_file", "_handle_store_error", "_handle_store_success", "_start_new_batch", "item_scraped", "_load_components", "_exporter_supported", "_settings_are_valid", "_storage_supported", "_get_storage", "_get_uri_params", "_load_filter", "ItemFilter"], "ast_kind": "class_or_type", "text": "    def __init__(\n        self,\n        uri: str,\n        _stdout: IO[bytes] | None = None,\n        *,\n        feed_options: dict[str, Any] | None = None,\n    ):\n        if not _stdout:\n            _stdout = sys.stdout.buffer\n        self._stdout: IO[bytes] = _stdout\n        if feed_options and feed_options.get(\"overwrite\", False) is True:\n            logger.warning(\n                \"Standard output (stdout) storage does not support \"\n                \"overwriting. To suppress this warning, remove the \"\n                \"overwrite option from your FEEDS setting, or set \"\n                \"it to False.\"\n            )\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        return self._stdout\n\n    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n        pass\n\n\n@implementer(IFeedStorage)\nclass FileFeedStorage:\n    def __init__(self, uri: str, *, feed_options: dict[str, Any] | None = None):\n        self.path: str = file_uri_to_path(uri) if uri.startswith(\"file://\") else uri\n        feed_options = feed_options or {}\n        self.write_mode: OpenBinaryMode = (\n            \"wb\" if feed_options.get(\"overwrite\", False) else \"ab\"\n        )\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        dirname = Path(self.path).parent\n        if dirname and not dirname.exists():\n            dirname.mkdir(parents=True)\n        return Path(self.path).open(self.write_mode)\n\n    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n        file.close()\n        return None\n\n\nclass S3FeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        access_key: str | None = None,\n        secret_key: str | None = None,\n        acl: str | None = None,\n        endpoint_url: str | None = None,\n        *,\n        feed_options: dict[str, Any] | None = None,\n        session_token: str | None = None,\n        region_name: str | None = None,\n    ):\n        try:\n            import boto3.session  # noqa: PLC0415\n        except ImportError:\n            raise NotConfigured(\"missing boto3 library\")\n        u = urlparse(uri)\n        assert u.hostname\n        self.bucketname: str = u.hostname\n        self.access_key: str | None = u.username or access_key\n        self.secret_key: str | None = u.password or secret_key\n        self.session_token: str | None = session_token\n        self.keyname: str = u.path[1:]  # remove first \"/\"\n        self.acl: str | None = acl\n        self.endpoint_url: str | None = endpoint_url\n        self.region_name: str | None = region_name\n\n        boto3_session = boto3.session.Session()\n        self.s3_client = boto3_session.client(\n            \"s3\",\n            aws_access_key_id=self.access_key,\n            aws_secret_access_key=self.secret_key,\n            aws_session_token=self.session_token,\n            endpoint_url=self.endpoint_url,\n            region_name=self.region_name,\n        )\n\n        if feed_options and feed_options.get(\"overwrite\", True) is False:\n            logger.warning(\n                \"S3 does not support appending to files. To \"\n                \"suppress this warning, remove the overwrite \"\n                \"option from your FEEDS setting or set it to True.\"\n            )\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: dict[str, Any] | None = None,\n    ) -> Self:\n        return cls(\n            uri,\n            access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n            secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n            session_token=crawler.settings[\"AWS_SESSION_TOKEN\"],\n            acl=crawler.settings[\"FEED_STORAGE_S3_ACL\"] or None,\n            endpoint_url=crawler.settings[\"AWS_ENDPOINT_URL\"] or None,\n            region_name=crawler.settings[\"AWS_REGION_NAME\"] or None,\n            feed_options=feed_options,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        file.seek(0)\n        kwargs: dict[str, Any] = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n        self.s3_client.upload_fileobj(\n            Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n        )\n        file.close()\n\n\nclass GCSFeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        project_id: str | None,\n        acl: str | None,\n        *,\n        feed_options: dict[str, Any] | None = None,\n    ):\n        self.project_id: str | None = project_id\n        self.acl: str | None = acl\n        u = urlparse(uri)\n        assert u.hostname\n        self.bucket_name: str = u.hostname\n        self.blob_name: str = u.path[1:]  # remove first \"/\"\n\n        if feed_options and feed_options.get(\"overwrite\", True) is False:\n            logger.warning(\n                \"GCS does not support appending to files. To \"\n                \"suppress this warning, remove the overwrite \"\n                \"option from your FEEDS setting or set it to True.\"\n            )\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: dict[str, Any] | None = None,\n    ) -> Self:\n        return cls(\n            uri,\n            crawler.settings[\"GCS_PROJECT_ID\"],\n            crawler.settings[\"FEED_STORAGE_GCS_ACL\"] or None,\n            feed_options=feed_options,\n        )\n", "n_tokens": 1210, "byte_len": 5292, "file_sha1": "30926a3be6391008c5bc61bde30a035db585097e", "start_line": 162, "end_line": 317}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py", "rel_path": "scrapy/extensions/feedexport.py", "module": "scrapy.extensions.feedexport", "ext": "py", "chunk_number": 3, "symbols": ["_store_in_thread", "__init__", "from_crawler", "start_exporting", "_get_exporter", "finish_exporting", "FTPFeedStorage", "FeedSlot", "FeedExporter", "encoding", "bool", "batch", "item", "filter", "export", "fileloaded", "spider", "exporter", "port", "username", "settings", "google", "none", "type", "without", "feed", "exporters", "start", "exporting", "uri", "build_storage", "accepts", "open", "store", "open_spider", "_close_slot", "get_file", "_handle_store_error", "_handle_store_success", "_start_new_batch", "item_scraped", "_load_components", "_exporter_supported", "_settings_are_valid", "_storage_supported", "_get_storage", "_get_uri_params", "_load_filter", "ItemFilter", "IFeedStorage"], "ast_kind": "class_or_type", "text": "    def _store_in_thread(self, file: IO[bytes]) -> None:\n        file.seek(0)\n        from google.cloud.storage import Client  # noqa: PLC0415\n\n        client = Client(project=self.project_id)\n        bucket = client.get_bucket(self.bucket_name)\n        blob = bucket.blob(self.blob_name)\n        blob.upload_from_file(file, predefined_acl=self.acl)\n\n\nclass FTPFeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        use_active_mode: bool = False,\n        *,\n        feed_options: dict[str, Any] | None = None,\n    ):\n        u = urlparse(uri)\n        if not u.hostname:\n            raise ValueError(f\"Got a storage URI without a hostname: {uri}\")\n        self.host: str = u.hostname\n        self.port: int = int(u.port or \"21\")\n        self.username: str = u.username or \"\"\n        self.password: str = unquote(u.password or \"\")\n        self.path: str = u.path\n        self.use_active_mode: bool = use_active_mode\n        self.overwrite: bool = not feed_options or feed_options.get(\"overwrite\", True)\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: dict[str, Any] | None = None,\n    ) -> Self:\n        return cls(\n            uri,\n            use_active_mode=crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n            feed_options=feed_options,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        ftp_store_file(\n            path=self.path,\n            file=file,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.use_active_mode,\n            overwrite=self.overwrite,\n        )\n\n\nclass FeedSlot:\n    def __init__(\n        self,\n        storage: FeedStorageProtocol,\n        uri: str,\n        format: str,  # noqa: A002\n        store_empty: bool,\n        batch_id: int,\n        uri_template: str,\n        filter: ItemFilter,  # noqa: A002\n        feed_options: dict[str, Any],\n        spider: Spider,\n        exporters: dict[str, type[BaseItemExporter]],\n        settings: BaseSettings,\n        crawler: Crawler,\n    ):\n        self.file: IO[bytes] | None = None\n        self.exporter: BaseItemExporter | None = None\n        self.storage: FeedStorageProtocol = storage\n        # feed params\n        self.batch_id: int = batch_id\n        self.format: str = format\n        self.store_empty: bool = store_empty\n        self.uri_template: str = uri_template\n        self.uri: str = uri\n        self.filter: ItemFilter = filter\n        # exporter params\n        self.feed_options: dict[str, Any] = feed_options\n        self.spider: Spider = spider\n        self.exporters: dict[str, type[BaseItemExporter]] = exporters\n        self.settings: BaseSettings = settings\n        self.crawler: Crawler = crawler\n        # flags\n        self.itemcount: int = 0\n        self._exporting: bool = False\n        self._fileloaded: bool = False\n\n    def start_exporting(self) -> None:\n        if not self._fileloaded:\n            self.file = self.storage.open(self.spider)\n            if \"postprocessing\" in self.feed_options:\n                self.file = cast(\n                    \"IO[bytes]\",\n                    PostProcessingManager(\n                        self.feed_options[\"postprocessing\"],\n                        self.file,\n                        self.feed_options,\n                    ),\n                )\n            self.exporter = self._get_exporter(\n                file=self.file,\n                format_=self.feed_options[\"format\"],\n                fields_to_export=self.feed_options[\"fields\"],\n                encoding=self.feed_options[\"encoding\"],\n                indent=self.feed_options[\"indent\"],\n                **self.feed_options[\"item_export_kwargs\"],\n            )\n            self._fileloaded = True\n\n        if not self._exporting:\n            assert self.exporter\n            self.exporter.start_exporting()\n            self._exporting = True\n\n    def _get_exporter(\n        self, file: IO[bytes], format_: str, *args: Any, **kwargs: Any\n    ) -> BaseItemExporter:\n        return build_from_crawler(\n            self.exporters[format_], self.crawler, file, *args, **kwargs\n        )\n\n    def finish_exporting(self) -> None:\n        if self._exporting:\n            assert self.exporter\n            self.exporter.finish_exporting()\n            self._exporting = False\n\n\nclass FeedExporter:\n    _pending_deferreds: list[Deferred[None]] = []\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        exporter = cls(crawler)\n        crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n        crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n        return exporter\n", "n_tokens": 1071, "byte_len": 4868, "file_sha1": "30926a3be6391008c5bc61bde30a035db585097e", "start_line": 318, "end_line": 462}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py", "rel_path": "scrapy/extensions/feedexport.py", "module": "scrapy.extensions.feedexport", "ext": "py", "chunk_number": 4, "symbols": ["__init__", "open_spider", "_close_slot", "get_file", "_handle_store_error", "_handle_store_success", "failure", "takes", "async", "send", "batch", "item", "filter", "append", "storage", "supported", "signal", "inc", "value", "case", "spider", "backward", "error", "deprecated", "more", "elif", "path", "deferreds", "settings", "maybe", "build_storage", "accepts", "open", "store", "_store_in_thread", "from_crawler", "start_exporting", "_get_exporter", "finish_exporting", "_start_new_batch", "item_scraped", "_load_components", "_exporter_supported", "_settings_are_valid", "_storage_supported", "_get_storage", "_get_uri_params", "_load_filter", "ItemFilter", "IFeedStorage"], "ast_kind": "function_or_method", "text": "    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        self.settings: Settings = crawler.settings\n        self.feeds = {}\n        self.slots: list[FeedSlot] = []\n        self.filters: dict[str, ItemFilter] = {}\n\n        if not self.settings[\"FEEDS\"] and not self.settings[\"FEED_URI\"]:\n            raise NotConfigured\n\n        # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings\n        if self.settings[\"FEED_URI\"]:\n            warnings.warn(\n                \"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of \"\n                \"the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            uri = self.settings[\"FEED_URI\"]\n            # handle pathlib.Path objects\n            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()\n            feed_options = {\"format\": self.settings[\"FEED_FORMAT\"]}\n            self.feeds[uri] = feed_complete_default_values_from_settings(\n                feed_options, self.settings\n            )\n            self.filters[uri] = self._load_filter(feed_options)\n        # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n\n        # 'FEEDS' setting takes precedence over 'FEED_URI'\n        for uri, feed_options in self.settings.getdict(\"FEEDS\").items():\n            # handle pathlib.Path objects\n            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()\n            self.feeds[uri] = feed_complete_default_values_from_settings(\n                feed_options, self.settings\n            )\n            self.filters[uri] = self._load_filter(feed_options)\n\n        self.storages: dict[str, type[FeedStorageProtocol]] = self._load_components(\n            \"FEED_STORAGES\"\n        )\n        self.exporters: dict[str, type[BaseItemExporter]] = self._load_components(\n            \"FEED_EXPORTERS\"\n        )\n        for uri, feed_options in self.feeds.items():\n            if not self._storage_supported(uri, feed_options):\n                raise NotConfigured\n            if not self._settings_are_valid():\n                raise NotConfigured\n            if not self._exporter_supported(feed_options[\"format\"]):\n                raise NotConfigured\n\n    def open_spider(self, spider: Spider) -> None:\n        for uri, feed_options in self.feeds.items():\n            uri_params = self._get_uri_params(spider, feed_options[\"uri_params\"])\n            self.slots.append(\n                self._start_new_batch(\n                    batch_id=1,\n                    uri=uri % uri_params,\n                    feed_options=feed_options,\n                    spider=spider,\n                    uri_template=uri,\n                )\n            )\n\n    async def close_spider(self, spider: Spider) -> None:\n        for slot in self.slots:\n            self._close_slot(slot, spider)\n\n        # Await all deferreds\n        if self._pending_deferreds:\n            await maybe_deferred_to_future(DeferredList(self._pending_deferreds))\n\n        # Send FEED_EXPORTER_CLOSED signal\n        await self.crawler.signals.send_catch_log_async(signals.feed_exporter_closed)\n\n    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Deferred[None] | None:\n        def get_file(slot_: FeedSlot) -> IO[bytes]:\n            assert slot_.file\n            if isinstance(slot_.file, PostProcessingManager):\n                slot_.file.close()\n                return slot_.file.file\n            return slot_.file\n\n        if slot.itemcount:\n            # Normal case\n            slot.finish_exporting()\n        elif slot.store_empty and slot.batch_id == 1:\n            # Need to store the empty file\n            slot.start_exporting()\n            slot.finish_exporting()\n        else:\n            # In this case, the file is not stored, so no processing is required.\n            return None\n\n        logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n        d: Deferred[None] = maybeDeferred(slot.storage.store, get_file(slot))  # type: ignore[call-overload]\n\n        d.addCallback(\n            self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n        )\n        d.addErrback(\n            self._handle_store_error, logmsg, spider, type(slot.storage).__name__\n        )\n        self._pending_deferreds.append(d)\n        d.addCallback(\n            lambda _: self.crawler.signals.send_catch_log_deferred(\n                signals.feed_slot_closed, slot=slot\n            )\n        )\n        d.addBoth(lambda _: self._pending_deferreds.remove(d))\n\n        return d\n\n    def _handle_store_error(\n        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n    ) -> None:\n        logger.error(\n            \"Error storing %s\",\n            logmsg,\n            exc_info=failure_to_exc_info(f),\n            extra={\"spider\": spider},\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n\n    def _handle_store_success(\n        self, result: Any, logmsg: str, spider: Spider, slot_type: str\n    ) -> None:\n        logger.info(\"Stored %s\", logmsg, extra={\"spider\": spider})\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n", "n_tokens": 1182, "byte_len": 5339, "file_sha1": "30926a3be6391008c5bc61bde30a035db585097e", "start_line": 463, "end_line": 594}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py#5", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py", "rel_path": "scrapy/extensions/feedexport.py", "module": "scrapy.extensions.feedexport", "ext": "py", "chunk_number": 5, "symbols": ["_start_new_batch", "item_scraped", "_load_components", "_exporter_supported", "_settings_are_valid", "_storage_supported", "_get_storage", "bool", "batch", "setting", "prefix", "append", "storage", "supported", "each", "stream", "spider", "exporter", "doesn", "more", "https", "contextlib", "settings", "items", "unknown", "than", "object", "none", "docs", "html", "build_storage", "__init__", "accepts", "open", "store", "_store_in_thread", "from_crawler", "start_exporting", "_get_exporter", "finish_exporting", "open_spider", "_close_slot", "get_file", "_handle_store_error", "_handle_store_success", "_get_uri_params", "_load_filter", "ItemFilter", "IFeedStorage", "FeedStorageProtocol"], "ast_kind": "function_or_method", "text": "    def _start_new_batch(\n        self,\n        batch_id: int,\n        uri: str,\n        feed_options: dict[str, Any],\n        spider: Spider,\n        uri_template: str,\n    ) -> FeedSlot:\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n        :param batch_id: sequence number of current batch\n        :param uri: uri of the new batch to start\n        :param feed_options: dict with parameters of feed\n        :param spider: user spider\n        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n        \"\"\"\n        storage = self._get_storage(uri, feed_options)\n        return FeedSlot(\n            storage=storage,\n            uri=uri,\n            format=feed_options[\"format\"],\n            store_empty=feed_options[\"store_empty\"],\n            batch_id=batch_id,\n            uri_template=uri_template,\n            filter=self.filters[uri_template],\n            feed_options=feed_options,\n            spider=spider,\n            exporters=self.exporters,\n            settings=self.settings,\n            crawler=self.crawler,\n        )\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        slots = []\n        for slot in self.slots:\n            if not slot.filter.accepts(item):\n                slots.append(\n                    slot\n                )  # if slot doesn't accept item, continue with next slot\n                continue\n\n            slot.start_exporting()\n            assert slot.exporter\n            slot.exporter.export_item(item)\n            slot.itemcount += 1\n            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n            if (\n                self.feeds[slot.uri_template][\"batch_item_count\"]\n                and slot.itemcount >= self.feeds[slot.uri_template][\"batch_item_count\"]\n            ):\n                uri_params = self._get_uri_params(\n                    spider, self.feeds[slot.uri_template][\"uri_params\"], slot\n                )\n                self._close_slot(slot, spider)\n                slots.append(\n                    self._start_new_batch(\n                        batch_id=slot.batch_id + 1,\n                        uri=slot.uri_template % uri_params,\n                        feed_options=self.feeds[slot.uri_template],\n                        spider=spider,\n                        uri_template=slot.uri_template,\n                    )\n                )\n            else:\n                slots.append(slot)\n        self.slots = slots\n\n    def _load_components(self, setting_prefix: str) -> dict[str, Any]:\n        conf = without_none_values(\n            cast(\"dict[str, str]\", self.settings.getwithbase(setting_prefix))\n        )\n        d = {}\n        for k, v in conf.items():\n            with contextlib.suppress(NotConfigured):\n                d[k] = load_object(v)\n        return d\n\n    def _exporter_supported(self, format_: str) -> bool:\n        if format_ in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {\"format\": format_})\n        return False\n\n    def _settings_are_valid(self) -> bool:\n        \"\"\"\n        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n        \"\"\"\n        for uri_template, values in self.feeds.items():\n            if values[\"batch_item_count\"] and not re.search(\n                r\"%\\(batch_time\\)s|%\\(batch_id\\)\", uri_template\n            ):\n                logger.error(\n                    \"%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT \"\n                    \"setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: \"\n                    \"https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count\",\n                    uri_template,\n                )\n                return False\n        return True\n\n    def _storage_supported(self, uri: str, feed_options: dict[str, Any]) -> bool:\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages or PureWindowsPath(uri).drive:\n            try:\n                self._get_storage(uri, feed_options)\n                return True\n            except NotConfigured as e:\n                logger.error(\n                    \"Disabled feed storage scheme: %(scheme)s. Reason: %(reason)s\",\n                    {\"scheme\": scheme, \"reason\": str(e)},\n                )\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n        return False\n\n    def _get_storage(\n        self, uri: str, feed_options: dict[str, Any]\n    ) -> FeedStorageProtocol:\n        \"\"\"Build a storage object for the specified *uri* with the specified\n        *feed_options*.\"\"\"\n        cls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n        return build_from_crawler(cls, self.crawler, uri, feed_options=feed_options)\n", "n_tokens": 1062, "byte_len": 5142, "file_sha1": "30926a3be6391008c5bc61bde30a035db585097e", "start_line": 595, "end_line": 719}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py#6", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/feedexport.py", "rel_path": "scrapy/extensions/feedexport.py", "module": "scrapy.extensions.feedexport", "ext": "py", "chunk_number": 6, "symbols": ["_get_uri_params", "_load_filter", "uri", "params", "feed", "options", "declared", "batch", "item", "filter", "spider", "get", "slot", "utc", "now", "dict", "return", "replace", "lambda", "class", "load", "time", "new", "isoformat", "uripar", "function", "microsecond", "getattr", "none", "type", "build_storage", "__init__", "accepts", "open", "store", "_store_in_thread", "from_crawler", "start_exporting", "_get_exporter", "finish_exporting", "open_spider", "_close_slot", "get_file", "_handle_store_error", "_handle_store_success", "_start_new_batch", "item_scraped", "_load_components", "_exporter_supported", "_settings_are_valid"], "ast_kind": "function_or_method", "text": "    def _get_uri_params(\n        self,\n        spider: Spider,\n        uri_params_function: str | UriParamsCallableT | None,\n        slot: FeedSlot | None = None,\n    ) -> dict[str, Any]:\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        utc_now = datetime.now(tz=timezone.utc)\n        params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n        params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n        params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n        uripar_function: UriParamsCallableT = (\n            load_object(uri_params_function)\n            if uri_params_function\n            else lambda params, _: params\n        )\n        new_params = uripar_function(params, spider)\n        return new_params if new_params is not None else params\n\n    def _load_filter(self, feed_options: dict[str, Any]) -> ItemFilter:\n        # load the item filter if declared else load the default filter class\n        item_filter_class: type[ItemFilter] = load_object(\n            feed_options.get(\"item_filter\", ItemFilter)\n        )\n        return item_filter_class(feed_options)\n", "n_tokens": 273, "byte_len": 1178, "file_sha1": "30926a3be6391008c5bc61bde30a035db585097e", "start_line": 720, "end_line": 747}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/corestats.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/corestats.py", "rel_path": "scrapy/extensions/corestats.py", "module": "scrapy.extensions.corestats", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "spider_closed", "item_scraped", "response_received", "item_dropped", "CoreStats", "set", "value", "statscollectors", "core", "requires", "inc", "signal", "typing", "extensions", "spider", "exception", "finish", "python", "return", "time", "extension", "scraped", "annotations", "elapsed", "class", "item", "name", "stats", "like", "scrapy", "future", "typ", "checking", "classmethod", "closed", "dropped", "times", "init", "connect", "from", "crawler", "base", "collector", "assert", "signals", "items", "collecting"], "ast_kind": "class_or_type", "text": "\"\"\"\nExtension for collecting core stats like items scraped and start/finish times\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy import Spider, signals\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nclass CoreStats:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n        self.start_time: datetime | None = None\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.start_time = datetime.now(tz=timezone.utc)\n        self.stats.set_value(\"start_time\", self.start_time)\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        assert self.start_time is not None\n        finish_time = datetime.now(tz=timezone.utc)\n        elapsed_time = finish_time - self.start_time\n        elapsed_time_seconds = elapsed_time.total_seconds()\n        self.stats.set_value(\"elapsed_time_seconds\", elapsed_time_seconds)\n        self.stats.set_value(\"finish_time\", finish_time)\n        self.stats.set_value(\"finish_reason\", reason)\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        self.stats.inc_value(\"item_scraped_count\")\n\n    def response_received(self, spider: Spider) -> None:\n        self.stats.inc_value(\"response_received_count\")\n\n    def item_dropped(self, item: Any, spider: Spider, exception: BaseException) -> None:\n        reason = exception.__class__.__name__\n        self.stats.inc_value(\"item_dropped_count\")\n        self.stats.inc_value(f\"item_dropped_reasons_count/{reason}\")\n", "n_tokens": 492, "byte_len": 2248, "file_sha1": "61dc5618878923088cc9d9a0e61252198c203883", "start_line": 1, "end_line": 59}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/logstats.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/logstats.py", "rel_path": "scrapy/extensions/logstats.py", "module": "scrapy.extensions.logstats", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "log", "calculate_stats", "spider_closed", "calculate_final_stats", "LogStats", "signal", "python", "spider", "pages", "future", "typ", "checking", "requests", "stats", "collector", "settings", "items", "asyncio", "looping", "none", "stop", "seconds", "irate", "reason", "pagerate", "get", "value", "pagesprev", "internet", "typing", "extensions", "create", "minute", "return", "scraped", "annotations", "class", "not", "configured", "classmethod", "rpm", "final", "from", "crawler", "crawled", "signals", "exceptions"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.asyncio import AsyncioLoopingCall, create_looping_call\n\nif TYPE_CHECKING:\n    from twisted.internet.task import LoopingCall\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass LogStats:\n    \"\"\"Log basic scraping stats periodically like:\n    * RPM - Requests per Minute\n    * IPM - Items per Minute\n    \"\"\"\n\n    def __init__(self, stats: StatsCollector, interval: float = 60.0):\n        self.stats: StatsCollector = stats\n        self.interval: float = interval\n        self.multiplier: float = 60.0 / self.interval\n        self.task: AsyncioLoopingCall | LoopingCall | None = None\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n        if not interval:\n            raise NotConfigured\n        assert crawler.stats\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.pagesprev: int = 0\n        self.itemsprev: int = 0\n\n        self.task = create_looping_call(self.log, spider)\n        self.task.start(self.interval)\n\n    def log(self, spider: Spider) -> None:\n        self.calculate_stats()\n\n        msg = (\n            \"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n            \"scraped %(items)d items (at %(itemrate)d items/min)\"\n        )\n        log_args = {\n            \"pages\": self.pages,\n            \"pagerate\": self.prate,\n            \"items\": self.items,\n            \"itemrate\": self.irate,\n        }\n        logger.info(msg, log_args, extra={\"spider\": spider})\n\n    def calculate_stats(self) -> None:\n        self.items: int = self.stats.get_value(\"item_scraped_count\", 0)\n        self.pages: int = self.stats.get_value(\"response_received_count\", 0)\n        self.irate: float = (self.items - self.itemsprev) * self.multiplier\n        self.prate: float = (self.pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = self.pages, self.items\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        if self.task and self.task.running:\n            self.task.stop()\n\n        rpm_final, ipm_final = self.calculate_final_stats(spider)\n        self.stats.set_value(\"responses_per_minute\", rpm_final)\n        self.stats.set_value(\"items_per_minute\", ipm_final)\n\n    def calculate_final_stats(\n        self, spider: Spider\n    ) -> tuple[None, None] | tuple[float, float]:\n        start_time = self.stats.get_value(\"start_time\")\n        finish_time = self.stats.get_value(\"finish_time\")\n\n        if not start_time or not finish_time:\n            return None, None\n\n        mins_elapsed = (finish_time - start_time).seconds / 60\n\n        if mins_elapsed == 0:\n            return None, None\n\n        items = self.stats.get_value(\"item_scraped_count\", 0)\n        pages = self.stats.get_value(\"response_received_count\", 0)\n\n        return (pages / mins_elapsed), (items / mins_elapsed)\n", "n_tokens": 797, "byte_len": 3415, "file_sha1": "c02ecaed17c201c017d9a1e55bab2541ba7ba824", "start_line": 1, "end_line": 101}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/memdebug.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/memdebug.py", "rel_path": "scrapy/extensions/memdebug.py", "module": "scrapy.extensions.memdebug", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_closed", "MemoryDebugger", "set", "value", "statscollectors", "getbool", "requires", "signal", "typing", "extensions", "memdebu", "enabled", "spider", "garbage", "python", "return", "wdict", "collect", "annotations", "name", "class", "scrapy", "not", "configured", "future", "typ", "checking", "classmethod", "memdebug", "continue", "closed", "extension", "topics", "init", "connect", "count", "documentation", "from", "crawler", "memory", "debugger", "stats", "collector", "settings", "assert", "signals", "items", "exceptions"], "ast_kind": "class_or_type", "text": "\"\"\"\nMemoryDebugger extension\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport gc\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.trackref import live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nclass MemoryDebugger:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"MEMDEBUG_ENABLED\"):\n            raise NotConfigured\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        gc.collect()\n        self.stats.set_value(\"memdebug/gc_garbage_count\", len(gc.garbage))\n        for cls, wdict in live_refs.items():\n            if not wdict:\n                continue\n            self.stats.set_value(f\"memdebug/live_refs/{cls.__name__}\", len(wdict))\n", "n_tokens": 277, "byte_len": 1253, "file_sha1": "fef03696eec58a1b67b310d2f6afaa61574afe3a", "start_line": 1, "end_line": 44}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/closespider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/closespider.py", "rel_path": "scrapy/extensions/closespider.py", "module": "scrapy.extensions.closespider", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "error_count", "page_count", "spider_opened", "item_scraped", "spider_closed", "spider_opened_no_item", "item_scraped_no_item", "_count_items_produced", "CloseSpider", "failure", "errorcount", "call", "later", "item", "scraped", "signal", "were", "pagecount", "since", "python", "after", "spider", "opened", "close", "spiders", "future", "typ", "checking", "_close_spider", "settings", "items", "asyncio", "looping", "none", "stop", "seconds", "period", "docs", "closing", "reason", "error", "http", "count", "closespide", "closespider", "defaultdict", "response", "values"], "ast_kind": "class_or_type", "text": "\"\"\"CloseSpider is an extension that forces spiders to be closed after certain\nconditions are met.\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.asyncio import (\n    AsyncioLoopingCall,\n    CallLaterResult,\n    call_later,\n    create_looping_call,\n)\nfrom scrapy.utils.defer import _schedule_coro\n\nif TYPE_CHECKING:\n    from twisted.internet.task import LoopingCall\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass CloseSpider:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n\n        # for CLOSESPIDER_TIMEOUT\n        self.task: CallLaterResult | None = None\n\n        # for CLOSESPIDER_TIMEOUT_NO_ITEM\n        self.task_no_item: AsyncioLoopingCall | LoopingCall | None = None\n\n        self.close_on: dict[str, Any] = {\n            \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n            \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n            \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n            \"errorcount\": crawler.settings.getint(\"CLOSESPIDER_ERRORCOUNT\"),\n            \"timeout_no_item\": crawler.settings.getint(\"CLOSESPIDER_TIMEOUT_NO_ITEM\"),\n            \"pagecount_no_item\": crawler.settings.getint(\n                \"CLOSESPIDER_PAGECOUNT_NO_ITEM\"\n            ),\n        }\n\n        if not any(self.close_on.values()):\n            raise NotConfigured\n\n        self.counter: defaultdict[str, int] = defaultdict(int)\n\n        if self.close_on.get(\"errorcount\"):\n            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n        if self.close_on.get(\"pagecount\") or self.close_on.get(\"pagecount_no_item\"):\n            crawler.signals.connect(self.page_count, signal=signals.response_received)\n        if self.close_on.get(\"timeout\"):\n            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        if self.close_on.get(\"itemcount\") or self.close_on.get(\"pagecount_no_item\"):\n            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n        if self.close_on.get(\"timeout_no_item\"):\n            self.timeout_no_item: int = self.close_on[\"timeout_no_item\"]\n            self.items_in_period: int = 0\n            crawler.signals.connect(\n                self.spider_opened_no_item, signal=signals.spider_opened\n            )\n            crawler.signals.connect(\n                self.item_scraped_no_item, signal=signals.item_scraped\n            )\n\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def error_count(self, failure: Failure, response: Response, spider: Spider) -> None:\n        self.counter[\"errorcount\"] += 1\n        if self.counter[\"errorcount\"] == self.close_on[\"errorcount\"]:\n            self._close_spider(\"closespider_errorcount\")\n\n    def page_count(self, response: Response, request: Request, spider: Spider) -> None:\n        self.counter[\"pagecount\"] += 1\n        self.counter[\"pagecount_since_last_item\"] += 1\n        if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n            self._close_spider(\"closespider_pagecount\")\n            return\n        if self.close_on[\"pagecount_no_item\"] and (\n            self.counter[\"pagecount_since_last_item\"]\n            >= self.close_on[\"pagecount_no_item\"]\n        ):\n            self._close_spider(\"closespider_pagecount_no_item\")\n\n    def spider_opened(self, spider: Spider) -> None:\n        assert self.crawler.engine\n        self.task = call_later(\n            self.close_on[\"timeout\"], self._close_spider, \"closespider_timeout\"\n        )\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        self.counter[\"itemcount\"] += 1\n        self.counter[\"pagecount_since_last_item\"] = 0\n        if self.counter[\"itemcount\"] == self.close_on[\"itemcount\"]:\n            self._close_spider(\"closespider_itemcount\")\n\n    def spider_closed(self, spider: Spider) -> None:\n        if self.task:\n            self.task.cancel()\n            self.task = None\n\n        if self.task_no_item:\n            if self.task_no_item.running:\n                self.task_no_item.stop()\n            self.task_no_item = None\n\n    def spider_opened_no_item(self, spider: Spider) -> None:\n        self.task_no_item = create_looping_call(self._count_items_produced)\n        self.task_no_item.start(self.timeout_no_item, now=False)\n\n        logger.info(\n            f\"Spider will stop when no items are produced after \"\n            f\"{self.timeout_no_item} seconds.\"\n        )\n\n    def item_scraped_no_item(self, item: Any, spider: Spider) -> None:\n        self.items_in_period += 1\n\n    def _count_items_produced(self) -> None:\n        if self.items_in_period >= 1:\n            self.items_in_period = 0\n        else:\n            logger.info(\n                f\"Closing spider since no items were produced in the last \"\n                f\"{self.timeout_no_item} seconds.\"\n            )\n            self._close_spider(\"closespider_timeout_no_item\")\n", "n_tokens": 1213, "byte_len": 5450, "file_sha1": "8128f4413db9661c1ad5d19e48c601e0310c39e0", "start_line": 1, "end_line": 147}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/closespider.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/closespider.py", "rel_path": "scrapy/extensions/closespider.py", "module": "scrapy.extensions.closespider", "ext": "py", "chunk_number": 2, "symbols": ["_close_spider", "reason", "close", "spider", "engine", "crawler", "assert", "self", "schedule", "coro", "none", "__init__", "from_crawler", "error_count", "page_count", "spider_opened", "item_scraped", "spider_closed", "spider_opened_no_item", "item_scraped_no_item", "_count_items_produced", "CloseSpider", "failure", "errorcount", "call", "later", "item", "scraped", "signal", "were", "pagecount", "since", "python", "after", "opened", "spiders", "future", "typ", "checking", "settings", "items", "asyncio", "looping", "stop", "seconds", "period", "docs", "closing", "error", "http"], "ast_kind": "function_or_method", "text": "    def _close_spider(self, reason: str) -> None:\n        assert self.crawler.engine\n        _schedule_coro(self.crawler.engine.close_spider_async(reason=reason))\n", "n_tokens": 39, "byte_len": 163, "file_sha1": "8128f4413db9661c1ad5d19e48c601e0310c39e0", "start_line": 148, "end_line": 151}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/spiderstate.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/spiderstate.py", "rel_path": "scrapy/extensions/spiderstate.py", "module": "scrapy.extensions.spiderstate", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_closed", "spider_opened", "statefn", "SpiderState", "protocol", "hasattr", "jobdir", "requires", "store", "spider", "typing", "extensions", "signal", "python", "property", "return", "annotations", "class", "load", "with", "attr", "ignore", "defined", "scrapy", "not", "configured", "future", "typ", "checking", "classmethod", "pathlib", "path", "closed", "scraping", "init", "connect", "job", "dir", "open", "from", "crawler", "noqa", "settings", "state", "assert", "signals", "during", "exceptions"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport pickle\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.job import job_dir\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nclass SpiderState:\n    \"\"\"Store and load spider state during a scraping job\"\"\"\n\n    def __init__(self, jobdir: str | None = None):\n        self.jobdir: str | None = jobdir\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj\n\n    def spider_closed(self, spider: Spider) -> None:\n        if self.jobdir:\n            with Path(self.statefn).open(\"wb\") as f:\n                assert hasattr(spider, \"state\")  # set in spider_opened\n                pickle.dump(spider.state, f, protocol=4)\n\n    def spider_opened(self, spider: Spider) -> None:\n        if self.jobdir and Path(self.statefn).exists():\n            with Path(self.statefn).open(\"rb\") as f:\n                spider.state = pickle.load(f)  # type: ignore[attr-defined]  # noqa: S301\n        else:\n            spider.state = {}  # type: ignore[attr-defined]\n\n    @property\n    def statefn(self) -> str:\n        assert self.jobdir\n        return str(Path(self.jobdir, \"spider.state\"))\n", "n_tokens": 383, "byte_len": 1626, "file_sha1": "16064548b5b18a6a42001ab0c6447af03da3e545", "start_line": 1, "end_line": 52}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/debug.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/debug.py", "rel_path": "scrapy/extensions/debug.py", "module": "scrapy.extensions.debug", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "dump_stacktrace", "_thread_stacks", "_enter_debugger", "StackTraceDump", "Debugger", "trace", "dumps", "signal", "python", "traceback", "name", "frame", "type", "future", "typ", "checking", "contextlib", "liverefs", "back", "items", "none", "thread", "join", "docs", "signum", "sigus", "sigusr2", "suppress", "current", "frames", "typing", "extensions", "return", "platforms", "debugger", "annotations", "format", "live", "class", "ignore", "classmethod", "sigquit", "from", "crawler", "stacks", "engine", "signals", "enumerate"], "ast_kind": "class_or_type", "text": "\"\"\"\nExtensions for debugging Scrapy\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport logging\nimport signal\nimport sys\nimport threading\nimport traceback\nfrom pdb import Pdb\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.utils.engine import format_engine_status\nfrom scrapy.utils.trackref import format_live_refs\n\nif TYPE_CHECKING:\n    from types import FrameType\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass StackTraceDump:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        try:\n            signal.signal(signal.SIGUSR2, self.dump_stacktrace)  # type: ignore[attr-defined]\n            signal.signal(signal.SIGQUIT, self.dump_stacktrace)  # type: ignore[attr-defined]\n        except AttributeError:\n            # win32 platforms don't support SIGUSR signals\n            pass\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def dump_stacktrace(self, signum: int, frame: FrameType | None) -> None:\n        assert self.crawler.engine\n        log_args = {\n            \"stackdumps\": self._thread_stacks(),\n            \"enginestatus\": format_engine_status(self.crawler.engine),\n            \"liverefs\": format_live_refs(),\n        }\n        logger.info(\n            \"Dumping stack trace and engine status\\n\"\n            \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n            log_args,\n            extra={\"crawler\": self.crawler},\n        )\n\n    def _thread_stacks(self) -> str:\n        id2name = {th.ident: th.name for th in threading.enumerate()}\n        dumps = \"\"\n        for id_, frame in sys._current_frames().items():\n            name = id2name.get(id_, \"\")\n            dump = \"\".join(traceback.format_stack(frame))\n            dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n        return dumps\n\n\nclass Debugger:\n    def __init__(self) -> None:\n        # win32 platforms don't support SIGUSR signals\n        with contextlib.suppress(AttributeError):\n            signal.signal(signal.SIGUSR2, self._enter_debugger)  # type: ignore[attr-defined]\n\n    def _enter_debugger(self, signum: int, frame: FrameType | None) -> None:\n        assert frame\n        Pdb().set_trace(frame.f_back)\n", "n_tokens": 544, "byte_len": 2367, "file_sha1": "11807ac9f1ad6be4baa44d43c9fccfb14185747e", "start_line": 1, "end_line": 80}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/postprocessing.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/postprocessing.py", "rel_path": "scrapy/extensions/postprocessing.py", "module": "scrapy.extensions.postprocessing", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "write", "close", "tell", "writable", "_load_plugins", "_get_head_plugin", "GzipPlugin", "Bz2Plugin", "LZMAPlugin", "PostProcessingManager", "gzip", "compresslevel", "processed", "bool", "gzipfile", "instance", "accepted", "exported", "lzma", "check", "file", "bz2file", "extension", "about", "target", "bz2", "passed", "more", "https", "plugin", "text", "wrapper", "process", "here", "written", "object", "older", "none", "get", "head", "type", "misc", "exporters", "could", "format", "number", "rtype", "typing", "return"], "ast_kind": "class_or_type", "text": "\"\"\"\nExtension for processing data before they are exported to feeds.\n\"\"\"\n\nfrom bz2 import BZ2File\nfrom gzip import GzipFile\nfrom io import IOBase\nfrom lzma import LZMAFile\nfrom typing import IO, Any, BinaryIO, cast\n\nfrom scrapy.utils.misc import load_object\n\n\nclass GzipPlugin:\n    \"\"\"\n    Compresses received data using `gzip <https://en.wikipedia.org/wiki/Gzip>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `gzip_compresslevel`\n    - `gzip_mtime`\n    - `gzip_filename`\n\n    See :py:class:`gzip.GzipFile` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n        compress_level = self.feed_options.get(\"gzip_compresslevel\", 9)\n        mtime = self.feed_options.get(\"gzip_mtime\")\n        filename = self.feed_options.get(\"gzip_filename\")\n        self.gzipfile = GzipFile(\n            fileobj=self.file,\n            mode=\"wb\",\n            compresslevel=compress_level,\n            mtime=mtime,\n            filename=filename,\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.gzipfile.write(data)\n\n    def close(self) -> None:\n        self.gzipfile.close()\n\n\nclass Bz2Plugin:\n    \"\"\"\n    Compresses received data using `bz2 <https://en.wikipedia.org/wiki/Bzip2>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `bz2_compresslevel`\n\n    See :py:class:`bz2.BZ2File` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n        compress_level = self.feed_options.get(\"bz2_compresslevel\", 9)\n        self.bz2file = BZ2File(\n            filename=self.file, mode=\"wb\", compresslevel=compress_level\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.bz2file.write(data)\n\n    def close(self) -> None:\n        self.bz2file.close()\n\n\nclass LZMAPlugin:\n    \"\"\"\n    Compresses received data using `lzma <https://en.wikipedia.org/wiki/Lempel–Ziv–Markov_chain_algorithm>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `lzma_format`\n    - `lzma_check`\n    - `lzma_preset`\n    - `lzma_filters`\n\n    .. note::\n        ``lzma_filters`` cannot be used in pypy version 7.3.1 and older.\n\n    See :py:class:`lzma.LZMAFile` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n\n        format_ = self.feed_options.get(\"lzma_format\")\n        check = self.feed_options.get(\"lzma_check\", -1)\n        preset = self.feed_options.get(\"lzma_preset\")\n        filters = self.feed_options.get(\"lzma_filters\")\n        self.lzmafile = LZMAFile(\n            filename=self.file,\n            mode=\"wb\",\n            format=format_,\n            check=check,\n            preset=preset,\n            filters=filters,\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.lzmafile.write(data)\n\n    def close(self) -> None:\n        self.lzmafile.close()\n\n\n# io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n# instance as a file like writable object. This could be needed by some exporters\n# such as CsvItemExporter which wraps the feed storage with io.TextIOWrapper.\nclass PostProcessingManager(IOBase):\n    \"\"\"\n    This will manage and use declared plugins to process data in a\n    pipeline-ish way.\n    :param plugins: all the declared plugins for the feed\n    :type plugins: list\n    :param file: final target file where the processed data will be written\n    :type file: file like object\n    \"\"\"\n\n    def __init__(\n        self, plugins: list[Any], file: IO[bytes], feed_options: dict[str, Any]\n    ) -> None:\n        self.plugins = self._load_plugins(plugins)\n        self.file = file\n        self.feed_options = feed_options\n        self.head_plugin = self._get_head_plugin()\n\n    def write(self, data: bytes) -> int:\n        \"\"\"\n        Uses all the declared plugins to process data first, then writes\n        the processed data to target file.\n        :param data: data passed to be written to target file\n        :type data: bytes\n        :return: returns number of bytes written\n        :rtype: int\n        \"\"\"\n        return cast(\"int\", self.head_plugin.write(data))\n\n    def tell(self) -> int:\n        return self.file.tell()\n\n    def close(self) -> None:\n        \"\"\"\n        Close the target file along with all the plugins.\n        \"\"\"\n        self.head_plugin.close()\n\n    def writable(self) -> bool:\n        return True\n\n    def _load_plugins(self, plugins: list[Any]) -> list[Any]:\n        return [load_object(plugin) for plugin in plugins]\n\n    def _get_head_plugin(self) -> Any:\n        prev = self.file\n        for plugin in self.plugins[::-1]:\n            prev = plugin(prev, self.feed_options)\n        return prev\n", "n_tokens": 1175, "byte_len": 4904, "file_sha1": "184e386ebec4d9a7f9ab0a17ed8a7314cef806c4", "start_line": 1, "end_line": 167}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/periodic_log.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/periodic_log.py", "rel_path": "scrapy/extensions/periodic_log.py", "module": "scrapy.extensions.periodic_log", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "spider_opened", "log", "log_delta", "log_timing", "log_crawler_stats", "param_allowed", "spider_closed", "PeriodicLog", "stats", "bool", "signal", "python", "serialize", "spider", "enabled", "future", "typ", "checking", "periodi", "collector", "settings", "delta", "prev", "num", "items", "isinstance", "asyncio", "looping", "crawler", "ext", "none", "encode", "stop", "timing", "reason", "timezone", "param", "allowed", "internet", "scrapy", "json", "typing", "extensions", "create", "time", "getdict", "return", "update"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.asyncio import AsyncioLoopingCall, create_looping_call\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\nif TYPE_CHECKING:\n    from json import JSONEncoder\n\n    from twisted.internet.task import LoopingCall\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.statscollectors import StatsCollector\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass PeriodicLog:\n    \"\"\"Log basic scraping stats periodically\"\"\"\n\n    def __init__(\n        self,\n        stats: StatsCollector,\n        interval: float = 60.0,\n        ext_stats: dict[str, Any] = {},\n        ext_delta: dict[str, Any] = {},\n        ext_timing_enabled: bool = False,\n    ):\n        self.stats: StatsCollector = stats\n        self.interval: float = interval\n        self.multiplier: float = 60.0 / self.interval\n        self.task: AsyncioLoopingCall | LoopingCall | None = None\n        self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n        self.ext_stats_enabled: bool = bool(ext_stats)\n        self.ext_stats_include: list[str] = ext_stats.get(\"include\", [])\n        self.ext_stats_exclude: list[str] = ext_stats.get(\"exclude\", [])\n        self.ext_delta_enabled: bool = bool(ext_delta)\n        self.ext_delta_include: list[str] = ext_delta.get(\"include\", [])\n        self.ext_delta_exclude: list[str] = ext_delta.get(\"exclude\", [])\n        self.ext_timing_enabled: bool = ext_timing_enabled\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n        if not interval:\n            raise NotConfigured\n        try:\n            ext_stats: dict[str, Any] | None = crawler.settings.getdict(\n                \"PERIODIC_LOG_STATS\"\n            )\n        except (TypeError, ValueError):\n            ext_stats = (\n                {\"enabled\": True}\n                if crawler.settings.getbool(\"PERIODIC_LOG_STATS\")\n                else None\n            )\n        try:\n            ext_delta: dict[str, Any] | None = crawler.settings.getdict(\n                \"PERIODIC_LOG_DELTA\"\n            )\n        except (TypeError, ValueError):\n            ext_delta = (\n                {\"enabled\": True}\n                if crawler.settings.getbool(\"PERIODIC_LOG_DELTA\")\n                else None\n            )\n\n        ext_timing_enabled: bool = crawler.settings.getbool(\n            \"PERIODIC_LOG_TIMING_ENABLED\"\n        )\n        if not (ext_stats or ext_delta or ext_timing_enabled):\n            raise NotConfigured\n        assert crawler.stats\n        assert ext_stats is not None\n        assert ext_delta is not None\n        o = cls(\n            crawler.stats,\n            interval,\n            ext_stats,\n            ext_delta,\n            ext_timing_enabled,\n        )\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.time_prev: datetime = datetime.now(tz=timezone.utc)\n        self.delta_prev: dict[str, int | float] = {}\n        self.stats_prev: dict[str, int | float] = {}\n\n        self.task = create_looping_call(self.log)\n        self.task.start(self.interval)\n\n    def log(self) -> None:\n        data: dict[str, Any] = {}\n        if self.ext_timing_enabled:\n            data.update(self.log_timing())\n        if self.ext_delta_enabled:\n            data.update(self.log_delta())\n        if self.ext_stats_enabled:\n            data.update(self.log_crawler_stats())\n        logger.info(self.encoder.encode(data))\n\n    def log_delta(self) -> dict[str, Any]:\n        num_stats: dict[str, int | float] = {\n            k: v\n            for k, v in self.stats._stats.items()\n            if isinstance(v, (int, float))\n            and self.param_allowed(k, self.ext_delta_include, self.ext_delta_exclude)\n        }\n        delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}\n        self.delta_prev = num_stats\n        return {\"delta\": delta}\n\n    def log_timing(self) -> dict[str, Any]:\n        now = datetime.now(tz=timezone.utc)\n        time = {\n            \"log_interval\": self.interval,\n            \"start_time\": self.stats._stats[\"start_time\"],\n            \"utcnow\": now,\n            \"log_interval_real\": (now - self.time_prev).total_seconds(),\n            \"elapsed\": (now - self.stats._stats[\"start_time\"]).total_seconds(),\n        }\n        self.time_prev = now\n        return {\"time\": time}\n\n    def log_crawler_stats(self) -> dict[str, Any]:\n        stats = {\n            k: v\n            for k, v in self.stats._stats.items()\n            if self.param_allowed(k, self.ext_stats_include, self.ext_stats_exclude)\n        }\n        return {\"stats\": stats}\n\n    def param_allowed(\n        self, stat_name: str, include: list[str], exclude: list[str]\n    ) -> bool:\n        if not include and not exclude:\n            return True\n        for p in exclude:\n            if p in stat_name:\n                return False\n        if exclude and not include:\n            return True\n        return any(p in stat_name for p in include)\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        self.log()\n        if self.task and self.task.running:\n            self.task.stop()\n", "n_tokens": 1225, "byte_len": 5576, "file_sha1": "26beb01e1e83db0682534ef2e4c9c109f614c617", "start_line": 1, "end_line": 161}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/throttle.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/throttle.py", "rel_path": "scrapy/extensions/throttle.py", "module": "scrapy.extensions.throttle", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "_spider_opened", "_min_delay", "_max_delay", "_start_delay", "_response_downloaded", "_get_slot", "_adjust_delay", "AutoThrottle", "processed", "autothrottl", "star", "bool", "parallel", "signal", "adjustment", "python", "each", "latency", "spider", "small", "target", "make", "pages", "mean", "concurrency", "future", "typ", "checking", "max", "delay", "start", "debug", "settings", "maxdelay", "targe", "than", "none", "seconds", "server", "type", "delaydiff", "http", "slot", "response", "download", "policy", "requests", "typing"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import NotConfigured\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.core.downloader import Slot\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoThrottle:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        if not crawler.settings.getbool(\"AUTOTHROTTLE_ENABLED\"):\n            raise NotConfigured\n\n        self.debug: bool = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n        self.target_concurrency: float = crawler.settings.getfloat(\n            \"AUTOTHROTTLE_TARGET_CONCURRENCY\"\n        )\n        if self.target_concurrency <= 0.0:\n            raise NotConfigured(\n                f\"AUTOTHROTTLE_TARGET_CONCURRENCY \"\n                f\"({self.target_concurrency!r}) must be higher than 0.\"\n            )\n        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(\n            self._response_downloaded, signal=signals.response_downloaded\n        )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def _spider_opened(self, spider: Spider) -> None:\n        self.mindelay = self._min_delay(spider)\n        self.maxdelay = self._max_delay(spider)\n        spider.download_delay = self._start_delay(spider)  # type: ignore[attr-defined]\n\n    def _min_delay(self, spider: Spider) -> float:\n        s = self.crawler.settings\n        return getattr(spider, \"download_delay\", s.getfloat(\"DOWNLOAD_DELAY\"))\n\n    def _max_delay(self, spider: Spider) -> float:\n        return self.crawler.settings.getfloat(\"AUTOTHROTTLE_MAX_DELAY\")\n\n    def _start_delay(self, spider: Spider) -> float:\n        return max(\n            self.mindelay, self.crawler.settings.getfloat(\"AUTOTHROTTLE_START_DELAY\")\n        )\n\n    def _response_downloaded(\n        self, response: Response, request: Request, spider: Spider\n    ) -> None:\n        key, slot = self._get_slot(request, spider)\n        latency = request.meta.get(\"download_latency\")\n        if (\n            latency is None\n            or slot is None\n            or request.meta.get(\"autothrottle_dont_adjust_delay\", False) is True\n        ):\n            return\n\n        olddelay = slot.delay\n        self._adjust_delay(slot, latency, response)\n        if self.debug:\n            diff = slot.delay - olddelay\n            size = len(response.body)\n            conc = len(slot.transferring)\n            logger.info(\n                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                {\n                    \"slot\": key,\n                    \"concurrency\": conc,\n                    \"delay\": slot.delay * 1000,\n                    \"delaydiff\": diff * 1000,\n                    \"latency\": latency * 1000,\n                    \"size\": size,\n                },\n                extra={\"spider\": spider},\n            )\n\n    def _get_slot(\n        self, request: Request, spider: Spider\n    ) -> tuple[str | None, Slot | None]:\n        key: str | None = request.meta.get(\"download_slot\")\n        if key is None:\n            return None, None\n        assert self.crawler.engine\n        return key, self.crawler.engine.downloader.slots.get(key)\n\n    def _adjust_delay(self, slot: Slot, latency: float, response: Response) -> None:\n        \"\"\"Define delay adjustment policy\"\"\"\n\n        # If a server needs `latency` seconds to respond then\n        # we should send a request each `latency/N` seconds\n        # to have N requests processed in parallel\n        target_delay = latency / self.target_concurrency\n\n        # Adjust the delay to make it closer to target_delay\n        new_delay = (slot.delay + target_delay) / 2.0\n\n        # If target delay is bigger than old delay, then use it instead of mean.\n        # It works better with problematic sites.\n        new_delay = max(target_delay, new_delay)\n\n        # Make sure self.mindelay <= new_delay <= self.max_delay\n        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n        # Dont adjust delay if response status != 200 and new delay is smaller\n        # than old one, as error pages (and redirections) are usually small and\n        # so tend to reduce latency, thus provoking a positive feedback by\n        # reducing delay instead of increase.\n        if response.status != 200 and new_delay <= slot.delay:\n            return\n\n        slot.delay = new_delay\n", "n_tokens": 1079, "byte_len": 4732, "file_sha1": "c2e0d32d30473b2513ca7c09de35a47409034787", "start_line": 1, "end_line": 130}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py", "rel_path": "scrapy/extensions/httpcache.py", "module": "scrapy.extensions.httpcache", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "should_cache_request", "should_cache_response", "is_cached_response_fresh", "is_cached_response_valid", "_parse_cachecontrol", "DummyPolicy", "RFC2616Policy", "does", "sense", "bool", "rfc", "rfc2616", "python", "lib", "w3lib", "parse", "cachecontrol", "spider", "ignore", "http", "responses", "import", "module", "should", "cache", "spiders", "future", "typ", "checking", "_set_conditional_validators", "_get_max_age", "_compute_freshness_lifetime", "_compute_current_age", "open_spider", "close_spider", "retrieve_response", "store_response", "_read_data", "_get_request_path", "_read_meta", "parse_cachecontrol", "rfc1123_to_epoch", "DbmCacheStorage", "FilesystemCacheStorage", "method", "connection", "load", "from", "good"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport gzip\nimport logging\nimport pickle\nfrom email.utils import mktime_tz, parsedate_tz\nfrom importlib import import_module\nfrom pathlib import Path\nfrom time import time\nfrom typing import IO, TYPE_CHECKING, Any, cast\nfrom weakref import WeakKeyDictionary\n\nfrom w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.project import data_path\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    import os\n    from collections.abc import Callable\n    from types import ModuleType\n\n    # typing.Concatenate requires Python 3.10\n    from typing_extensions import Concatenate\n\n    from scrapy.http.request import Request\n    from scrapy.settings import BaseSettings\n    from scrapy.spiders import Spider\n    from scrapy.utils.request import RequestFingerprinterProtocol\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DummyPolicy:\n    def __init__(self, settings: BaseSettings):\n        self.ignore_schemes: list[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n        self.ignore_http_codes: list[int] = [\n            int(x) for x in settings.getlist(\"HTTPCACHE_IGNORE_HTTP_CODES\")\n        ]\n\n    def should_cache_request(self, request: Request) -> bool:\n        return urlparse_cached(request).scheme not in self.ignore_schemes\n\n    def should_cache_response(self, response: Response, request: Request) -> bool:\n        return response.status not in self.ignore_http_codes\n\n    def is_cached_response_fresh(\n        self, cachedresponse: Response, request: Request\n    ) -> bool:\n        return True\n\n    def is_cached_response_valid(\n        self, cachedresponse: Response, response: Response, request: Request\n    ) -> bool:\n        return True\n\n\nclass RFC2616Policy:\n    MAXAGE = 3600 * 24 * 365  # one year\n\n    def __init__(self, settings: BaseSettings):\n        self.always_store: bool = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n        self.ignore_schemes: list[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n        self._cc_parsed: WeakKeyDictionary[\n            Request | Response, dict[bytes, bytes | None]\n        ] = WeakKeyDictionary()\n        self.ignore_response_cache_controls: list[bytes] = [\n            to_bytes(cc)\n            for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n        ]\n\n    def _parse_cachecontrol(self, r: Request | Response) -> dict[bytes, bytes | None]:\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b\"Cache-Control\", b\"\")\n            assert cch is not None\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]\n\n    def should_cache_request(self, request: Request) -> bool:\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        return b\"no-store\" not in cc\n\n    def should_cache_response(self, response: Response, request: Request) -> bool:\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b\"no-store\" in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        if response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        if self.always_store:\n            return True\n        # Any hint on response expiration is good\n        if b\"max-age\" in cc or b\"Expires\" in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        if response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        if response.status in (200, 203, 401):\n            return b\"Last-Modified\" in response.headers or b\"ETag\" in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        return False\n", "n_tokens": 1049, "byte_len": 4678, "file_sha1": "d2b9af3155dd93e00954ea417d796f5790d9e1b3", "start_line": 1, "end_line": 121}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py", "rel_path": "scrapy/extensions/httpcache.py", "module": "scrapy.extensions.httpcache", "ext": "py", "chunk_number": 2, "symbols": ["is_cached_response_fresh", "is_cached_response_valid", "_set_conditional_validators", "_get_max_age", "_compute_freshness_lifetime", "_compute_current_age", "__init__", "DbmCacheStorage", "parse", "synthesize", "protocol", "bool", "get", "max", "cachecontrol", "freshnesslifetime", "connection", "didn", "says", "fallback", "import", "module", "more", "from", "https", "date", "cachedir", "compute", "freshness", "settings", "should_cache_request", "should_cache_response", "_parse_cachecontrol", "open_spider", "close_spider", "retrieve_response", "store_response", "_read_data", "_get_request_path", "_read_meta", "parse_cachecontrol", "rfc1123_to_epoch", "DummyPolicy", "RFC2616Policy", "FilesystemCacheStorage", "method", "lib", "w3lib", "spider", "load"], "ast_kind": "class_or_type", "text": "    def is_cached_response_fresh(\n        self, cachedresponse: Response, request: Request\n    ) -> bool:\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b\"no-cache\" in cc or b\"no-cache\" in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(\n            cachedresponse, request, now\n        )\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b\"max-stale\" in ccreq and b\"must-revalidate\" not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b\"max-stale\"]\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False\n\n    def is_cached_response_valid(\n        self, cachedresponse: Response, response: Response, request: Request\n    ) -> bool:\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b\"must-revalidate\" not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304\n\n    def _set_conditional_validators(\n        self, request: Request, cachedresponse: Response\n    ) -> None:\n        if b\"Last-Modified\" in cachedresponse.headers:\n            request.headers[b\"If-Modified-Since\"] = cachedresponse.headers[\n                b\"Last-Modified\"\n            ]\n\n        if b\"ETag\" in cachedresponse.headers:\n            request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n\n    def _get_max_age(self, cc: dict[bytes, bytes | None]) -> int | None:\n        try:\n            return max(0, int(cc[b\"max-age\"]))  # type: ignore[arg-type]\n        except (KeyError, ValueError):\n            return None\n\n    def _compute_freshness_lifetime(\n        self, response: Response, request: Request, now: float\n    ) -> float:\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n\n        # Try HTTP/1.0 Expires header\n        if b\"Expires\" in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b\"Expires\"])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b\"Last-Modified\"))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute freshness lifetime\n        return 0\n\n    def _compute_current_age(\n        self, response: Response, request: Request, now: float\n    ) -> float:\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage: float = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n        if now > date:\n            currentage = now - date\n\n        if b\"Age\" in response.headers:\n            try:\n                age = int(response.headers[b\"Age\"])  # type: ignore[arg-type]\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage\n\n\nclass DbmCacheStorage:\n    def __init__(self, settings: BaseSettings):\n        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"], createdir=True)\n        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n        self.dbmodule: ModuleType = import_module(settings[\"HTTPCACHE_DBM_MODULE\"])\n        self.db: Any = None  # the real type is private\n", "n_tokens": 1245, "byte_len": 5512, "file_sha1": "d2b9af3155dd93e00954ea417d796f5790d9e1b3", "start_line": 122, "end_line": 256}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py", "rel_path": "scrapy/extensions/httpcache.py", "module": "scrapy.extensions.httpcache", "ext": "py", "chunk_number": 3, "symbols": ["open_spider", "close_spider", "retrieve_response", "store_response", "_read_data", "__init__", "_get_request_path", "FilesystemCacheStorage", "dumps", "method", "protocol", "rawheaders", "use", "gzip", "loads", "bool", "pickled", "meta", "read", "data", "spider", "name", "https", "path", "filesystem", "cache", "cachedir", "debug", "settings", "fingerprint", "should_cache_request", "should_cache_response", "is_cached_response_fresh", "is_cached_response_valid", "_parse_cachecontrol", "_set_conditional_validators", "_get_max_age", "_compute_freshness_lifetime", "_compute_current_age", "_read_meta", "parse_cachecontrol", "rfc1123_to_epoch", "DummyPolicy", "RFC2616Policy", "DbmCacheStorage", "parse", "cachecontrol", "lib", "w3lib", "connection"], "ast_kind": "class_or_type", "text": "    def open_spider(self, spider: Spider) -> None:\n        dbpath = Path(self.cachedir, f\"{spider.name}.db\")\n        self.db = self.dbmodule.open(str(dbpath), \"c\")\n\n        logger.debug(\n            \"Using DBM cache storage in %(cachepath)s\",\n            {\"cachepath\": dbpath},\n            extra={\"spider\": spider},\n        )\n\n        assert spider.crawler.request_fingerprinter\n        self._fingerprinter: RequestFingerprinterProtocol = (\n            spider.crawler.request_fingerprinter\n        )\n\n    def close_spider(self, spider: Spider) -> None:\n        self.db.close()\n\n    def retrieve_response(self, spider: Spider, request: Request) -> Response | None:\n        data = self._read_data(spider, request)\n        if data is None:\n            return None  # not cached\n        url = data[\"url\"]\n        status = data[\"status\"]\n        headers = Headers(data[\"headers\"])\n        body = data[\"body\"]\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        return respcls(url=url, headers=headers, status=status, body=body)\n\n    def store_response(\n        self, spider: Spider, request: Request, response: Response\n    ) -> None:\n        key = self._fingerprinter.fingerprint(request).hex()\n        data = {\n            \"status\": response.status,\n            \"url\": response.url,\n            \"headers\": dict(response.headers),\n            \"body\": response.body,\n        }\n        self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n        self.db[f\"{key}_time\"] = str(time())\n\n    def _read_data(self, spider: Spider, request: Request) -> dict[str, Any] | None:\n        key = self._fingerprinter.fingerprint(request).hex()\n        db = self.db\n        tkey = f\"{key}_time\"\n        if tkey not in db:\n            return None  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return None  # expired\n\n        return cast(\"dict[str, Any]\", pickle.loads(db[f\"{key}_data\"]))  # noqa: S301\n\n\nclass FilesystemCacheStorage:\n    def __init__(self, settings: BaseSettings):\n        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"])\n        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n        self.use_gzip: bool = settings.getbool(\"HTTPCACHE_GZIP\")\n        # https://github.com/python/mypy/issues/10740\n        self._open: Callable[Concatenate[str | os.PathLike, str, ...], IO[bytes]] = (\n            gzip.open if self.use_gzip else open  # type: ignore[assignment]\n        )\n\n    def open_spider(self, spider: Spider) -> None:\n        logger.debug(\n            \"Using filesystem cache storage in %(cachedir)s\",\n            {\"cachedir\": self.cachedir},\n            extra={\"spider\": spider},\n        )\n\n        assert spider.crawler.request_fingerprinter\n        self._fingerprinter = spider.crawler.request_fingerprinter\n\n    def close_spider(self, spider: Spider) -> None:\n        pass\n\n    def retrieve_response(self, spider: Spider, request: Request) -> Response | None:\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return None  # not cached\n        rpath = Path(self._get_request_path(spider, request))\n        with self._open(rpath / \"response_body\", \"rb\") as f:\n            body = f.read()\n        with self._open(rpath / \"response_headers\", \"rb\") as f:\n            rawheaders = f.read()\n        url = metadata[\"response_url\"]\n        status = metadata[\"status\"]\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        return respcls(url=url, headers=headers, status=status, body=body)\n\n    def store_response(\n        self, spider: Spider, request: Request, response: Response\n    ) -> None:\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = Path(self._get_request_path(spider, request))\n        if not rpath.exists():\n            rpath.mkdir(parents=True)\n        metadata = {\n            \"url\": request.url,\n            \"method\": request.method,\n            \"status\": response.status,\n            \"response_url\": response.url,\n            \"timestamp\": time(),\n        }\n        with self._open(rpath / \"meta\", \"wb\") as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(rpath / \"pickled_meta\", \"wb\") as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(rpath / \"response_headers\", \"wb\") as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(rpath / \"response_body\", \"wb\") as f:\n            f.write(response.body)\n        with self._open(rpath / \"request_headers\", \"wb\") as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(rpath / \"request_body\", \"wb\") as f:\n            f.write(request.body)\n\n    def _get_request_path(self, spider: Spider, request: Request) -> str:\n        key = self._fingerprinter.fingerprint(request).hex()\n        return str(Path(self.cachedir, spider.name, key[0:2], key))\n", "n_tokens": 1206, "byte_len": 5073, "file_sha1": "d2b9af3155dd93e00954ea417d796f5790d9e1b3", "start_line": 257, "end_line": 382}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py#4", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/extensions/httpcache.py", "rel_path": "scrapy/extensions/httpcache.py", "module": "scrapy.extensions.httpcache", "ext": "py", "chunk_number": 4, "symbols": ["_read_meta", "parse_cachecontrol", "rfc1123_to_epoch", "encoding", "parse", "partition", "stat", "except", "rfc", "rfc2616", "pickled", "meta", "spider", "mtime", "cast", "directive", "return", "dict", "date", "str", "load", "with", "time", "ignore", "cache", "https", "directives", "path", "cachecontrol", "exception", "__init__", "should_cache_request", "should_cache_response", "is_cached_response_fresh", "is_cached_response_valid", "_parse_cachecontrol", "_set_conditional_validators", "_get_max_age", "_compute_freshness_lifetime", "_compute_current_age", "open_spider", "close_spider", "retrieve_response", "store_response", "_read_data", "_get_request_path", "DummyPolicy", "RFC2616Policy", "DbmCacheStorage", "FilesystemCacheStorage"], "ast_kind": "function_or_method", "text": "    def _read_meta(self, spider: Spider, request: Request) -> dict[str, Any] | None:\n        rpath = Path(self._get_request_path(spider, request))\n        metapath = rpath / \"pickled_meta\"\n        if not metapath.exists():\n            return None  # not found\n        mtime = metapath.stat().st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return None  # expired\n        with self._open(metapath, \"rb\") as f:\n            return cast(\"dict[str, Any]\", pickle.load(f))  # noqa: S301\n\n\ndef parse_cachecontrol(header: bytes) -> dict[bytes, bytes | None]:\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b\",\"):\n        key, sep, val = directive.strip().partition(b\"=\")\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives\n\n\ndef rfc1123_to_epoch(date_str: str | bytes | None) -> int | None:\n    try:\n        date_str = to_unicode(date_str, encoding=\"ascii\")  # type: ignore[arg-type]\n        return mktime_tz(parsedate_tz(date_str))  # type: ignore[arg-type]\n    except Exception:\n        return None\n", "n_tokens": 364, "byte_len": 1391, "file_sha1": "d2b9af3155dd93e00954ea417d796f5790d9e1b3", "start_line": 383, "end_line": 421}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/loader/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/loader/__init__.py", "rel_path": "scrapy/loader/__init__.py", "module": "scrapy.loader.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "ItemLoader", "method", "those", "subclasses", "populate", "processor", "instance", "case", "extraction", "selector", "accessible", "pages", "attempting", "future", "typ", "checking", "extract", "default", "input", "items", "processors", "object", "created", "item", "none", "docs", "type", "http", "either", "automatically", "response", "abstraction", "instantiate", "overridden", "argument", "typing", "scraped", "annotations", "loader", "class", "ignored", "update", "output", "when", "fields", "applying", "currently", "being", "through"], "ast_kind": "class_or_type", "text": "\"\"\"\nItem Loader\n\nSee documentation in docs/topics/loaders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nimport itemloaders\n\nfrom scrapy.item import Item\nfrom scrapy.selector import Selector\n\nif TYPE_CHECKING:\n    from scrapy.http import TextResponse\n\n\nclass ItemLoader(itemloaders.ItemLoader):\n    \"\"\"\n    A user-friendly abstraction to populate an :ref:`item <topics-items>` with data\n    by applying :ref:`field processors <topics-loaders-processors>` to scraped data.\n    When instantiated with a ``selector`` or a ``response`` it supports\n    data extraction from web pages using :ref:`selectors <topics-selectors>`.\n\n    :param item: The item instance to populate using subsequent calls to\n        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n        or :meth:`~ItemLoader.add_value`.\n    :type item: scrapy.item.Item\n\n    :param selector: The selector to extract data from, when using the\n        :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or\n        :meth:`replace_css` method.\n    :type selector: :class:`~scrapy.Selector` object\n\n    :param response: The response used to construct the selector using the\n        :attr:`default_selector_class`, unless the selector argument is given,\n        in which case this argument is ignored.\n    :type response: :class:`~scrapy.http.Response` object\n\n    If no item is given, one is instantiated automatically using the class in\n    :attr:`default_item_class`.\n\n    The item, selector, response and remaining keyword arguments are\n    assigned to the Loader context (accessible through the :attr:`context` attribute).\n\n    .. attribute:: item\n\n        The item object being parsed by this Item Loader.\n        This is mostly used as a property so, when attempting to override this\n        value, you may want to check out :attr:`default_item_class` first.\n\n    .. attribute:: context\n\n        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n\n    .. attribute:: default_item_class\n\n        An :ref:`item <topics-items>` class (or factory), used to instantiate\n        items when not given in the ``__init__`` method.\n\n    .. attribute:: default_input_processor\n\n        The default input processor to use for those fields which don't specify\n        one.\n\n    .. attribute:: default_output_processor\n\n        The default output processor to use for those fields which don't specify\n        one.\n\n    .. attribute:: default_selector_class\n\n        The class used to construct the :attr:`selector` of this\n        :class:`ItemLoader`, if only a response is given in the ``__init__`` method.\n        If a selector is given in the ``__init__`` method this attribute is ignored.\n        This attribute is sometimes overridden in subclasses.\n\n    .. attribute:: selector\n\n        The :class:`~scrapy.Selector` object to extract data from.\n        It's either the selector given in the ``__init__`` method or one created from\n        the response given in the ``__init__`` method using the\n        :attr:`default_selector_class`. This attribute is meant to be\n        read-only.\n    \"\"\"\n\n    default_item_class: type = Item\n    default_selector_class = Selector\n\n    def __init__(\n        self,\n        item: Any = None,\n        selector: Selector | None = None,\n        response: TextResponse | None = None,\n        parent: itemloaders.ItemLoader | None = None,\n        **context: Any,\n    ):\n        if selector is None and response is not None:\n            try:\n                selector = self.default_selector_class(response)\n            except AttributeError:\n                selector = None\n        context.update(response=response)\n        super().__init__(item=item, selector=selector, parent=parent, **context)\n", "n_tokens": 810, "byte_len": 3757, "file_sha1": "f4eb9d6bc1ec48af10e68895db14a4e0e0c38206", "start_line": 1, "end_line": 107}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/cookies.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/cookies.py", "rel_path": "scrapy/http/cookies.py", "module": "scrapy.http.cookies", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "extract_cookies", "add_cookie_header", "_cookies", "clear_session_cookies", "clear", "__iter__", "__len__", "set_policy", "make_cookies", "set_cookie", "set_cookie_if_ok", "potential_domain_matches", "acquire", "release", "get_full_url", "get_host", "get_type", "CookieJar", "_DummyLock", "WrappedRequest", "while", "processed", "library", "potential", "cookie", "jar", "cookies", "append", "netloc", "is_unverifiable", "full_url", "host", "type", "unverifiable", "origin_req_host", "has_header", "get_header", "header_items", "add_unredirected_header", "info", "get_all", "WrappedResponse", "bool", "check", "expired", "python", "automatic", "name", "domain"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport re\nimport time\nfrom http.cookiejar import Cookie, CookiePolicy, DefaultCookiePolicy\nfrom http.cookiejar import CookieJar as _CookieJar\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator, Sequence\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request\n    from scrapy.http import Response\n\n\n# Defined in the http.cookiejar module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\nIPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n\n\nclass CookieJar:\n    def __init__(\n        self,\n        policy: CookiePolicy | None = None,\n        check_expired_frequency: int = 10000,\n    ):\n        self.policy: CookiePolicy = policy or DefaultCookiePolicy()\n        self.jar: _CookieJar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()  # type: ignore[attr-defined]\n        self.check_expired_frequency: int = check_expired_frequency\n        self.processed: int = 0\n\n    def extract_cookies(self, response: Response, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        self.jar.extract_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def add_cookie_header(self, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())  # type: ignore[attr-defined]\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if \".\" not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:  # type: ignore[attr-defined]\n                cookies += self.jar._cookies_for_domain(host, wreq)  # type: ignore[attr-defined]\n\n        attrs = self.jar._cookie_attrs(cookies)  # type: ignore[attr-defined]\n        if attrs and not wreq.has_header(\"Cookie\"):\n            wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()\n\n    @property\n    def _cookies(self) -> dict[str, dict[str, dict[str, Cookie]]]:\n        return self.jar._cookies  # type: ignore[attr-defined,no-any-return]\n\n    def clear_session_cookies(self) -> None:\n        return self.jar.clear_session_cookies()\n\n    def clear(\n        self,\n        domain: str | None = None,\n        path: str | None = None,\n        name: str | None = None,\n    ) -> None:\n        self.jar.clear(domain, path, name)\n\n    def __iter__(self) -> Iterator[Cookie]:\n        return iter(self.jar)\n\n    def __len__(self) -> int:\n        return len(self.jar)\n\n    def set_policy(self, pol: CookiePolicy) -> None:\n        self.jar.set_policy(pol)\n\n    def make_cookies(self, response: Response, request: Request) -> Sequence[Cookie]:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def set_cookie(self, cookie: Cookie) -> None:\n        self.jar.set_cookie(cookie)\n\n    def set_cookie_if_ok(self, cookie: Cookie, request: Request) -> None:\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))  # type: ignore[arg-type]\n\n\ndef potential_domain_matches(domain: str) -> list[str]:\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index(\".\") + 1\n        end = domain.rindex(\".\")\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index(\".\", start) + 1\n    except ValueError:\n        pass\n    return matches + [\".\" + d for d in matches]\n\n\nclass _DummyLock:\n    def acquire(self) -> None:\n        pass\n\n    def release(self) -> None:\n        pass\n\n\nclass WrappedRequest:\n    \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n\n    see http://docs.python.org/library/urllib2.html#urllib2.Request\n    \"\"\"\n\n    def __init__(self, request: Request):\n        self.request = request\n\n    def get_full_url(self) -> str:\n        return self.request.url\n\n    def get_host(self) -> str:\n        return urlparse_cached(self.request).netloc\n\n    def get_type(self) -> str:\n        return urlparse_cached(self.request).scheme\n", "n_tokens": 1141, "byte_len": 5003, "file_sha1": "0976ac0cc9472a5df0630db5c17400958c9caf83", "start_line": 1, "end_line": 155}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/cookies.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/cookies.py", "rel_path": "scrapy/http/cookies.py", "module": "scrapy.http.cookies", "ext": "py", "chunk_number": 2, "symbols": ["is_unverifiable", "full_url", "host", "type", "unverifiable", "origin_req_host", "has_header", "get_header", "header_items", "add_unredirected_header", "__init__", "info", "get_all", "WrappedResponse", "appendlist", "get", "header", "user", "approve", "image", "bool", "false", "add", "unredirected", "origin", "req", "all", "cast", "errors", "automatic", "extract_cookies", "add_cookie_header", "_cookies", "clear_session_cookies", "clear", "__iter__", "__len__", "set_policy", "make_cookies", "set_cookie", "set_cookie_if_ok", "potential_domain_matches", "acquire", "release", "get_full_url", "get_host", "get_type", "CookieJar", "_DummyLock", "WrappedRequest"], "ast_kind": "class_or_type", "text": "    def is_unverifiable(self) -> bool:\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return cast(\"bool\", self.request.meta.get(\"is_unverifiable\", False))\n\n    @property\n    def full_url(self) -> str:\n        return self.get_full_url()\n\n    @property\n    def host(self) -> str:\n        return self.get_host()\n\n    @property\n    def type(self) -> str:\n        return self.get_type()\n\n    @property\n    def unverifiable(self) -> bool:\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self) -> str:\n        return cast(\"str\", urlparse_cached(self.request).hostname)\n\n    def has_header(self, name: str) -> bool:\n        return name in self.request.headers\n\n    def get_header(self, name: str, default: str | None = None) -> str | None:\n        value = self.request.headers.get(name, default)\n        return to_unicode(value, errors=\"replace\") if value is not None else None\n\n    def header_items(self) -> list[tuple[str, list[str]]]:\n        return [\n            (\n                to_unicode(k, errors=\"replace\"),\n                [to_unicode(x, errors=\"replace\") for x in v],\n            )\n            for k, v in self.request.headers.items()\n        ]\n\n    def add_unredirected_header(self, name: str, value: str) -> None:\n        self.request.headers.appendlist(name, value)\n\n\nclass WrappedResponse:\n    def __init__(self, response: Response):\n        self.response = response\n\n    def info(self) -> Self:\n        return self\n\n    def get_all(self, name: str, default: Any = None) -> list[str]:\n        return [\n            to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n        ]\n", "n_tokens": 465, "byte_len": 2014, "file_sha1": "0976ac0cc9472a5df0630db5c17400958c9caf83", "start_line": 156, "end_line": 217}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/__init__.py", "rel_path": "scrapy/http/__init__.py", "module": "scrapy.http.__init__", "ext": "py", "chunk_number": 1, "symbols": ["importing", "text", "response", "containing", "module", "http", "related", "classes", "instead", "form", "request", "json", "scrapy", "more", "headers", "all", "from", "xml", "ones", "when", "html", "specific", "outside", "import", "rpc", "this"], "ast_kind": "imports", "text": "\"\"\"\nModule containing all HTTP related classes\n\nUse this module (instead of the more specific ones) when importing Headers,\nRequest and Response outside this module.\n\"\"\"\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.http.request import Request\nfrom scrapy.http.request.form import FormRequest\nfrom scrapy.http.request.json_request import JsonRequest\nfrom scrapy.http.request.rpc import XmlRpcRequest\nfrom scrapy.http.response import Response\nfrom scrapy.http.response.html import HtmlResponse\nfrom scrapy.http.response.json import JsonResponse\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.http.response.xml import XmlResponse\n\n__all__ = [\n    \"FormRequest\",\n    \"Headers\",\n    \"HtmlResponse\",\n    \"JsonRequest\",\n    \"JsonResponse\",\n    \"Request\",\n    \"Response\",\n    \"TextResponse\",\n    \"XmlResponse\",\n    \"XmlRpcRequest\",\n]\n", "n_tokens": 169, "byte_len": 850, "file_sha1": "533fa810f8a1243604598dc86ba08f1ccdde8e08", "start_line": 1, "end_line": 31}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/headers.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/headers.py", "rel_path": "scrapy/http/headers.py", "module": "scrapy.http.headers", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "update", "normkey", "normvalue", "_tobytes", "__getitem__", "get", "getlist", "setlist", "setlistdefault", "appendlist", "items", "values", "to_string", "to_unicode_dict", "__copy__", "Headers", "encoding", "default", "list", "unsupported", "python", "lib", "w3lib", "caseless", "dict", "unicode", "elif", "future", "typ", "checking", "https", "pull", "compatible", "string", "getitem", "isinstance", "case", "insensitive", "index", "error", "none", "encode", "join", "type", "http", "either", "iter", "any", "str"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, Any, AnyStr, Union, cast\n\nfrom w3lib.http import headers_dict_to_raw\n\nfrom scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_RawValueT = Union[bytes, str, int]\n\n\n# isn't fully compatible typing-wise with either dict or CaselessDict,\n# but it needs refactoring anyway, see also https://github.com/scrapy/scrapy/pull/5146\nclass Headers(CaselessDict):\n    \"\"\"Case insensitive http headers dictionary\"\"\"\n\n    def __init__(\n        self,\n        seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        encoding: str = \"utf-8\",\n    ):\n        self.encoding: str = encoding\n        super().__init__(seq)\n\n    def update(  # type: ignore[override]\n        self, seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]]\n    ) -> None:\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq: dict[bytes, list[bytes]] = {}\n        for k, v in seq:\n            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n        super().update(iseq)\n\n    def normkey(self, key: AnyStr) -> bytes:  # type: ignore[override]\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())\n\n    def normvalue(self, value: _RawValueT | Iterable[_RawValueT]) -> list[bytes]:\n        \"\"\"Normalize values to bytes\"\"\"\n        _value: Iterable[_RawValueT]\n        if value is None:\n            _value = []\n        elif isinstance(value, (str, bytes)):\n            _value = [value]\n        elif hasattr(value, \"__iter__\"):\n            _value = value\n        else:\n            _value = [value]\n\n        return [self._tobytes(x) for x in _value]\n\n    def _tobytes(self, x: _RawValueT) -> bytes:\n        if isinstance(x, bytes):\n            return x\n        if isinstance(x, str):\n            return x.encode(self.encoding)\n        if isinstance(x, int):\n            return str(x).encode(self.encoding)\n        raise TypeError(f\"Unsupported value type: {type(x)}\")\n\n    def __getitem__(self, key: AnyStr) -> bytes | None:\n        try:\n            return cast(\"list[bytes]\", super().__getitem__(key))[-1]\n        except IndexError:\n            return None\n\n    def get(self, key: AnyStr, def_val: Any = None) -> bytes | None:\n        try:\n            return cast(\"list[bytes]\", super().get(key, def_val))[-1]\n        except IndexError:\n            return None\n\n    def getlist(self, key: AnyStr, def_val: Any = None) -> list[bytes]:\n        try:\n            return cast(\"list[bytes]\", super().__getitem__(key))\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []\n\n    def setlist(self, key: AnyStr, list_: Iterable[_RawValueT]) -> None:\n        self[key] = list_\n\n    def setlistdefault(\n        self, key: AnyStr, default_list: Iterable[_RawValueT] = ()\n    ) -> Any:\n        return self.setdefault(key, default_list)\n\n    def appendlist(self, key: AnyStr, value: Iterable[_RawValueT]) -> None:\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst\n\n    def items(self) -> Iterable[tuple[bytes, list[bytes]]]:  # type: ignore[override]\n        return ((k, self.getlist(k)) for k in self.keys())\n\n    def values(self) -> list[bytes | None]:  # type: ignore[override]\n        return [\n            self[k]\n            for k in self.keys()  # pylint: disable=consider-using-dict-items\n        ]\n\n    def to_string(self) -> bytes:\n        return headers_dict_to_raw(self)\n\n    def to_unicode_dict(self) -> CaseInsensitiveDict:\n        \"\"\"Return headers as a CaseInsensitiveDict with str keys\n        and str values. Multiple values are joined with ','.\n        \"\"\"\n        return CaseInsensitiveDict(\n            (\n                to_unicode(key, encoding=self.encoding),\n                to_unicode(b\",\".join(value), encoding=self.encoding),\n            )\n            for key, value in self.items()\n        )\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n", "n_tokens": 1025, "byte_len": 4258, "file_sha1": "d66bd5746b8fcf25361067dbbdc30e057f859eb9", "start_line": 1, "end_line": 131}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/html.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/html.py", "rel_path": "scrapy/http/response/html.py", "module": "scrapy.http.response.html", "ext": "py", "chunk_number": 1, "symbols": ["HtmlResponse", "encoding", "text", "response", "pass", "module", "declarations", "class", "scrapy", "adds", "topics", "documentation", "from", "implements", "discovering", "through", "html", "which", "docs", "import", "http", "this", "request"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the HtmlResponse class which adds encoding\ndiscovering through HTML encoding declarations to the TextResponse class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass HtmlResponse(TextResponse):\n    pass\n", "n_tokens": 53, "byte_len": 300, "file_sha1": "af3c96a7ff60f9e3edc33f25668ce6aa52d63209", "start_line": 1, "end_line": 13}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/xml.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/xml.py", "rel_path": "scrapy/http/response/xml.py", "module": "scrapy.http.response.xml", "ext": "py", "chunk_number": 1, "symbols": ["XmlResponse", "encoding", "text", "response", "pass", "module", "declarations", "class", "scrapy", "topics", "adds", "documentation", "from", "implements", "xml", "discovering", "through", "which", "docs", "import", "http", "this", "request"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the XmlResponse class which adds encoding\ndiscovering through XML encoding declarations to the TextResponse class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass XmlResponse(TextResponse):\n    pass\n", "n_tokens": 53, "byte_len": 297, "file_sha1": "43a31204c51f45443810b148ed191ef0b20a3edf", "start_line": 1, "end_line": 13}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/__init__.py", "rel_path": "scrapy/http/response/__init__.py", "module": "scrapy.http.response.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "cb_kwargs", "meta", "url", "_set_url", "body", "_set_body", "__repr__", "copy", "replace", "urljoin", "text", "css", "jmespath", "Response", "failure", "method", "protocol", "those", "subclasses", "containing", "python", "shortcut", "currently", "possible", "name", "responses", "future", "typ", "checking", "xpath", "follow", "follow_all", "encoding", "bool", "cookies", "instance", "dont", "filter", "selector", "list", "elif", "isinstance", "join", "all", "object", "none", "parameter", "represent", "docs"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the Response class which is used to represent HTTP\nresponses in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, AnyStr, TypeVar, overload\nfrom urllib.parse import urljoin\n\nfrom scrapy.exceptions import NotSupported\nfrom scrapy.http.headers import Headers\nfrom scrapy.http.request import Request\nfrom scrapy.link import Link\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable, Mapping\n    from ipaddress import IPv4Address, IPv6Address\n\n    from twisted.internet.ssl import Certificate\n    from twisted.python.failure import Failure\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.http.request import CallbackT, CookiesT\n    from scrapy.selector import SelectorList\n\n\nResponseTypeVar = TypeVar(\"ResponseTypeVar\", bound=\"Response\")\n\n\nclass Response(object_ref):\n    \"\"\"An object that represents an HTTP response, which is usually\n    downloaded (by the Downloader) and fed to the Spiders for processing.\n    \"\"\"\n\n    attributes: tuple[str, ...] = (\n        \"url\",\n        \"status\",\n        \"headers\",\n        \"body\",\n        \"flags\",\n        \"request\",\n        \"certificate\",\n        \"ip_address\",\n        \"protocol\",\n    )\n    \"\"\"A tuple of :class:`str` objects containing the name of all public\n    attributes of the class that are also keyword parameters of the\n    ``__init__()`` method.\n\n    Currently used by :meth:`Response.replace`.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        status: int = 200,\n        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        body: bytes = b\"\",\n        flags: list[str] | None = None,\n        request: Request | None = None,\n        certificate: Certificate | None = None,\n        ip_address: IPv4Address | IPv6Address | None = None,\n        protocol: str | None = None,\n    ):\n        self.headers: Headers = Headers(headers or {})\n        self.status: int = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request: Request | None = request\n        self.flags: list[str] = [] if flags is None else list(flags)\n        self.certificate: Certificate | None = certificate\n        self.ip_address: IPv4Address | IPv6Address | None = ip_address\n        self.protocol: str | None = protocol\n\n    @property\n    def cb_kwargs(self) -> dict[str, Any]:\n        try:\n            return self.request.cb_kwargs  # type: ignore[union-attr]\n        except AttributeError:\n            raise AttributeError(\n                \"Response.cb_kwargs not available, this response \"\n                \"is not tied to any request\"\n            )\n\n    @property\n    def meta(self) -> dict[str, Any]:\n        try:\n            return self.request.meta  # type: ignore[union-attr]\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response is not tied to any request\"\n            )\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    def _set_url(self, url: str) -> None:\n        if isinstance(url, str):\n            self._url: str = url\n        else:\n            raise TypeError(\n                f\"{type(self).__name__} url must be str, got {type(url).__name__}\"\n            )\n\n    @property\n    def body(self) -> bytes:\n        return self._body\n\n    def _set_body(self, body: bytes | None) -> None:\n        if body is None:\n            self._body = b\"\"\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\"\n            )\n        else:\n            self._body = body\n\n    def __repr__(self) -> str:\n        return f\"<{self.status} {self.url}>\"\n\n    def copy(self) -> Self:\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()\n\n    @overload\n    def replace(\n        self, *args: Any, cls: type[ResponseTypeVar], **kwargs: Any\n    ) -> ResponseTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: type[Response] | None = None, **kwargs: Any\n    ) -> Response:\n        \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n        for x in self.attributes:\n            kwargs.setdefault(x, getattr(self, x))\n        if cls is None:\n            cls = self.__class__\n        return cls(*args, **kwargs)\n\n    def urljoin(self, url: str) -> str:\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)\n\n    @property\n    def text(self) -> str:\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as str\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")\n\n    def css(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def jmespath(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n", "n_tokens": 1237, "byte_len": 5533, "file_sha1": "f5b2825999c639856bd95b28d42d9e1b7af79e58", "start_line": 1, "end_line": 176}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/__init__.py", "rel_path": "scrapy/http/response/__init__.py", "module": "scrapy.http.response.__init__", "ext": "py", "chunk_number": 2, "symbols": ["xpath", "follow", "follow_all", "encoding", "failure", "method", "subclasses", "bool", "cookies", "instance", "shortcut", "responses", "dont", "filter", "selector", "list", "elif", "isinstance", "all", "object", "none", "parameter", "callable", "response", "any", "str", "iter", "not", "supported", "addition", "__init__", "cb_kwargs", "meta", "url", "_set_url", "body", "_set_body", "__repr__", "copy", "replace", "urljoin", "text", "css", "jmespath", "Response", "protocol", "those", "containing", "python", "currently"], "ast_kind": "function_or_method", "text": "    def xpath(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def follow(\n        self,\n        url: str | Link,\n        callback: CallbackT | None = None,\n        method: str = \"GET\",\n        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        body: bytes | str | None = None,\n        cookies: CookiesT | None = None,\n        meta: dict[str, Any] | None = None,\n        encoding: str | None = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Callable[[Failure], Any] | None = None,\n        cb_kwargs: dict[str, Any] | None = None,\n        flags: list[str] | None = None,\n    ) -> Request:\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__()`` method,\n        but ``url`` can be a relative URL or a :class:`~scrapy.link.Link` object,\n        not only an absolute URL.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n\n        .. versionadded:: 2.0\n           The *flags* parameter.\n        \"\"\"\n        if encoding is None:\n            raise ValueError(\"encoding can't be None\")\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n\n        return Request(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n    def follow_all(\n        self,\n        urls: Iterable[str | Link],\n        callback: CallbackT | None = None,\n        method: str = \"GET\",\n        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        body: bytes | str | None = None,\n        cookies: CookiesT | None = None,\n        meta: dict[str, Any] | None = None,\n        encoding: str | None = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Callable[[Failure], Any] | None = None,\n        cb_kwargs: dict[str, Any] | None = None,\n        flags: list[str] | None = None,\n    ) -> Iterable[Request]:\n        \"\"\"\n        .. versionadded:: 2.0\n\n        Return an iterable of :class:`~.Request` instances to follow all links\n        in ``urls``. It accepts the same arguments as ``Request.__init__()`` method,\n        but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n        not only absolute URLs.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if not hasattr(urls, \"__iter__\"):\n            raise TypeError(\"'urls' argument must be an iterable\")\n        return (\n            self.follow(\n                url=url,\n                callback=callback,\n                method=method,\n                headers=headers,\n                body=body,\n                cookies=cookies,\n                meta=meta,\n                encoding=encoding,\n                priority=priority,\n                dont_filter=dont_filter,\n                errback=errback,\n                cb_kwargs=cb_kwargs,\n                flags=flags,\n            )\n            for url in urls\n        )\n", "n_tokens": 852, "byte_len": 3809, "file_sha1": "f5b2825999c639856bd95b28d42d9e1b7af79e58", "start_line": 177, "end_line": 284}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/text.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/text.py", "rel_path": "scrapy/http/response/text.py", "module": "scrapy.http.response.text", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_set_body", "encoding", "_declared_encoding", "json", "text", "urljoin", "_headers_encoding", "_body_inferred_encoding", "_auto_detect_fun", "_body_declared_encoding", "_bom_encoding", "selector", "jmespath", "xpath", "css", "TextResponse", "failure", "body", "loads", "html", "python", "lib", "w3lib", "possible", "cp1252", "future", "typ", "checking", "list", "follow", "follow_all", "_url_from_selector", "_InvalidSelector", "method", "does", "bool", "url", "from", "unsupported", "append", "cookies", "generator", "instance", "each", "extraction", "dont", "filter", "passing", "elif"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the TextResponse class which adds encoding handling and\ndiscovering (through HTTP headers) to base Response class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom contextlib import suppress\nfrom typing import TYPE_CHECKING, Any, AnyStr, cast\nfrom urllib.parse import urljoin\n\nimport parsel\nfrom w3lib.encoding import (\n    html_body_declared_encoding,\n    html_to_unicode,\n    http_content_type_encoding,\n    read_bom,\n    resolve_encoding,\n)\nfrom w3lib.html import strip_html5_whitespace\n\nfrom scrapy.http.response import Response\nfrom scrapy.utils.python import memoizemethod_noargs, to_unicode\nfrom scrapy.utils.response import get_base_url\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable, Mapping\n\n    from twisted.python.failure import Failure\n\n    from scrapy.http.request import CallbackT, CookiesT, Request\n    from scrapy.link import Link\n    from scrapy.selector import Selector, SelectorList\n\n\n_NONE = object()\n\n\nclass TextResponse(Response):\n    _DEFAULT_ENCODING = \"ascii\"\n    _cached_decoded_json = _NONE\n\n    attributes: tuple[str, ...] = (*Response.attributes, \"encoding\")\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        self._encoding: str | None = kwargs.pop(\"encoding\", None)\n        self._cached_benc: str | None = None\n        self._cached_ubody: str | None = None\n        self._cached_selector: Selector | None = None\n        super().__init__(*args, **kwargs)\n\n    def _set_body(self, body: str | bytes | None) -> None:\n        self._body: bytes = b\"\"  # used by encoding detection\n        if isinstance(body, str):\n            if self._encoding is None:\n                raise TypeError(\n                    \"Cannot convert unicode body - \"\n                    f\"{type(self).__name__} has no encoding\"\n                )\n            self._body = body.encode(self._encoding)\n        else:\n            super()._set_body(body)\n\n    @property\n    def encoding(self) -> str:\n        return self._declared_encoding() or self._body_inferred_encoding()\n\n    def _declared_encoding(self) -> str | None:\n        return (\n            self._encoding\n            or self._bom_encoding()\n            or self._headers_encoding()\n            or self._body_declared_encoding()\n        )\n\n    def json(self) -> Any:\n        \"\"\"\n        .. versionadded:: 2.2\n\n        Deserialize a JSON document to a Python object.\n        \"\"\"\n        if self._cached_decoded_json is _NONE:\n            self._cached_decoded_json = json.loads(self.body)\n        return self._cached_decoded_json\n\n    @property\n    def text(self) -> str:\n        \"\"\"Body as unicode\"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = f\"charset={benc}\"\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody\n\n    def urljoin(self, url: str) -> str:\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)\n\n    @memoizemethod_noargs\n    def _headers_encoding(self) -> str | None:\n        content_type = cast(\"bytes\", self.headers.get(b\"Content-Type\", b\"\"))\n        return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n\n    def _body_inferred_encoding(self) -> str:\n        if self._cached_benc is None:\n            content_type = to_unicode(\n                cast(\"bytes\", self.headers.get(b\"Content-Type\", b\"\")),\n                encoding=\"latin-1\",\n            )\n            benc, ubody = html_to_unicode(\n                content_type,\n                self.body,\n                auto_detect_fun=self._auto_detect_fun,\n                default_encoding=self._DEFAULT_ENCODING,\n            )\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc\n\n    def _auto_detect_fun(self, text: bytes) -> str | None:\n        for enc in (self._DEFAULT_ENCODING, \"utf-8\", \"cp1252\"):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)\n        return None\n\n    @memoizemethod_noargs\n    def _body_declared_encoding(self) -> str | None:\n        return html_body_declared_encoding(self.body)\n\n    @memoizemethod_noargs\n    def _bom_encoding(self) -> str | None:\n        return read_bom(self.body)[0]\n\n    @property\n    def selector(self) -> Selector:\n        # circular import\n        from scrapy.selector import Selector  # noqa: PLC0415\n\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector\n\n    def jmespath(self, query: str, **kwargs: Any) -> SelectorList:\n        if not hasattr(self.selector, \"jmespath\"):\n            raise AttributeError(\n                \"Please install parsel >= 1.8.1 to get jmespath support\"\n            )\n        return cast(\"SelectorList\", self.selector.jmespath(query, **kwargs))\n\n    def xpath(self, query: str, **kwargs: Any) -> SelectorList:\n        return cast(\"SelectorList\", self.selector.xpath(query, **kwargs))\n\n    def css(self, query: str) -> SelectorList:\n        return cast(\"SelectorList\", self.selector.css(query))\n", "n_tokens": 1210, "byte_len": 5406, "file_sha1": "feae558ecf71f26e3343a82bb9c97d482b74ec51", "start_line": 1, "end_line": 164}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/text.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/text.py", "rel_path": "scrapy/http/response/text.py", "module": "scrapy.http.response.text", "ext": "py", "chunk_number": 2, "symbols": ["follow", "follow_all", "encoding", "failure", "method", "does", "bool", "url", "from", "xpath", "append", "cookies", "generator", "instance", "each", "extraction", "selector", "dont", "filter", "passing", "list", "elif", "accepted", "href", "isinstance", "following", "all", "object", "none", "parameter", "__init__", "_set_body", "_declared_encoding", "json", "text", "urljoin", "_headers_encoding", "_body_inferred_encoding", "_auto_detect_fun", "_body_declared_encoding", "_bom_encoding", "jmespath", "css", "_url_from_selector", "TextResponse", "_InvalidSelector", "body", "loads", "html", "unsupported"], "ast_kind": "function_or_method", "text": "    def follow(\n        self,\n        url: str | Link | parsel.Selector,\n        callback: CallbackT | None = None,\n        method: str = \"GET\",\n        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        body: bytes | str | None = None,\n        cookies: CookiesT | None = None,\n        meta: dict[str, Any] | None = None,\n        encoding: str | None = None,\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Callable[[Failure], Any] | None = None,\n        cb_kwargs: dict[str, Any] | None = None,\n        flags: list[str] | None = None,\n    ) -> Request:\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__()`` method,\n        but ``url`` can be not only an absolute URL, but also\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        See :ref:`response-follow-example` for usage examples.\n        \"\"\"\n        if isinstance(url, parsel.Selector):\n            url = _url_from_selector(url)\n        elif isinstance(url, parsel.SelectorList):\n            raise ValueError(\"SelectorList is not supported\")\n        encoding = self.encoding if encoding is None else encoding\n        return super().follow(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n    def follow_all(\n        self,\n        urls: Iterable[str | Link] | parsel.SelectorList | None = None,\n        callback: CallbackT | None = None,\n        method: str = \"GET\",\n        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        body: bytes | str | None = None,\n        cookies: CookiesT | None = None,\n        meta: dict[str, Any] | None = None,\n        encoding: str | None = None,\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Callable[[Failure], Any] | None = None,\n        cb_kwargs: dict[str, Any] | None = None,\n        flags: list[str] | None = None,\n        css: str | None = None,\n        xpath: str | None = None,\n    ) -> Iterable[Request]:\n        \"\"\"\n        A generator that produces :class:`~.Request` instances to follow all\n        links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n        ``__init__()`` method, except that each ``urls`` element does not need to be\n        an absolute URL, it can be any of the following:\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n        within the ``follow_all()`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n\n        Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n        using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n        selectors from which links cannot be obtained (for instance, anchor tags without an\n        ``href`` attribute)\n        \"\"\"\n        arguments = [x for x in (urls, css, xpath) if x is not None]\n        if len(arguments) != 1:\n            raise ValueError(\n                \"Please supply exactly one of the following arguments: urls, css, xpath\"\n            )\n        if not urls:\n            if css:\n                urls = self.css(css)\n            if xpath:\n                urls = self.xpath(xpath)\n        if isinstance(urls, parsel.SelectorList):\n            selectors = urls\n            urls = []\n            for sel in selectors:\n                with suppress(_InvalidSelector):\n                    urls.append(_url_from_selector(sel))\n        return super().follow_all(\n            urls=cast(\"Iterable[str | Link]\", urls),\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n", "n_tokens": 1199, "byte_len": 5077, "file_sha1": "feae558ecf71f26e3343a82bb9c97d482b74ec51", "start_line": 165, "end_line": 291}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/text.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/text.py", "rel_path": "scrapy/http/response/text.py", "module": "scrapy.http.response.text", "ext": "py", "chunk_number": 3, "symbols": ["_url_from_selector", "_InvalidSelector", "hasattr", "url", "from", "result", "unsupported", "supported", "return", "value", "error", "cannot", "selector", "class", "only", "attr", "strip", "html", "href", "parsel", "isinstance", "when", "attribute", "none", "raised", "elements", "link", "root", "invalid", "raise", "__init__", "_set_body", "encoding", "_declared_encoding", "json", "text", "urljoin", "_headers_encoding", "_body_inferred_encoding", "_auto_detect_fun", "_body_declared_encoding", "_bom_encoding", "jmespath", "xpath", "css", "follow", "follow_all", "TextResponse", "failure", "method"], "ast_kind": "class_or_type", "text": "class _InvalidSelector(ValueError):\n    \"\"\"\n    Raised when a URL cannot be obtained from a Selector\n    \"\"\"\n\n\ndef _url_from_selector(sel: parsel.Selector) -> str:\n    if isinstance(sel.root, str):\n        # e.g. ::attr(href) result\n        return strip_html5_whitespace(sel.root)\n    if not hasattr(sel.root, \"tag\"):\n        raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n    if sel.root.tag not in (\"a\", \"link\"):\n        raise _InvalidSelector(\n            f\"Only <a> and <link> elements are supported; got <{sel.root.tag}>\"\n        )\n    href = sel.root.get(\"href\")\n    if href is None:\n        raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n    return strip_html5_whitespace(href)\n", "n_tokens": 177, "byte_len": 727, "file_sha1": "feae558ecf71f26e3343a82bb9c97d482b74ec51", "start_line": 292, "end_line": 312}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/json.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/response/json.py", "rel_path": "scrapy/http/response/json.py", "module": "scrapy.http.response.json", "ext": "py", "chunk_number": 1, "symbols": ["JsonResponse", "used", "text", "response", "mime", "pass", "module", "type", "class", "scrapy", "topics", "documentation", "from", "implements", "when", "content", "json", "docs", "import", "header", "http", "this", "request", "that"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the JsonResponse class that is used when the response\nhas a JSON MIME type in its Content-Type header.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass JsonResponse(TextResponse):\n    pass\n", "n_tokens": 53, "byte_len": 286, "file_sha1": "981e810becfb437711b45ba85b97a95db5e4ac65", "start_line": 1, "end_line": 13}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/__init__.py", "rel_path": "scrapy/http/request/__init__.py", "module": "scrapy.http.request.__init__", "ext": "py", "chunk_number": 1, "symbols": ["NO_CALLBACK", "VerboseCookie", "Request", "encoding", "failure", "method", "bool", "containing", "cookies", "python", "lib", "w3lib", "currently", "curl", "request", "name", "domain", "callback", "spider", "type", "target", "dont", "filter", "indicates", "more", "future", "typ", "checking", "https", "from", "__init__", "cb_kwargs", "meta", "url", "_set_url", "body", "_set_body", "__repr__", "copy", "replace", "from_curl", "to_dict", "_find_method", "those", "sending", "filtered", "back", "following", "them", "parameter"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the Request class which is used to represent HTTP\nrequests in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    NoReturn,\n    TypedDict,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom w3lib.url import safe_url_string\n\n# a workaround for the docs \"more than one target found\" problem\nimport scrapy  # noqa: TC001\nfrom scrapy.http.headers import Headers\nfrom scrapy.utils.curl import curl_to_request_kwargs\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterable, Mapping\n\n    from twisted.python.failure import Failure\n\n    # typing.Concatenate requires Python 3.10\n    # typing.NotRequired and typing.Self require Python 3.11\n    from typing_extensions import Concatenate, NotRequired, Self\n\n    from scrapy.http import Response\n\n    CallbackT = Callable[Concatenate[Response, ...], Any]\n\n\nclass VerboseCookie(TypedDict):\n    name: str | bytes\n    value: str | bytes | bool | float | int\n    domain: NotRequired[str | bytes]\n    path: NotRequired[str | bytes]\n    secure: NotRequired[bool]\n\n\nCookiesT = Union[dict[str, str], list[VerboseCookie]]\n\n\nRequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n\n\ndef NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n    \"\"\"When assigned to the ``callback`` parameter of\n    :class:`~scrapy.Request`, it indicates that the request is not meant\n    to have a spider callback at all.\n\n    For example:\n\n    .. code-block:: python\n\n       Request(\"https://example.com\", callback=NO_CALLBACK)\n\n    This value should be used by :ref:`components <topics-components>` that\n    create and handle their own requests, e.g. through\n    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader\n    middlewares handling such requests can treat them differently from requests\n    intended for the :meth:`~scrapy.Spider.parse` callback.\n    \"\"\"\n    raise RuntimeError(\n        \"The NO_CALLBACK callback has been called. This is a special callback \"\n        \"value intended for requests whose callback is never meant to be \"\n        \"called.\"\n    )\n\n\nclass Request(object_ref):\n    \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n    executed by the Downloader, thus generating a :class:`~scrapy.http.Response`.\n    \"\"\"\n\n    attributes: tuple[str, ...] = (\n        \"url\",\n        \"callback\",\n        \"method\",\n        \"headers\",\n        \"body\",\n        \"cookies\",\n        \"meta\",\n        \"encoding\",\n        \"priority\",\n        \"dont_filter\",\n        \"errback\",\n        \"flags\",\n        \"cb_kwargs\",\n    )\n    \"\"\"A tuple of :class:`str` objects containing the name of all public\n    attributes of the class that are also keyword parameters of the\n    ``__init__()`` method.\n\n    Currently used by :meth:`.Request.replace`, :meth:`.Request.to_dict` and\n    :func:`~scrapy.utils.request.request_from_dict`.\n    \"\"\"\n", "n_tokens": 695, "byte_len": 3047, "file_sha1": "93d582e0553969c078a44b53c95b5b0651dd53d2", "start_line": 1, "end_line": 110}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/__init__.py", "rel_path": "scrapy/http/request/__init__.py", "module": "scrapy.http.request.__init__", "ext": "py", "chunk_number": 2, "symbols": ["__init__", "cb_kwargs", "meta", "url", "_set_url", "body", "_set_body", "encoding", "failure", "method", "whether", "those", "bool", "schedulers", "cookies", "filtering", "spider", "callback", "about", "responses", "scheduler", "enabled", "sending", "dont", "filter", "indicates", "filtered", "isinstance", "following", "handler", "NO_CALLBACK", "__repr__", "copy", "replace", "from_curl", "to_dict", "_find_method", "VerboseCookie", "Request", "containing", "lib", "w3lib", "back", "them", "parameter", "methods", "verbose", "cookie", "http", "runtime"], "ast_kind": "function_or_method", "text": "    def __init__(\n        self,\n        url: str,\n        callback: CallbackT | None = None,\n        method: str = \"GET\",\n        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        body: bytes | str | None = None,\n        cookies: CookiesT | None = None,\n        meta: dict[str, Any] | None = None,\n        encoding: str = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Callable[[Failure], Any] | None = None,\n        flags: list[str] | None = None,\n        cb_kwargs: dict[str, Any] | None = None,\n    ) -> None:\n        self._encoding: str = encoding  # this one has to be set first\n        self.method: str = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        if not isinstance(priority, int):\n            raise TypeError(f\"Request priority not an integer: {priority!r}\")\n\n        #: Default: ``0``\n        #:\n        #: Value that the :ref:`scheduler <topics-scheduler>` may use for\n        #: request prioritization.\n        #:\n        #: Built-in schedulers prioritize requests with a higher priority\n        #: value.\n        #:\n        #: Negative values are allowed.\n        self.priority: int = priority\n\n        if not (callable(callback) or callback is None):\n            raise TypeError(\n                f\"callback must be a callable, got {type(callback).__name__}\"\n            )\n        if not (callable(errback) or errback is None):\n            raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n\n        #: :class:`~collections.abc.Callable` to parse the\n        #: :class:`~scrapy.http.Response` to this request once received.\n        #:\n        #: The callable must expect the response as its first parameter, and\n        #: support any additional keyword arguments set through\n        #: :attr:`cb_kwargs`.\n        #:\n        #: In addition to an arbitrary callable, the following values are also\n        #: supported:\n        #:\n        #: -   ``None`` (default), which indicates that the\n        #:     :meth:`~scrapy.Spider.parse` method of the spider must be used.\n        #:\n        #: -   :func:`~scrapy.http.request.NO_CALLBACK`.\n        #:\n        #: If an unhandled exception is raised during request or response\n        #: processing, i.e. by a :ref:`spider middleware\n        #: <topics-spider-middleware>`, :ref:`downloader middleware\n        #: <topics-downloader-middleware>` or download handler\n        #: (:setting:`DOWNLOAD_HANDLERS`), :attr:`errback` is called instead.\n        #:\n        #: .. tip::\n        #:     :class:`~scrapy.spidermiddlewares.httperror.HttpErrorMiddleware`\n        #:     raises exceptions for non-2xx responses by default, sending them\n        #:     to the :attr:`errback` instead.\n        #:\n        #: .. seealso::\n        #:     :ref:`topics-request-response-ref-request-callback-arguments`\n        self.callback: CallbackT | None = callback\n\n        #: :class:`~collections.abc.Callable` to handle exceptions raised\n        #: during request or response processing.\n        #:\n        #: The callable must expect a :exc:`~twisted.python.failure.Failure` as\n        #: its first parameter.\n        #:\n        #: .. seealso:: :ref:`topics-request-response-ref-errbacks`\n        self.errback: Callable[[Failure], Any] | None = errback\n\n        self.cookies: CookiesT = cookies or {}\n        self.headers: Headers = Headers(headers or {}, encoding=encoding)\n\n        #: Whether this request may be filtered out by :ref:`components\n        #: <topics-components>` that support filtering out requests (``False``,\n        #: default), or those components should not filter out this request\n        #: (``True``).\n        #:\n        #: This attribute is commonly set to ``True`` to prevent duplicate\n        #: requests from being filtered out.\n        #:\n        #: When defining the start URLs of a spider through\n        #: :attr:`~scrapy.Spider.start_urls`, this attribute is enabled by\n        #: default. See :meth:`~scrapy.Spider.start`.\n        self.dont_filter: bool = dont_filter\n\n        self._meta: dict[str, Any] | None = dict(meta) if meta else None\n        self._cb_kwargs: dict[str, Any] | None = dict(cb_kwargs) if cb_kwargs else None\n        self.flags: list[str] = [] if flags is None else list(flags)\n\n    @property\n    def cb_kwargs(self) -> dict[str, Any]:\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs\n\n    @property\n    def meta(self) -> dict[str, Any]:\n        if self._meta is None:\n            self._meta = {}\n        return self._meta\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    def _set_url(self, url: str) -> None:\n        if not isinstance(url, str):\n            raise TypeError(f\"Request url must be str, got {type(url).__name__}\")\n\n        self._url = safe_url_string(url, self.encoding)\n\n        if (\n            \"://\" not in self._url\n            and not self._url.startswith(\"about:\")\n            and not self._url.startswith(\"data:\")\n        ):\n            raise ValueError(f\"Missing scheme in request url: {self._url}\")\n\n    @property\n    def body(self) -> bytes:\n        return self._body\n\n    def _set_body(self, body: str | bytes | None) -> None:\n        self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n", "n_tokens": 1248, "byte_len": 5337, "file_sha1": "93d582e0553969c078a44b53c95b5b0651dd53d2", "start_line": 111, "end_line": 246}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/__init__.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/__init__.py", "rel_path": "scrapy/http/request/__init__.py", "module": "scrapy.http.request.__init__", "ext": "py", "chunk_number": 3, "symbols": ["encoding", "__repr__", "copy", "replace", "from_curl", "to_dict", "_find_method", "method", "unidiomatic", "those", "bool", "from", "curl", "subclasses", "containing", "ismethod", "cookies", "instance", "retrieved", "each", "request", "spider", "name", "type", "enabled", "scrapy", "passing", "string", "https", "predicate", "NO_CALLBACK", "__init__", "cb_kwargs", "meta", "url", "_set_url", "body", "_set_body", "VerboseCookie", "Request", "failure", "lib", "w3lib", "sending", "filtered", "back", "following", "them", "parameter", "methods"], "ast_kind": "function_or_method", "text": "    @property\n    def encoding(self) -> str:\n        return self._encoding\n\n    def __repr__(self) -> str:\n        return f\"<{self.method} {self.url}>\"\n\n    def copy(self) -> Self:\n        return self.replace()\n\n    @overload\n    def replace(\n        self, *args: Any, cls: type[RequestTypeVar], **kwargs: Any\n    ) -> RequestTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: type[Request] | None = None, **kwargs: Any\n    ) -> Request:\n        \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n        for x in self.attributes:\n            kwargs.setdefault(x, getattr(self, x))\n        if cls is None:\n            cls = self.__class__\n        return cls(*args, **kwargs)\n\n    @classmethod\n    def from_curl(\n        cls,\n        curl_command: str,\n        ignore_unknown_options: bool = True,\n        **kwargs: Any,\n    ) -> Self:\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.Request`\n                     subclasses, such as :class:`~scrapy.http.JsonRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.Request` object.\n\n        To translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n        \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n\n    def to_dict(self, *, spider: scrapy.Spider | None = None) -> dict[str, Any]:\n        \"\"\"Return a dictionary containing the Request's data.\n\n        Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n        If a spider is given, this method will try to find out the name of the spider methods used as callback\n        and errback and include them in the output dict, raising an exception if they cannot be found.\n        \"\"\"\n        d = {\n            \"url\": self.url,  # urls are safe (safe_string_url)\n            \"callback\": (\n                _find_method(spider, self.callback)\n                if callable(self.callback)\n                else self.callback\n            ),\n            \"errback\": (\n                _find_method(spider, self.errback)\n                if callable(self.errback)\n                else self.errback\n            ),\n            \"headers\": dict(self.headers),\n        }\n        for attr in self.attributes:\n            d.setdefault(attr, getattr(self, attr))\n        if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck\n            d[\"_class\"] = self.__module__ + \".\" + self.__class__.__name__\n        return d\n\n\ndef _find_method(obj: Any, func: Callable[..., Any]) -> str:\n    \"\"\"Helper function for Request.to_dict\"\"\"\n    # Only instance methods contain ``__func__``\n    if obj and hasattr(func, \"__func__\"):\n        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n        for name, obj_func in members:\n            # We need to use __func__ to access the original function object because instance\n            # method objects are generated each time attribute is retrieved from instance.\n            #\n            # Reference: The standard type hierarchy\n            # https://docs.python.org/3/reference/datamodel.html\n            if obj_func.__func__ is func.__func__:\n                return name\n    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")\n", "n_tokens": 1015, "byte_len": 4680, "file_sha1": "93d582e0553969c078a44b53c95b5b0651dd53d2", "start_line": 247, "end_line": 356}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/json_request.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/json_request.py", "rel_path": "scrapy/http/request/json_request.py", "module": "scrapy.http.request.json_request", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "dumps_kwargs", "replace", "_dumps", "JsonRequest", "method", "dumps", "bool", "python", "request", "type", "passed", "more", "future", "typ", "checking", "requests", "elif", "both", "than", "none", "json", "docs", "accept", "generate", "http", "typing", "extensions", "return", "annotations", "class", "ignored", "sort", "keys", "headers", "warnings", "kwargs", "super", "self", "this", "response", "else", "post", "setdefault", "application", "requires", "module", "dict", "body", "convenient"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the JsonRequest class which is a more convenient class\n(than Request) to generate JSON Requests.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING, Any, overload\n\nfrom scrapy.http.request import Request, RequestTypeVar\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass JsonRequest(Request):\n    attributes: tuple[str, ...] = (*Request.attributes, \"dumps_kwargs\")\n\n    def __init__(\n        self, *args: Any, dumps_kwargs: dict[str, Any] | None = None, **kwargs: Any\n    ) -> None:\n        dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n        dumps_kwargs.setdefault(\"sort_keys\", True)\n        self._dumps_kwargs: dict[str, Any] = dumps_kwargs\n\n        body_passed = kwargs.get(\"body\") is not None\n        data: Any = kwargs.pop(\"data\", None)\n        data_passed: bool = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn(\"Both body and data passed. data will be ignored\")\n        elif not body_passed and data_passed:\n            kwargs[\"body\"] = self._dumps(data)\n            if \"method\" not in kwargs:\n                kwargs[\"method\"] = \"POST\"\n\n        super().__init__(*args, **kwargs)\n        self.headers.setdefault(\"Content-Type\", \"application/json\")\n        self.headers.setdefault(\n            \"Accept\", \"application/json, text/javascript, */*; q=0.01\"\n        )\n\n    @property\n    def dumps_kwargs(self) -> dict[str, Any]:\n        return self._dumps_kwargs\n\n    @overload\n    def replace(\n        self, *args: Any, cls: type[RequestTypeVar], **kwargs: Any\n    ) -> RequestTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: type[Request] | None = None, **kwargs: Any\n    ) -> Request:\n        body_passed = kwargs.get(\"body\") is not None\n        data: Any = kwargs.pop(\"data\", None)\n        data_passed: bool = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn(\"Both body and data passed. data will be ignored\")\n        elif not body_passed and data_passed:\n            kwargs[\"body\"] = self._dumps(data)\n\n        return super().replace(*args, cls=cls, **kwargs)\n\n    def _dumps(self, data: Any) -> str:\n        \"\"\"Convert to JSON\"\"\"\n        return json.dumps(data, **self._dumps_kwargs)\n", "n_tokens": 598, "byte_len": 2504, "file_sha1": "2ae258787871d66c3fbaf32b5a29851ecbf0f558", "start_line": 1, "end_line": 78}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/rpc.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/rpc.py", "rel_path": "scrapy/http/request/rpc.py", "module": "scrapy.http.request.rpc", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "XmlRpcRequest", "encoding", "post", "dumps", "method", "requests", "text", "setdefault", "module", "python", "spec", "xmlrpc", "typing", "over", "type", "annotations", "monkey", "patch", "class", "dont", "filter", "scrapy", "more", "future", "convenient", "headers", "body", "topics", "times", "init", "documentation", "defines", "query", "dump", "args", "true", "from", "implements", "xmlrpclib", "kwargs", "params", "request", "content", "multiples", "restore", "client", "none", "super", "which"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the XmlRpcRequest class which is a more convenient class\n(that Request) to generate xml-rpc requests.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport xmlrpc.client as xmlrpclib\nfrom typing import Any\n\nimport defusedxml.xmlrpc\n\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import get_func_args\n\ndefusedxml.xmlrpc.monkey_patch()\n\nDUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n\n\nclass XmlRpcRequest(Request):\n    def __init__(self, *args: Any, encoding: str | None = None, **kwargs: Any):\n        if \"body\" not in kwargs and \"params\" in kwargs:\n            kw = {k: kwargs.pop(k) for k in DUMPS_ARGS if k in kwargs}\n            kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault(\"method\", \"POST\")\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault(\"dont_filter\", True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs[\"encoding\"] = encoding\n\n        super().__init__(*args, **kwargs)\n        self.headers.setdefault(\"Content-Type\", \"text/xml\")\n", "n_tokens": 270, "byte_len": 1178, "file_sha1": "1a7518f37aedf2ca95e1cdc9e8307588b8386326", "start_line": 1, "end_line": 41}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/form.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/form.py", "rel_path": "scrapy/http/request/form.py", "module": "scrapy.http.request.form", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_response", "_get_form_url", "_urlencode", "_get_form", "FormRequest", "encoding", "while", "method", "wanted", "bool", "html", "translator", "xpath", "css", "formxpath", "python", "lib", "w3lib", "name", "formdata", "type", "more", "future", "requests", "typ", "checking", "formnumber", "items", "isinstance", "_get_inputs", "_value", "_select_value", "_get_clickable", "button", "inputs", "browser", "append", "expression", "compare", "didn", "here", "than", "selector", "index", "error", "selected", "none", "urlencode", "reset"], "ast_kind": "class_or_type", "text": "\"\"\"\nThis module implements the FormRequest class which is a more convenient class\n(than Request) to generate Requests based on form data.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\nfrom urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n\nfrom parsel.csstranslator import HTMLTranslator\nfrom w3lib.html import strip_html5_whitespace\n\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import is_listlike, to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from lxml.html import (\n        FormElement,\n        InputElement,\n        MultipleSelectOptions,\n        SelectElement,\n        TextareaElement,\n    )\n    from typing_extensions import Self\n\n    from scrapy.http.response.text import TextResponse\n\n\nFormdataVType = Union[str, Iterable[str]]\nFormdataKVType = tuple[str, FormdataVType]\nFormdataType = Optional[Union[dict[str, FormdataVType], list[FormdataKVType]]]\n\n\nclass FormRequest(Request):\n    valid_form_methods = [\"GET\", \"POST\"]\n\n    def __init__(\n        self, *args: Any, formdata: FormdataType = None, **kwargs: Any\n    ) -> None:\n        if formdata and kwargs.get(\"method\") is None:\n            kwargs[\"method\"] = \"POST\"\n\n        super().__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            form_query_str = _urlencode(items, self.encoding)\n            if self.method == \"POST\":\n                self.headers.setdefault(\n                    b\"Content-Type\", b\"application/x-www-form-urlencoded\"\n                )\n                self._set_body(form_query_str)\n            else:\n                self._set_url(\n                    urlunsplit(urlsplit(self.url)._replace(query=form_query_str))\n                )\n\n    @classmethod\n    def from_response(\n        cls,\n        response: TextResponse,\n        formname: str | None = None,\n        formid: str | None = None,\n        formnumber: int = 0,\n        formdata: FormdataType = None,\n        clickdata: dict[str, str | int] | None = None,\n        dont_click: bool = False,\n        formxpath: str | None = None,\n        formcss: str | None = None,\n        **kwargs: Any,\n    ) -> Self:\n        kwargs.setdefault(\"encoding\", response.encoding)\n\n        if formcss is not None:\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata)\n        url = _get_form_url(form, kwargs.pop(\"url\", None))\n\n        method = kwargs.pop(\"method\", form.method)\n        if method is not None:\n            method = method.upper()\n            if method not in cls.valid_form_methods:\n                method = \"GET\"\n\n        return cls(url=url, method=method, formdata=formdata, **kwargs)\n\n\ndef _get_form_url(form: FormElement, url: str | None) -> str:\n    assert form.base_url is not None  # typing\n    if url is None:\n        action = form.get(\"action\")\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n    return urljoin(form.base_url, url)\n\n\ndef _urlencode(seq: Iterable[FormdataKVType], enc: str) -> str:\n    values = [\n        (to_bytes(k, enc), to_bytes(v, enc))\n        for k, vs in seq\n        for v in (cast(\"Iterable[str]\", vs) if is_listlike(vs) else [cast(\"str\", vs)])\n    ]\n    return urlencode(values, doseq=True)\n\n\ndef _get_form(\n    response: TextResponse,\n    formname: str | None,\n    formid: str | None,\n    formnumber: int,\n    formxpath: str | None,\n) -> FormElement:\n    \"\"\"Find the wanted form element within the given response.\"\"\"\n    root = response.selector.root\n    forms = root.xpath(\"//form\")\n    if not forms:\n        raise ValueError(f\"No <form> element found in {response}\")\n\n    if formname is not None:\n        f = root.xpath(f'//form[@name=\"{formname}\"]')\n        if f:\n            return cast(\"FormElement\", f[0])\n\n    if formid is not None:\n        f = root.xpath(f'//form[@id=\"{formid}\"]')\n        if f:\n            return cast(\"FormElement\", f[0])\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == \"form\":\n                    return cast(\"FormElement\", el)\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError(f\"No <form> element found with {formxpath}\")\n\n    # If we get here, it means that either formname was None or invalid\n    try:\n        form = forms[formnumber]\n    except IndexError:\n        raise IndexError(f\"Form number {formnumber} not found in {response}\")\n    return cast(\"FormElement\", form)\n\n", "n_tokens": 1154, "byte_len": 4945, "file_sha1": "cab5274d12ba27f55ead7d6d1b73d6cce3a9e696", "start_line": 1, "end_line": 157}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/form.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/http/request/form.py", "rel_path": "scrapy/http/request/form.py", "module": "scrapy.http.request.form", "ext": "py", "chunk_number": 2, "symbols": ["_get_inputs", "_value", "_select_value", "_get_clickable", "button", "inputs", "bool", "browser", "xpath", "append", "expression", "compare", "name", "didn", "formdata", "type", "items", "isinstance", "selected", "index", "error", "none", "reset", "returns", "clickables", "dont", "click", "join", "without", "uniquely", "__init__", "from_response", "_get_form_url", "_urlencode", "_get_form", "FormRequest", "encoding", "while", "method", "wanted", "html", "translator", "css", "formxpath", "python", "lib", "w3lib", "more", "future", "requests"], "ast_kind": "function_or_method", "text": "def _get_inputs(\n    form: FormElement,\n    formdata: FormdataType,\n    dont_click: bool,\n    clickdata: dict[str, str | int] | None,\n) -> list[FormdataKVType]:\n    \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n    try:\n        formdata_keys = dict(formdata or ()).keys()\n    except (ValueError, TypeError):\n        raise ValueError(\"formdata should be a dict or iterable of tuples\")\n\n    if not formdata:\n        formdata = []\n    inputs = form.xpath(\n        \"descendant::textarea\"\n        \"|descendant::select\"\n        \"|descendant::input[not(@type) or @type[\"\n        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n        \" and (../@checked or\"\n        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n        namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n    )\n    values: list[FormdataKVType] = [\n        (k, \"\" if v is None else v)\n        for k, v in (_value(e) for e in inputs)\n        if k and k not in formdata_keys\n    ]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and clickable[0] is not None:\n            values.append(clickable)\n\n    formdata_items = formdata.items() if isinstance(formdata, dict) else formdata\n    values.extend((k, v) for k, v in formdata_items if v is not None)\n    return values\n\n\ndef _value(\n    ele: InputElement | SelectElement | TextareaElement,\n) -> tuple[str | None, str | MultipleSelectOptions | None]:\n    n = ele.name\n    v = ele.value\n    if ele.tag == \"select\":\n        return _select_value(cast(\"SelectElement\", ele), n, v)\n    return n, v\n\n\ndef _select_value(\n    ele: SelectElement, n: str | None, v: str | MultipleSelectOptions | None\n) -> tuple[str | None, str | MultipleSelectOptions | None]:\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags without options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    return n, v\n\n\ndef _get_clickable(\n    clickdata: dict[str, str | int] | None, form: FormElement\n) -> tuple[str, str] | None:\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = list(\n        form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n            namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n        )\n    )\n    if not clickables:\n        return None\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get(\"name\"), el.get(\"value\") or \"\")\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get(\"nr\", None)\n    if nr is not None:\n        assert isinstance(nr, int)\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (cast(\"str\", el.get(\"name\")), el.get(\"value\") or \"\")\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = \".//*\" + \"\".join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get(\"name\"), el[0].get(\"value\") or \"\")\n    if len(el) > 1:\n        raise ValueError(\n            f\"Multiple elements found ({el!r}) matching the \"\n            f\"criteria in clickdata: {clickdata!r}\"\n        )\n    raise ValueError(f\"No clickable element matching clickdata: {clickdata!r}\")\n", "n_tokens": 1022, "byte_len": 3899, "file_sha1": "cab5274d12ba27f55ead7d6d1b73d6cce3a9e696", "start_line": 158, "end_line": 267}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/templates/project/module/spiders/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/templates/project/module/spiders/__init__.py", "rel_path": "scrapy/templates/project/module/spiders/__init__.py", "module": "scrapy.templates.project.module.spiders.__init__", "ext": "py", "chunk_number": 1, "symbols": ["create", "contain", "will", "package", "refer", "your", "project", "this", "spiders", "scrapy", "manage", "please", "information", "documentation"], "ast_kind": "unknown", "text": "# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n", "n_tokens": 33, "byte_len": 161, "file_sha1": "212923a8da79a2b00a154978408a19b697129289", "start_line": 1, "end_line": 5}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/referer.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/referer.py", "rel_path": "scrapy/spidermiddlewares/referer.py", "module": "scrapy.spidermiddlewares.referer", "ext": "py", "chunk_number": 1, "symbols": ["referrer", "stripped_referrer", "origin_referrer", "strip_url", "origin", "potentially_trustworthy", "tls_protected", "ReferrerPolicy", "NoReferrerPolicy", "NoReferrerWhenDowngradePolicy", "SameOriginPolicy", "originated", "does", "referer", "bool", "unsafe", "made", "python", "lib", "w3lib", "polic", "sam", "when", "spider", "name", "strict", "about", "strip", "url", "credentials", "_load_policy_class", "__init__", "from_crawler", "policy", "get_processed_request", "request_scheduled", "OriginPolicy", "StrictOriginPolicy", "OriginWhenCrossOriginPolicy", "StrictOriginWhenCrossOriginPolicy", "UnsafeUrlPolicy", "DefaultReferrerPolicy", "RefererMiddleware", "resp", "surrogate", "signal", "latin", "latin1", "interpret", "doesn"], "ast_kind": "class_or_type", "text": "\"\"\"\nRefererMiddleware: populates Request referer field, based on the Response which\noriginated it.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, cast\nfrom urllib.parse import urlparse\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import Spider, signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.spidermiddlewares.base import BaseSpiderMiddleware\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.url import strip_url\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n\nLOCAL_SCHEMES: tuple[str, ...] = (\n    \"about\",\n    \"blob\",\n    \"data\",\n    \"filesystem\",\n)\n\nPOLICY_NO_REFERRER = \"no-referrer\"\nPOLICY_NO_REFERRER_WHEN_DOWNGRADE = \"no-referrer-when-downgrade\"\nPOLICY_SAME_ORIGIN = \"same-origin\"\nPOLICY_ORIGIN = \"origin\"\nPOLICY_STRICT_ORIGIN = \"strict-origin\"\nPOLICY_ORIGIN_WHEN_CROSS_ORIGIN = \"origin-when-cross-origin\"\nPOLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN = \"strict-origin-when-cross-origin\"\nPOLICY_UNSAFE_URL = \"unsafe-url\"\nPOLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n\n\nclass ReferrerPolicy(ABC):\n    \"\"\"Abstract base class for referrer policies.\"\"\"\n\n    NOREFERRER_SCHEMES: tuple[str, ...] = LOCAL_SCHEMES\n    name: str\n\n    @abstractmethod\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        raise NotImplementedError\n\n    def stripped_referrer(self, url: str) -> str | None:\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.strip_url(url)\n        return None\n\n    def origin_referrer(self, url: str) -> str | None:\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.origin(url)\n        return None\n\n    def strip_url(self, url: str, origin_only: bool = False) -> str | None:\n        \"\"\"\n        https://www.w3.org/TR/referrer-policy/#strip-url\n\n        If url is null, return no referrer.\n        If url's scheme is a local scheme, then return no referrer.\n        Set url's username to the empty string.\n        Set url's password to null.\n        Set url's fragment to null.\n        If the origin-only flag is true, then:\n            Set url's path to null.\n            Set url's query to null.\n        Return url.\n        \"\"\"\n        if not url:\n            return None\n        return strip_url(\n            url,\n            strip_credentials=True,\n            strip_fragment=True,\n            strip_default_port=True,\n            origin_only=origin_only,\n        )\n\n    def origin(self, url: str) -> str | None:\n        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n        return self.strip_url(url, origin_only=True)\n\n    def potentially_trustworthy(self, url: str) -> bool:\n        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in (\"data\",):\n            return False\n        return self.tls_protected(url)\n\n    def tls_protected(self, url: str) -> bool:\n        return urlparse(url).scheme in (\"https\", \"ftps\")\n\n\nclass NoReferrerPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer\n\n    The simplest policy is \"no-referrer\", which specifies that no referrer information\n    is to be sent along with requests made from a particular request client to any origin.\n    The header will be omitted entirely.\n    \"\"\"\n\n    name: str = POLICY_NO_REFERRER\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        return None\n\n\nclass NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n\n    The \"no-referrer-when-downgrade\" policy sends a full URL along with requests\n    from a TLS-protected environment settings object to a potentially trustworthy URL,\n    and requests from clients which are not TLS-protected to any origin.\n\n    Requests from TLS-protected clients to non-potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n\n    This is a user agent's default behavior, if no policy is otherwise specified.\n    \"\"\"\n\n    name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n            return self.stripped_referrer(response_url)\n        return None\n\n\nclass SameOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin\n\n    The \"same-origin\" policy specifies that a full URL, stripped for use as a referrer,\n    is sent as referrer information when making same-origin requests from a particular request client.\n\n    Cross-origin requests, on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_SAME_ORIGIN\n", "n_tokens": 1228, "byte_len": 5230, "file_sha1": "58b73125cf71cc2e322b5ced307f2f75801de79a", "start_line": 1, "end_line": 158}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/referer.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/referer.py", "rel_path": "scrapy/spidermiddlewares/referer.py", "module": "scrapy.spidermiddlewares.referer", "ext": "py", "chunk_number": 2, "symbols": ["referrer", "OriginPolicy", "StrictOriginPolicy", "OriginWhenCrossOriginPolicy", "StrictOriginWhenCrossOriginPolicy", "UnsafeUrlPolicy", "DefaultReferrerPolicy", "origin", "unsafe", "made", "when", "name", "strict", "doesn", "documents", "https", "requests", "string", "sent", "particular", "loca", "schemes", "settings", "serialization", "protected", "object", "polic", "origi", "none", "type", "stripped_referrer", "origin_referrer", "strip_url", "potentially_trustworthy", "tls_protected", "_load_policy_class", "__init__", "from_crawler", "policy", "get_processed_request", "request_scheduled", "ReferrerPolicy", "NoReferrerPolicy", "NoReferrerWhenDowngradePolicy", "SameOriginPolicy", "RefererMiddleware", "originated", "does", "resp", "url"], "ast_kind": "class_or_type", "text": "    def referrer(self, response_url: str, request_url: str) -> str | None:\n        if self.origin(response_url) == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        return None\n\n\nclass OriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-origin\n\n    The \"origin\" policy specifies that only the ASCII serialization\n    of the origin of the request client is sent as referrer information\n    when making both same-origin requests and cross-origin requests\n    from a particular request client.\n    \"\"\"\n\n    name: str = POLICY_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        return self.origin_referrer(response_url)\n\n\nclass StrictOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin\n\n    The \"strict-origin\" policy sends the ASCII serialization\n    of the origin of the request client when making requests:\n    - from a TLS-protected environment settings object to a potentially trustworthy URL, and\n    - from non-TLS-protected environment settings objects to any origin.\n\n    Requests from TLS-protected request clients to non- potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_STRICT_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        if (\n            self.tls_protected(response_url)\n            and self.potentially_trustworthy(request_url)\n        ) or not self.tls_protected(response_url):\n            return self.origin_referrer(response_url)\n        return None\n\n\nclass OriginWhenCrossOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin\n\n    The \"origin-when-cross-origin\" policy specifies that a full URL,\n    stripped for use as a referrer, is sent as referrer information\n    when making same-origin requests from a particular request client,\n    and only the ASCII serialization of the origin of the request client\n    is sent as referrer information when making cross-origin requests\n    from a particular request client.\n    \"\"\"\n\n    name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        return origin\n\n\nclass StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin\n\n    The \"strict-origin-when-cross-origin\" policy specifies that a full URL,\n    stripped for use as a referrer, is sent as referrer information\n    when making same-origin requests from a particular request client,\n    and only the ASCII serialization of the origin of the request client\n    when making cross-origin requests:\n\n    - from a TLS-protected environment settings object to a potentially trustworthy URL, and\n    - from non-TLS-protected environment settings objects to any origin.\n\n    Requests from TLS-protected clients to non- potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        if (\n            self.tls_protected(response_url)\n            and self.potentially_trustworthy(request_url)\n        ) or not self.tls_protected(response_url):\n            return self.origin_referrer(response_url)\n        return None\n\n\nclass UnsafeUrlPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url\n\n    The \"unsafe-url\" policy specifies that a full URL, stripped for use as a referrer,\n    is sent along with both cross-origin requests\n    and same-origin requests made from a particular request client.\n\n    Note: The policy's name doesn't lie; it is unsafe.\n    This policy will leak origins and paths from TLS-protected resources\n    to insecure origins.\n    Carefully consider the impact of setting such a policy for potentially sensitive documents.\n    \"\"\"\n\n    name: str = POLICY_UNSAFE_URL\n\n    def referrer(self, response_url: str, request_url: str) -> str | None:\n        return self.stripped_referrer(response_url)\n\n\nclass DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n    \"\"\"\n    A variant of \"no-referrer-when-downgrade\",\n    with the addition that \"Referer\" is not sent if the parent request was\n    using ``file://`` or ``s3://`` scheme.\n    \"\"\"\n\n    NOREFERRER_SCHEMES: tuple[str, ...] = (*LOCAL_SCHEMES, \"file\", \"s3\")\n    name: str = POLICY_SCRAPY_DEFAULT\n\n\n_policy_classes: dict[str, type[ReferrerPolicy]] = {\n    p.name: p\n    for p in (\n        NoReferrerPolicy,\n        NoReferrerWhenDowngradePolicy,\n        SameOriginPolicy,\n        OriginPolicy,\n        StrictOriginPolicy,\n        OriginWhenCrossOriginPolicy,\n        StrictOriginWhenCrossOriginPolicy,\n        UnsafeUrlPolicy,\n        DefaultReferrerPolicy,\n    )\n}\n\n# Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string\n_policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n\n", "n_tokens": 1225, "byte_len": 5516, "file_sha1": "58b73125cf71cc2e322b5ced307f2f75801de79a", "start_line": 159, "end_line": 308}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/referer.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/referer.py", "rel_path": "scrapy/spidermiddlewares/referer.py", "module": "scrapy.spidermiddlewares.referer", "ext": "py", "chunk_number": 3, "symbols": ["_load_policy_class", "__init__", "from_crawler", "policy", "get_processed_request", "request_scheduled", "RefererMiddleware", "resp", "url", "bool", "surrogate", "signal", "latin", "latin1", "interpret", "spider", "runtime", "warning", "load", "string", "https", "policies", "sent", "acts", "settings", "unicode", "lower", "isinstance", "referrer", "safe", "stripped_referrer", "origin_referrer", "strip_url", "origin", "potentially_trustworthy", "tls_protected", "ReferrerPolicy", "NoReferrerPolicy", "NoReferrerWhenDowngradePolicy", "SameOriginPolicy", "OriginPolicy", "StrictOriginPolicy", "OriginWhenCrossOriginPolicy", "StrictOriginWhenCrossOriginPolicy", "UnsafeUrlPolicy", "DefaultReferrerPolicy", "originated", "does", "referer", "unsafe"], "ast_kind": "class_or_type", "text": "def _load_policy_class(\n    policy: str, warning_only: bool = False\n) -> type[ReferrerPolicy] | None:\n    \"\"\"\n    Expect a string for the path to the policy class,\n    otherwise try to interpret the string as a standard value\n    from https://www.w3.org/TR/referrer-policy/#referrer-policies\n    \"\"\"\n    try:\n        return cast(\"type[ReferrerPolicy]\", load_object(policy))\n    except ValueError:\n        tokens = [token.strip() for token in policy.lower().split(\",\")]\n        # https://www.w3.org/TR/referrer-policy/#parse-referrer-policy-from-header\n        for token in tokens[::-1]:\n            if token in _policy_classes:\n                return _policy_classes[token]\n\n        msg = f\"Could not load referrer policy {policy!r}\"\n        if not warning_only:\n            raise RuntimeError(msg)\n        warnings.warn(msg, RuntimeWarning)\n        return None\n\n\nclass RefererMiddleware(BaseSpiderMiddleware):\n    def __init__(self, settings: BaseSettings | None = None):  # pylint: disable=super-init-not-called\n        self.default_policy: type[ReferrerPolicy] = DefaultReferrerPolicy\n        if settings is not None:\n            settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n            assert settings_policy\n            self.default_policy = settings_policy\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"REFERER_ENABLED\"):\n            raise NotConfigured\n        mw = cls(crawler.settings)\n\n        # Note: this hook is a bit of a hack to intercept redirections\n        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n\n        return mw\n\n    def policy(self, resp_or_url: Response | str, request: Request) -> ReferrerPolicy:\n        \"\"\"\n        Determine Referrer-Policy to use from a parent Response (or URL),\n        and a Request to be sent.\n\n        - if a valid policy is set in Request meta, it is used.\n        - if the policy is set in meta but is wrong (e.g. a typo error),\n          the policy from settings is used\n        - if the policy is not set in Request meta,\n          but there is a Referrer-policy header in the parent response,\n          it is used if valid\n        - otherwise, the policy from settings is used.\n        \"\"\"\n        policy_name = request.meta.get(\"referrer_policy\")\n        if policy_name is None and isinstance(resp_or_url, Response):\n            policy_header = resp_or_url.headers.get(\"Referrer-Policy\")\n            if policy_header is not None:\n                policy_name = to_unicode(policy_header.decode(\"latin1\"))\n        if policy_name is None:\n            return self.default_policy()\n\n        cls = _load_policy_class(policy_name, warning_only=True)\n        return cls() if cls else self.default_policy()\n\n    def get_processed_request(\n        self, request: Request, response: Response | None\n    ) -> Request | None:\n        if response is None:\n            # start requests\n            return request\n        referrer = self.policy(response, request).referrer(response.url, request.url)\n        if referrer is not None:\n            request.headers.setdefault(\"Referer\", referrer)\n        return request\n\n    def request_scheduled(self, request: Request, spider: Spider) -> None:\n        # check redirected request to patch \"Referer\" header if necessary\n        redirected_urls = request.meta.get(\"redirect_urls\", [])\n        if redirected_urls:\n            request_referrer = request.headers.get(\"Referer\")\n            # we don't patch the referrer value if there is none\n            if request_referrer is not None:\n                # the request's referrer header value acts as a surrogate\n                # for the parent response URL\n                #\n                # Note: if the 3xx response contained a Referrer-Policy header,\n                #       the information is not available using this hook\n                parent_url = safe_url_string(request_referrer)\n                policy_referrer = self.policy(parent_url, request).referrer(\n                    parent_url, request.url\n                )\n                if policy_referrer != request_referrer.decode(\"latin1\"):\n                    if policy_referrer is None:\n                        request.headers.pop(\"Referer\")\n                    else:\n                        request.headers[\"Referer\"] = policy_referrer\n", "n_tokens": 901, "byte_len": 4363, "file_sha1": "58b73125cf71cc2e322b5ced307f2f75801de79a", "start_line": 309, "end_line": 408}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/urllength.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/urllength.py", "rel_path": "scrapy/spidermiddlewares/urllength.py", "module": "scrapy.spidermiddlewares.urllength", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "get_processed_request", "UrlLengthMiddleware", "urllength", "length", "ignoring", "requires", "inc", "value", "spider", "typing", "extensions", "middleware", "extra", "python", "return", "name", "annotations", "getint", "class", "url", "scrapy", "not", "configured", "future", "typ", "checking", "logger", "classmethod", "get", "pylint", "processed", "topics", "init", "logging", "info", "documentation", "base", "from", "crawler", "urllengt", "limit", "settings", "assert", "maxlength", "request", "exceptions", "called", "spidermiddlewares"], "ast_kind": "class_or_type", "text": "\"\"\"\nUrl Length Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Request, Response\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass UrlLengthMiddleware(BaseSpiderMiddleware):\n    crawler: Crawler\n\n    def __init__(self, maxlength: int):  # pylint: disable=super-init-not-called\n        self.maxlength: int = maxlength\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        maxlength = crawler.settings.getint(\"URLLENGTH_LIMIT\")\n        if not maxlength:\n            raise NotConfigured\n        o = cls(maxlength)\n        o.crawler = crawler\n        return o\n\n    def get_processed_request(\n        self, request: Request, response: Response | None\n    ) -> Request | None:\n        if len(request.url) <= self.maxlength:\n            return request\n        logger.info(\n            \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n            {\"maxlength\": self.maxlength, \"url\": request.url},\n            extra={\"spider\": self.crawler.spider},\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\"urllength/request_ignored_count\")\n        return None\n", "n_tokens": 325, "byte_len": 1494, "file_sha1": "0bb1540542c7ca1e67a424682d206b0657b127e8", "start_line": 1, "end_line": 54}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/start.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/start.py", "rel_path": "scrapy/spidermiddlewares/start.py", "module": "scrapy.spidermiddlewares.start", "ext": "py", "chunk_number": 1, "symbols": ["get_processed_request", "StartSpiderMiddleware", "downloader", "reqmeta", "requests", "setdefault", "typing", "return", "tell", "middleware", "annotations", "allowing", "class", "meta", "start", "request", "attr", "middlewares", "scrapy", "future", "typ", "checking", "get", "processed", "apart", "other", "topics", "base", "spider", "true", "from", "none", "import", "http", "self", "that", "response"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom .base import BaseSpiderMiddleware\n\nif TYPE_CHECKING:\n    from scrapy.http import Request\n    from scrapy.http.response import Response\n\n\nclass StartSpiderMiddleware(BaseSpiderMiddleware):\n    \"\"\"Set :reqmeta:`is_start_request`.\n\n    .. reqmeta:: is_start_request\n\n    is_start_request\n    ----------------\n\n    :attr:`~scrapy.Request.meta` key that is set to ``True`` in :ref:`start\n    requests <start-requests>`, allowing you to tell start requests apart from\n    other requests, e.g. in :ref:`downloader middlewares\n    <topics-downloader-middleware>`.\n    \"\"\"\n\n    def get_processed_request(\n        self, request: Request, response: Response | None\n    ) -> Request | None:\n        if response is None:\n            request.meta.setdefault(\"is_start_request\", True)\n        return request\n", "n_tokens": 192, "byte_len": 868, "file_sha1": "e9fb7bc5e4654bb50fd65662f074c0500f00e078", "start_line": 1, "end_line": 32}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/depth.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/depth.py", "rel_path": "scrapy/spidermiddlewares/depth.py", "module": "scrapy.spidermiddlewares.depth", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_spider_output", "_init_depth", "get_processed_request", "DepthMiddleware", "dept", "stat", "async", "bool", "inc", "value", "middleware", "verbose", "stats", "case", "python", "spider", "requrl", "warn", "future", "typ", "checking", "request", "depth", "debug", "collector", "settings", "none", "docs", "init", "http", "response", "requests", "process", "ignoring", "priority", "typing", "extensions", "return", "annotations", "class", "meta", "classmethod", "get", "processed", "decorators", "from", "crawler", "spidermiddlewares"], "ast_kind": "class_or_type", "text": "\"\"\"\nDepth Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.spidermiddlewares.base import BaseSpiderMiddleware\nfrom scrapy.utils.decorators import _warn_spider_arg\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Request, Response\n    from scrapy.statscollectors import StatsCollector\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DepthMiddleware(BaseSpiderMiddleware):\n    crawler: Crawler\n\n    def __init__(  # pylint: disable=super-init-not-called\n        self,\n        maxdepth: int,\n        stats: StatsCollector,\n        verbose_stats: bool = False,\n        prio: int = 1,\n    ):\n        self.maxdepth = maxdepth\n        self.stats = stats\n        self.verbose_stats = verbose_stats\n        self.prio = prio\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        settings = crawler.settings\n        maxdepth = settings.getint(\"DEPTH_LIMIT\")\n        verbose = settings.getbool(\"DEPTH_STATS_VERBOSE\")\n        prio = settings.getint(\"DEPTH_PRIORITY\")\n        assert crawler.stats\n        o = cls(maxdepth, crawler.stats, verbose, prio)\n        o.crawler = crawler\n        return o\n\n    @_warn_spider_arg\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider | None = None\n    ) -> Iterable[Any]:\n        self._init_depth(response)\n        yield from super().process_spider_output(response, result)\n\n    @_warn_spider_arg\n    async def process_spider_output_async(\n        self,\n        response: Response,\n        result: AsyncIterator[Any],\n        spider: Spider | None = None,\n    ) -> AsyncIterator[Any]:\n        self._init_depth(response)\n        async for o in super().process_spider_output_async(response, result):\n            yield o\n\n    def _init_depth(self, response: Response) -> None:\n        # base case (depth=0)\n        if \"depth\" not in response.meta:\n            response.meta[\"depth\"] = 0\n            if self.verbose_stats:\n                self.stats.inc_value(\"request_depth_count/0\")\n\n    def get_processed_request(\n        self, request: Request, response: Response | None\n    ) -> Request | None:\n        if response is None:\n            # start requests\n            return request\n        depth = response.meta[\"depth\"] + 1\n        request.meta[\"depth\"] = depth\n        if self.prio:\n            request.priority -= depth * self.prio\n        if self.maxdepth and depth > self.maxdepth:\n            logger.debug(\n                \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                {\"maxdepth\": self.maxdepth, \"requrl\": request.url},\n                extra={\"spider\": self.crawler.spider},\n            )\n            return None\n        if self.verbose_stats:\n            self.stats.inc_value(f\"request_depth_count/{depth}\")\n        self.stats.max_value(\"request_depth_max\", depth)\n        return request\n", "n_tokens": 688, "byte_len": 3153, "file_sha1": "5c62bb2783521fcf8d0a7e43f4d59b43addf81dd", "start_line": 1, "end_line": 102}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/httperror.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/httperror.py", "rel_path": "scrapy/spidermiddlewares/httperror.py", "module": "scrapy.spidermiddlewares.httperror", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_spider_input", "process_spider_exception", "HttpError", "HttpErrorMiddleware", "bool", "http", "error", "inc", "value", "middleware", "case", "python", "spider", "filtered", "warn", "future", "typ", "checking", "handle", "httpstatus", "elif", "settings", "isinstance", "none", "common", "docs", "httperror", "code", "response", "ignoring", "ignore", "request", "typing", "extensions", "return", "handled", "annotations", "process", "class", "meta", "getlist", "classmethod", "decorators", "from", "crawler", "list", "kwargs", "exceptions"], "ast_kind": "class_or_type", "text": "\"\"\"\nHttpError Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.utils.decorators import _warn_spider_arg\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n    from scrapy.settings import BaseSettings\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HttpError(IgnoreRequest):\n    \"\"\"A non-200 response was filtered\"\"\"\n\n    def __init__(self, response: Response, *args: Any, **kwargs: Any):\n        self.response = response\n        super().__init__(*args, **kwargs)\n\n\nclass HttpErrorMiddleware:\n    crawler: Crawler\n\n    def __init__(self, settings: BaseSettings):\n        self.handle_httpstatus_all: bool = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n        self.handle_httpstatus_list: list[int] = settings.getlist(\n            \"HTTPERROR_ALLOWED_CODES\"\n        )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings)\n        o.crawler = crawler\n        return o\n\n    @_warn_spider_arg\n    def process_spider_input(\n        self, response: Response, spider: Spider | None = None\n    ) -> None:\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if meta.get(\"handle_httpstatus_all\", False):\n            return\n        if \"handle_httpstatus_list\" in meta:\n            allowed_statuses = meta[\"handle_httpstatus_list\"]\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(\n                self.crawler.spider,\n                \"handle_httpstatus_list\",\n                self.handle_httpstatus_list,\n            )\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, \"Ignoring non-200 response\")\n\n    @_warn_spider_arg\n    def process_spider_exception(\n        self, response: Response, exception: Exception, spider: Spider | None = None\n    ) -> Iterable[Any] | None:\n        if isinstance(exception, HttpError):\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(\"httperror/response_ignored_count\")\n            self.crawler.stats.inc_value(\n                f\"httperror/response_ignored_status_count/{response.status}\"\n            )\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {\"response\": response},\n                extra={\"spider\": self.crawler.spider},\n            )\n            return []\n        return None\n", "n_tokens": 589, "byte_len": 2816, "file_sha1": "53cee43d132bfe25da6efa18deabb07f4af8d87d", "start_line": 1, "end_line": 93}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/base.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/spidermiddlewares/base.py", "rel_path": "scrapy/spidermiddlewares/base.py", "module": "scrapy.spidermiddlewares.base", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "from_crawler", "process_start_requests", "process_spider_output", "_get_processed", "get_processed_request", "get_processed_item", "BaseSpiderMiddleware", "method", "processed", "async", "python", "spider", "seeds", "warn", "these", "future", "typ", "checking", "items", "isinstance", "object", "none", "process", "start", "type", "code", "methods", "http", "either", "response", "middlewares", "requests", "typing", "extensions", "asynchronous", "return", "item", "annotations", "class", "output", "ignore", "get", "classmethod", "decorators", "from", "crawler", "being", "spidermiddlewares", "param"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy import Request, Spider\nfrom scrapy.utils.decorators import _warn_spider_arg\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.http import Response\n\n\nclass BaseSpiderMiddleware:\n    \"\"\"Optional base class for spider middlewares.\n\n    .. versionadded:: 2.13\n\n    This class provides helper methods for asynchronous\n    ``process_spider_output()`` and ``process_start()`` methods. Middlewares\n    that don't have either of these methods don't need to use this class.\n\n    You can override the\n    :meth:`~scrapy.spidermiddlewares.base.BaseSpiderMiddleware.get_processed_request`\n    method to add processing code for requests and the\n    :meth:`~scrapy.spidermiddlewares.base.BaseSpiderMiddleware.get_processed_item`\n    method to add processing code for items. These methods take a single\n    request or item from the spider output iterable and return a request or\n    item (the same or a new one), or ``None`` to remove this request or item\n    from the processing.\n    \"\"\"\n\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def process_start_requests(\n        self, start: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        for o in start:\n            if (o := self._get_processed(o, None)) is not None:\n                yield o\n\n    async def process_start(self, start: AsyncIterator[Any]) -> AsyncIterator[Any]:\n        async for o in start:\n            if (o := self._get_processed(o, None)) is not None:\n                yield o\n\n    @_warn_spider_arg\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider | None = None\n    ) -> Iterable[Any]:\n        for o in result:\n            if (o := self._get_processed(o, response)) is not None:\n                yield o\n\n    @_warn_spider_arg\n    async def process_spider_output_async(\n        self,\n        response: Response,\n        result: AsyncIterator[Any],\n        spider: Spider | None = None,\n    ) -> AsyncIterator[Any]:\n        async for o in result:\n            if (o := self._get_processed(o, response)) is not None:\n                yield o\n\n    def _get_processed(self, o: Any, response: Response | None) -> Any:\n        if isinstance(o, Request):\n            return self.get_processed_request(o, response)\n        return self.get_processed_item(o, response)\n\n    def get_processed_request(\n        self, request: Request, response: Response | None\n    ) -> Request | None:\n        \"\"\"Return a processed request from the spider output.\n\n        This method is called with a single request from the start seeds or the\n        spider output. It should return the same or a different request, or\n        ``None`` to ignore it.\n\n        :param request: the input request\n        :type request: :class:`~scrapy.Request` object\n\n        :param response: the response being processed\n        :type response: :class:`~scrapy.http.Response` object or ``None`` for\n            start seeds\n\n        :return: the processed request or ``None``\n        \"\"\"\n        return request\n\n    def get_processed_item(self, item: Any, response: Response | None) -> Any:\n        \"\"\"Return a processed item from the spider output.\n\n        This method is called with a single item from the start seeds or the\n        spider output. It should return the same or a different item, or\n        ``None`` to ignore it.\n\n        :param item: the input item\n        :type item: item object\n\n        :param response: the response being processed\n        :type response: :class:`~scrapy.http.Response` object or ``None`` for\n            start seeds\n\n        :return: the processed item or ``None``\n        \"\"\"\n        return item\n", "n_tokens": 887, "byte_len": 3994, "file_sha1": "458383ac95b44d8532ed712f46e320355e39a1e1", "start_line": 1, "end_line": 117}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/fetch.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/fetch.py", "rel_path": "scrapy/commands/fetch.py", "module": "scrapy.commands.fetch", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "long_desc", "add_options", "_print_headers", "_print_response", "_print_bytes", "run", "Command", "method", "spidercls", "async", "usage", "error", "lib", "w3lib", "print", "headers", "spider", "loader", "bytes", "dont", "filter", "load", "command", "future", "typ", "checking", "handle", "httpstatus", "namespace", "add", "options", "items", "none", "dest", "type", "http", "default", "long", "desc", "response", "values", "url", "typing", "return", "stdout", "annotations", "commands", "class"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport sys\nfrom argparse import Namespace  # noqa: TC003\nfrom typing import TYPE_CHECKING\n\nfrom w3lib.url import is_url\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.spider import DefaultSpider, spidercls_for_request\n\nif TYPE_CHECKING:\n    from argparse import ArgumentParser\n\n    from scrapy import Spider\n\n\nclass Command(ScrapyCommand):\n    def syntax(self) -> str:\n        return \"[options] <url>\"\n\n    def short_desc(self) -> str:\n        return \"Fetch a URL using the Scrapy downloader\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Fetch a URL using the Scrapy downloader and print its content\"\n            \" to stdout. You may want to use --nolog to disable logging\"\n        )\n\n    def add_options(self, parser: ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_argument(\n            \"--headers\",\n            dest=\"headers\",\n            action=\"store_true\",\n            help=\"print response HTTP headers instead of body\",\n        )\n        parser.add_argument(\n            \"--no-redirect\",\n            dest=\"no_redirect\",\n            action=\"store_true\",\n            default=False,\n            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n        )\n\n    def _print_headers(self, headers: dict[bytes, list[bytes]], prefix: bytes) -> None:\n        for key, values in headers.items():\n            for value in values:\n                self._print_bytes(prefix + b\" \" + key + b\": \" + value)\n\n    def _print_response(self, response: Response, opts: Namespace) -> None:\n        if opts.headers:\n            assert response.request\n            self._print_headers(response.request.headers, b\">\")\n            print(\">\")\n            self._print_headers(response.headers, b\"<\")\n        else:\n            self._print_bytes(response.body)\n\n    def _print_bytes(self, bytes_: bytes) -> None:\n        sys.stdout.buffer.write(bytes_ + b\"\\n\")\n\n    def run(self, args: list[str], opts: Namespace) -> None:\n        if len(args) != 1 or not is_url(args[0]):\n            raise UsageError\n        request = Request(\n            args[0],\n            callback=self._print_response,\n            cb_kwargs={\"opts\": opts},\n            dont_filter=True,\n        )\n        # by default, let the framework handle redirects,\n        # i.e. command handles all codes expect 3xx\n        if not opts.no_redirect:\n            request.meta[\"handle_httpstatus_list\"] = SequenceExclude(range(300, 400))\n        else:\n            request.meta[\"handle_httpstatus_all\"] = True\n\n        spidercls: type[Spider] = DefaultSpider\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        else:\n            spidercls = spidercls_for_request(spider_loader, request, spidercls)\n\n        async def start(self):\n            yield request\n\n        spidercls.start = start  # type: ignore[method-assign,attr-defined]\n\n        self.crawler_process.crawl(spidercls)\n        self.crawler_process.start()\n", "n_tokens": 712, "byte_len": 3316, "file_sha1": "3069e35174436892f2a04d9c25a793c1c819258f", "start_line": 1, "end_line": 99}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/crawl.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/crawl.py", "rel_path": "scrapy/commands/crawl.py", "module": "scrapy.commands.crawl", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "run", "Command", "argparse", "short", "desc", "usage", "error", "spider", "bootstrap", "failed", "typing", "supported", "return", "base", "annotations", "commands", "class", "opts", "options", "with", "command", "scrapy", "more", "future", "typ", "checking", "running", "namespace", "spname", "true", "from", "requires", "project", "list", "assert", "spargs", "exceptions", "than", "none", "import", "start", "exitcode", "self", "raise", "crawl", "args", "crawler", "process"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\n\nif TYPE_CHECKING:\n    import argparse\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = True\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Run a spider\"\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        if len(args) < 1:\n            raise UsageError\n        if len(args) > 1:\n            raise UsageError(\n                \"running 'scrapy crawl' with more than one spider is not supported\"\n            )\n        spname = args[0]\n\n        assert self.crawler_process\n        self.crawler_process.crawl(spname, **opts.spargs)\n        self.crawler_process.start()\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1\n", "n_tokens": 205, "byte_len": 913, "file_sha1": "647c0f1a95718c85755bcd310d45032f0eb29ab3", "start_line": 1, "end_line": 35}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/version.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/version.py", "rel_path": "scrapy/commands/version.py", "module": "scrapy.commands.version", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "add_options", "run", "Command", "argparse", "short", "desc", "false", "python", "width", "twisted", "useful", "return", "enabled", "log", "display", "name", "commands", "class", "opts", "action", "command", "scrapy", "add", "argument", "print", "requires", "crawler", "versions", "also", "get", "info", "options", "namespace", "default", "settings", "version", "from", "list", "platform", "verbose", "store", "true", "help", "super", "none", "reports", "utils", "dest"], "ast_kind": "class_or_type", "text": "import argparse\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.utils.versions import get_versions\n\n\nclass Command(ScrapyCommand):\n    requires_crawler_process = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[-v]\"\n\n    def short_desc(self) -> str:\n        return \"Print Scrapy version\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--verbose\",\n            \"-v\",\n            dest=\"verbose\",\n            action=\"store_true\",\n            help=\"also display twisted/python/platform info (useful for bug reports)\",\n        )\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        if opts.verbose:\n            versions = get_versions()\n            width = max(len(n) for (n, _) in versions)\n            for name, version in versions:\n                print(f\"{name:<{width}} : {version}\")\n        else:\n            print(f\"Scrapy {scrapy.__version__}\")\n", "n_tokens": 224, "byte_len": 1044, "file_sha1": "bad86fa22e8c5ebe868639d992d91f93705e5f24", "start_line": 1, "end_line": 36}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/list.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/list.py", "rel_path": "scrapy/commands/list.py", "module": "scrapy.commands.list", "ext": "py", "chunk_number": 1, "symbols": ["short_desc", "run", "Command", "argparse", "short", "desc", "false", "typing", "enabled", "log", "return", "spider", "loader", "list", "args", "annotations", "commands", "class", "opts", "command", "spiders", "scrapy", "future", "typ", "checking", "print", "requires", "crawler", "get", "namespace", "true", "default", "settings", "from", "project", "assert", "spiderloader", "none", "import", "sorted", "self", "available"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.spiderloader import get_spider_loader\n\nif TYPE_CHECKING:\n    import argparse\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    requires_crawler_process = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def short_desc(self) -> str:\n        return \"List available spiders\"\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        assert self.settings is not None\n        spider_loader = get_spider_loader(self.settings)\n        for s in sorted(spider_loader.list()):\n            print(s)\n", "n_tokens": 140, "byte_len": 655, "file_sha1": "72f5b4e1ce1c58ca27f36971cd89ba5213e43d9f", "start_line": 1, "end_line": 25}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/check.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/check.py", "rel_path": "scrapy/commands/check.py", "module": "scrapy.commands.check", "ext": "py", "chunk_number": 1, "symbols": ["printSummary", "syntax", "short_desc", "add_options", "run", "TextTestResult", "Command", "method", "async", "spidercls", "append", "was", "successful", "stream", "spider", "loader", "from", "name", "load", "command", "spiders", "elif", "text", "test", "namespace", "add", "options", "failed", "default", "settings", "print", "summary", "requires", "project", "conman", "spidername", "tested", "methods", "verbose", "items", "them", "stop", "none", "checking", "join", "dest", "plural", "without", "type", "misc"], "ast_kind": "class_or_type", "text": "import argparse\nimport time\nfrom collections import defaultdict\nfrom unittest import TextTestResult as _TextTestResult\nfrom unittest import TextTestRunner\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.contracts import ContractsManager\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import load_object, set_environ\n\n\nclass TextTestResult(_TextTestResult):\n    def printSummary(self, start: float, stop: float) -> None:\n        write = self.stream.write\n        writeln = self.stream.writeln\n\n        run = self.testsRun\n        plural = \"s\" if run != 1 else \"\"\n\n        writeln(self.separator2)\n        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n        writeln()\n\n        infos = []\n        if not self.wasSuccessful():\n            write(\"FAILED\")\n            failed, errored = map(len, (self.failures, self.errors))\n            if failed:\n                infos.append(f\"failures={failed}\")\n            if errored:\n                infos.append(f\"errors={errored}\")\n        else:\n            write(\"OK\")\n\n        if infos:\n            writeln(f\" ({', '.join(infos)})\")\n        else:\n            write(\"\\n\")\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Check spider contracts\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"only list contracts, without checking them\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            default=False,\n            action=\"store_true\",\n            help=\"print contract tests for all spiders\",\n        )\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        # load contracts\n        assert self.settings is not None\n        contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n        conman = ContractsManager(load_object(c) for c in contracts)\n        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n        # contract requests\n        contract_reqs = defaultdict(list)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        async def start(self):\n            for request in conman.from_spider(self, result):\n                yield request\n\n        with set_environ(SCRAPY_CHECK=\"true\"):\n            for spidername in args or spider_loader.list():\n                spidercls = spider_loader.load(spidername)\n                spidercls.start = start  # type: ignore[assignment,method-assign,return-value]\n\n                tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                if opts.list:\n                    for method in tested_methods:\n                        contract_reqs[spidercls.name].append(method)\n                elif tested_methods:\n                    self.crawler_process.crawl(spidercls)\n\n            # start checks\n            if opts.list:\n                for spider, methods in sorted(contract_reqs.items()):\n                    if not methods and not opts.verbose:\n                        continue\n                    print(spider)\n                    for method in sorted(methods):\n                        print(f\"  * {method}\")\n            else:\n                start_time = time.time()\n                self.crawler_process.start()\n                stop = time.time()\n\n                result.printErrors()\n                result.printSummary(start_time, stop)\n                self.exitcode = int(not result.wasSuccessful())\n", "n_tokens": 781, "byte_len": 3906, "file_sha1": "a61b96ccee36f6c510e76e1a1ecdbd6fef183ad0", "start_line": 1, "end_line": 116}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/genspider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/genspider.py", "rel_path": "scrapy/commands/genspider.py", "module": "scrapy.commands.genspider", "ext": "py", "chunk_number": 1, "symbols": ["sanitize_module_name", "extract_domain", "verify_url_scheme", "syntax", "short_desc", "add_options", "run", "_generate_template_variables", "_genspider", "Command", "encoding", "usage", "error", "your", "netloc", "file", "after", "spider", "name", "domain", "doesn", "import", "module", "command", "future", "string", "typ", "checking", "https", "path", "_find_template", "_list_templates", "_spider_exists", "templates_dir", "spidercls", "bool", "loader", "iterdir", "target", "load", "spiders", "created", "get", "genspider", "classname", "namespace", "add", "options", "default", "settings"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport os\nimport shutil\nimport string\nfrom importlib import import_module\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, cast\nfrom urllib.parse import urlparse\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.spiderloader import get_spider_loader\nfrom scrapy.utils.template import render_templatefile, string_camelcase\n\nif TYPE_CHECKING:\n    import argparse\n\n\ndef sanitize_module_name(module_name: str) -> str:\n    \"\"\"Sanitize the given module name, by replacing dashes and points\n    with underscores and prefixing it with a letter if it doesn't start\n    with one\n    \"\"\"\n    module_name = module_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n    if module_name[0] not in string.ascii_letters:\n        module_name = \"a\" + module_name\n    return module_name\n\n\ndef extract_domain(url: str) -> str:\n    \"\"\"Extract domain name from URL string\"\"\"\n    o = urlparse(url)\n    if o.scheme == \"\" and o.netloc == \"\":\n        o = urlparse(\"//\" + url.lstrip(\"/\"))\n    return o.netloc\n\n\ndef verify_url_scheme(url: str) -> str:\n    \"\"\"Check url for scheme and insert https if none found.\"\"\"\n    parsed = urlparse(url)\n    if parsed.scheme == \"\" and parsed.netloc == \"\":\n        parsed = urlparse(\"//\" + url)._replace(scheme=\"https\")\n    return parsed.geturl()\n\n\nclass Command(ScrapyCommand):\n    requires_crawler_process = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <name> <domain>\"\n\n    def short_desc(self) -> str:\n        return \"Generate new spider using pre-defined templates\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"List available templates\",\n        )\n        parser.add_argument(\n            \"-e\",\n            \"--edit\",\n            dest=\"edit\",\n            action=\"store_true\",\n            help=\"Edit spider after creating it\",\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--dump\",\n            dest=\"dump\",\n            metavar=\"TEMPLATE\",\n            help=\"Dump template to standard output\",\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--template\",\n            dest=\"template\",\n            default=\"basic\",\n            help=\"Uses a custom template.\",\n        )\n        parser.add_argument(\n            \"--force\",\n            dest=\"force\",\n            action=\"store_true\",\n            help=\"If the spider already exists, overwrite it with the template\",\n        )\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        assert self.settings is not None\n        if opts.list:\n            self._list_templates()\n            return\n        if opts.dump:\n            template_file = self._find_template(opts.dump)\n            if template_file:\n                print(template_file.read_text(encoding=\"utf-8\"))\n            return\n        if len(args) != 2:\n            raise UsageError\n\n        name, url = args[0:2]\n        url = verify_url_scheme(url)\n        module = sanitize_module_name(name)\n\n        if self.settings.get(\"BOT_NAME\") == module:\n            print(\"Cannot create a spider with the same name as your project\")\n            return\n\n        if not opts.force and self._spider_exists(name):\n            return\n\n        template_file = self._find_template(opts.template)\n        if template_file:\n            self._genspider(module, name, url, opts.template, template_file)\n            if opts.edit:\n                self.exitcode = os.system(f'scrapy edit \"{name}\"')  # noqa: S605\n\n    def _generate_template_variables(\n        self,\n        module: str,\n        name: str,\n        url: str,\n        template_name: str,\n    ) -> dict[str, Any]:\n        assert self.settings is not None\n        capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n        return {\n            \"project_name\": self.settings.get(\"BOT_NAME\"),\n            \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n            \"module\": module,\n            \"name\": name,\n            \"url\": url,\n            \"domain\": extract_domain(url),\n            \"classname\": f\"{capitalized_module}Spider\",\n        }\n\n    def _genspider(\n        self,\n        module: str,\n        name: str,\n        url: str,\n        template_name: str,\n        template_file: str | os.PathLike,\n    ) -> None:\n        \"\"\"Generate the spider module, based on the given template\"\"\"\n        assert self.settings is not None\n        tvars = self._generate_template_variables(module, name, url, template_name)\n        if self.settings.get(\"NEWSPIDER_MODULE\"):\n            spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n            assert spiders_module.__file__\n            spiders_dir = Path(spiders_module.__file__).parent.resolve()\n        else:\n            spiders_module = None\n            spiders_dir = Path()\n        spider_file = f\"{spiders_dir / module}.py\"\n        shutil.copyfile(template_file, spider_file)\n        render_templatefile(spider_file, **tvars)\n        print(\n            f\"Created spider {name!r} using template {template_name!r} \",\n            end=(\"\" if spiders_module else \"\\n\"),\n        )\n        if spiders_module:\n            print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n", "n_tokens": 1165, "byte_len": 5466, "file_sha1": "c914769ba227c8811f25593f194bb9e222410b93", "start_line": 1, "end_line": 171}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/genspider.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/genspider.py", "rel_path": "scrapy/commands/genspider.py", "module": "scrapy.commands.genspider", "ext": "py", "chunk_number": 2, "symbols": ["_find_template", "_list_templates", "_spider_exists", "templates_dir", "spiders", "dir", "unable", "module", "spidercls", "bool", "false", "except", "pass", "command", "spider", "cast", "file", "property", "return", "loader", "newspide", "name", "genspider", "iterdir", "else", "target", "import", "load", "with", "path", "sanitize_module_name", "extract_domain", "verify_url_scheme", "syntax", "short_desc", "add_options", "run", "_generate_template_variables", "_genspider", "Command", "encoding", "usage", "error", "your", "netloc", "after", "domain", "doesn", "future", "string"], "ast_kind": "function_or_method", "text": "    def _find_template(self, template: str) -> Path | None:\n        template_file = Path(self.templates_dir, f\"{template}.tmpl\")\n        if template_file.exists():\n            return template_file\n        print(f\"Unable to find template: {template}\\n\")\n        print('Use \"scrapy genspider --list\" to see all available templates.')\n        return None\n\n    def _list_templates(self) -> None:\n        print(\"Available templates:\")\n        for file in sorted(Path(self.templates_dir).iterdir()):\n            if file.suffix == \".tmpl\":\n                print(f\"  {file.stem}\")\n\n    def _spider_exists(self, name: str) -> bool:\n        assert self.settings is not None\n        if not self.settings.get(\"NEWSPIDER_MODULE\"):\n            # if run as a standalone command and file with same filename already exists\n            path = Path(name + \".py\")\n            if path.exists():\n                print(f\"{path.resolve()} already exists\")\n                return True\n            return False\n\n        spider_loader = get_spider_loader(self.settings)\n        try:\n            spidercls = spider_loader.load(name)\n        except KeyError:\n            pass\n        else:\n            # if spider with same name exists\n            print(f\"Spider {name!r} already exists in module:\")\n            print(f\"  {spidercls.__module__}\")\n            return True\n\n        # a file with the same name exists in the target directory\n        spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n        spiders_dir = Path(cast(\"str\", spiders_module.__file__)).parent\n        spiders_dir_abs = spiders_dir.resolve()\n        path = spiders_dir_abs / (name + \".py\")\n        if path.exists():\n            print(f\"{path} already exists\")\n            return True\n\n        return False\n\n    @property\n    def templates_dir(self) -> str:\n        assert self.settings is not None\n        return str(\n            Path(\n                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n                \"spiders\",\n            )\n        )\n", "n_tokens": 430, "byte_len": 2038, "file_sha1": "c914769ba227c8811f25593f194bb9e222410b93", "start_line": 172, "end_line": 227}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/shell.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/shell.py", "rel_path": "scrapy/commands/shell.py", "module": "scrapy.commands.shell", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "long_desc", "add_options", "update_vars", "run", "_start_crawler_thread", "Command", "method", "spidercls", "spider", "loader", "target", "load", "command", "future", "typ", "checking", "elif", "console", "namespace", "add", "options", "default", "settings", "create", "engine", "persistent", "created", "thread", "kee", "alive", "none", "docs", "html", "dest", "code", "type", "since", "http", "long", "desc", "apply", "argument", "shell", "typing", "return", "update", "annotations", "commands"], "ast_kind": "class_or_type", "text": "\"\"\"\nScrapy Shell\n\nSee documentation in docs/topics/shell.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom threading import Thread\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.http import Request\nfrom scrapy.shell import Shell\nfrom scrapy.utils.defer import _schedule_coro\nfrom scrapy.utils.spider import DefaultSpider, spidercls_for_request\nfrom scrapy.utils.url import guess_scheme\n\nif TYPE_CHECKING:\n    from argparse import ArgumentParser, Namespace\n\n    from scrapy import Spider\n\n\nclass Command(ScrapyCommand):\n    default_settings = {\n        \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n        \"KEEP_ALIVE\": True,\n        \"LOGSTATS_INTERVAL\": 0,\n    }\n\n    def syntax(self) -> str:\n        return \"[url|file]\"\n\n    def short_desc(self) -> str:\n        return \"Interactive scraping console\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Interactive console for scraping the given url or file. \"\n            \"Use ./file.html syntax or full path for local file.\"\n        )\n\n    def add_options(self, parser: ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-c\",\n            dest=\"code\",\n            help=\"evaluate the code in the shell, print the result and exit\",\n        )\n        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_argument(\n            \"--no-redirect\",\n            dest=\"no_redirect\",\n            action=\"store_true\",\n            default=False,\n            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n        )\n\n    def update_vars(self, vars: dict[str, Any]) -> None:  # noqa: A002\n        \"\"\"You can use this function to update the Scrapy objects that will be\n        available in the shell\n        \"\"\"\n\n    def run(self, args: list[str], opts: Namespace) -> None:\n        url = args[0] if args else None\n        if url:\n            # first argument may be a local file\n            url = guess_scheme(url)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        spidercls: type[Spider] = DefaultSpider\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        elif url:\n            spidercls = spidercls_for_request(\n                spider_loader, Request(url), spidercls, log_multiple=True\n            )\n\n        # The crawler is created this way since the Shell manually handles the\n        # crawling engine, so the set up in the crawl method won't work\n        crawler = self.crawler_process._create_crawler(spidercls)\n        crawler._apply_settings()\n        # The Shell class needs a persistent engine in the crawler\n        crawler.engine = crawler._create_engine()\n        _schedule_coro(crawler.engine.start_async(_start_request_processing=False))\n\n        self._start_crawler_thread()\n\n        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n        shell.start(url=url, redirect=not opts.no_redirect)\n\n    def _start_crawler_thread(self) -> None:\n        assert self.crawler_process\n        t = Thread(\n            target=self.crawler_process.start,\n            kwargs={\"stop_after_crawl\": False, \"install_signal_handlers\": False},\n        )\n        t.daemon = True\n        t.start()\n", "n_tokens": 718, "byte_len": 3319, "file_sha1": "e2ba3f94bfe33444fb1f172282b026da123e2435", "start_line": 1, "end_line": 103}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/runspider.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/runspider.py", "rel_path": "scrapy/commands/runspider.py", "module": "scrapy.commands.runspider", "ext": "py", "chunk_number": 1, "symbols": ["_import_file", "syntax", "short_desc", "long_desc", "run", "Command", "spidercls", "usage", "error", "python", "import", "module", "load", "command", "future", "typ", "checking", "path", "namespace", "default", "settings", "spiderloader", "none", "without", "found", "long", "desc", "like", "abspath", "bootstrap", "failed", "spider", "typing", "base", "return", "annotations", "commands", "class", "options", "dirname", "contained", "finally", "list", "exists", "exceptions", "filename", "file", "self", "raise", "types"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport sys\nfrom importlib import import_module\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.spiderloader import DummySpiderLoader\nfrom scrapy.utils.spider import iter_spider_classes\n\nif TYPE_CHECKING:\n    import argparse\n    from os import PathLike\n    from types import ModuleType\n\n\ndef _import_file(filepath: str | PathLike[str]) -> ModuleType:\n    abspath = Path(filepath).resolve()\n    if abspath.suffix not in (\".py\", \".pyw\"):\n        raise ValueError(f\"Not a Python source file: {abspath}\")\n    dirname = str(abspath.parent)\n    sys.path = [dirname, *sys.path]\n    try:\n        module = import_module(abspath.stem)\n    finally:\n        sys.path.pop(0)\n    return module\n\n\nclass Command(BaseRunSpiderCommand):\n    default_settings = {\"SPIDER_LOADER_CLASS\": DummySpiderLoader}\n\n    def syntax(self) -> str:\n        return \"[options] <spider_file>\"\n\n    def short_desc(self) -> str:\n        return \"Run a self-contained spider (without creating a project)\"\n\n    def long_desc(self) -> str:\n        return \"Run the spider defined in the given file\"\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        if len(args) != 1:\n            raise UsageError\n        filename = Path(args[0])\n        if not filename.exists():\n            raise UsageError(f\"File not found: {filename}\\n\")\n        try:\n            module = _import_file(filename)\n        except (ImportError, ValueError) as e:\n            raise UsageError(f\"Unable to load {str(filename)!r}: {e}\\n\")\n        spclasses = list(iter_spider_classes(module))\n        if not spclasses:\n            raise UsageError(f\"No spider found in file: {filename}\\n\")\n        spidercls = spclasses.pop()\n\n        assert self.crawler_process\n        self.crawler_process.crawl(spidercls, **opts.spargs)\n        self.crawler_process.start()\n\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1\n", "n_tokens": 465, "byte_len": 2039, "file_sha1": "6e73248922b0bfc6ec0f8299f489d6713bf28af3", "start_line": 1, "end_line": 65}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/__init__.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/__init__.py", "rel_path": "scrapy/commands/__init__.py", "module": "scrapy.commands.__init__", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "set_crawler", "syntax", "short_desc", "long_desc", "help", "add_options", "process_options", "run", "ScrapyCommand", "BaseRunSpiderCommand", "encoding", "usage", "error", "bool", "file", "append", "extensive", "name", "common", "loglevel", "command", "between", "future", "typ", "checking", "path", "feed", "process", "runspider", "_join_parts", "format_part_strings", "ScrapyHelpFormatter", "max", "underline", "case", "indent", "increment", "namespace", "add", "options", "overwrite", "output", "format", "part", "default", "settings", "requires", "project", "items"], "ast_kind": "class_or_type", "text": "\"\"\"\nBase class for Scrapy commands\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport builtins\nimport os\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nfrom twisted.python import failure\n\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from scrapy.crawler import Crawler, CrawlerProcessBase\n    from scrapy.settings import Settings\n\n\nclass ScrapyCommand(ABC):\n    requires_project: bool = False\n    requires_crawler_process: bool = True\n    crawler_process: CrawlerProcessBase | None = None  # set in scrapy.cmdline\n\n    # default settings to be used for this command instead of global defaults\n    default_settings: dict[str, Any] = {}\n\n    exitcode: int = 0\n\n    def __init__(self) -> None:\n        self.settings: Settings | None = None  # set in scrapy.cmdline\n\n    def set_crawler(self, crawler: Crawler) -> None:\n        if hasattr(self, \"_crawler\"):\n            raise RuntimeError(\"crawler already set\")\n        self._crawler: Crawler = crawler\n\n    def syntax(self) -> str:\n        \"\"\"\n        Command syntax (preferably one-line). Do not include command name.\n        \"\"\"\n        return \"\"\n\n    @abstractmethod\n    def short_desc(self) -> str:\n        \"\"\"\n        A short description of the command\n        \"\"\"\n        return \"\"\n\n    def long_desc(self) -> str:\n        \"\"\"A long description of the command. Return short description when not\n        available. It cannot contain newlines since contents will be formatted\n        by optparser which removes newlines and wraps text.\n        \"\"\"\n        return self.short_desc()\n\n    def help(self) -> str:\n        \"\"\"An extensive help for the command. It will be shown when using the\n        \"help\" command. It can contain newlines since no post-formatting will\n        be applied to its contents.\n        \"\"\"\n        return self.long_desc()\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        \"\"\"\n        Populate option parse with options available for this command\n        \"\"\"\n        assert self.settings is not None\n        group = parser.add_argument_group(title=\"Global Options\")\n        group.add_argument(\n            \"--logfile\", metavar=\"FILE\", help=\"log file. if omitted stderr will be used\"\n        )\n        group.add_argument(\n            \"-L\",\n            \"--loglevel\",\n            metavar=\"LEVEL\",\n            default=None,\n            help=f\"log level (default: {self.settings['LOG_LEVEL']})\",\n        )\n        group.add_argument(\n            \"--nolog\", action=\"store_true\", help=\"disable logging completely\"\n        )\n        group.add_argument(\n            \"--profile\",\n            metavar=\"FILE\",\n            default=None,\n            help=\"write python cProfile stats to FILE\",\n        )\n        group.add_argument(\"--pidfile\", metavar=\"FILE\", help=\"write process ID to FILE\")\n        group.add_argument(\n            \"-s\",\n            \"--set\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set/override setting (may be repeated)\",\n        )\n        group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n\n    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n        assert self.settings is not None\n        try:\n            self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n        except ValueError:\n            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n\n        if opts.logfile:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_FILE\", opts.logfile, priority=\"cmdline\")\n\n        if opts.loglevel:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_LEVEL\", opts.loglevel, priority=\"cmdline\")\n\n        if opts.nolog:\n            self.settings.set(\"LOG_ENABLED\", False, priority=\"cmdline\")\n\n        if opts.pidfile:\n            Path(opts.pidfile).write_text(\n                str(os.getpid()) + os.linesep, encoding=\"utf-8\"\n            )\n\n        if opts.pdb:\n            failure.startDebugMode()\n\n    @abstractmethod\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        \"\"\"\n        Entry point for running commands\n        \"\"\"\n        raise NotImplementedError\n\n\nclass BaseRunSpiderCommand(ScrapyCommand):\n    \"\"\"\n    Common class used to share functionality between the crawl, parse and runspider commands\n    \"\"\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-a\",\n            dest=\"spargs\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set spider argument (may be repeated)\",\n        )\n        parser.add_argument(\n            \"-o\",\n            \"--output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"append scraped items to the end of FILE (use - for stdout),\"\n            \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\",\n        )\n        parser.add_argument(\n            \"-O\",\n            \"--overwrite-output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"dump scraped items into FILE, overwriting any existing file,\"\n            \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\",\n        )\n", "n_tokens": 1170, "byte_len": 5588, "file_sha1": "ca5295f7f10b2c9db68a13316af5f95895352368", "start_line": 1, "end_line": 172}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/__init__.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/__init__.py", "rel_path": "scrapy/commands/__init__.py", "module": "scrapy.commands.__init__", "ext": "py", "chunk_number": 2, "symbols": ["process_options", "__init__", "_join_parts", "format_part_strings", "ScrapyHelpFormatter", "argparse", "usage", "error", "false", "except", "headings", "command", "priority", "max", "help", "width", "case", "feeds", "underline", "value", "arglist", "dict", "return", "commands", "opts", "class", "output", "startswith", "name", "print", "set_crawler", "syntax", "short_desc", "long_desc", "add_options", "run", "ScrapyCommand", "BaseRunSpiderCommand", "encoding", "bool", "file", "append", "extensive", "common", "loglevel", "between", "future", "typ", "checking", "path"], "ast_kind": "class_or_type", "text": "    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n        try:\n            opts.spargs = arglist_to_dict(opts.spargs)\n        except ValueError:\n            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n        if opts.output or opts.overwrite_output:\n            assert self.settings is not None\n            feeds = feed_process_params_from_cli(\n                self.settings,\n                opts.output,\n                overwrite_output=opts.overwrite_output,\n            )\n            self.settings.set(\"FEEDS\", feeds, priority=\"cmdline\")\n\n\nclass ScrapyHelpFormatter(argparse.HelpFormatter):\n    \"\"\"\n    Help Formatter for scrapy command line help messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        prog: str,\n        indent_increment: int = 2,\n        max_help_position: int = 24,\n        width: int | None = None,\n    ):\n        super().__init__(\n            prog,\n            indent_increment=indent_increment,\n            max_help_position=max_help_position,\n            width=width,\n        )\n\n    def _join_parts(self, part_strings: Iterable[str]) -> str:\n        # scrapy.commands.list shadows builtins.list\n        parts = self.format_part_strings(builtins.list(part_strings))\n        return super()._join_parts(parts)\n\n    def format_part_strings(self, part_strings: list[str]) -> list[str]:\n        \"\"\"\n        Underline and title case command line help message headers.\n        \"\"\"\n        if part_strings and part_strings[0].startswith(\"usage: \"):\n            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len(\"usage: \") :]\n        headings = [\n            i for i in range(len(part_strings)) if part_strings[i].endswith(\":\\n\")\n        ]\n        for index in headings[::-1]:\n            char = \"-\" if \"Global Options\" in part_strings[index] else \"=\"\n            part_strings[index] = part_strings[index][:-2].title()\n            underline = \"\".join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n            part_strings.insert(index + 1, underline)\n        return part_strings\n", "n_tokens": 455, "byte_len": 2117, "file_sha1": "ca5295f7f10b2c9db68a13316af5f95895352368", "start_line": 173, "end_line": 228}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/edit.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/edit.py", "rel_path": "scrapy/commands/edit.py", "module": "scrapy.commands.edit", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "long_desc", "_err", "run", "Command", "argparse", "spidercls", "short", "desc", "usage", "error", "false", "editor", "except", "spider", "file", "enabled", "log", "return", "loader", "replace", "commands", "class", "opts", "modules", "load", "command", "defined", "scrapy", "module", "requires", "crawler", "get", "stderr", "variable", "namespace", "true", "default", "settings", "key", "from", "project", "list", "err", "system", "assert", "noqa", "spiderloader", "exceptions"], "ast_kind": "class_or_type", "text": "import argparse\nimport os\nimport sys\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.spiderloader import get_spider_loader\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    requires_crawler_process = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"<spider>\"\n\n    def short_desc(self) -> str:\n        return \"Edit spider\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Edit a spider using the editor defined in the EDITOR environment\"\n            \" variable or else the EDITOR setting\"\n        )\n\n    def _err(self, msg: str) -> None:\n        sys.stderr.write(msg + os.linesep)\n        self.exitcode = 1\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        if len(args) != 1:\n            raise UsageError\n\n        assert self.settings is not None\n        editor = self.settings[\"EDITOR\"]\n        spider_loader = get_spider_loader(self.settings)\n        try:\n            spidercls = spider_loader.load(args[0])\n        except KeyError:\n            self._err(f\"Spider not found: {args[0]}\")\n            return\n\n        sfile = sys.modules[spidercls.__module__].__file__\n        assert sfile\n        sfile = sfile.replace(\".pyc\", \".py\")\n        self.exitcode = os.system(f'{editor} \"{sfile}\"')  # noqa: S605\n", "n_tokens": 321, "byte_len": 1358, "file_sha1": "6267a048014316b5f788128639636b46b6a680ba", "start_line": 1, "end_line": 48}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/bench.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/bench.py", "rel_path": "scrapy/commands/bench.py", "module": "scrapy.commands.bench", "ext": "py", "chunk_number": 1, "symbols": ["short_desc", "run", "__enter__", "__exit__", "parse", "Command", "_BenchServer", "_BenchSpider", "async", "traceback", "spider", "name", "sleep", "dont", "filter", "command", "future", "typ", "checking", "enter", "namespace", "default", "settings", "isinstance", "link", "extractor", "none", "localhost", "bench", "server", "http", "response", "quick", "s603", "get", "testenv", "typing", "return", "stdout", "annotations", "commands", "class", "level", "log", "follows", "extract", "links", "noqa", "list", "info"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport subprocess\nimport sys\nimport time\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import urlencode\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.utils.test import get_testenv\n\nif TYPE_CHECKING:\n    import argparse\n    from collections.abc import AsyncIterator\n\n\nclass Command(ScrapyCommand):\n    default_settings = {\n        \"LOG_LEVEL\": \"INFO\",\n        \"LOGSTATS_INTERVAL\": 1,\n        \"CLOSESPIDER_TIMEOUT\": 10,\n    }\n\n    def short_desc(self) -> str:\n        return \"Run quick benchmark test\"\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        with _BenchServer():\n            assert self.crawler_process\n            self.crawler_process.crawl(_BenchSpider, total=100000)\n            self.crawler_process.start()\n\n\nclass _BenchServer:\n    def __enter__(self) -> None:\n        pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n        self.proc = subprocess.Popen(  # noqa: S603\n            pargs, stdout=subprocess.PIPE, env=get_testenv()\n        )\n        assert self.proc.stdout\n        self.proc.stdout.readline()\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:\n        self.proc.kill()\n        self.proc.wait()\n        time.sleep(0.2)\n\n\nclass _BenchSpider(scrapy.Spider):\n    \"\"\"A spider that follows all links\"\"\"\n\n    name = \"follow\"\n    total = 10000\n    show = 20\n    baseurl = \"http://localhost:8998\"\n    link_extractor = LinkExtractor()\n\n    async def start(self) -> AsyncIterator[Any]:\n        qargs = {\"total\": self.total, \"show\": self.show}\n        url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n        yield scrapy.Request(url, dont_filter=True)\n\n    def parse(self, response: Response) -> Any:\n        assert isinstance(response, TextResponse)\n        for link in self.link_extractor.extract_links(response):\n            yield scrapy.Request(link.url, callback=self.parse)\n", "n_tokens": 472, "byte_len": 2027, "file_sha1": "6c3a11458516c2c9bf2d6678b58a0e4a506538e3", "start_line": 1, "end_line": 70}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/view.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/view.py", "rel_path": "scrapy/commands/view.py", "module": "scrapy.commands.view", "ext": "py", "chunk_number": 1, "symbols": ["short_desc", "long_desc", "add_options", "_print_response", "Command", "fetch", "text", "response", "downloader", "argparse", "view", "short", "desc", "contents", "browser", "seen", "return", "name", "commands", "class", "opts", "show", "command", "open", "scrapy", "logger", "add", "argument", "headers", "print", "get", "cannot", "logging", "namespace", "options", "from", "isinstance", "suppress", "help", "super", "none", "utils", "import", "http", "self", "long", "parser", "error", "using"], "ast_kind": "class_or_type", "text": "import argparse\nimport logging\n\nfrom scrapy.commands import fetch\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.utils.response import open_in_browser\n\nlogger = logging.getLogger(__name__)\n\n\nclass Command(fetch.Command):\n    def short_desc(self) -> str:\n        return \"Open URL in browser, as seen by Scrapy\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n        )\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\"--headers\", help=argparse.SUPPRESS)\n\n    def _print_response(self, response: Response, opts: argparse.Namespace) -> None:\n        if not isinstance(response, TextResponse):\n            logger.error(\"Cannot view a non-text response.\")\n            return\n        open_in_browser(response)\n", "n_tokens": 181, "byte_len": 892, "file_sha1": "f55f540c08c1e89c2c0fe671d4f6511a530cb6a5", "start_line": 1, "end_line": 29}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/settings.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/settings.py", "rel_path": "scrapy/commands/settings.py", "module": "scrapy.commands.settings", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "add_options", "run", "Command", "dumps", "argparse", "short", "desc", "false", "getbool", "getfloat", "copy", "dict", "return", "enabled", "log", "setting", "commands", "class", "getint", "options", "getlist", "opts", "json", "command", "scrapy", "add", "argument", "print", "elif", "requires", "crawler", "value", "namespace", "default", "settings", "integer", "from", "interpreted", "list", "base", "assert", "isinstance", "help", "super", "none", "float", "dest", "import"], "ast_kind": "class_or_type", "text": "import argparse\nimport json\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.settings import BaseSettings\n\n\nclass Command(ScrapyCommand):\n    requires_crawler_process = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options]\"\n\n    def short_desc(self) -> str:\n        return \"Get settings values\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--get\", dest=\"get\", metavar=\"SETTING\", help=\"print raw setting value\"\n        )\n        parser.add_argument(\n            \"--getbool\",\n            dest=\"getbool\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a boolean\",\n        )\n        parser.add_argument(\n            \"--getint\",\n            dest=\"getint\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as an integer\",\n        )\n        parser.add_argument(\n            \"--getfloat\",\n            dest=\"getfloat\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a float\",\n        )\n        parser.add_argument(\n            \"--getlist\",\n            dest=\"getlist\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a list\",\n        )\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        assert self.settings is not None\n        settings = self.settings\n        if opts.get:\n            s = settings.get(opts.get)\n            if isinstance(s, BaseSettings):\n                print(json.dumps(s.copy_to_dict()))\n            else:\n                print(s)\n        elif opts.getbool:\n            print(settings.getbool(opts.getbool))\n        elif opts.getint:\n            print(settings.getint(opts.getint))\n        elif opts.getfloat:\n            print(settings.getfloat(opts.getfloat))\n        elif opts.getlist:\n            print(settings.getlist(opts.getlist))\n", "n_tokens": 392, "byte_len": 1986, "file_sha1": "89320400834a7b2e41e3e13666310ac110789ed2", "start_line": 1, "end_line": 65}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/parse.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/parse.py", "rel_path": "scrapy/commands/parse.py", "module": "scrapy.commands.parse", "ext": "py", "chunk_number": 1, "symbols": ["syntax", "short_desc", "add_options", "max_level", "handle_exception", "iterate_spider_output", "add_items", "add_requests", "print_items", "Command", "max", "requests", "failure", "parse", "items", "spidercls", "while", "async", "usage", "error", "bool", "new", "reqs", "colour", "each", "lib", "w3lib", "coroutine", "spider", "rules", "print_requests", "print_results", "_get_items_and_requests", "run_callback", "get_callback_from_rules", "set_spidercls", "start_parsing", "scraped_data", "_get_callback", "prepare_request", "callback", "process_options", "process_request_meta", "process_request_cb_kwargs", "run", "method", "loads", "append", "itemproc", "were"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport functools\nimport inspect\nimport json\nimport logging\nfrom typing import TYPE_CHECKING, Any, TypeVar, overload\n\nfrom itemadapter import ItemAdapter\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom w3lib.url import is_url\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.http import Request, Response\nfrom scrapy.utils import display\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.defer import aiter_errback, deferred_from_coro\nfrom scrapy.utils.deprecate import argument_is_required\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.spider import spidercls_for_request\n\nif TYPE_CHECKING:\n    import argparse\n    from collections.abc import AsyncGenerator, AsyncIterator, Coroutine, Iterable\n\n    from twisted.python.failure import Failure\n\n    from scrapy.http.request import CallbackT\n    from scrapy.spiders import Spider\n\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = True\n\n    spider: Spider | None = None\n    items: dict[int, list[Any]] = {}\n    requests: dict[int, list[Request]] = {}\n    spidercls: type[Spider] | None\n\n    first_response = None\n\n    def syntax(self) -> str:\n        return \"[options] <url>\"\n\n    def short_desc(self) -> str:\n        return \"Parse URL (using its spider) and print the results\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--spider\",\n            dest=\"spider\",\n            default=None,\n            help=\"use this spider without looking for one\",\n        )\n        parser.add_argument(\n            \"--pipelines\", action=\"store_true\", help=\"process items through pipelines\"\n        )\n        parser.add_argument(\n            \"--nolinks\",\n            dest=\"nolinks\",\n            action=\"store_true\",\n            help=\"don't show links to follow (extracted requests)\",\n        )\n        parser.add_argument(\n            \"--noitems\",\n            dest=\"noitems\",\n            action=\"store_true\",\n            help=\"don't show scraped items\",\n        )\n        parser.add_argument(\n            \"--nocolour\",\n            dest=\"nocolour\",\n            action=\"store_true\",\n            help=\"avoid using pygments to colorize the output\",\n        )\n        parser.add_argument(\n            \"-r\",\n            \"--rules\",\n            dest=\"rules\",\n            action=\"store_true\",\n            help=\"use CrawlSpider rules to discover the callback\",\n        )\n        parser.add_argument(\n            \"-c\",\n            \"--callback\",\n            dest=\"callback\",\n            help=\"use this callback for parsing, instead looking for a callback\",\n        )\n        parser.add_argument(\n            \"-m\",\n            \"--meta\",\n            dest=\"meta\",\n            help=\"inject extra meta into the Request, it must be a valid raw json string\",\n        )\n        parser.add_argument(\n            \"--cbkwargs\",\n            dest=\"cbkwargs\",\n            help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\",\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--depth\",\n            dest=\"depth\",\n            type=int,\n            default=1,\n            help=\"maximum depth for parsing requests [default: %(default)s]\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            action=\"store_true\",\n            help=\"print each depth level one by one\",\n        )\n\n    @property\n    def max_level(self) -> int:\n        max_items, max_requests = 0, 0\n        if self.items:\n            max_items = max(self.items)\n        if self.requests:\n            max_requests = max(self.requests)\n        return max(max_items, max_requests)\n\n    def handle_exception(self, _failure: Failure) -> None:\n        logger.error(\n            \"An error is caught while iterating the async iterable\",\n            exc_info=failure_to_exc_info(_failure),\n        )\n\n    @overload\n    def iterate_spider_output(\n        self, result: AsyncGenerator[_T] | Coroutine[Any, Any, _T]\n    ) -> Deferred[_T]: ...\n\n    @overload\n    def iterate_spider_output(self, result: _T) -> Iterable[Any]: ...\n\n    def iterate_spider_output(self, result: Any) -> Iterable[Any] | Deferred[Any]:\n        if inspect.isasyncgen(result):\n            d = deferred_from_coro(\n                collect_asyncgen(aiter_errback(result, self.handle_exception))\n            )\n            d.addCallback(self.iterate_spider_output)\n            return d\n        if inspect.iscoroutine(result):\n            d = deferred_from_coro(result)\n            d.addCallback(self.iterate_spider_output)\n            return d\n        return arg_to_iter(deferred_from_coro(result))\n\n    def add_items(self, lvl: int, new_items: list[Any]) -> None:\n        old_items = self.items.get(lvl, [])\n        self.items[lvl] = old_items + new_items\n\n    def add_requests(self, lvl: int, new_reqs: list[Request]) -> None:\n        old_reqs = self.requests.get(lvl, [])\n        self.requests[lvl] = old_reqs + new_reqs\n\n    def print_items(self, lvl: int | None = None, colour: bool = True) -> None:\n        if lvl is None:\n            items = [item for lst in self.items.values() for item in lst]\n        else:\n            items = self.items.get(lvl, [])\n\n        print(\"# Scraped Items \", \"-\" * 60)\n        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n", "n_tokens": 1207, "byte_len": 5593, "file_sha1": "82a367070600df3e2268a49f23396746fd252059", "start_line": 1, "end_line": 176}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/parse.py#2", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/parse.py", "rel_path": "scrapy/commands/parse.py", "module": "scrapy.commands.parse", "ext": "py", "chunk_number": 2, "symbols": ["print_requests", "print_results", "_get_items_and_requests", "run_callback", "get_callback_from_rules", "set_spidercls", "start_parsing", "scraped_data", "method", "spidercls", "async", "bool", "append", "colour", "itemproc", "spider", "loader", "name", "load", "rules", "elif", "requests", "namespace", "nocolour", "maybe", "deferred", "verbose", "items", "isinstance", "none", "syntax", "short_desc", "add_options", "max_level", "handle_exception", "iterate_spider_output", "add_items", "add_requests", "print_items", "_get_callback", "prepare_request", "callback", "process_options", "process_request_meta", "process_request_cb_kwargs", "run", "Command", "max", "failure", "parse"], "ast_kind": "function_or_method", "text": "    def print_requests(self, lvl: int | None = None, colour: bool = True) -> None:\n        if lvl is not None:\n            requests = self.requests.get(lvl, [])\n        elif self.requests:\n            requests = self.requests[max(self.requests)]\n        else:\n            requests = []\n\n        print(\"# Requests \", \"-\" * 65)\n        display.pprint(requests, colorize=colour)\n\n    def print_results(self, opts: argparse.Namespace) -> None:\n        colour = not opts.nocolour\n\n        if opts.verbose:\n            for level in range(1, self.max_level + 1):\n                print(f\"\\n>>> DEPTH LEVEL: {level} <<<\")\n                if not opts.noitems:\n                    self.print_items(level, colour)\n                if not opts.nolinks:\n                    self.print_requests(level, colour)\n        else:\n            print(f\"\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<\")\n            if not opts.noitems:\n                self.print_items(colour=colour)\n            if not opts.nolinks:\n                self.print_requests(colour=colour)\n\n    def _get_items_and_requests(\n        self,\n        spider_output: Iterable[Any],\n        opts: argparse.Namespace,\n        depth: int,\n        spider: Spider,\n        callback: CallbackT,\n    ) -> tuple[list[Any], list[Request], argparse.Namespace, int, Spider, CallbackT]:\n        items, requests = [], []\n        for x in spider_output:\n            if isinstance(x, Request):\n                requests.append(x)\n            else:\n                items.append(x)\n        return items, requests, opts, depth, spider, callback\n\n    def run_callback(\n        self,\n        response: Response,\n        callback: CallbackT,\n        cb_kwargs: dict[str, Any] | None = None,\n    ) -> Deferred[Any]:\n        cb_kwargs = cb_kwargs or {}\n        return maybeDeferred(\n            self.iterate_spider_output, callback(response, **cb_kwargs)\n        )\n\n    def get_callback_from_rules(\n        self, spider: Spider, response: Response\n    ) -> CallbackT | str | None:\n        if getattr(spider, \"rules\", None):\n            for rule in spider.rules:  # type: ignore[attr-defined]\n                if rule.link_extractor.matches(response.url):\n                    return rule.callback or \"parse\"\n        else:\n            logger.error(\n                \"No CrawlSpider rules found in spider %(spider)r, \"\n                \"please specify a callback to use for parsing\",\n                {\"spider\": spider.name},\n            )\n        return None\n\n    def set_spidercls(self, url: str, opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            try:\n                self.spidercls = spider_loader.load(opts.spider)\n            except KeyError:\n                logger.error(\n                    \"Unable to find spider: %(spider)s\", {\"spider\": opts.spider}\n                )\n        else:\n            self.spidercls = spidercls_for_request(spider_loader, Request(url))\n            if not self.spidercls:\n                logger.error(\"Unable to find spider for: %(url)s\", {\"url\": url})\n\n        async def start(spider: Spider) -> AsyncIterator[Any]:\n            yield self.prepare_request(spider, Request(url), opts)\n\n        if self.spidercls:\n            self.spidercls.start = start  # type: ignore[assignment,method-assign]\n\n    def start_parsing(self, url: str, opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        assert self.spidercls\n        self.crawler_process.crawl(self.spidercls, **opts.spargs)\n        self.pcrawler = next(iter(self.crawler_process.crawlers))\n        self.crawler_process.start()\n\n        if not self.first_response:\n            logger.error(\"No response downloaded for: %(url)s\", {\"url\": url})\n\n    def scraped_data(\n        self,\n        args: tuple[\n            list[Any], list[Request], argparse.Namespace, int, Spider, CallbackT\n        ],\n    ) -> list[Any]:\n        items, requests, opts, depth, spider, callback = args\n        if opts.pipelines:\n            assert self.pcrawler.engine\n            itemproc = self.pcrawler.engine.scraper.itemproc\n            needs_spider = argument_is_required(itemproc.process_item, \"spider\")\n            for item in items:\n                if needs_spider:\n                    itemproc.process_item(item, spider)\n                else:\n                    itemproc.process_item(item)\n        self.add_items(depth, items)\n        self.add_requests(depth, requests)\n\n        scraped_data = items if opts.output else []\n        if depth < opts.depth:\n            for req in requests:\n                req.meta[\"_depth\"] = depth + 1\n                req.meta[\"_callback\"] = req.callback\n                req.callback = callback\n            scraped_data += requests\n\n        return scraped_data\n", "n_tokens": 1017, "byte_len": 4817, "file_sha1": "82a367070600df3e2268a49f23396746fd252059", "start_line": 177, "end_line": 306}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/parse.py#3", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/parse.py", "rel_path": "scrapy/commands/parse.py", "module": "scrapy.commands.parse", "ext": "py", "chunk_number": 3, "symbols": ["_get_callback", "prepare_request", "callback", "process_options", "process_request_meta", "process_request_cb_kwargs", "run", "spidercls", "usage", "error", "loads", "were", "spider", "name", "rules", "passed", "elif", "memorize", "string", "namespace", "items", "method", "cbkwargs", "process", "request", "none", "prepare", "update", "wrapper", "parse", "syntax", "short_desc", "add_options", "max_level", "handle_exception", "iterate_spider_output", "add_items", "add_requests", "print_items", "print_requests", "print_results", "_get_items_and_requests", "run_callback", "get_callback_from_rules", "set_spidercls", "start_parsing", "scraped_data", "Command", "max", "requests"], "ast_kind": "function_or_method", "text": "    def _get_callback(\n        self,\n        *,\n        spider: Spider,\n        opts: argparse.Namespace,\n        response: Response | None = None,\n    ) -> CallbackT:\n        cb: str | CallbackT | None = None\n        if response:\n            cb = response.meta[\"_callback\"]\n        if not cb:\n            if opts.callback:\n                cb = opts.callback\n            elif response and opts.rules and self.first_response == response:\n                cb = self.get_callback_from_rules(spider, response)\n                if not cb:\n                    raise ValueError(\n                        f\"Cannot find a rule that matches {response.url!r} in spider: \"\n                        f\"{spider.name}\"\n                    )\n            else:\n                cb = \"parse\"\n\n        if not callable(cb):\n            assert cb is not None\n            cb_method = getattr(spider, cb, None)\n            if callable(cb_method):\n                cb = cb_method\n            else:\n                raise ValueError(\n                    f\"Cannot find callback {cb!r} in spider: {spider.name}\"\n                )\n        assert callable(cb)\n        return cb\n\n    def prepare_request(\n        self, spider: Spider, request: Request, opts: argparse.Namespace\n    ) -> Request:\n        def callback(response: Response, **cb_kwargs: Any) -> Deferred[list[Any]]:\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            cb = self._get_callback(spider=spider, opts=opts, response=response)\n\n            # parse items and requests\n            depth: int = response.meta[\"_depth\"]\n\n            d = self.run_callback(response, cb, cb_kwargs)\n            d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)\n            d.addCallback(self.scraped_data)\n            return d\n\n        # update request meta if any extra meta was passed through the --meta/-m opts.\n        if opts.meta:\n            request.meta.update(opts.meta)\n\n        # update cb_kwargs if any extra values were was passed through the --cbkwargs option.\n        if opts.cbkwargs:\n            request.cb_kwargs.update(opts.cbkwargs)\n\n        request.meta[\"_depth\"] = 1\n        request.meta[\"_callback\"] = request.callback\n        if not request.callback and not opts.rules:\n            cb = self._get_callback(spider=spider, opts=opts)\n            functools.update_wrapper(callback, cb)\n        request.callback = callback\n        return request\n\n    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n\n        self.process_request_meta(opts)\n        self.process_request_cb_kwargs(opts)\n\n    def process_request_meta(self, opts: argparse.Namespace) -> None:\n        if opts.meta:\n            try:\n                opts.meta = json.loads(opts.meta)\n            except ValueError:\n                raise UsageError(\n                    \"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n                    'Example: --meta=\\'{\"foo\" : \"bar\"}\\'',\n                    print_help=False,\n                )\n\n    def process_request_cb_kwargs(self, opts: argparse.Namespace) -> None:\n        if opts.cbkwargs:\n            try:\n                opts.cbkwargs = json.loads(opts.cbkwargs)\n            except ValueError:\n                raise UsageError(\n                    \"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n                    'Example: --cbkwargs=\\'{\"foo\" : \"bar\"}\\'',\n                    print_help=False,\n                )\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        # parse arguments\n        if not len(args) == 1 or not is_url(args[0]):\n            raise UsageError\n        url = args[0]\n\n        # prepare spidercls\n        self.set_spidercls(url, opts)\n\n        if self.spidercls and opts.depth > 0:\n            self.start_parsing(url, opts)\n            self.print_results(opts)\n", "n_tokens": 814, "byte_len": 3976, "file_sha1": "82a367070600df3e2268a49f23396746fd252059", "start_line": 307, "end_line": 416}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/startproject.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/scrapy/commands/startproject.py", "rel_path": "scrapy/commands/startproject.py", "module": "scrapy.commands.startproject", "ext": "py", "chunk_number": 1, "symbols": ["_make_writable", "syntax", "short_desc", "_is_valid_name", "_module_exists", "_copytree", "run", "templates_dir", "Command", "usage", "error", "bool", "stat", "your", "case", "spec", "name", "template", "iterdir", "command", "future", "string", "typ", "checking", "elif", "path", "https", "begin", "numbers", "pull", "namespace", "default", "settings", "letter", "tmpl", "items", "created", "none", "parents", "dir", "since", "loader", "chmod", "paths", "camelcase", "spider", "module", "exists", "typing", "render"], "ast_kind": "class_or_type", "text": "from __future__ import annotations\n\nimport re\nimport string\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom shutil import copy2, copystat, ignore_patterns, move\nfrom stat import S_IWUSR as OWNER_WRITE_PERMISSION\nfrom typing import TYPE_CHECKING\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.template import render_templatefile, string_camelcase\n\nif TYPE_CHECKING:\n    import argparse\n\nTEMPLATES_TO_RENDER: tuple[tuple[str, ...], ...] = (\n    (\"scrapy.cfg\",),\n    (\"${project_name}\", \"settings.py.tmpl\"),\n    (\"${project_name}\", \"items.py.tmpl\"),\n    (\"${project_name}\", \"pipelines.py.tmpl\"),\n    (\"${project_name}\", \"middlewares.py.tmpl\"),\n)\n\nIGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n\n\ndef _make_writable(path: Path) -> None:\n    current_permissions = path.stat().st_mode\n    path.chmod(current_permissions | OWNER_WRITE_PERMISSION)\n\n\nclass Command(ScrapyCommand):\n    requires_crawler_process = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"<project_name> [project_dir]\"\n\n    def short_desc(self) -> str:\n        return \"Create new project\"\n\n    def _is_valid_name(self, project_name: str) -> bool:\n        def _module_exists(module_name: str) -> bool:\n            spec = find_spec(module_name)\n            return spec is not None and spec.loader is not None\n\n        if not re.search(r\"^[_a-zA-Z]\\w*$\", project_name):\n            print(\n                \"Error: Project names must begin with a letter and contain\"\n                \" only\\nletters, numbers and underscores\"\n            )\n        elif _module_exists(project_name):\n            print(f\"Error: Module {project_name!r} already exists\")\n        else:\n            return True\n        return False\n\n    def _copytree(self, src: Path, dst: Path) -> None:\n        \"\"\"\n        Since the original function always creates the directory, to resolve\n        the issue a new function had to be created. It's a simple copy and\n        was reduced for this case.\n\n        More info at:\n        https://github.com/scrapy/scrapy/pull/2005\n        \"\"\"\n        ignore = IGNORE\n        names = [x.name for x in src.iterdir()]\n        ignored_names = ignore(src, names)\n\n        if not dst.exists():\n            dst.mkdir(parents=True)\n\n        for name in names:\n            if name in ignored_names:\n                continue\n\n            srcname = src / name\n            dstname = dst / name\n            if srcname.is_dir():\n                self._copytree(srcname, dstname)\n            else:\n                copy2(srcname, dstname)\n                _make_writable(dstname)\n\n        copystat(src, dst)\n        _make_writable(dst)\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        if len(args) not in (1, 2):\n            raise UsageError\n\n        project_name = args[0]\n\n        project_dir = Path(args[-1])\n\n        if (project_dir / \"scrapy.cfg\").exists():\n            self.exitcode = 1\n            print(f\"Error: scrapy.cfg already exists in {project_dir.resolve()}\")\n            return\n\n        if not self._is_valid_name(project_name):\n            self.exitcode = 1\n            return\n\n        self._copytree(Path(self.templates_dir), project_dir.resolve())\n        move(project_dir / \"module\", project_dir / project_name)\n        for paths in TEMPLATES_TO_RENDER:\n            tplfile = Path(\n                project_dir,\n                *(\n                    string.Template(s).substitute(project_name=project_name)\n                    for s in paths\n                ),\n            )\n            render_templatefile(\n                tplfile,\n                project_name=project_name,\n                ProjectName=string_camelcase(project_name),\n            )\n        print(\n            f\"New Scrapy project '{project_name}', using template directory \"\n            f\"'{self.templates_dir}', created in:\"\n        )\n        print(f\"    {project_dir.resolve()}\\n\")\n        print(\"You can start your first spider with:\")\n        print(f\"    cd {project_dir}\")\n        print(\"    scrapy genspider example example.com\")\n\n    @property\n    def templates_dir(self) -> str:\n        assert self.settings is not None\n        return str(\n            Path(\n                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n                \"project\",\n            )\n        )\n", "n_tokens": 972, "byte_len": 4418, "file_sha1": "6339e2153819177213c582cc65b05131f624a90c", "start_line": 1, "end_line": 142}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/extras/qps-bench-server.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/extras/qps-bench-server.py", "rel_path": "extras/qps-bench-server.py", "module": "extras.qps-bench-server", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_reset_stats", "getChild", "render", "_finish", "Root", "get", "child", "finish", "disconnected", "root", "factory", "internet", "python", "resource", "twisted", "return", "stats", "latency", "name", "listen", "tcp", "class", "time", "restarts", "collections", "print", "caused", "iter", "deque", "call", "later", "times", "init", "site", "noqa", "from", "clear", "delta", "maxlen", "client", "reset", "seconds", "don", "yet", "samplesize", "reactor", "server", "tail", "concurrent"], "ast_kind": "class_or_type", "text": "#!/usr/bin/env python\nfrom collections import deque\nfrom time import time\n\nfrom twisted.internet import reactor  # noqa: TID253\nfrom twisted.web.resource import Resource\nfrom twisted.web.server import NOT_DONE_YET, Site\n\n\nclass Root(Resource):\n    def __init__(self):\n        Resource.__init__(self)\n        self.concurrent = 0\n        self.tail = deque(maxlen=100)\n        self._reset_stats()\n\n    def _reset_stats(self):\n        self.tail.clear()\n        self.start = self.lastmark = self.lasttime = time()\n\n    def getChild(self, request, name):\n        return self\n\n    def render(self, request):\n        now = time()\n        delta = now - self.lasttime\n\n        # reset stats on high iter-request times caused by client restarts\n        if delta > 3:  # seconds\n            self._reset_stats()\n            return \"\"\n\n        self.tail.appendleft(delta)\n        self.lasttime = now\n        self.concurrent += 1\n\n        if now - self.lastmark >= 3:\n            self.lastmark = now\n            qps = len(self.tail) / sum(self.tail)\n            print(\n                f\"samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}\"\n            )\n\n        if \"latency\" in request.args:\n            latency = float(request.args[\"latency\"][0])\n            reactor.callLater(latency, self._finish, request)\n            return NOT_DONE_YET\n\n        self.concurrent -= 1\n        return \"\"\n\n    def _finish(self, request):\n        self.concurrent -= 1\n        if not request.finished and not request._disconnected:\n            request.finish()\n\n\nroot = Root()\nfactory = Site(root)\nreactor.listenTCP(8880, factory)\nreactor.run()\n", "n_tokens": 373, "byte_len": 1634, "file_sha1": "f088f6ff8bb2708a11750cb88cc00ddc22f13eec", "start_line": 1, "end_line": 62}
{"id": "/Users/zack.alatrash/Downloads/scrapy-master/extras/qpsclient.py#1", "repo_id": "scrapy-master", "language": "python", "path": "/Users/zack.alatrash/Downloads/scrapy-master/extras/qpsclient.py", "rel_path": "extras/qpsclient.py", "module": "extras.qpsclient", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "start_requests", "parse", "QPSSpider", "async", "requests", "global", "number", "pass", "spider", "second", "latency", "name", "replace", "responses", "class", "goal", "else", "limited", "concurren", "dont", "filter", "time", "spiders", "scrapy", "benchurl", "elif", "usage", "init", "runspider", "qps", "delay", "measure", "start", "max", "concurrent", "from", "download", "yield", "true", "slots", "concurrency", "request", "throughput", "range", "super", "none", "localhost", "seconds", "same"], "ast_kind": "class_or_type", "text": "\"\"\"\nA spider that generate light requests to measure QPS throughput\n\nusage:\n\n    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0\n     --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n\n\"\"\"\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\n\n\nclass QPSSpider(Spider):\n    name = \"qps\"\n    benchurl = \"http://localhost:8880/\"\n\n    # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n    max_concurrent_requests = 8\n    # Requests per second goal\n    qps = None  # same as: 1 / download_delay\n    download_delay = None\n    # time in seconds to delay server responses\n    latency = None\n    # number of slots to create\n    slots = 1\n\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        if self.qps is not None:\n            self.qps = float(self.qps)\n            self.download_delay = 1 / self.qps\n        elif self.download_delay is not None:\n            self.download_delay = float(self.download_delay)\n\n    async def start(self):\n        for item_or_request in self.start_requests():\n            yield item_or_request\n\n    def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += f\"?latency={self.latency}\"\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace(\"localhost\", f\"127.0.0.{x + 1}\") for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1\n\n    def parse(self, response):\n        pass\n", "n_tokens": 412, "byte_len": 1623, "file_sha1": "1fd93c5a34fbf84b0460aff0221645344d282fd1", "start_line": 1, "end_line": 60}
