#!/usr/bin/env python3
"""
Chunk Quality Analysis

This script analyzes the quality of chunks generated by the Chunk Doctor pipeline
and provides detailed insights into the chunking improvements.
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """Load JSONL file and return list of chunks."""
    chunks = []
    with open(file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                chunks.append(json.loads(line))
    return chunks

def analyze_chunk_quality(chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze chunk quality and return detailed statistics."""
    stats = {
        'total_chunks': len(chunks),
        'go_chunks': 0,
        'file_headers': 0,
        'interfaces': 0,
        'structs': 0,
        'functions': 0,
        'methods': 0,
        'avg_tokens': 0,
        'chunks_with_imports': 0,
        'chunks_with_symbols': 0,
        'neighbor_chains_complete': 0,
        'chunks_with_summary': 0,
        'chunks_with_qa_terms': 0,
        'file_types': {},
        'ast_paths': {},
        'import_usage': {},
        'symbol_usage': {},
        'token_distribution': {
            'under_50': 0,
            '50_100': 0,
            '100_200': 0,
            '200_300': 0,
            'over_300': 0
        }
    }
    
    total_tokens = 0
    neighbor_chains = 0
    
    for chunk in chunks:
        # Count by language
        language = chunk.get('language', 'unknown')
        stats['file_types'][language] = stats['file_types'].get(language, 0) + 1
        
        if language == 'go':
            stats['go_chunks'] += 1
        
        # Count by ast_path
        ast_path = chunk.get('ast_path', '')
        stats['ast_paths'][ast_path] = stats['ast_paths'].get(ast_path, 0) + 1
        
        if ast_path == 'go:file_header':
            stats['file_headers'] += 1
        elif 'interface' in ast_path:
            stats['interfaces'] += 1
        elif 'struct' in ast_path:
            stats['structs'] += 1
        elif 'function' in ast_path:
            stats['functions'] += 1
        elif 'method' in ast_path:
            stats['methods'] += 1
        
        # Count tokens
        token_counts = chunk.get('token_counts', {})
        if isinstance(token_counts, dict):
            total_tokens += token_counts.get('total', 0)
            
            # Token distribution
            token_count = token_counts.get('total', 0)
            if token_count < 50:
                stats['token_distribution']['under_50'] += 1
            elif token_count < 100:
                stats['token_distribution']['50_100'] += 1
            elif token_count < 200:
                stats['token_distribution']['100_200'] += 1
            elif token_count < 300:
                stats['token_distribution']['200_300'] += 1
            else:
                stats['token_distribution']['over_300'] += 1
        
        # Count chunks with imports
        imports_used = chunk.get('imports_used', [])
        if imports_used:
            stats['chunks_with_imports'] += 1
            for imp in imports_used:
                stats['import_usage'][imp] = stats['import_usage'].get(imp, 0) + 1
        
        # Count chunks with symbols
        symbols_referenced = chunk.get('symbols_referenced', [])
        if symbols_referenced:
            stats['chunks_with_symbols'] += 1
            for symbol in symbols_referenced:
                stats['symbol_usage'][symbol] = stats['symbol_usage'].get(symbol, 0) + 1
        
        # Check neighbor chains
        neighbors = chunk.get('neighbors', {})
        if neighbors.get('prev') is not None or neighbors.get('next') is not None:
            neighbor_chains += 1
        
        # Check for summary and QA terms
        if chunk.get('summary_1l'):
            stats['chunks_with_summary'] += 1
        
        if chunk.get('qa_terms'):
            stats['chunks_with_qa_terms'] += 1
    
    stats['avg_tokens'] = total_tokens / len(chunks) if chunks else 0
    stats['neighbor_chains_complete'] = neighbor_chains
    
    return stats

def print_quality_report(stats: Dict[str, Any]):
    """Print a detailed quality report."""
    logger.info("📊 Chunk Quality Analysis Report")
    logger.info("=" * 50)
    
    # Basic statistics
    logger.info(f"📈 Basic Statistics:")
    logger.info(f"  Total chunks: {stats['total_chunks']}")
    logger.info(f"  Go chunks: {stats['go_chunks']} ({stats['go_chunks']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  Average tokens per chunk: {stats['avg_tokens']:.1f}")
    
    # Go-specific statistics
    logger.info(f"\n🔧 Go Chunk Breakdown:")
    logger.info(f"  File headers: {stats['file_headers']}")
    logger.info(f"  Interfaces: {stats['interfaces']}")
    logger.info(f"  Structs: {stats['structs']}")
    logger.info(f"  Functions: {stats['functions']}")
    logger.info(f"  Methods: {stats['methods']}")
    
    # Quality metrics
    logger.info(f"\n✅ Quality Metrics:")
    logger.info(f"  Chunks with imports: {stats['chunks_with_imports']} ({stats['chunks_with_imports']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  Chunks with symbols: {stats['chunks_with_symbols']} ({stats['chunks_with_symbols']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  Chunks with summary: {stats['chunks_with_summary']} ({stats['chunks_with_summary']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  Chunks with QA terms: {stats['chunks_with_qa_terms']} ({stats['chunks_with_qa_terms']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  Neighbor chains: {stats['neighbor_chains_complete']} ({stats['neighbor_chains_complete']/stats['total_chunks']*100:.1f}%)")
    
    # Token distribution
    logger.info(f"\n📏 Token Distribution:")
    dist = stats['token_distribution']
    logger.info(f"  Under 50 tokens: {dist['under_50']} ({dist['under_50']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  50-100 tokens: {dist['50_100']} ({dist['50_100']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  100-200 tokens: {dist['100_200']} ({dist['100_200']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  200-300 tokens: {dist['200_300']} ({dist['200_300']/stats['total_chunks']*100:.1f}%)")
    logger.info(f"  Over 300 tokens: {dist['over_300']} ({dist['over_300']/stats['total_chunks']*100:.1f}%)")
    
    # File types
    logger.info(f"\n📁 File Types:")
    for file_type, count in sorted(stats['file_types'].items()):
        logger.info(f"  {file_type}: {count} ({count/stats['total_chunks']*100:.1f}%)")
    
    # Top AST paths
    logger.info(f"\n🎯 Top AST Paths:")
    sorted_ast_paths = sorted(stats['ast_paths'].items(), key=lambda x: x[1], reverse=True)
    for ast_path, count in sorted_ast_paths[:10]:
        logger.info(f"  {ast_path}: {count}")
    
    # Top imports
    logger.info(f"\n📦 Top Imports:")
    sorted_imports = sorted(stats['import_usage'].items(), key=lambda x: x[1], reverse=True)
    for imp, count in sorted_imports[:10]:
        logger.info(f"  {imp}: {count}")
    
    # Top symbols
    logger.info(f"\n🔗 Top Symbols:")
    sorted_symbols = sorted(stats['symbol_usage'].items(), key=lambda x: x[1], reverse=True)
    for symbol, count in sorted_symbols[:10]:
        logger.info(f"  {symbol}: {count}")

def main():
    """Main analysis function."""
    if len(sys.argv) != 2:
        logger.error("Usage: python analyze_chunk_quality.py <chunks.jsonl>")
        sys.exit(1)
    
    chunks_file = sys.argv[1]
    if not Path(chunks_file).exists():
        logger.error(f"File not found: {chunks_file}")
        sys.exit(1)
    
    logger.info(f"📖 Loading chunks from {chunks_file}")
    chunks = load_jsonl(chunks_file)
    logger.info(f"📊 Loaded {len(chunks)} chunks")
    
    stats = analyze_chunk_quality(chunks)
    print_quality_report(stats)

if __name__ == "__main__":
    main()
