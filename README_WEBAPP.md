Perfect! Here's a clear step-by-step guide to get your RAG Code Search & Q&A system running:

## ğŸš€ **Step-by-Step Setup Guide**

### **Step 1: Prerequisites Setup**

**1.1 Make sure you're in the project directory:**
```bash
cd /Users/zack.alatrash/Internship/Code/Code-Chunker
```

**1.2 Start OpenSearch (if not already running):**
```bash
cd ops/
docker-compose up -d
cd ..
```

**1.3 Install/Start Ollama (for Q&A functionality):**
```bash
# Install Ollama (if not installed)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull the model
ollama pull qwen2.5-coder:7b-instruct

# Ollama should now be running on http://localhost:11434
```

---

### **Step 2: Backend Setup**

**2.1 Kill any existing backend processes:**
```bash
pkill -f "python backend/app.py"
```

**2.2 Activate the virtual environment and start backend:**
```bash
source backend_venv/bin/activate
python backend/app.py
```

**âœ… You should see this output:**
```
ğŸš€ Starting RAG Code Search & Q&A API...
ğŸ“ API docs will be available at: http://localhost:8000/docs
ğŸ” Simple search test: http://localhost:8000/search/simple?q=authentication
ğŸ¤– Simple Q&A test: http://localhost:8000/qa/simple?q=how%20does%20this%20work
ğŸŒ Frontend: Open frontend/index.html and frontend/qa.html
ğŸ”„ Connecting to OpenSearch...
âœ… Connected to OpenSearch at http://localhost:9200
ğŸ”„ Loading embedding model...
âœ… Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**ğŸ¯ Backend is ready when you see:** `Uvicorn running on http://0.0.0.0:8000`

---

### **Step 3: Frontend Setup**

**3.1 Open a new terminal (keep backend running in the first terminal)**

**3.2 Navigate to project directory:**
```bash
cd /Users/zack.alatrash/Internship/Code/Code-Chunker
```

**3.3 Open the frontends:**

**Option A - Direct file opening:**
```bash
# Open Code Search interface
open frontend/index.html

# Open Q&A Assistant interface  
open frontend/qa.html
```

**Option B - Serve via HTTP (recommended):**
```bash
# Start a simple HTTP server
cd frontend/
python3 -m http.server 3000

# Then open in browser:
# http://localhost:3000/index.html (Code Search)
# http://localhost:3000/qa.html (Q&A Assistant)
```

---

### **Step 4: Verify Everything Works**

**4.1 Test Backend API:**
- Open: http://localhost:8000/docs
- Check: http://localhost:8000/health

**4.2 Test Frontend:**
- **Code Search**: Try searching for "function" or "class"
- **Q&A Assistant**: Ask "How does this system work?"

---

### **Step 5: Usage**

**ğŸ” Code Search Interface (`frontend/index.html`):**
- Search for code chunks
- Filter by repository
- View syntax-highlighted results

**ğŸ¤– Q&A Assistant Interface (`frontend/qa.html`):**
- Ask questions about your codebase
- Get comprehensive answers with code examples
- Adjust spotlight chunks (2-6 key code blocks)
- Configure LLM model and settings

---

### **ğŸ› ï¸ Quick Troubleshooting**

**If backend fails to start:**
```bash
# Kill existing processes and restart
pkill -f "python backend/app.py"
source backend_venv/bin/activate
python backend/app.py
```

**If frontend can't connect:**
- Make sure backend is running on port 8000
- Try serving frontend via HTTP server instead of file:// 

**If Q&A doesn't work:**
- Make sure Ollama is running: `ollama list`
- Check model is available: `ollama pull qwen2.5-coder:7b-instruct`

---

### **ğŸ“‹ Final Checklist**

- âœ… **OpenSearch**: Running on port 9200
- âœ… **Ollama**: Running on port 11434 with model
- âœ… **Backend**: Running on port 8000 
- âœ… **Frontend**: Accessible in browser
- âœ… **Both interfaces**: Code Search + Q&A working

**ğŸ¯ You're ready to go!** The system will search your indexed code and provide intelligent answers about your codebase.