{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/Chunker.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/Chunker.py", "rel_path": "Chunker.py", "module": "Chunker", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "chunk", "get_chunk", "print_chunks", "consolidate_chunks_into_file", "count_lines", "Chunker", "CodeChunker", "default", "breakpoints", "code", "reached", "dict", "consolidated", "chunks", "adjusted", "current", "encoding", "name", "find", "line", "need", "chunked", "codebase", "chunker", "using", "items", "ensure", "whitespace", "checks", "count", "lines", "content", "next", "been", "class", "parser", "get", "number", "strip", "super", "stop", "move", "sorted", "else", "staticmethod", "split", "init", "means", "file"], "ast_kind": "class_or_type", "text": "from abc import ABC, abstractmethod\nfrom CodeParser import CodeParser\nfrom utils import count_tokens\n\n\nclass Chunker(ABC):\n    def __init__(self, encoding_name=\"gpt-4\"):\n        self.encoding_name = encoding_name\n\n    @abstractmethod\n    def chunk(self, content, token_limit):\n        pass\n\n    @abstractmethod\n    def get_chunk(self, chunked_content, chunk_number):\n        pass\n\n    @staticmethod\n    def print_chunks(chunks):\n        for chunk_number, chunk_code in chunks.items():\n            print(f\"Chunk {chunk_number}:\")\n            print(\"=\" * 40)\n            print(chunk_code)\n            print(\"=\" * 40)\n\n    @staticmethod\n    def consolidate_chunks_into_file(chunks):\n        return \"\\n\".join(chunks.values())\n\n    @staticmethod\n    def count_lines(consolidated_chunks):\n        lines = consolidated_chunks.split(\"\\n\")\n        return len(lines)\n\n\nclass CodeChunker(Chunker):\n    def __init__(self, file_extension, encoding_name=\"gpt-4\"):\n        super().__init__(encoding_name)\n        self.file_extension = file_extension\n\n    def chunk(self, code, token_limit) -> dict:\n        code_parser = CodeParser(self.file_extension)\n        chunks = {}\n        current_chunk = \"\"\n        token_count = 0\n        lines = code.split(\"\\n\")\n        i = 0\n        chunk_number = 1\n        start_line = 0\n        breakpoints = sorted(code_parser.get_lines_for_points_of_interest(code, self.file_extension))\n        comments = sorted(code_parser.get_lines_for_comments(code, self.file_extension))\n        adjusted_breakpoints = []\n        for bp in breakpoints:\n            current_line = bp - 1\n            highest_comment_line = None  # Initialize with None to indicate no comment line has been found yet\n            while current_line in comments:\n                highest_comment_line = current_line  # Update highest comment line found\n                current_line -= 1  # Move to the previous line\n\n            if highest_comment_line:  # If a highest comment line exists, add it\n                adjusted_breakpoints.append(highest_comment_line)\n            else:\n                adjusted_breakpoints.append(\n                    bp)  # If no comments were found before the breakpoint, add the original breakpoint\n\n        breakpoints = sorted(set(adjusted_breakpoints))  # Ensure breakpoints are unique and sorted\n\n        while i < len(lines):\n            line = lines[i]\n            new_token_count = count_tokens(line, self.encoding_name)\n            if token_count + new_token_count > token_limit:\n\n                # Set the stop line to the last breakpoint before the current line\n                if i in breakpoints:\n                    stop_line = i\n                else:\n                    stop_line = max(max([x for x in breakpoints if x < i], default=start_line), start_line)\n\n                # If the stop line is the same as the start line, it means we haven't reached a breakpoint yet and we need to move to the next line to find one\n                if stop_line == start_line and i not in breakpoints:\n                    token_count += new_token_count\n                    i += 1\n\n                # If the stop line is the same as the start line and the current line is a breakpoint, it means we can create a chunk with just the current line\n                elif stop_line == start_line and i == stop_line:\n                    token_count += new_token_count\n                    i += 1\n\n\n                # If the stop line is the same as the start line and the current line is a breakpoint, it means we can create a chunk with just the current line\n                elif stop_line == start_line and i in breakpoints:\n                    current_chunk = \"\\n\".join(lines[start_line:stop_line])\n                    if current_chunk.strip():  # If the current chunk is not just whitespace\n                        chunks[chunk_number] = current_chunk  # Using chunk_number as key\n                        chunk_number += 1\n\n                    token_count = 0\n                    start_line = i\n                    i += 1\n\n                # If the stop line is different from the start line, it means we're at the end of a block\n                else:\n                    current_chunk = \"\\n\".join(lines[start_line:stop_line])\n                    if current_chunk.strip():\n                        chunks[chunk_number] = current_chunk  # Using chunk_number as key\n                        chunk_number += 1\n\n                    i = stop_line\n                    token_count = 0\n                    start_line = stop_line\n            else:\n                # If the token count is still within the limit, add the line to the current chunk\n                token_count += new_token_count\n                i += 1\n\n        # Append remaining code, if any, ensuring it's not empty or whitespace\n        current_chunk_code = \"\\n\".join(lines[start_line:])\n        if current_chunk_code.strip():  # Checks if the chunk is not just whitespace\n            chunks[chunk_number] = current_chunk_code  # Using chunk_number as key\n\n        return chunks\n\n    def get_chunk(self, chunked_codebase, chunk_number):\n        return chunked_codebase[chunk_number]\n", "n_tokens": 1045, "byte_len": 5148, "file_sha1": "144f846db4183ec3957dd95c8e43656dec860a12", "start_line": 1, "end_line": 125}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/test_code_chunker.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/test_code_chunker.py", "rel_path": "test_code_chunker.py", "module": "test_code_chunker", "ext": "py", "chunk_number": 1, "symbols": ["mock_count_tokens", "setUp", "tearDown", "test_chunk_simple_code", "test_chunk_code_text_only", "test_chunk_code_with_routes", "test_chunk_code_with_models", "test_chunk_code_with_main", "test_chunk_code_with_utilities", "test_chunk_code_with_big_class", "TestCodeChunkerPython", "TestCodeChunkerJavaScript", "simple", "code", "contain", "test", "models", "patcher", "num", "lines", "chunk", "encoding", "name", "routes", "first", "handle", "world", "chunker", "side", "effect", "test_chunk_javascript_simple_code", "test_chunk_javascript_with_routes", "test_chunk_javascript_with_models", "test_chunk_javascript_with_main", "test_chunk_javascript_with_utilities", "test_chunk_javascript_with_big_class", "test_chunk_javascript_with_react_component", "test_chunk_css_with_media_query", "test_chunk_css_with_simple_css", "test_chunk_typescript_code", "test_chunk_ruby_code", "test_chunk_php_code", "test_chunk_golang_simple_code", "test_chunk_golang_with_structs", "test_chunk_golang_with_interfaces", "test_chunk_golang_with_goroutines", "TestCodeChunkerCSS", "TestCodeChunkerTypeScript", "TestCodeChunkerRuby", "TestCodeChunkerPHP"], "ast_kind": "class_or_type", "text": "import unittest\nfrom unittest.mock import patch\nfrom Chunker import Chunker, CodeChunker\nimport tiktoken\nfrom utils import load_json\n\n\n                        \n# Mocking the count_tokens function as it's external and not the focus of these tests\ndef mock_count_tokens(string: str, encoding_name='gpt-4') -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.encoding_for_model(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n# Python Test Class\nclass TestCodeChunkerPython(unittest.TestCase):\n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='py')\n        self.mock_codebase = load_json('mock_codefiles.json')\n        \n    def tearDown(self):\n        self.patcher.stop()\n\n    def test_chunk_simple_code(self):\n        py_code = self.mock_codebase['simple.py']\n        first_chunk_token_limit = mock_count_tokens(\"import sys\")\n        print(f\"first_chunk_token_limit = {first_chunk_token_limit}\")\n        chunks = self.code_chunker.chunk(py_code, token_limit=25)\n        token_count = self.mock_count_tokens(py_code)\n        print(f\"token_count = {token_count}\")\n        print(f\"original code:\\n {py_code}\")\n        Chunker.print_chunks(chunks)\n        full_code = Chunker.consolidate_chunks_into_file(chunks)\n        print(f\"code after consolidation:\\n {full_code}\")\n        num_lines = Chunker.count_lines(full_code)\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\"))) # The number of lines should be the same\n        self.assertIn(full_code, py_code) # The full code should be in the original code\n        self.assertEqual(len(chunks), 2) # There should be 2 chunks\n        self.assertIn(\"import sys\", chunks[1]) # The first chunk should contain the import statement\n        self.assertIn(\"print('Hello, world!')\", chunks[2]) # The second chunk should contain the print statement\n\n    def test_chunk_code_text_only(self):\n        py_code = self.mock_codebase['text_only.py']\n        chunks = self.code_chunker.chunk(py_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\"))) # The number of lines should be the same\n        self.assertIn(py_code, final_code) # The full code should be in the original code\n        self.assertEqual(len(chunks), 1)\n        self.assertIn(\"This file is empty and should test the chunker's ability to handle empty files\", chunks[1])\n\n\n    def test_chunk_code_with_routes(self):\n        py_code = self.mock_codebase['routes.py']\n        chunks = self.code_chunker.chunk(py_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\"))) # The number of lines should be the same\n        self.assertIn(py_code, final_code) # The full code should be in the original code\n\n\n    def test_chunk_code_with_models(self):\n        py_code = self.mock_codebase['models.py']\n        chunks = self.code_chunker.chunk(py_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\")))\n        self.assertIn(py_code, final_code)\n\n    def test_chunk_code_with_main(self):\n        py_code = self.mock_codebase['main.py']\n        chunks = self.code_chunker.chunk(py_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\")))\n        self.assertIn(py_code, final_code)\n\n    def test_chunk_code_with_utilities(self):\n        py_code = self.mock_codebase['utilities.py']\n        chunks = self.code_chunker.chunk(py_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\")))\n        self.assertIn(py_code, final_code)\n\n    def test_chunk_code_with_big_class(self):\n        py_code = self.mock_codebase['big_class.py']\n        chunks = self.code_chunker.chunk(py_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(py_code.split(\"\\n\")))\n        self.assertIn(py_code, final_code)\n\n# JavaScript Test Class\nclass TestCodeChunkerJavaScript(unittest.TestCase):\n\n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='js')\n        self.mock_codebase = load_json('mock_codefiles.json')\n", "n_tokens": 1215, "byte_len": 5451, "file_sha1": "f17dfca45b9da77c94467a17ee891df89eec0d05", "start_line": 1, "end_line": 111}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/test_code_chunker.py#2", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/test_code_chunker.py", "rel_path": "test_code_chunker.py", "module": "test_code_chunker", "ext": "py", "chunk_number": 2, "symbols": ["tearDown", "test_chunk_javascript_simple_code", "test_chunk_javascript_with_routes", "test_chunk_javascript_with_models", "test_chunk_javascript_with_main", "test_chunk_javascript_with_utilities", "test_chunk_javascript_with_big_class", "test_chunk_javascript_with_react_component", "setUp", "test_chunk_css_with_media_query", "test_chunk_css_with_simple_css", "TestCodeChunkerCSS", "TestCodeChunkerTypeScript", "main", "react", "component", "test", "code", "patch", "assert", "equal", "simple", "class", "start", "chunk", "mock", "count", "print", "chunks", "chunker", "mock_count_tokens", "test_chunk_simple_code", "test_chunk_code_text_only", "test_chunk_code_with_routes", "test_chunk_code_with_models", "test_chunk_code_with_main", "test_chunk_code_with_utilities", "test_chunk_code_with_big_class", "test_chunk_typescript_code", "test_chunk_ruby_code", "test_chunk_php_code", "test_chunk_golang_simple_code", "test_chunk_golang_with_structs", "test_chunk_golang_with_interfaces", "test_chunk_golang_with_goroutines", "TestCodeChunkerPython", "TestCodeChunkerJavaScript", "TestCodeChunkerRuby", "TestCodeChunkerPHP", "TestCodeChunkerGolang"], "ast_kind": "class_or_type", "text": "    def tearDown(self):\n        self.patcher.stop()\n\n    def test_chunk_javascript_simple_code(self):\n        js_code = self.mock_codebase['simple.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n\n    def test_chunk_javascript_with_routes(self):\n        js_code = self.mock_codebase['routes.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n\n    def test_chunk_javascript_with_models(self):\n        js_code = self.mock_codebase['models.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n    def test_chunk_javascript_with_main(self):\n        js_code = self.mock_codebase['main.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n    def test_chunk_javascript_with_utilities(self):\n        js_code = self.mock_codebase['utilities.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n    def test_chunk_javascript_with_big_class(self):\n        js_code = self.mock_codebase['big_class.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n    def test_chunk_javascript_with_react_component(self):\n        js_code = self.mock_codebase['react_component.js']\n        chunks = self.code_chunker.chunk(js_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(js_code.split(\"\\n\")))\n        self.assertIn(js_code, final_code)\n\n# CSS Test Class\nclass TestCodeChunkerCSS(unittest.TestCase):\n   \n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='css')\n         #Load the JSON data\n        self.mock_codebase = load_json('mock_codefiles.json')\n\n    def tearDown(self):\n        self.patcher.stop()\n\n    def test_chunk_css_with_media_query(self):\n        css_code = self.mock_codebase['media_queries.css']\n        chunks = self.code_chunker.chunk(css_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(css_code.split(\"\\n\")))\n        self.assertIn(css_code, final_code)\n\n    def test_chunk_css_with_simple_css(self):\n        css_code = self.mock_codebase['simple_styles.css']\n        chunks = self.code_chunker.chunk(css_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(Chunker.consolidate_chunks_into_file(chunks))\n        self.assertEqual(num_lines, len(css_code.split(\"\\n\")))\n        self.assertIn(css_code, final_code)\n\n\n    \n# TypeScript Test Class\nclass TestCodeChunkerTypeScript(unittest.TestCase):\n\n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='ts')\n        self.mock_codebase = load_json('mock_codefiles.json')\n\n\n    def tearDown(self):\n        self.patcher.stop()\n", "n_tokens": 1129, "byte_len": 5087, "file_sha1": "f17dfca45b9da77c94467a17ee891df89eec0d05", "start_line": 112, "end_line": 225}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/test_code_chunker.py#3", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/test_code_chunker.py", "rel_path": "test_code_chunker.py", "module": "test_code_chunker", "ext": "py", "chunk_number": 3, "symbols": ["test_chunk_typescript_code", "setUp", "tearDown", "test_chunk_ruby_code", "test_chunk_php_code", "test_chunk_golang_simple_code", "test_chunk_golang_with_structs", "test_chunk_golang_with_interfaces", "test_chunk_golang_with_goroutines", "TestCodeChunkerRuby", "TestCodeChunkerPHP", "TestCodeChunkerGolang", "main", "patch", "test", "code", "assert", "equal", "start", "class", "simple", "chunk", "example", "print", "chunks", "chunker", "ruby", "mock", "count", "self", "mock_count_tokens", "test_chunk_simple_code", "test_chunk_code_text_only", "test_chunk_code_with_routes", "test_chunk_code_with_models", "test_chunk_code_with_main", "test_chunk_code_with_utilities", "test_chunk_code_with_big_class", "test_chunk_javascript_simple_code", "test_chunk_javascript_with_routes", "test_chunk_javascript_with_models", "test_chunk_javascript_with_main", "test_chunk_javascript_with_utilities", "test_chunk_javascript_with_big_class", "test_chunk_javascript_with_react_component", "test_chunk_css_with_media_query", "test_chunk_css_with_simple_css", "TestCodeChunkerPython", "TestCodeChunkerJavaScript", "TestCodeChunkerCSS"], "ast_kind": "class_or_type", "text": "    def test_chunk_typescript_code(self):\n        ts_code = self.mock_codebase['example.ts']\n        chunks = self.code_chunker.chunk(ts_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(ts_code.split(\"\\n\")))\n        self.assertIn(ts_code, final_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\n# Ruby Test Class\nclass TestCodeChunkerRuby(unittest.TestCase):\n\n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='rb')\n        self.mock_codebase = load_json('mock_codefiles.json')\n\n\n    def tearDown(self):\n        self.patcher.stop()\n\n    def test_chunk_ruby_code(self):\n        rb_code = self.mock_codebase['example.rb']\n        chunks = self.code_chunker.chunk(rb_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(rb_code.split(\"\\n\")))\n        self.assertIn(rb_code, final_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\n# PHP Test Class\nclass TestCodeChunkerPHP(unittest.TestCase):\n\n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='php')\n        self.mock_codebase = load_json('mock_codefiles.json')\n\n    def tearDown(self):\n        self.patcher.stop()\n\n    def test_chunk_php_code(self):\n        php_code = self.mock_codebase['example.php']\n        chunks = self.code_chunker.chunk(php_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(php_code.split(\"\\n\")))\n        self.assertIn(php_code, final_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\n# Golang Test Class\nclass TestCodeChunkerGolang(unittest.TestCase):\n\n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.code_chunker = CodeChunker(file_extension='go')\n        self.mock_codebase = load_json('mock_codefiles.json')\n\n    def tearDown(self):\n        self.patcher.stop()\n\n    def test_chunk_golang_simple_code(self):\n        go_code = self.mock_codebase['simple.go']\n        chunks = self.code_chunker.chunk(go_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(go_code.split(\"\\n\")))\n        self.assertIn(go_code, final_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\n    def test_chunk_golang_with_structs(self):\n        go_code = self.mock_codebase['structs.go']\n        chunks = self.code_chunker.chunk(go_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(go_code.split(\"\\n\")))\n        self.assertIn(go_code, final_code)\n        self.assertGreater(len(chunks), 1)\n\n    def test_chunk_golang_with_interfaces(self):\n        go_code = self.mock_codebase['interfaces.go']\n        chunks = self.code_chunker.chunk(go_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(go_code.split(\"\\n\")))\n        self.assertIn(go_code, final_code)\n        self.assertGreater(len(chunks), 1)\n\n    def test_chunk_golang_with_goroutines(self):\n        go_code = self.mock_codebase['goroutines.go']\n        chunks = self.code_chunker.chunk(go_code, token_limit=20)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        self.assertEqual(num_lines, len(go_code.split(\"\\n\")))\n        self.assertIn(go_code, final_code)\n        self.assertGreater(len(chunks), 1)\n\nif __name__ == '__main__':\n    unittest.main()\n", "n_tokens": 1054, "byte_len": 4663, "file_sha1": "f17dfca45b9da77c94467a17ee891df89eec0d05", "start_line": 226, "end_line": 335}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/utils.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/utils.py", "rel_path": "utils.py", "module": "utils", "ext": "py", "chunk_number": 1, "symbols": ["count_tokens", "load_json", "load", "returns", "count", "tokens", "json", "return", "open", "encoding", "name", "num", "import", "file", "number", "encode", "for", "with", "text", "string", "tiktoken"], "ast_kind": "function_or_method", "text": "import tiktoken\nimport json\n\ndef count_tokens(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.encoding_for_model(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\ndef load_json(json_file):\n    with open(json_file) as f:\n        return json.load(f)\n", "n_tokens": 80, "byte_len": 356, "file_sha1": "97ddb2ddb017cf2cc98a606cea99ad2b4954d327", "start_line": 1, "end_line": 13}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/app.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/app.py", "rel_path": "app.py", "module": "app", "ext": "py", "chunk_number": 1, "symbols": ["load_json_file", "read_code_from_file", "get_language_by_extension", "page", "sections", "typescript", "significantly", "breakpoints", "code", "content", "guideline", "slightly", "constraints", "choose", "chunk", "coherent", "than", "middle", "logic", "original", "col", "strict", "achieve", "logical", "dirname", "attempting", "selectbox", "handling", "files", "contents", "readable", "flexibility", "chunker", "items", "larger", "language", "depending", "function", "filenames", "list", "expander", "file", "min", "value", "read", "streamlit", "parameter", "emphasizing", "rather", "uploader"], "ast_kind": "function_or_method", "text": "import streamlit as st\nimport json\nimport os\nfrom Chunker import CodeChunker\n\n# Set Streamlit page config at the very beginning\nst.set_page_config(page_title=\"Cintra Code Chunker\", layout=\"wide\")\n\n# Function to load JSON data\ndef load_json_file(file_path):\n    with open(file_path, 'r') as file:\n        return json.load(file)\n\n# Function to read code from an uploaded file\ndef read_code_from_file(uploaded_file):\n    return uploaded_file.getvalue().decode(\"utf-8\")\n\nst.link_button('Contribute on GitHub', 'https://github.com/CintraAI/code-chunker', help=None, type=\"secondary\", disabled=False, use_container_width=False)\n\njson_file_path = os.path.join(os.path.dirname(__file__), 'mock_codefiles.json')\ncode_files_data = load_json_file(json_file_path)\n\n# Extract filenames and contents\ncode_files = list(code_files_data.keys())\n\nst.title('Cintra Code Chunker')\n\nselection_col, upload_col = st.columns(2)\nwith selection_col:\n    # File selection dropdown\n    selected_file_name = st.selectbox(\"Select an example code file\", code_files)\n\nwith upload_col:\n    # File upload\n    uploaded_file = st.file_uploader(\"Or upload your code file\", type=['py', 'js', 'css', 'jsx'])\n\n# Determine the content and file extension based on selection or upload\nif uploaded_file is not None:\n    code_content = read_code_from_file(uploaded_file)\n    file_extension = uploaded_file.name.split('.')[-1]\nelse:\n    code_content = code_files_data.get(selected_file_name, \"\")\n    file_extension = selected_file_name.split('.')[-1] if selected_file_name else None\n\n# Determine the language for syntax highlighting\ndef get_language_by_extension(file_extension):\n    if file_extension in ['py', 'python']:\n        return 'python'\n    elif file_extension in ['js', 'jsx', 'javascript']:\n        return 'javascript'\n    elif file_extension == 'css':\n        return 'css'\n    elif file_extension in ['ts', 'typescript', 'tsx']:\n        return 'typescript'\n    elif file_extension in ['rb', 'ruby']:\n        return 'ruby'\n    elif file_extension == 'php':\n        return 'php'\n    elif file_extension == 'go':\n        return 'go'\n    else:\n        return None\n\nlanguage = get_language_by_extension(file_extension)\n\nst.write(\"\"\"\n### Choose Chunk Size Target\"\"\")\ntoken_chunk_size = st.number_input('Target Chunk Size Target', min_value=5, max_value=1000, value=25, help=\"The token limit guides the chunk size in tokens (tiktoken, gpt-4), aiming for readability without enforcing a strict upper limit.\")\n\nwith st.expander(\"Learn more about the chunk size target\"):\n    st.markdown(\"\"\"\nThe `token_limit` parameter in the `chunk` function serves as a guideline to optimize the size of code chunks produced. It is not a hard limit but rather an ideal target, attempting to achieve a balance between chunk size and maintaining logical coherence within the code.\n\n- **Adherence to Logical Breakpoints:** The chunking logic respects logical breakpoints in the code, ensuring that chunks are coherent and maintain readability.\n- **Flexibility in Chunk Size:** Chunks might be slightly smaller or larger than the specified `token_limit` to avoid breaking the code in the middle of logical sections.\n- **Handling Final Chunks:** The last chunk of code captures any remaining code, which may vary significantly in size depending on the remaining code's structure.\n\nThis approach allows for flexibility in how code is segmented into chunks, emphasizing the balance between readable, logical code segments and size constraints.\n    \"\"\")\n\noriginal_col, chunked_col = st.columns(2)\n\nwith original_col:\n    st.subheader('Original File')\n    st.code(code_content, language=language)\n\n# Initialize the code chunker\ncode_chunker = CodeChunker(file_extension=file_extension)\n\n# Chunk the code content\nchunked_code_dict = code_chunker.chunk(code_content, token_chunk_size)\n\nwith chunked_col:\n    st.subheader('Chunked Code')\n    for chunk_key, chunk_code in chunked_code_dict.items():\n        st.code(chunk_code, language=language)", "n_tokens": 875, "byte_len": 3975, "file_sha1": "c97f56e6a4f1d8d3b850d48618923b559218d0df", "start_line": 1, "end_line": 96}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/CodeParser.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/CodeParser.py", "rel_path": "CodeParser.py", "module": "CodeParser", "ext": "py", "chunk_number": 1, "symbols": ["__init__", "_install_parsers", "_is_repo_valid", "parse_code", "extract_points_of_interest", "CodeParser", "typing", "cache", "typescript", "language", "name", "tree", "sitter", "clone", "code", "expanduser", "levelname", "bytes", "subprocess", "languages", "makedirs", "type", "script", "set", "check", "info", "names", "raise", "logging", "update", "_get_node_types_of_interest", "_get_nodes_for_comments", "extract_comments", "get_lines_for_points_of_interest", "get_lines_for_comments", "print_all_line_types", "map_line_to_node_type", "module", "converting", "adjusting", "attribute", "package", "clause", "map", "line", "comment", "class", "definition", "num", "alias"], "ast_kind": "class_or_type", "text": "import os\nimport subprocess\nfrom typing import List, Dict, Union, Tuple\nfrom tree_sitter import Language, Parser, Node\nimport logging\n\nclass CodeParser:\n    # Added a CACHE_DIR class attribute for caching\n    CACHE_DIR = os.path.expanduser(\"~/.code_parser_cache\")\n\n    def __init__(self, file_extensions: Union[None, List[str], str] = None):\n        if isinstance(file_extensions, str):\n            file_extensions = [file_extensions]\n        self.language_extension_map = {\n            \"py\": \"python\",\n            \"js\": \"javascript\",\n            \"jsx\": \"javascript\",\n            \"css\": \"css\",\n            \"ts\": \"typescript\",\n            \"tsx\": \"typescript\",\n            \"php\": \"php\",\n            \"rb\": \"ruby\",\n            \"go\": \"go\"\n        }\n        if file_extensions is None:\n            self.language_names = []\n        else:\n            self.language_names = [self.language_extension_map.get(ext) for ext in file_extensions if\n                                   ext in self.language_extension_map]\n        self.languages = {}\n        self._install_parsers()\n\n    def _install_parsers(self):\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n        try:\n            # Ensure cache directory exists\n            if not os.path.exists(self.CACHE_DIR):\n                os.makedirs(self.CACHE_DIR)\n\n            for language in self.language_names:\n                repo_path = os.path.join(self.CACHE_DIR, f\"tree-sitter-{language}\")\n\n                # Check if the repository exists and contains necessary files\n                if not os.path.exists(repo_path) or not self._is_repo_valid(repo_path, language):\n                    try:\n                        if os.path.exists(repo_path):\n                            logging.info(f\"Updating existing repository for {language}\")\n                            update_command = f\"cd {repo_path} && git pull\"\n                            subprocess.run(update_command, shell=True, check=True)\n                        else:\n                            logging.info(f\"Cloning repository for {language}\")\n                            clone_command = f\"git clone https://github.com/tree-sitter/tree-sitter-{language} {repo_path}\"\n                            subprocess.run(clone_command, shell=True, check=True)\n                    except subprocess.CalledProcessError as e:\n                        logging.error(f\"Failed to clone/update repository for {language}. Error: {e}\")\n                        continue\n\n                try:\n                    build_path = os.path.join(self.CACHE_DIR, f\"build/{language}.so\")\n                    \n                    # Special handling for TypeScript\n                    if language == 'typescript':\n                        ts_dir = os.path.join(repo_path, 'typescript')\n                        tsx_dir = os.path.join(repo_path, 'tsx')\n                        if os.path.exists(ts_dir) and os.path.exists(tsx_dir):\n                            Language.build_library(build_path, [ts_dir, tsx_dir])\n                        else:\n                            raise FileNotFoundError(f\"TypeScript or TSX directory not found in {repo_path}\")\n                    elif language == 'php':\n                        php_dir = os.path.join(repo_path, 'php')\n                        if os.path.exists(php_dir):\n                            Language.build_library(build_path, [php_dir])\n                        else:\n                            raise FileNotFoundError(f\"PHP directory not found in {repo_path}\")\n                    else:\n                        Language.build_library(build_path, [repo_path])\n                    \n                    self.languages[language] = Language(build_path, language)\n                    logging.info(f\"Successfully built and loaded {language} parser\")\n                except Exception as e:\n                    logging.error(f\"Failed to build or load language {language}. Error: {str(e)}\")\n                    logging.error(f\"Repository path: {repo_path}\")\n                    logging.error(f\"Build path: {build_path}\")\n                    if language == 'typescript':\n                        logging.error(f\"TypeScript dir exists: {os.path.exists(ts_dir)}\")\n                        logging.error(f\"TSX dir exists: {os.path.exists(tsx_dir)}\")\n                    elif language == 'php':\n                        logging.error(f\"PHP dir exists: {os.path.exists(php_dir)}\")\n\n        except Exception as e:\n            logging.error(f\"An unexpected error occurred during parser installation: {str(e)}\")\n\n    def _is_repo_valid(self, repo_path: str, language: str) -> bool:\n        \"\"\"Check if the repository contains necessary files.\"\"\"\n        if language == 'typescript':\n            return (os.path.exists(os.path.join(repo_path, 'typescript', 'src', 'parser.c')) and\n                     os.path.exists(os.path.join(repo_path, 'tsx', 'src', 'parser.c')))\n        elif language == 'php':\n            return os.path.exists(os.path.join(repo_path, 'php', 'src', 'parser.c'))\n        else:\n            return os.path.exists(os.path.join(repo_path, 'src', 'parser.c'))\n\n    def parse_code(self, code: str, file_extension: str) -> Union[None, Node]:\n        language_name = self.language_extension_map.get(file_extension)\n        if language_name is None:\n            print(f\"Unsupported file type: {file_extension}\")\n            return None\n\n        language = self.languages.get(language_name)\n        if language is None:\n            print(\"Language parser not found\")\n            return None\n\n        parser = Parser()\n        parser.set_language(language)\n        tree = parser.parse(bytes(code, \"utf8\"))\n\n        if tree is None:\n            print(\"Failed to parse the code\")\n            return None\n\n        return tree.root_node\n\n    def extract_points_of_interest(self, node: Node, file_extension: str) -> List[Tuple[Node, str]]:\n        node_types_of_interest = self._get_node_types_of_interest(file_extension)\n\n        points_of_interest = []\n        if node.type in node_types_of_interest.keys():\n            points_of_interest.append((node, node_types_of_interest[node.type]))\n\n        for child in node.children:\n            points_of_interest.extend(self.extract_points_of_interest(child, file_extension))\n\n        return points_of_interest\n", "n_tokens": 1203, "byte_len": 6308, "file_sha1": "be77cb314efd0ad52cd815815bfe8f6ffc25e813", "start_line": 1, "end_line": 136}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/CodeParser.py#2", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/CodeParser.py", "rel_path": "CodeParser.py", "module": "CodeParser", "ext": "py", "chunk_number": 2, "symbols": ["_get_node_types_of_interest", "_get_nodes_for_comments", "extract_comments", "get_lines_for_points_of_interest", "module", "language", "name", "attribute", "code", "package", "clause", "comment", "bytes", "languages", "class", "definition", "set", "alias", "raise", "value", "error", "children", "media", "items", "function", "decorator", "dict", "extract", "points", "child", "__init__", "_install_parsers", "_is_repo_valid", "parse_code", "extract_points_of_interest", "get_lines_for_comments", "print_all_line_types", "map_line_to_node_type", "CodeParser", "typing", "cache", "converting", "adjusting", "typescript", "tree", "sitter", "clone", "expanduser", "levelname", "map"], "ast_kind": "function_or_method", "text": "    def _get_node_types_of_interest(self, file_extension: str) -> Dict[str, str]:\n        node_types = {\n            'py': {\n                'import_statement': 'Import',\n                'export_statement': 'Export',\n                'class_definition': 'Class',\n                'function_definition': 'Function',\n            },\n            'css': {\n                'tag_name': 'Tag',\n                '@media': 'Media Query',\n            },\n            'js': {\n                'import_statement': 'Import',\n                'export_statement': 'Export',\n                'class_declaration': 'Class',\n                'function_declaration': 'Function',\n                'arrow_function': 'Arrow Function',\n                'statement_block': 'Block',\n            },\n            'ts': {\n                'import_statement': 'Import',\n                'export_statement': 'Export',\n                'class_declaration': 'Class',\n                'function_declaration': 'Function',\n                'arrow_function': 'Arrow Function',\n                'statement_block': 'Block',\n                'interface_declaration': 'Interface',\n                'type_alias_declaration': 'Type Alias',\n            },\n            'php': {\n                'namespace_definition': 'Namespace',\n                'class_declaration': 'Class',\n                'method_declaration': 'Method',\n                'function_definition': 'Function',\n                'interface_declaration': 'Interface',\n                'trait_declaration': 'Trait',\n            },\n            'rb': {\n                'class': 'Class',\n                'method': 'Method',\n                'module': 'Module',\n                'singleton_class': 'Singleton Class',\n                'begin': 'Begin Block',\n            },\n            'go': {\n                'import_declaration': 'Import',\n                'function_declaration': 'Function',\n                'method_declaration': 'Method',\n                'type_declaration': 'Type',\n                'struct_type': 'Struct',\n                'interface_type': 'Interface',\n                'package_clause': 'Package'\n            }\n        }\n\n        if file_extension in node_types.keys():\n            return node_types[file_extension]\n        elif file_extension == \"jsx\":\n            return node_types[\"js\"]\n        elif file_extension == \"tsx\":\n            return node_types[\"ts\"]\n        else:\n            raise ValueError(\"Unsupported file type\")\n        \n\n    def _get_nodes_for_comments(self, file_extension: str) -> Dict[str, str]:\n        node_types = {\n            'py': {\n                'comment': 'Comment',\n                'decorator': 'Decorator',  # Broadened category\n            },\n            'css': {\n                'comment': 'Comment'\n            },\n            'js': {\n                'comment': 'Comment',\n                'decorator': 'Decorator',  # Broadened category\n            },\n            'ts': {\n                'comment': 'Comment',\n                'decorator': 'Decorator',\n            },\n            'php': {\n                'comment': 'Comment',\n                'attribute': 'Attribute',\n            },\n            'rb': {\n                'comment': 'Comment',\n            },\n            'go': {\n                'comment': 'Comment',\n            }\n        }\n\n        if file_extension in node_types.keys():\n            return node_types[file_extension]\n        elif file_extension == \"jsx\":\n            return node_types[\"js\"]\n        elif file_extension == \"tsx\":\n            return node_types[\"ts\"]\n        else:\n            raise ValueError(\"Unsupported file type\")\n        \n    def extract_comments(self, node: Node, file_extension: str) -> List[Tuple[Node, str]]:\n        node_types_of_interest = self._get_nodes_for_comments(file_extension)\n\n        comments = []\n        if node.type in node_types_of_interest:\n            comments.append((node, node_types_of_interest[node.type]))\n\n        for child in node.children:\n            comments.extend(self.extract_comments(child, file_extension))\n\n        return comments\n\n    def get_lines_for_points_of_interest(self, code: str, file_extension: str) -> List[int]:\n        language_name = self.language_extension_map.get(file_extension)\n        if language_name is None:\n            raise ValueError(\"Unsupported file type\")\n\n        language = self.languages.get(language_name)\n        if language is None:\n            raise ValueError(\"Language parser not found\")\n\n        parser = Parser()\n        parser.set_language(language)\n\n        tree = parser.parse(bytes(code, \"utf8\"))\n\n        root_node = tree.root_node\n        points_of_interest = self.extract_points_of_interest(root_node, file_extension)\n\n        line_numbers_with_type_of_interest = {}\n\n        for node, type_of_interest in points_of_interest:\n            start_line = node.start_point[0] \n            if type_of_interest not in line_numbers_with_type_of_interest:\n                line_numbers_with_type_of_interest[type_of_interest] = []\n\n            if start_line not in line_numbers_with_type_of_interest[type_of_interest]:\n                line_numbers_with_type_of_interest[type_of_interest].append(start_line)\n\n        lines_of_interest = []\n        for _, line_numbers in line_numbers_with_type_of_interest.items():\n            lines_of_interest.extend(line_numbers)\n\n        return lines_of_interest\n", "n_tokens": 1000, "byte_len": 5356, "file_sha1": "be77cb314efd0ad52cd815815bfe8f6ffc25e813", "start_line": 137, "end_line": 285}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/CodeParser.py#3", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/CodeParser.py", "rel_path": "CodeParser.py", "module": "CodeParser", "ext": "py", "chunk_number": 3, "symbols": ["get_lines_for_comments", "print_all_line_types", "map_line_to_node_type", "found", "child", "code", "converting", "node", "types", "adjusting", "start", "language", "name", "tree", "indexed", "type", "interest", "self", "map", "line", "none", "bytes", "index", "languages", "parser", "lines", "get", "return", "list", "set", "__init__", "_install_parsers", "_is_repo_valid", "parse_code", "extract_points_of_interest", "_get_node_types_of_interest", "_get_nodes_for_comments", "extract_comments", "get_lines_for_points_of_interest", "CodeParser", "module", "typing", "cache", "typescript", "sitter", "attribute", "clone", "package", "clause", "expanduser"], "ast_kind": "function_or_method", "text": "    def get_lines_for_comments(self, code: str, file_extension: str) -> List[int]:\n        language_name = self.language_extension_map.get(file_extension)\n        if language_name is None:\n            raise ValueError(\"Unsupported file type\")\n\n        language = self.languages.get(language_name)\n        if language is None:\n            raise ValueError(\"Language parser not found\")\n\n        parser = Parser()\n        parser.set_language(language)\n\n        tree = parser.parse(bytes(code, \"utf8\"))\n\n        root_node = tree.root_node\n        comments = self.extract_comments(root_node, file_extension)\n\n        line_numbers_with_comments = {}\n\n        for node, type_of_interest in comments:\n            start_line = node.start_point[0] \n            if type_of_interest not in line_numbers_with_comments:\n                line_numbers_with_comments[type_of_interest] = []\n\n            if start_line not in line_numbers_with_comments[type_of_interest]:\n                line_numbers_with_comments[type_of_interest].append(start_line)\n\n        lines_of_interest = []\n        for _, line_numbers in line_numbers_with_comments.items():\n            lines_of_interest.extend(line_numbers)\n\n        return lines_of_interest\n\n    def print_all_line_types(self, code: str, file_extension: str):\n        language_name = self.language_extension_map.get(file_extension)\n        if language_name is None:\n            print(f\"Unsupported file type: {file_extension}\")\n            return\n\n        language = self.languages.get(language_name)\n        if language is None:\n            print(\"Language parser not found\")\n            return\n\n        parser = Parser()\n        parser.set_language(language)\n        tree = parser.parse(bytes(code, \"utf8\"))\n\n        root_node = tree.root_node\n        line_to_node_type = self.map_line_to_node_type(root_node)\n\n        code_lines = code.split('\\n')\n\n        for line_num, node_types in line_to_node_type.items():\n            line_content = code_lines[line_num - 1]  # Adjusting index for zero-based indexing\n            print(f\"line {line_num}: {', '.join(node_types)} | Code: {line_content}\")\n\n\n    def map_line_to_node_type(self, node, line_to_node_type=None, depth=0):\n        if line_to_node_type is None:\n            line_to_node_type = {}\n\n        start_line = node.start_point[0] + 1  # Tree-sitter lines are 0-indexed; converting to 1-indexed\n\n        # Only add the node type if it's the start line of the node\n        if start_line not in line_to_node_type:\n            line_to_node_type[start_line] = []\n        line_to_node_type[start_line].append(node.type)\n\n        for child in node.children:\n            self.map_line_to_node_type(child, line_to_node_type, depth + 1)\n\n        return line_to_node_type\n    ", "n_tokens": 570, "byte_len": 2749, "file_sha1": "be77cb314efd0ad52cd815815bfe8f6ffc25e813", "start_line": 286, "end_line": 359}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/mcp_server.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/mcp_server.py", "rel_path": "mcp_server.py", "module": "mcp_server", "ext": "py", "chunk_number": 1, "symbols": ["chunk_code", "get_supported_file_types", "typing", "default", "resource", "code", "get", "supported", "extension", "list", "logical", "create", "fast", "mcp", "chunker", "process", "language", "string", "when", "dict", "parser", "chunked", "into", "this", "file", "limit", "display", "name", "extensions", "token", "keys", "chunk", "specified", "fastmcp", "directly", "return", "chunks", "tool", "target", "functionality", "import", "script", "values", "exposing", "based", "size", "containing", "with", "provided", "source"], "ast_kind": "function_or_method", "text": "\"\"\"\nMCP server exposing Code Chunker functionality.\n\nThis server provides a tool to chunk code into smaller, logical segments\nand a resource to list supported file extensions.\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\nfrom CodeParser import CodeParser\nfrom Chunker import CodeChunker\nfrom typing import Dict, List, Optional\n\n# Create an MCP server with the name \"Code Chunker Server\"\nmcp = FastMCP(\"Code Chunker Server\")\n\n\n@mcp.tool()\ndef chunk_code(code: str, file_extension: str, token_limit: int = 25) -> Dict[int, str]:\n    \"\"\"\n    Chunks the provided code into logical segments based on token limit.\n    \n    Args:\n        code: The source code to be chunked\n        file_extension: The file extension (e.g., 'py', 'js', 'ts', 'css')\n        token_limit: Target size of each chunk in tokens (default: 25)\n        \n    Returns:\n        A dictionary with chunk numbers as keys and code segments as values\n    \"\"\"\n    # Create a code chunker for the specified file extension\n    chunker = CodeChunker(file_extension=file_extension)\n    \n    # Process the code through the chunker\n    chunks = chunker.chunk(code, token_limit)\n    \n    return chunks\n\n\n@mcp.resource(\"supported-file-types://list\")\ndef get_supported_file_types() -> str:\n    \"\"\"\n    Returns a list of file extensions supported by the Code Chunker.\n    \n    Returns:\n        A string containing the list of supported file extensions\n    \"\"\"\n    # Get the file extensions from CodeParser's language extension map\n    code_parser = CodeParser()\n    supported_extensions = list(code_parser.language_extension_map.keys())\n    \n    # Format the list for display\n    extension_list = \", \".join(supported_extensions)\n    return f\"Supported file extensions: {extension_list}\"\n\n\nif __name__ == \"__main__\":\n    # Run the server when the script is executed directly\n    mcp.run()", "n_tokens": 412, "byte_len": 1835, "file_sha1": "17895de16b2597226d9b0f2b119cfb26a7280a9d", "start_line": 1, "end_line": 58}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_javascript.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_javascript.py", "rel_path": "tests/test_javascript.py", "module": "tests.test_javascript", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_javascript_simple_code", "test_chunk_javascript_with_routes", "test_chunk_javascript_with_models", "test_chunk_javascript_with_main", "test_chunk_javascript_with_utilities", "test_chunk_javascript_with_big_class", "test_chunk_javascript_with_react_component", "TestCodeChunkerJavaScript", "main", "react", "component", "test", "base", "code", "simple", "class", "chunk", "chunker", "self", "models", "super", "unittest", "run", "from", "set", "setup", "import", "routes", "utilities", "file", "extension", "name", "mock", "codebase", "big"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerJavaScript(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='js')\n\n    def test_chunk_javascript_simple_code(self):\n        js_code = self.mock_codebase['simple.js']\n        self.run_chunker_test(js_code)\n\n    def test_chunk_javascript_with_routes(self):\n        js_code = self.mock_codebase['routes.js']\n        self.run_chunker_test(js_code)\n\n    def test_chunk_javascript_with_models(self):\n        js_code = self.mock_codebase['models.js']\n        self.run_chunker_test(js_code)\n\n    def test_chunk_javascript_with_main(self):\n        js_code = self.mock_codebase['main.js']\n        self.run_chunker_test(js_code)\n\n    def test_chunk_javascript_with_utilities(self):\n        js_code = self.mock_codebase['utilities.js']\n        self.run_chunker_test(js_code)\n\n    def test_chunk_javascript_with_big_class(self):\n        js_code = self.mock_codebase['big_class.js']\n        self.run_chunker_test(js_code)\n\n    def test_chunk_javascript_with_react_component(self):\n        js_code = self.mock_codebase['react_component.js']\n        self.run_chunker_test(js_code)\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 290, "byte_len": 1290, "file_sha1": "d8689ac40c3245544acd46caf41ee1d54b56e271", "start_line": 1, "end_line": 39}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_python.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_python.py", "rel_path": "tests/test_python.py", "module": "tests.test_python", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_simple_code", "test_chunk_code_text_only", "test_chunk_code_with_routes", "test_chunk_code_with_models", "test_chunk_code_with_main", "test_chunk_code_with_utilities", "test_chunk_code_with_big_class", "TestCodeChunkerPython", "main", "test", "code", "first", "chunk", "base", "assert", "equal", "simple", "class", "mock", "count", "text", "only", "chunker", "contain", "self", "full", "statement", "models", "super", "unittest", "original", "run", "from", "set", "setup", "final", "token", "limit", "big", "import", "routes", "this", "handle", "assertin", "world", "chunks", "utilities", "file", "extension"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest, mock_count_tokens\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerPython(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='py')\n    \n    def test_chunk_simple_code(self):\n        py_code = self.mock_codebase['simple.py']\n        first_chunk_token_limit = mock_count_tokens(\"import sys\")\n        print(f\"first_chunk_token_limit = {first_chunk_token_limit}\")\n        chunks = self.code_chunker.chunk(py_code, token_limit=25)\n        token_count = self.mock_count_tokens(py_code)\n        print(f\"token_count = {token_count}\")\n        print(f\"original code:\\n {py_code}\")\n        \n        chunks, full_code = self.run_chunker_test(py_code, token_limit=25)\n        \n        self.assertEqual(len(chunks), 2) # There should be 2 chunks\n        self.assertIn(\"import sys\", chunks[1]) # The first chunk should contain the import statement\n        self.assertIn(\"print('Hello, world!')\", chunks[2]) # The second chunk should contain the print statement\n\n    def test_chunk_code_text_only(self):\n        py_code = self.mock_codebase['text_only.py']\n        chunks, final_code = self.run_chunker_test(py_code)\n        \n        self.assertEqual(len(chunks), 1)\n        self.assertIn(\"This file is empty and should test the chunker's ability to handle empty files\", chunks[1])\n\n    def test_chunk_code_with_routes(self):\n        py_code = self.mock_codebase['routes.py']\n        self.run_chunker_test(py_code)\n\n    def test_chunk_code_with_models(self):\n        py_code = self.mock_codebase['models.py']\n        self.run_chunker_test(py_code)\n\n    def test_chunk_code_with_main(self):\n        py_code = self.mock_codebase['main.py']\n        self.run_chunker_test(py_code)\n\n    def test_chunk_code_with_utilities(self):\n        py_code = self.mock_codebase['utilities.py']\n        self.run_chunker_test(py_code)\n\n    def test_chunk_code_with_big_class(self):\n        py_code = self.mock_codebase['big_class.py']\n        self.run_chunker_test(py_code)\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 481, "byte_len": 2114, "file_sha1": "e67af8e253f8b01eef407a0a4965e9b22b9c354d", "start_line": 1, "end_line": 53}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_golang.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_golang.py", "rel_path": "tests/test_golang.py", "module": "tests.test_golang", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_golang_simple_code", "test_chunk_golang_with_structs", "test_chunk_golang_with_interfaces", "test_chunk_golang_with_goroutines", "TestCodeChunkerGolang", "main", "test", "base", "simple", "class", "code", "chunker", "self", "chunk", "actually", "super", "unittest", "run", "from", "set", "setup", "assert", "greater", "chunked", "import", "structs", "chunks", "interfaces", "file", "extension", "ensure", "name", "goroutines", "mock", "codebase"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerGolang(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='go')\n\n    def test_chunk_golang_simple_code(self):\n        go_code = self.mock_codebase['simple.go']\n        chunks, _ = self.run_chunker_test(go_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\n    def test_chunk_golang_with_structs(self):\n        go_code = self.mock_codebase['structs.go']\n        chunks, _ = self.run_chunker_test(go_code)\n        self.assertGreater(len(chunks), 1)\n\n    def test_chunk_golang_with_interfaces(self):\n        go_code = self.mock_codebase['interfaces.go']\n        chunks, _ = self.run_chunker_test(go_code)\n        self.assertGreater(len(chunks), 1)\n\n    def test_chunk_golang_with_goroutines(self):\n        go_code = self.mock_codebase['goroutines.go']\n        chunks, _ = self.run_chunker_test(go_code)\n        self.assertGreater(len(chunks), 1)\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 268, "byte_len": 1104, "file_sha1": "122708790c01f4f40d02e4bdde57c17e7e6f85cd", "start_line": 1, "end_line": 31}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_ruby.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_ruby.py", "rel_path": "tests/test_ruby.py", "module": "tests.test_ruby", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_ruby_code", "TestCodeChunkerRuby", "main", "test", "base", "code", "class", "example", "chunker", "self", "actually", "super", "unittest", "run", "from", "set", "setup", "assert", "greater", "chunked", "import", "chunks", "file", "extension", "ensure", "name", "chunk", "mock", "codebase"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerRuby(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='rb')\n\n    def test_chunk_ruby_code(self):\n        rb_code = self.mock_codebase['example.rb']\n        chunks, _ = self.run_chunker_test(rb_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 123, "byte_len": 506, "file_sha1": "b4c60eaf2bdbfd43e6917e018bde0b786f728228", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/__init__.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/__init__.py", "rel_path": "tests/__init__.py", "module": "tests.__init__", "ext": "py", "chunk_number": 1, "symbols": ["this", "python", "makes", "package", "tests", "file", "directory"], "ast_kind": "unknown", "text": "# This file makes the tests directory a Python package ", "n_tokens": 11, "byte_len": 55, "file_sha1": "1237e6188211668d830cdc63c1c750b4eb24875f", "start_line": 1, "end_line": 1}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_php.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_php.py", "rel_path": "tests/test_php.py", "module": "tests.test_php", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_php_code", "TestCodeChunkerPHP", "main", "test", "base", "class", "example", "code", "chunker", "self", "actually", "super", "unittest", "run", "from", "set", "setup", "assert", "greater", "chunked", "import", "chunk", "chunks", "file", "extension", "ensure", "name", "php", "mock", "codebase"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerPHP(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='php')\n\n    def test_chunk_php_code(self):\n        php_code = self.mock_codebase['example.php']\n        chunks, _ = self.run_chunker_test(php_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 123, "byte_len": 508, "file_sha1": "aeee0cd9b44be678dbffa36b54074f40f228e883", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_typescript.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_typescript.py", "rel_path": "tests/test_typescript.py", "module": "tests.test_typescript", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_typescript_code", "TestCodeChunkerTypeScript", "main", "test", "base", "class", "example", "code", "chunker", "self", "actually", "super", "unittest", "run", "from", "set", "setup", "chunk", "assert", "greater", "chunked", "import", "chunks", "file", "extension", "ensure", "name", "mock", "codebase"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerTypeScript(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='ts')\n\n    def test_chunk_typescript_code(self):\n        ts_code = self.mock_codebase['example.ts']\n        chunks, _ = self.run_chunker_test(ts_code)\n        self.assertGreater(len(chunks), 1)  # Ensure the code is actually chunked\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 124, "byte_len": 518, "file_sha1": "85e6190d533af79deebc569b2fcef774816d7fa2", "start_line": 1, "end_line": 16}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_css.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_css.py", "rel_path": "tests/test_css.py", "module": "tests.test_css", "ext": "py", "chunk_number": 1, "symbols": ["setUp", "test_chunk_css_with_media_query", "test_chunk_css_with_simple_css", "TestCodeChunkerCSS", "main", "test", "base", "code", "class", "chunker", "self", "super", "unittest", "run", "from", "set", "setup", "import", "css", "media", "queries", "chunk", "file", "extension", "name", "simple", "styles", "mock", "codebase"], "ast_kind": "class_or_type", "text": "import unittest\nfrom test_base import BaseChunkerTest\nfrom Chunker import CodeChunker\n\nclass TestCodeChunkerCSS(BaseChunkerTest):\n    def setUp(self):\n        super().setUp()\n        self.code_chunker = CodeChunker(file_extension='css')\n\n    def test_chunk_css_with_media_query(self):\n        css_code = self.mock_codebase['media_queries.css']\n        self.run_chunker_test(css_code)\n\n    def test_chunk_css_with_simple_css(self):\n        css_code = self.mock_codebase['simple_styles.css']\n        self.run_chunker_test(css_code)\n\nif __name__ == '__main__':\n    unittest.main() ", "n_tokens": 132, "byte_len": 578, "file_sha1": "77c01327ae16e5477703d1c26216101f5c6a7a55", "start_line": 1, "end_line": 19}
{"id": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_base.py#1", "repo_id": "code-chunker-main", "language": "python", "path": "/Users/zack.alatrash/Downloads/code-chunker-main/tests/test_base.py", "rel_path": "tests/test_base.py", "module": "tests.test_base", "ext": "py", "chunk_number": 1, "symbols": ["mock_count_tokens", "setUp", "tearDown", "run_chunker_test", "BaseChunkerTest", "parent", "allow", "code", "patcher", "num", "lines", "dirname", "encoding", "name", "chunker", "side", "effect", "count", "string", "patch", "assert", "equal", "class", "mock", "unittest", "external", "these", "tear", "down", "load", "json", "file", "encode", "for", "base", "codefiles", "imports", "codebase", "utils", "start", "mocking", "chunk", "print", "chunks", "self", "run", "tests", "return", "stop", "token"], "ast_kind": "class_or_type", "text": "import unittest\nfrom unittest.mock import patch\nimport tiktoken\nimport sys\nimport os\n\n# Add parent directory to path to allow imports\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom Chunker import Chunker, CodeChunker\nfrom utils import load_json\n\n# Mocking the count_tokens function as it's external and not the focus of these tests\ndef mock_count_tokens(string: str, encoding_name='gpt-4') -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.encoding_for_model(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nclass BaseChunkerTest(unittest.TestCase):\n    \"\"\"Base class for all code chunker tests with common setup and utilities.\"\"\"\n    \n    def setUp(self):\n        self.patcher = patch('utils.count_tokens', side_effect=mock_count_tokens)\n        self.mock_count_tokens = self.patcher.start()\n        self.mock_codebase = load_json('mock_codefiles.json')\n        \n    def tearDown(self):\n        self.patcher.stop()\n    \n    def run_chunker_test(self, code, token_limit=20):\n        \"\"\"Helper method to run standard chunker tests.\"\"\"\n        chunks = self.code_chunker.chunk(code, token_limit=token_limit)\n        Chunker.print_chunks(chunks)\n        final_code = Chunker.consolidate_chunks_into_file(chunks)\n        num_lines = Chunker.count_lines(final_code)\n        \n        # Common assertions\n        self.assertEqual(num_lines, len(code.split(\"\\n\")))\n        self.assertIn(code, final_code)\n        \n        return chunks, final_code ", "n_tokens": 338, "byte_len": 1556, "file_sha1": "e27364c80999b4182350b865dcf7563d8ef3b4c2", "start_line": 1, "end_line": 42}
